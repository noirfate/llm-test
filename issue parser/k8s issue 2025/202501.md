# Issue ÂÆâÂÖ®ÂàÜÊûêÊä•Âëä

# üö® Â≠òÂú®È´òÈ£éÈô©ÁöÑ Issues (4 ‰∏™)

## Issue #129907 Pods with postStart lifecycle hook are stuck in Pending state.

- Issue ÈìæÊé•Ôºö[#129907](https://github.com/kubernetes/kubernetes/issues/129907)

### Issue ÂÜÖÂÆπ

#### What happened?

Upon kubernetes control plane v1.32 upgrade pods with **postStart** lifecycle hook configured are unable to start.
Pod is failing to report its state and remains in **Pending** state forever.
Also it's impossible to path or update deployment like change replicas number or image tag, not possible to delete pod without --force flag.
But it allows to update deployment to remove **lifecycle** section.

#### What did you expect to happen?

Pod should start normally, lifecycle hook must be processed.

#### How can we reproduce it (as minimally and precisely as possible)?

Tested on v1.32.0 (v1.32.0-gke.1448000, v1.32.0-eks-5ca49cb) and v1.32.1 (v1.32.1-gke.1002000)

Create deployment on cluster v1.32, postStart lifecycle hook must be defined as below:
```
lifecycle:
  postStart:
    sleep:
      seconds: 15
```

#### Anything else we need to know?

Some details.

1. No issues with PreStop hooks.

2. Deployment definition used to reproduce the issue:

```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: inflate
  labels:
    app: inflate
automountServiceAccountToken: false


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      serviceAccountName: inflate
      containers:
      - args:
          - infinity
        command:
          - sleep
        image: python:3
        name: inflate
        lifecycle:
          postStart:
            sleep:
              seconds: 15
```

3. Error on deployment scaling attempt:
```
$ kubectl scale deployment inflate --replicas 2
Unable to connect to the server: stream error: stream ID 3; INTERNAL_ERROR; received from peer
```

4. Error on editing deployment (image tag change):
```
$ kubectl edit deployments inflate
error: deployments.apps "inflate" could not be patched: Patch "https://<edited>/apis/apps/v1/namespaces/default/deployments/inflate?fieldManager=kubectl-edit&fieldValidation=Strict": stream error: stream ID 5; INTERNAL_ERROR; received from peer
You can run `kubectl replace -f /var/folders/6b/qz1j7hs94kb8j238fb80vf800000gn/T/kubectl-edit-2502749312.yaml` to try this update again.
```

5. Apiserver (EKS) log on similar event:
```
E0129 14:29:50.819453      11 wrap.go:57] "apiserver panic'd" method="PATCH" URI="/apis/apps/v1/namespaces/<edited>/replicasets/<edited>" auditID="d7f993f8-ef27-4544-982d-c8913890fcba"
http2: panic serving 172.16.42.188:34518: runtime error: invalid memory address or nil pointer dereference
goroutine 6006616 [running]:
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0xa5
panic({0x3201e40?, 0x5cd2e70?})
	runtime/panic.go:785 +0x132
k8s.io/kubernetes/pkg/api/pod.podLifecycleSleepActionZeroValueInUse.func1(0x0?, 0x0?)
	k8s.io/kubernetes/pkg/api/pod/util.go:775 +0x42
k8s.io/kubernetes/pkg/api/pod.VisitContainers(0xc037a98688, 0x7, 0xc03c12cfe8)
	k8s.io/kubernetes/pkg/api/pod/util.go:73 +0x11e
k8s.io/kubernetes/pkg/api/pod.podLifecycleSleepActionZeroValueInUse(0xc037a61888?)
	k8s.io/kubernetes/pkg/api/pod/util.go:767 +0x3d
k8s.io/kubernetes/pkg/api/pod.GetValidationOptionsFromPodSpecAndMeta(0xc037a98688, 0xc037a61888, 0xc03d1b5a10?, 0xc037a617a0)
	k8s.io/kubernetes/pkg/api/pod/util.go:418 +0x1fe
k8s.io/kubernetes/pkg/api/pod.GetValidationOptionsFromPodTemplate(0x0?, 0xedf170fc1?)
	k8s.io/kubernetes/pkg/api/pod/util.go:551 +0x57
k8s.io/kubernetes/pkg/registry/apps/replicaset.rsStrategy.ValidateUpdate({{0x0?, 0x3773560?}, {0xc037a61688?, 0xc037a98488?}}, {0x3df1580, 0xc041182ab0}, {0x3dcd7d0?, 0xc037a98488}, {0x3dcd7d0, 0xc037a61688})
	k8s.io/kubernetes/pkg/registry/apps/replicaset/strategy.go:146 +0x95
k8s.io/apiserver/pkg/registry/rest.BeforeUpdate({0x3e0a9c0, 0xc006ec32c0}, {0x3df1580, 0xc041182ab0}, {0x3dcd7d0, 0xc037a98488}, {0x3dcd7d0, 0xc037a61688})
	k8s.io/apiserver/pkg/registry/rest/update.go:154 +0x48f
k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update.func1({0x3dcd7d0, 0xc037a61688}, {0xc037e933b0?, 0x379a6f8?})
	k8s.io/apiserver/pkg/registry/generic/registry/store.go:750 +0x585
k8s.io/apiserver/pkg/storage/etcd3.(*store).updateState(0xc006f86370, 0xc02de24400?, 0x34?)
	k8s.io/apiserver/pkg/storage/etcd3/store.go:1072 +0x3a
k8s.io/apiserver/pkg/storage/etcd3.(*store).GuaranteedUpdate(0xc006f86370, {0x3df1580, 0xc041182ab0}, {0xc037a22b70, 0x2b}, {0x3dcd7d0, 0xc037a61208}, 0x1, 0xc0261e5960, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/storage/etcd3/store.go:501 +0x985
k8s.io/apiserver/pkg/storage/cacher.(*Cacher).GuaranteedUpdate(0xc006bf6ea0, {0x3df1580, 0xc041182ab0}, {0xc037a22b70, 0x2b}, {0x3dcd7d0, 0xc037a61208}, 0x1, 0xc0261e5960, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/storage/cacher/cacher.go:981 +0x127
k8s.io/apiserver/pkg/registry/generic/registry.(*DryRunnableStorage).GuaranteedUpdate(0xc034e05d70?, {0x3df1580?, 0xc041182ab0?}, {0xc037a22b70?, 0xb?}, {0x3dcd7d0?, 0xc037a61208?}, 0x50?, 0x2?, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/registry/generic/registry/dryrun.go:107 +0x177
k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update(0xc006e9f680, {0x3df1580, 0xc041182ab0}, {0xc0470f861b, 0x18}, {0x3dc55d8, 0xc041182c90}, 0xc037a365f0, 0xc02b0abb80, 0x0, ...)
	k8s.io/apiserver/pkg/registry/generic/registry/store.go:638 +0x415
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func2()
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:704 +0xa7
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func3()
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:710 +0x35
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0x75
created by k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest in goroutine 6006615
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0xc8

goroutine 6006615 [running]:
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1.1()
	k8s.io/apiserver/pkg/server/filters/timeout.go:110 +0xb0
panic({0x302b360?, 0xc021473740?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1.deferwrap1()
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:84 +0x25
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:93 +0x223
panic({0x302b360?, 0xc021473740?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/endpoints/handlers/finisher.(*result).Return(0xc03b913128?)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:53 +0xad
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest({0x3df1580, 0xc041182ab0}, 0xc0291b50e0, 0x45d964b800, 0x39a76a0)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:126 +0x247
k8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest(...)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource(0xc028766960, {0x3df1580, 0xc041182ab0}, 0xc00788b6c0)
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:708 +0xa65
k8s.io/apiserver/pkg/endpoints/handlers.PatchResource.func1({0x3de3c30, 0xc0291b5020}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:247 +0x2858
k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulPatchResource.func12(0xc0291b5000, 0xc03a0cca10)
	k8s.io/apiserver/pkg/endpoints/installer.go:1329 +0x6a
k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func13(0xc0291b5000, 0xc03a0cca10)
	k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:627 +0x1ce
github.com/emicklei/go-restful/v3.(*Container).dispatch(0xc0018578c0, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	github.com/emicklei/go-restful/v3@v3.11.0/container.go:299 +0x9d7
github.com/emicklei/go-restful/v3.(*Container).Dispatch(...)
	github.com/emicklei/go-restful/v3@v3.11.0/container.go:204
k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x3784546?, 0x12?}, 0xc0018578c0?, 0xc00193b570?}, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/handler.go:145 +0x56c
k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP(0xc0098a4c70?, {0x3de3c30?, 0xc0291b4cc0?}, 0x43?)
	k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x25d
k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc00888dd40, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x410
k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc0470f85f0?, {0x3de3c30?, 0xc0291b4cc0?}, 0x410965?)
	k8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x66
k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x3786997?, 0xc03b9145a0?}, 0xc005f29cb0?, 0xc007a8f260?}, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/handler.go:153 +0x6f1
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4cc0?}, 0x4?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/authorization.go:83 +0x606
net/http.HandlerFunc.ServeHTTP(0xc032d92958?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc02b0ab940?, {0x3de3c30?, 0xc0291b4cc0?}, 0x1ec91b0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0xd3
k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
	k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x22a
k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1(0xc0377b1680?, 0x70?, 0x35e6de0?)
	k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x47
k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish(0xc0377b1680, 0xc03a0cc8c0)
	k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x39
k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle(0xc000240640, {0x3df1628, 0xc03a0cc4d0}, {0xc01a7c9790, {0x3df13d8, 0xc02b0ab8c0}}, 0x0?, 0x0?, 0xc02b0ab900?, 0xc02b0ab900)
	k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x74b
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11(0xc007a33090, {0x3df1580?, 0xc041182660?}, {0xc01a7c9790?, {0x3df13d8?, 0xc02b0ab8c0?}}, 0xc0291b4d20, 0xc0411826f0, 0xc0261e54c0, 0xc02b0ab900)
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xeb
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle(0xc007a33090, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x9f6
net/http.HandlerFunc.ServeHTTP(0xc032d93100?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc0411822d0?, {0x3de3c30?, 0xc0291b4cc0?}, 0xc0470f85fb?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x1b0
net/http.HandlerFunc.ServeHTTP(0xc032d93680?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc0012eee60?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3df1580?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:115 +0x395
net/http.HandlerFunc.ServeHTTP(0xc032d93a10?, {0x3de3c30?, 0xc0291b4c80?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d972c0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x7bf
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b180)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x354
net/http.HandlerFunc.ServeHTTP(0xc037a4b040?, {0x3de3c30?, 0xc0291b4c80?}, 0xc037a36500?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11({0x3de3c30, 0xc0291b4c80}, 0xc037a4b040)
	k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0xb9
net/http.HandlerFunc.ServeHTTP(0xc00c621f48?, {0x3de3c30?, 0xc0291b4c80?}, 0xb424ec?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
	k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0x5b
created by k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP in goroutine 6006574
	k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x198

goroutine 6006574 [running]:
golang.org/x/net/http2.(*serverConn).runHandler.func1()
	golang.org/x/net@v0.30.0/http2/server.go:2432 +0x145
panic({0x302b360?, 0xc021473820?})
	runtime/panic.go:785 +0x132
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3df0fa0, 0x5d564c0}, {0x302b360, 0xc021473820}, {0xc0293fca70, 0x1, 0xc03c6a7360?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0xc03c6a73a8, 0x1, 0xc027cb3180?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x302b360?, 0xc021473820?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP(0xc0065b6d80, {0x3de3c30, 0xc0291b4c00}, 0xdf8475800?)
	k8s.io/apiserver/pkg/server/filters/timeout.go:121 +0x32f
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x234
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c00?}, 0x7f8055d9c5b8?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x17d
net/http.HandlerFunc.ServeHTTP(0xc00178f100?, {0x3de3c30?, 0xc0291b4c00?}, 0x3782b82?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*goaway).ServeHTTP(0xc005daf720, {0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/filters/goaway.go:72 +0x142
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xa7
net/http.HandlerFunc.ServeHTTP(0x1?, {0x3de3c30?, 0xc0291b4c00?}, 0x410965?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/httplog/httplog.go:112 +0x95
net/http.HandlerFunc.ServeHTTP(0x3df1698?, {0x3de3c30?, 0xc0291b4c00?}, 0x30?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4ac80)
	k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x143
net/http.HandlerFunc.ServeHTTP(0xc037a4ab40?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a4ab40?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4ab40)
	k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x119
net/http.HandlerFunc.ServeHTTP(0xc037a4aa00?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xef2dbd6e4f3?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4aa00)
	k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xaf
net/http.HandlerFunc.ServeHTTP(0x30?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc0000a2e08?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19({0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a4aa00?)
	k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xd5
net/http.HandlerFunc.ServeHTTP(0xed50e219a1f68e5c?, {0x3de3ea0?, 0xc01b9dfa40?}, 0x410965?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4aa00)
	k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0xdc
net/http.HandlerFunc.ServeHTTP(0xc040891200?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a22ab0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34({0x3de3ea0, 0xc01b9dfa40}, 0xc0379e2a00)
	k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x135
net/http.HandlerFunc.ServeHTTP(0xc0003a4ac0?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc00043e330?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP(0xc00c621d40?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc0408fb950?)
	k8s.io/apiserver/pkg/server/handler.go:188 +0x25
net/http.serverHandler.ServeHTTP({0xc0474c3b60?}, {0x3de3ea0?, 0xc01b9dfa40?}, 0x3deb700?)
	net/http/server.go:3210 +0x8e
net/http.initALPNRequest.ServeHTTP({{0x3df1580?, 0xc0321de9c0?}, 0xc03799c388?, {0xc007a94960?}}, {0x3de3ea0, 0xc01b9dfa40}, 0xc0379e2a00)
	net/http/server.go:3819 +0x231
golang.org/x/net/http2.(*serverConn).runHandler(0x4469db?, 0xc032f50260?, 0x3db0780?, 0x5cd2c90?)
	golang.org/x/net@v0.30.0/http2/server.go:2439 +0xf5
created by golang.org/x/net/http2.(*serverConn).scheduleHandler in goroutine 2753346
	golang.org/x/net@v0.30.0/http2/server.go:2373 +0x21d
```

#### Kubernetes version

<details>

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0-gke.1448000
```

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.1-gke.1002000
```

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0-eks-5ca49cb
```

</details>


#### Cloud provider

<details>

AWS, GCP

</details>


#### OS version

<details>

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
È´òÈ£éÈô©

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ‰∏ªË¶ÅÊèèËø∞Âú® Kubernetes v1.32 ÂçáÁ∫ßÂêéÔºåÂΩì pod ÈÖçÁΩÆ‰∫Ü postStart ÁîüÂëΩÂë®ÊúüÈí©Â≠êÊó∂Ôºåpod Êó†Ê≥ïÊ≠£Â∏∏ÂêØÂä®ÔºåÂπ∂ÂØºËá¥‰∏ÄÁ≥ªÂàóÊìç‰ΩúÔºàÂ¶ÇÊâ©ÂÆπ„ÄÅ‰øÆÊîπÈïúÂÉèÁ≠âÔºâÂá∫Áé∞ÈîôËØØÂíåÂÜÖÈÉ® panic„ÄÇÈÄöËøáÈîôËØØÊó•ÂøóÂèØ‰ª•ÁúãÂà∞Ôºåapiserver Âú®Â§ÑÁêÜ PATCH Êìç‰ΩúÊó∂Âõ†ÂØπ postStart Èí©Â≠êËøõË°åÈ™åËØÅÊó∂‰∫ßÁîü‰∫Ü nil pointer dereferenceÔºå‰ªéËÄåËß¶Âèë panic„ÄÇ  
Â∞±ÂÆâÂÖ®È£éÈô©ËÄåË®ÄÔºåËØ•ÈóÆÈ¢ò‰ºöÂØºËá¥ apiserver Â¥©Ê∫ÉÊàñËÄÖÈïøÊó∂Èó¥ÈòªÂ°ûÔºå‰ªéËÄåÈÄ†ÊàêÈõÜÁæ§ÊéßÂà∂Âπ≥Èù¢‰∏çÂèØÁî®ÔºàÊãíÁªùÊúçÂä°ÔºåDoSÔºâÔºå‰ΩøÂæóÈõÜÁæ§ÁöÑÁÆ°ÁêÜÂäüËÉΩÂèóÂà∞‰∏•ÈáçÂΩ±Âìç„ÄÇÁî±‰∫é Kubernetes Êú¨Ë∫´ÂÖÅËÆ∏‰∏çÂêåÁßüÊà∑ÁöÑÁî®Êà∑Âú®Ëá™Â∑±ÁöÑÂëΩÂêçÁ©∫Èó¥ÂÜÖÂàõÂª∫ DeploymentÔºåÂ¶ÇÊûúÊîªÂáªËÄÖÊã•ÊúâÂ∏∏ËßÑÁöÑ pod ÂàõÂª∫ÊùÉÈôêÔºåÂàôÂèØ‰ª•ÊûÑÈÄ†ÂåÖÂê´ postStart Èí©Â≠êÁöÑ PodÔºå‰ªéËÄåËß¶ÂèëËØ•ÈóÆÈ¢ò„ÄÇÁâπÂà´ÊòØÂú®Â§öÁßüÊà∑ÈõÜÁæ§‰∏≠ÔºåÂ¶ÇÊûú‰ΩéÊùÉÈôêÁî®Êà∑ÂèØ‰ª•ÂàõÂª∫ DeploymentÔºåÂ∞±ÂèØËÉΩÂà©Áî®ËØ•ÊºèÊ¥ûÂΩ±ÂìçÂêå‰∏ÄÈõÜÁæ§‰∏≠ÂÖ∂‰ªñÁî®Êà∑ÁöÑÊ≠£Â∏∏ËøêË°å„ÄÇ

Âõ†Ê≠§ÔºåËØ•ÈóÆÈ¢òÂÖ∑ÊúâÊΩúÂú®ÁöÑÊãíÁªùÊúçÂä°È£éÈô©ÔºåÂπ∂‰∏îÊ†πÊçÆ CVSS 3.1 ÁöÑËØÑÂàÜÊ†áÂáÜÔºåÂ¶ÇÊûúÊîªÂáªËÄÖ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÈ´òÊùÉÈôêÂç≥ÂèØÂÆûÊñΩÔºàÂè™ÈúÄÊôÆÈÄöÁöÑ pod ÂàõÂª∫Êìç‰ΩúÔºâÔºåÂàôËØ•ÊºèÊ¥ûÂèØËØÑ‰∏∫È´òÈ£éÈô©„ÄÇ

Âà§Êñ≠‰æùÊçÆÔºö
1. ËØ• issue ÂØºËá¥ apiserver ÂÜÖÈÉ® panicÔºå‰∫ßÁîü nil pointer dereferenceÔºå‰ªéËÄå‰∏≠Êñ≠Ê≠£Â∏∏ÊúçÂä°„ÄÇ
2. ÊîªÂáªËÄÖÂè™ÈúÄÂàõÂª∫‰∏Ä‰∏™ÂåÖÂê´ postStart Èí©Â≠êÁöÑ Deployment Â∞±ÂèØËß¶ÂèëÔºåÊó†ÈúÄÈ´òÊùÉÈôêÊìç‰Ωú„ÄÇ
3. Âá∫Áé∞ÈóÆÈ¢òÂêéÔºåpod Êó†Ê≥ïÊ≠£Â∏∏ÂêØÂä®ÔºåÂπ∂‰∏îÂ∏∏ËßÅÁöÑÁÆ°ÁêÜÊìç‰ΩúÔºàÊâ©ÂÆπ„ÄÅÊõ¥Êñ∞Á≠âÔºâ‰πü‰ºöÂ§±Ë¥•ÔºåÊúÄÁªàÂèØËÉΩÂØºËá¥ÈõÜÁæ§ÊãíÁªùÊúçÂä°„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import time
import threading
import http.server
import socketserver
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# HTTPÊúçÂä°Âô®Áî®‰∫éÂ±ïÁ§∫Â§çÁé∞ËÑöÊú¨Âú®Êú¨Âú∞ËøêË°åÊó∂ÁöÑÊèêÁ§∫‰ø°ÊÅØÔºàÁõëÂê¨8080Á´ØÂè£Ôºâ
def start_http_server():
    class Handler(http.server.SimpleHTTPRequestHandler):
        def log_message(self, format, *args):
            print(format % args)
    with socketserver.TCPServer(("", 8080), Handler) as httpd:
        print("HTTP server serving at port 8080")
        httpd.serve_forever()

def create_deployment_with_poststart(api_instance, namespace="default", name="inflate"):
    # ÂÆö‰πâ deployment specÔºåÂÖ∂‰∏≠ÂåÖÂê´ postStart ÁîüÂëΩÂë®ÊúüÈí©Â≠ê: sleep 15 Áßí
    container = client.V1Container(
        name="inflate",
        image="python:3",
        command=["sleep"],
        args=["infinity"],
        lifecycle=client.V1Lifecycle(
            post_start=client.V1Handler(
                _exec=client.V1ExecAction(
                    command=["sleep", "15"]
                )
            )
        )
    )

    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": "inflate"}),
        spec=client.V1PodSpec(
            containers=[container],
            service_account_name="inflate",
            automount_service_account_token=False
        )
    )

    spec = client.V1DeploymentSpec(
        replicas=1,
        selector=client.V1LabelSelector(match_labels={"app": "inflate"}),
        template=template
    )

    deployment = client.V1Deployment(
        api_version="apps/v1",
        kind="Deployment",
        metadata=client.V1ObjectMeta(name=name),
        spec=spec
    )

    try:
        api_response = api_instance.create_namespaced_deployment(
            body=deployment,
            namespace=namespace
        )
        print(f"Deployment {name} created. Status='{api_response.metadata.name}'")
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)

def scale_deployment(api_instance, namespace="default", name="inflate", replicas=2):
    # Â∞ùËØïÊâ©ÂÆπÔºåËßÇÂØüÊòØÂê¶Ëß¶Âèë apiserver ÁöÑÈîôËØØ
    body = {"spec": {"replicas": replicas}}
    try:
        api_response = api_instance.patch_namespaced_deployment_scale(
            name=name,
            namespace=namespace,
            body=body
        )
        print(f"Scaled deployment {name} to {replicas} replicas.")
    except ApiException as e:
        print("Exception when scaling deployment: %s\n" % e)

def patch_deployment(api_instance, namespace="default", name="inflate"):
    # Â∞ùËØï‰øÆÊîπ deploymentÔºå‰æãÂ¶ÇÊõ¥Êñ∞ÈïúÂÉè tagÔºåËßÇÂØüÊòØÂê¶Ëß¶ÂèëÈîôËØØ
    body = {"spec": {"template": {"spec": {"containers": [{"name": "inflate", "image": "python:3.8"}]}}}}
    try:
        api_response = api_instance.patch_namespaced_deployment(
            name=name,
            namespace=namespace,
            body=body
        )
        print(f"Patched deployment {name} to update the image tag.")
    except ApiException as e:
        print("Exception when patching deployment: %s\n" % e)

def main():
    # ÂºÄÂêØ HTTP ÊúçÂä°Âô®ÁöÑÁ∫øÁ®ãÔºåÁî®‰∫éÁõëÂê¨8080Á´ØÂè£Âπ∂ËæìÂá∫Êó•Âøó‰ø°ÊÅØ
    threading.Thread(target=start_http_server, daemon=True).start()
    
    # ‰ªéÈªòËÆ§ kubeconfig Âä†ËΩΩÈÖçÁΩÆ
    try:
        config.load_kube_config()
    except Exception as e:
        print("Error loading kubeconfig: %s" % e)
        return

    apps_v1 = client.AppsV1Api()
    core_v1 = client.CoreV1Api()

    # ÂàõÂª∫ ServiceAccount "inflate"ÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ
    sa_body = client.V1ServiceAccount(
        metadata=client.V1ObjectMeta(name="inflate", labels={"app": "inflate"}),
        automount_service_account_token=False
    )
    try:
        core_v1.create_namespaced_service_account(namespace="default", body=sa_body)
        print("ServiceAccount 'inflate' created.")
    except ApiException as e:
        if e.status != 409:
            print("Exception when creating ServiceAccount: %s\n" % e)
        else:
            print("ServiceAccount 'inflate' already exists.")

    # ÂàõÂª∫ÂåÖÂê´ postStart ÁîüÂëΩÂë®ÊúüÈí©Â≠êÁöÑ deployment
    create_deployment_with_poststart(apps_v1, namespace="default", name="inflate")

    # Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥ÔºåËßÇÂØü deployment Áä∂ÊÄÅ
    print("Á≠âÂæÖ20ÁßíËÆ© deployment ËøõÂÖ• Pending Áä∂ÊÄÅ...")
    time.sleep(20)

    # Â∞ùËØïÊâ©ÂÆπ deploymentÔºåËßÇÂØüÊòØÂê¶Ëß¶ÂèëÈîôËØØÔºàDoSÊïàÊûúÔºâ
    print("Â∞ùËØïÊâ©ÂÆπ deployment ...")
    scale_deployment(apps_v1, namespace="default", name="inflate", replicas=2)
    
    # Â∞ùËØï patch ÈÉ®ÁΩ≤ÔºåÊõ¥Êñ∞ÈïúÂÉè tag
    print("Â∞ùËØï patch deployment ...")
    patch_deployment(apps_v1, namespace="default", name="inflate")

    print("Â§çÁé∞Êìç‰ΩúÂ∑≤ÂÆåÊàêÔºåËØ∑Ê£ÄÊü•ÈõÜÁæ§Áä∂ÊÄÅÂíå apiserver Êó•Âøó„ÄÇ")
    
    # ‰øùÊåÅËÑöÊú¨ËøêË°åÔºå‰ª•‰æø HTTP server ÊåÅÁª≠Êèê‰æõÊúçÂä°
    while True:
        time.sleep(30)

if __name__ == "__main__":
    main()
```


---


## Issue #129787 Compressed pod log files default to 644 permissions

- Issue ÈìæÊé•Ôºö[#129787](https://github.com/kubernetes/kubernetes/issues/129787)

### Issue ÂÜÖÂÆπ

#### What happened?

When the pod logs rotate and get compressed, the gzipped file defaults to the permissions of 644. With the directories set to 755 this allows the logs to be world readable at that point.

This appears to be where this issue is happening at - https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/logs/container_log_manager.go#L422

#### What did you expect to happen?

For the compressed logs to either inherit the original log file permissions (600) or for this file permission to be configurable in the kubelet configuration

#### How can we reproduce it (as minimally and precisely as possible)?

Generate logs & wait until they compress - https://kubernetes.io/docs/concepts/cluster-administration/logging/#basic-logging-in-kubernetes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
Rocky Linux 9.5
Version = 9.20250121.0.5 (Blue Onyx)
$ uname -a
5.14.0-503.21.1.el9_5.x86_64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
Êú¨ÈóÆÈ¢òÊèèËø∞ÁöÑÂÆâÂÖ®ÈóÆÈ¢òÂ±û‰∫éÂÆâÂÖ®ÈÖçÁΩÆÈîôËØØÂØºËá¥ÁöÑ‰ø°ÊÅØÊ≥ÑÈú≤ÈóÆÈ¢òÔºåÊ†πÊçÆ CVSS 3.1 ËØÑÂàÜÔºåËôΩÁÑ∂ÊîªÂáªÈúÄË¶ÅÂú®Êú¨Âú∞Êñá‰ª∂Á≥ªÁªü‰∏äÊìç‰ΩúÔºàÂπ∂ÈùûËøúÁ®ãÁΩëÁªúÊîªÂáªÔºâÔºå‰ΩÜÁî±‰∫éÔºö
1. ÊîªÂáªËÄÖ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊùÉÈôêÂç≥ÂèØËØªÂèñÊó•ÂøóÊñá‰ª∂ÔºàÂõ†‰∏∫ÈªòËÆ§ÊùÉÈôê‰∏∫ 644ÔºåÂç≥ÊâÄÊúâÁî®Êà∑ÂèØËØªÔºâÔºå
2. Êó•Âøó‰∏≠ÂèØËÉΩÂåÖÂê´ÊïèÊÑü‰ø°ÊÅØÔºå
3. Â§öÁßüÊà∑ÁéØÂ¢É‰∏ã‰ΩéÊùÉÈôêÁî®Êà∑ÂèØËÉΩËØªÂèñÂà∞ÂÖ∂‰ªñÁî®Êà∑ÁöÑÊó•ÂøóÔºå

ÊïÖÊåâÊÉÖÊôØËÄÉËôëÂèØËÉΩÂØºËá¥ËæÉ‰∏•ÈáçÁöÑ‰ø°ÊÅØÊ≥ÑÈú≤„ÄÇËã•Ê†πÊçÆ CVSS 3.1 ‰º∞ÂàÜÔºåÂèØËÉΩÂú® 7.0 Êàñ‰ª•‰∏äÔºåÂàôÂ∫îÂΩí‰∏∫È´òÈ£éÈô©Ôºõ‰ΩÜËÄÉËôëÂà∞ÊîªÂáªÈúÄË¶ÅÂú®ÂÆø‰∏ªÊú∫‰∏äËøõË°åÔºåÊú¨ÈóÆÈ¢òÂ≠òÂú®‰∏ÄÂÆö‰ΩøÁî®Âú∫ÊôØÈôêÂà∂ÔºåÂõ†Ê≠§Â§ßÈÉ®ÂàÜÊÉÖÂÜµ‰∏ãÂèØ‰ª•ËØÑ‰º∞‰∏∫‰ΩéÈ£éÈô©„ÄÇ‰∏çËøáÂú®Â≠òÂú®ÊïèÊÑü‰ø°ÊÅØÔºàÂ¶ÇÂá≠ÊçÆ„ÄÅÂØÜÈí•ÔºâÁöÑÂú∫ÊôØ‰∏ãÔºåÈ£éÈô©‰∏çÂèØÂ∞èËßëÔºåÂ∫îÂΩí‰∏∫È´òÈ£éÈô©„ÄÇ

Âú®Ê≠§ÂõûÁ≠î‰∏≠Ôºå‰∏∫ÁªºÂêàÊÉÖÂÜµÔºåÊàë‰ª¨ÁªôÂá∫È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰ΩéÈ£éÈô©‚ÄùÔºàÈâ¥‰∫éÂú®ÂæàÂ§öÈªòËÆ§Âú∫ÊôØ‰∏ãÔºåÊó•Âøó‰∏≠ÁöÑ‰ø°ÊÅØÂèØËÉΩÊ≤°ÊúâÊûÅÈ´òÊïèÊÑüÊÄßÔºâÔºå‰ΩÜÂú®ÊïèÊÑüÁéØÂ¢É‰∏ãÂèØËßÜ‰∏∫È´òÈ£éÈô©„ÄÇ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue Êä•ÂëäÁöÑÊòØÂú®ÂÆπÂô®Êó•ÂøóËΩÆËΩ¨Êó∂ÔºåÂéãÁº©ÂêéÁöÑÊó•ÂøóÊñá‰ª∂ÈªòËÆ§ÊùÉÈôê‰∏∫ 644ÔºåÂØºËá¥Âú®ÁõÆÂΩïÊùÉÈôê‰∏∫ 755 ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊó•ÂøóÊñá‰ª∂‰ºöË¢´ÊâÄÊúâÁî®Êà∑ËØªÂèñÂà∞„ÄÇÊó•ÂøóÊñá‰ª∂‰∏≠ÂèØËÉΩÂ≠òÂú®ÊïèÊÑü‰ø°ÊÅØÔºà‰æãÂ¶ÇÂá≠ÊçÆ„ÄÅË∞ÉËØï‰ø°ÊÅØÁ≠âÔºâÔºåÂõ†Ê≠§‰ΩøÂæóÈùûÁâπÊùÉÁî®Êà∑ÂèØ‰ª•ËΩªÊòìËØªÂèñÊïèÊÑüÊï∞ÊçÆ„ÄÇÊ≠§ÈóÆÈ¢òÂ±û‰∫éÊó•ÂøóÊùÉÈôêÈÖçÁΩÆ‰∏çÂΩìÂºïËµ∑ÁöÑ‰ø°ÊÅØÊ≥ÑÈú≤ÈóÆÈ¢ò„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºöÂ¶ÇÊûúÊó•ÂøóÂÜÖÂÆπÊú¨Ë∫´‰∏çÂåÖÂê´ÊïèÊÑüÊï∞ÊçÆÔºåÂàôÈ£éÈô©ËæÉ‰ΩéÔºõ‰ΩÜÂú®Â§ßÂ§öÊï∞Áîü‰∫ßÁéØÂ¢É‰∏≠ÔºåÊó•Âøó‰∏≠ÂæÄÂæÄ‰ºöËÆ∞ÂΩïÊïèÊÑüÊï∞ÊçÆÔºàÂ¶ÇÂÜÖÈÉ®ËØ∑Ê±Ç„ÄÅÈîôËØØËØ¶ÊÉÖÁîöËá≥Âá≠ÊçÆÔºâ‰∏î‰∏ÄËà¨ Kubernetes ÈõÜÁæ§ÂèØËÉΩÂ≠òÂú®Â§öÁßüÊà∑Âú∫ÊôØÔºå‰∏Ä‰∏™‰∏çÂÖ∑Â§áÈ´òÂ∫¶ÊùÉÈôêÁöÑÁî®Êà∑ÂèØËÉΩÂà©Áî®Ëøô‰∏ÄÈóÆÈ¢òËØªÂèñ‰ªñ‰∫∫ÂÆπÂô®ÁöÑÊó•Âøó‰ø°ÊÅØÔºå‰ªéËÄåÈÄ†ÊàêÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import os
import stat
import gzip
import shutil
import tempfile
import threading
from http.server import SimpleHTTPRequestHandler, HTTPServer

def create_log_file(log_path):
    # ÂàõÂª∫‰∏Ä‰∏™Êó•ÂøóÊñá‰ª∂ÔºåÂπ∂ËÆæÁΩÆ‰∏∫600ÊùÉÈôêÔºà‰ªÖÊã•ÊúâËÄÖÂèØËØªÂÜôÔºâ
    with open(log_path, "w") as f:
        f.write("This is a sample log entry that may contain sensitive data.\n" * 10)
    os.chmod(log_path, 0o600)
    print(f"[+] Created log file: {log_path} with mode: {oct(os.stat(log_path).st_mode & 0o777)}")

def compress_log_file(src_path, dst_path):
    # ‰ΩøÁî®gzipÂéãÁº©Êó•ÂøóÊñá‰ª∂
    with open(src_path, 'rb') as f_in, gzip.open(dst_path, 'wb') as f_out:
        shutil.copyfileobj(f_in, f_out)
    # Ê®°ÊãüÊó•ÂøóËΩÆËΩ¨‰∏≠ÁöÑÊÉÖÂÜµÔºöÂéãÁº©ÂêéÁöÑÊñá‰ª∂Êú™ÁªßÊâøÂéüÂßã600ÊùÉÈôêÔºåÈÄöÂ∏∏ÈªòËÆ§‰ºöÊòØ644ÔºàÂèóumaskÂΩ±ÂìçÔºâ
    print(f"[+] Compressed log file: {dst_path}")
    mode = os.stat(dst_path).st_mode & 0o777
    print(f"[+] Compressed file mode: {oct(mode)}")
    return mode

def start_http_server(directory, port=8080):
    # ÂàáÊç¢Â∑•‰ΩúÁõÆÂΩïÂà∞ÊåáÂÆöÁõÆÂΩï
    os.chdir(directory)
    
    handler = SimpleHTTPRequestHandler
    server = HTTPServer(("", port), handler)
    print(f"[+] HTTP server started at http://0.0.0.0:{port}")

    def serve():
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass
        server.server_close()
    
    thread = threading.Thread(target=serve, daemon=True)
    thread.start()
    return server

def main():
    # ÂàõÂª∫‰∏Ä‰∏™‰∏¥Êó∂ÁõÆÂΩï‰Ωú‰∏∫ÊµãËØïÁéØÂ¢É
    with tempfile.TemporaryDirectory() as tmpdir:
        print(f"[+] Using temporary directory: {tmpdir}")
        log_file = os.path.join(tmpdir, "pod.log")
        compressed_file = os.path.join(tmpdir, "pod.log.gz")
        
        # Ê®°ÊãüÁîüÊàêÊó•ÂøóÂíåËÆæÁΩÆ‰∏•Ê†ºÊùÉÈôê
        create_log_file(log_file)
        
        # Ê®°ÊãüÊó•ÂøóËΩÆËΩ¨ÔºåÂéãÁº©Êó•ÂøóÊñá‰ª∂
        final_mode = compress_log_file(log_file, compressed_file)
        
        # Ê£ÄÊü•Â¶ÇÊûúÂéãÁº©Êñá‰ª∂ÊùÉÈôê‰∏∫644ÔºåÂàôËØ¥ÊòéÂ≠òÂú®ËØ•ÈóÆÈ¢ò
        if final_mode == 0o644:
            print("[!] Á°ÆËÆ§Â§çÁé∞ÈóÆÈ¢òÔºöÂéãÁº©ÂêéÁöÑÊó•ÂøóÊñá‰ª∂ÊùÉÈôê‰∏∫ 644ÔºåÂèØËÉΩÂØºËá¥ÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤")
        else:
            print("[+] Êú™Â§çÁé∞ÈóÆÈ¢òÔºöÂéãÁº©Êó•ÂøóÊñá‰ª∂ÊùÉÈôê‰∏∫", oct(final_mode))
        
        # ÂêØÂä®HTTPÊúçÂä°Ôºå‰æø‰∫éËøúÁ®ãÊü•ÁúãÊàñ‰∏ãËΩΩÊñá‰ª∂
        http_server = start_http_server(tmpdir, port=8080)
        
        # ‰øùÊåÅÊúçÂä°ËøêË°åÔºåÁõ¥Âà∞Áî®Êà∑ÊâãÂä®ÁªàÊ≠¢
        try:
            while True:
                pass
        except KeyboardInterrupt:
            print("\n[+] ÁªàÊ≠¢HTTPÊúçÂä°")

if __name__ == "__main__":
    main()
```


---


## Issue #129584 Race condition in API server that can lead to leaked resources

- Issue ÈìæÊé•Ôºö[#129584](https://github.com/kubernetes/kubernetes/issues/129584)

### Issue ÂÜÖÂÆπ

#### What happened?

We believe to have come across a race condition in the API Server that can lead to scenarios where controllers leak external resources. We primarily observed this in an internal fork of API Server, but we also have some (internal) evidence to have observed this in Kubernetes directly, and our current hypothesis applies to Kubernetes' code as well, so we are sharing this here to seek confirmation and agreement for the solution.

Let's consider a scenario where a controller adds a finalizer to a newly created object, because it allocates some external resources that it needs to clean up upon deletion of the object. Some other actor issues a DELETE request that happens to be right _after_ the PUT request to add the finalizer. What we observe is that _both_ the DELETE and the PUT request (adding the finalizer) _succeed_, leading the controller to allocate the resources despite no finalizer existing anymore, because the object is now hard-deleted.

If we take a look at [`Store#Delete`](https://github.com/kubernetes/kubernetes/blob/36d316ebc524a47d05e479b848148c1acf1cee7b/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L1128-L1217), it consists roughly of the following steps:
1. Fetch the object.
2. Check whether there are any pending finalizers, and if so, do not delete immediately.
3. Send the request for immediate deletion to storage otherwise.

In the above scenario, the PUT request would occur after the DELETE request processing has (1) fetched the current object. This means it believes that the object has no pending finalizers, and thus proceeds with immediate deletion. Since the DELETE request did not specify an explicit precondition on the resource version, the concurrently added finalizer is ignored by storage, and the deletion is carried out.

Such a rV precondition is only set towards storage if either (a) the client specified it as part of the request, or (b) if within (2) a finalizer was added internally, because it caused an update to the object.

We believe that due to the read performed in (1), the DELETE logic should implicitly add a resource version precondition for the deletion towards storage even if no update was issued, and set this to the resource version of the object read in (1).

Though one could argue that the deleting client should've just set a precondition, and that the delete happening unconditionally is correct, I would urge to view this from the perspective of the controller. A delete can come from anywhere outside the control of a controller, but the controller now leaks resources because it made the reasonable assumption that its request to add a finalizer succeeding means the finalizer does indeed now exist. 

_Disclaimer_: I tried submitting this to the ML first, but somehow it seems my message never made it there, so reporting it here instead.

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a specific reproduction, since this is a very intricate timing issue.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
È´òÈ£éÈô©  
ÔºàCVSS 3.1 ËØÑÂàÜÈ¢Ñ‰º∞È´ò‰∫é 7.0ÔºåÂõ†ÂÖ∂ÂèØËÉΩÂØºËá¥ÂÖ≥ÈîÆÂ§ñÈÉ®ËµÑÊ∫êÊ≥ÑÈú≤ÂíåÂêéÁª≠ÊúçÂä°ÊãíÁªùÔºâ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫Ü API Server ‰∏≠ÁöÑ‰∏Ä‰∏™Á´ûÊÄÅÊù°‰ª∂ÈóÆÈ¢ò„ÄÇÂú∫ÊôØÊòØÔºöÂΩìÊéßÂà∂Âô®‰∏∫‰∏Ä‰∏™Êñ∞ÂàõÂª∫ÁöÑÂØπË±°Ê∑ªÂä† finalizerÔºà‰ª•‰æøÂú®ÂØπË±°Âà†Èô§Êó∂Ê∏ÖÁêÜÂ§ñÈÉ®ËµÑÊ∫êÔºâÊó∂ÔºåÂ¶ÇÊûúÊÅ∞Â•ΩÊúâÂè¶‰∏Ä‰∏™ actor Âú® finalizer Ê∑ªÂä†Êìç‰ΩúÊèê‰∫§‰πãÂêé„ÄÅ‰ΩÜÂú®Â≠òÂÇ® DELETE Êìç‰ΩúÂÆåÊàêÂâçÂèëËµ∑ DELETE ËØ∑Ê±ÇÔºåÂàôËØ• DELETE Êìç‰Ωú‰æùËµñ‰∫éÁ¨¨‰∏ÄÊ≠•Ëé∑ÂèñÁöÑÂØπË±°Áä∂ÊÄÅÔºàÊó† finalizerÔºâÔºåÂõ†Ê≠§Âà†Èô§Êìç‰Ωú‰ªçÁÑ∂ÊàêÂäüÔºåÂøΩÁï•‰∫ÜÂêéÊù•Ê∑ªÂä†ÁöÑ finalizerÔºåÁî±Ê≠§ÂØºËá¥Â§ñÈÉ®ËµÑÊ∫êË¢´ÂàÜÈÖç‰ΩÜÊ∞∏ËøúÂæó‰∏çÂà∞Ê∏ÖÁêÜÔºåÊúÄÁªàÂá∫Áé∞ËµÑÊ∫êÊ≥ÑÈú≤„ÄÇËøô‰∏ÄÊºèÊ¥ûÂèØËÉΩÂØºËá¥Â†ÜÁßØÁöÑÂ§ñÈÉ®ËµÑÊ∫êÊó†Ê≥ïÂõûÊî∂Ôºå‰ªéËÄåËÄóÂ∞ΩËµÑÊ∫ê„ÄÅÂºïÂèëÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁöÑÈ£éÈô©„ÄÇ

‰ªéÂÆâÂÖ®È£éÈô©ËßíÂ∫¶ÁúãÔºö
1. ËØ•ÈóÆÈ¢òÂ±û‰∫éÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†‰∏∫ÂÆÉÂΩ±Âìç‰∫ÜÂ§ñÈÉ®ËµÑÊ∫êÁöÑÊ≠£Â∏∏Ê∏ÖÁêÜÔºåÂπ∂ÂèØËÉΩË¢´ÊÅ∂ÊÑèÂà©Áî®Ôºõ
2. ËØ•ÊºèÊ¥ûÂèØ‰ª•‰ΩøÊîªÂáªËÄÖÂú®ÂÖ∑Â§áÂà†Èô§ÊùÉÈôêÁöÑÂâçÊèê‰∏ãÔºåÈÄöËøáÂà∂ÈÄ†Á´ûÊÄÅÊù°‰ª∂‰∏çÊñ≠Ëß¶ÂèëËµÑÊ∫êÊ≥ÑÈú≤ÔºåËøõËÄåËÄóÂ∞ΩÊüê‰∫õÂ§ñÈÉ®ËµÑÊ∫êÔºå‰ΩøÂæóÁ≥ªÁªüÊàñÊúçÂä°‰∏çÂèØÁî®Ôºõ
3. Ê†πÊçÆ CVSS 3.1 Ê†áÂáÜÔºåËØ•ÊºèÊ¥ûÂ¶ÇÊûúË¢´ÊÅ∂ÊÑèÁî®Êà∑Âà©Áî®ÔºåÂèØËÉΩÂØºËá¥‰∏•ÈáçÁöÑÊãíÁªùÊúçÂä°ÊîªÂáªÔºåÂõ†ËÄåÈ£éÈô©ËØÑÁ∫ßËææÂà∞È´òÈ£éÈô©„ÄÇËôΩÁÑ∂ÂÆûÊñΩÊîªÂáªÈúÄË¶ÅÊîªÂáªËÄÖÊúâÂØπÁâπÂÆöÂØπË±°ÊâßË°å DELETE Êìç‰ΩúÁöÑÊùÉÈôêÔºå‰ΩÜÂú®Â§öÁî®Êà∑ÁéØÂ¢É‰∏≠Âç≥‰ΩøÊòØ‰ΩéÊùÉÈôêÁî®Êà∑Â¶ÇÊûúËÉΩÊìçÁ∫µËá™Â∑±Êã•ÊúâÊùÉÈôêÁöÑÂØπË±°Ôºå‰πüÂèØËÉΩÂØπÊï¥‰∏™Á≥ªÁªüÈÄ†ÊàêÂΩ±ÂìçÔºåÂõ†Ê≠§‰∏çÂÆúÈôç‰ΩéÈ£éÈô©ËØÑÁ∫ß„ÄÇ

Âõ†Ê≠§ÔºåËØ•ÊºèÊ¥ûË¢´Âà§ÂÆö‰∏∫È´òÈ£éÈô©ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import threading
import time
from http.server import HTTPServer, BaseHTTPRequestHandler

# Ê®°ÊãüÁöÑËµÑÊ∫êÂØπË±°ÔºåÂåÖÂê´ finalizer ÂíåËµÑÊ∫êÁâàÊú¨Á≠âÂ±ûÊÄß
class ResourceObject:
    def __init__(self):
        self.finalizer = False
        self.resource_version = 1
        self.deleted = False
        self.lock = threading.Lock()

    def get_snapshot(self):
        with self.lock:
            return {
                "finalizer": self.finalizer,
                "resource_version": self.resource_version,
                "deleted": self.deleted
            }

    def add_finalizer(self):
        with self.lock:
            # Ê®°ÊãüÂ§ÑÁêÜÊó∂Èó¥
            time.sleep(0.1)
            # Êõ¥Êñ∞ finalizer Âπ∂Â¢ûÂä†ËµÑÊ∫êÁâàÊú¨
            self.finalizer = True
            self.resource_version += 1
            print(f"[PUT] Finalizer added, new resource_version: {self.resource_version}")

    def delete(self, expected_rv):
        with self.lock:
            # Âà†Èô§ÂâçÊ£ÄÊü•ËµÑÊ∫êÁâàÊú¨ÂåπÈÖç (Â¶ÇÊûú‰ΩøÁî®È¢ÑÊù°‰ª∂ÔºåÂàô‰ºöÊ£ÄÊü•)
            # Ê®°ÊãüÊ≤°ÊúâËÆæÁΩÆÈ¢ÑÊù°‰ª∂ÔºåÊâÄ‰ª•ÊØîËæÉÊóßËµÑÊ∫êÁâàÊú¨ÁöÑÊÉÖÂÜµ
            if self.deleted:
                print("[DELETE] Already deleted.")
                return False
            # Ê≠§Â§ÑÊ®°ÊãüÁ´ûÊÄÅÔºöÂ¶ÇÊûú object Âú®ËØªÂèñÂêéË¢´Êõ¥Êñ∞‰∫Ü finalizerÔºå‰ΩÜÂà†Èô§Êìç‰ΩúÊú™ËÉΩÊÑüÁü•
            snapshot = self.get_snapshot()
            # Â¶ÇÊûú snapshot.finalizer ‰∏∫ FalseÔºåÂàôËÆ§‰∏∫ÂèØ‰ª•Âà†Èô§
            if not snapshot["finalizer"]:
                # Ê≠§Â§Ñ‰∏çÊ£ÄÊü• expected_rv‰∏éÂΩìÂâç resource_version ÁöÑ‰∏ÄËá¥ÊÄß
                self.deleted = True
                print("[DELETE] Delete operation succeeded without noticing concurrent finalizer.")
                return True
            else:
                # Ê£ÄÊü•Â§±Ë¥•ÂàôÊãíÁªùÂà†Èô§
                print("[DELETE] Delete operation abandoned due to existing finalizer.")
                return False

# Ê®°Êãü API Server ÁöÑ DELETE Êìç‰Ωú
def delete_operation(resource_obj):
    # Ê®°ÊãüÂà†Èô§ËØ∑Ê±ÇÂºÄÂßãÔºåÂÖàËé∑ÂèñÂØπË±°Âø´ÁÖß
    snapshot = resource_obj.get_snapshot()
    print(f"[DELETE] Read snapshot: {snapshot}")
    # Ê®°ÊãüÁΩëÁªú/Â§ÑÁêÜÂª∂Ëøü
    time.sleep(0.2)
    # Â∞ùËØïÂà†Èô§Êìç‰ΩúÔºåÊ≤°ÊúâËÆæÁΩÆËµÑÊ∫êÁâàÊú¨È¢ÑÊù°‰ª∂ÔºåÂõ†Ê≠§‰ΩøÁî®‰πãÂâçËØªÂèñÁöÑÁä∂ÊÄÅ
    resource_obj.delete(expected_rv=snapshot["resource_version"])

# Ê®°ÊãüÊéßÂà∂Âô®Ê∑ªÂä† finalizer ÁöÑ PUT Êìç‰Ωú
def put_operation(resource_obj):
    # Ê®°ÊãüÂª∂ËøüÔºåÁ°Æ‰øù DELETE Âú®ËØªÂèñÂêéÔºåPUT Âú® DELETE ÊâßË°åÂâçÂÆåÊàê
    time.sleep(0.15)
    resource_obj.add_finalizer()

# ÂêåÊó∂ÂêØÂä® HTTP ÊúçÂä°Âô®Áî®‰∫éÁ¨¶ÂêàÈ¢òÁõÆË¶ÅÊ±Ç
class SimpleHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        message = "POC HTTP Server running on port 8080."
        self.wfile.write(message.encode('utf-8'))

def start_http_server():
    server_address = ("", 8080)
    httpd = HTTPServer(server_address, SimpleHandler)
    print("Starting HTTP server on port 8080...")
    httpd.serve_forever()

if __name__ == "__main__":
    # ÂêØÂä® HTTP ÊúçÂä°Âô®Á∫øÁ®ã
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # ÂàùÂßãÂåñÂÖ±‰∫´ÁöÑÊ®°ÊãüÂØπË±°
    resource_obj = ResourceObject()

    # ÂêØÂä® DELETE Âíå PUT Êìç‰ΩúÁ∫øÁ®ã‰ª•Âà∂ÈÄ†Á´ûÊÄÅÊù°‰ª∂
    delete_thread = threading.Thread(target=delete_operation, args=(resource_obj,))
    put_thread = threading.Thread(target=put_operation, args=(resource_obj,))

    # ÂêåÊó∂ÂêØÂä®‰∏§‰∏™Á∫øÁ®ã
    delete_thread.start()
    put_thread.start()

    # Á≠âÂæÖÁ∫øÁ®ãÂÆåÊàê
    delete_thread.join()
    put_thread.join()

    # ÊúÄÁªàÊ£ÄÊµãÂØπË±°Áä∂ÊÄÅ
    final_snapshot = resource_obj.get_snapshot()
    print(f"[RESULT] Final object state: {final_snapshot}")

    # ÊèêÁ§∫Â§çÁé∞ÁªìÊùü
    print("Â§çÁé∞ËÑöÊú¨ÊâßË°åÂÆåÊØï„ÄÇ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨‰∏≠ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ ResourceObject Á±ªÔºåÁî®‰∫éÊ®°Êãü‰∏Ä‰∏™Â∏¶Êúâ finalizer„ÄÅËµÑÊ∫êÁâàÊú¨ÂíåÂà†Èô§Áä∂ÊÄÅÁöÑÂØπË±°„ÄÇÂØπË±°ÂÜÖÈÉ®ÈÄöËøáÈîÅÔºàlockÔºâ‰øùËØÅÂØπÁä∂ÊÄÅÁöÑÂéüÂ≠êÊìç‰Ωú„ÄÇ
2. delete_operation ÂáΩÊï∞Ê®°Êãü DELETE ËØ∑Ê±ÇÔºåÈ¶ñÂÖàËØªÂèñÂØπË±°Âø´ÁÖßÔºåÁÑ∂ÂêéÂú®Âª∂Êó∂‰πãÂêéË∞ÉÁî® delete ÊñπÊ≥ï„ÄÇdelete ÊñπÊ≥ï‰∏çÊ£ÄÊü•ËµÑÊ∫êÁâàÊú¨È¢ÑÊù°‰ª∂Ôºå‰ªéËÄåÊ®°Êãü‰∫ÜÁî±‰∫éÁ´ûÊÄÅÊù°‰ª∂ÂØºËá¥ÁöÑ finalizer Ë¢´ÂøΩÁï•ÁöÑÈóÆÈ¢ò„ÄÇ
3. put_operation ÂáΩÊï∞Ê®°ÊãüÊéßÂà∂Âô®Âú® DELETE ËØ∑Ê±ÇËØªÂèñÂêé„ÄÅÂà†Èô§ÂâçÊ∑ªÂä† finalizer ÁöÑ PUT ËØ∑Ê±ÇÔºåÂêåÊó∂‰øÆÊîπÂØπË±°ÁöÑ finalizer Áä∂ÊÄÅÂèäËµÑÊ∫êÁâàÊú¨„ÄÇ
4. ‰∏∫‰∫ÜËØ¥ÊòéËøô‰∏™Á´ûÊÄÅÊù°‰ª∂ÂêåÊó∂Â≠òÂú® HTTP ÊúçÂä°Ë¶ÅÊ±ÇÔºåËÑöÊú¨‰πüÂú®ÂêéÂè∞ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁõëÂê¨ 8080 Á´ØÂè£ÁöÑ HTTP ÊúçÂä°Âô®ÔºåÂìçÂ∫îÁÆÄÂçïÁöÑ GET ËØ∑Ê±Ç„ÄÇ
5. Êï¥‰∏™ËÑöÊú¨ÈÄöËøáÁ∫øÁ®ãÁ´û‰∫âÁöÑÊñπÂºèÂ§çÁé∞‰∫ÜÂú® DELETE ‰∏é PUT ËØ∑Ê±Ç‰πãÈó¥Â≠òÂú®ÁöÑÊó∂Èó¥Á™óÂè£Ôºå‰ΩøÂæó DELETE ËØ∑Ê±ÇÊú™ËÉΩÊÑüÁü•Âà∞ÂêéÁª≠ÁöÑ finalizer Ê∑ªÂä†Ôºå‰ªéËÄåÂØºËá¥ËµÑÊ∫êÂà†Èô§Êìç‰ΩúÁöÑÂºÇÂ∏∏ÊàêÂäüÔºåÊ®°Êãü‰∫Ü API Server ‰∏≠ËµÑÊ∫êÊ≥ÑÈú≤ÁöÑÈóÆÈ¢ò„ÄÇ

ËØ•ËÑöÊú¨‰ªÖ‰Ωú‰∏∫Êú¨Âú∞Á†îÁ©∂ÂíåÊµãËØï‰πãÁî®ÔºåËØ∑ÂãøÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠‰ΩøÁî®„ÄÇ

---


## Issue #129463 kubelet could reject pods with NodeAffinity error due to incomplete informer cache on the node object

- Issue ÈìæÊé•Ôºö[#129463](https://github.com/kubernetes/kubernetes/issues/129463)

### Issue ÂÜÖÂÆπ

#### What happened?

In our end (AKS), we observed that there is a case that kubelet could reject pods with NodeAffinty status if the node watch call got closed due to unknown reason during kubelet start up.

An illustrated procedure is (using `Tx` to indicate time):

- T1: kubelet starts, spawns the processes for registering node (create node) and node informer for syncing the node object in parallel
- T2: the node informer successfully listed empty nodes list from kube-apiserver , proceeding to watch call
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/reflector.go#L401-L409
- T3: the watch call stopped due to unknown issue, for instance network closed. In this case, the watch call would exit with error like: 
  > very short watch: k8s.io/client-go/informers/factory.go:160: Unexpected watch close - watch lasted less than a second and no items received

  In this case, the node informer will back-off and retry with new list-watch steps:
  https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/reflector.go#L336-L340
- T4: kubelet successfully created the node object in kube-apiserver side. Kube-scheduler kicks in and assigns pods to the node
- T5: because in T3 the list call was succeeded with empty results, the node informer had been marked as synced:
  
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/pkg/kubelet/kubelet.go#L442-L444
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/delta_fifo.go#L308-L316
  In this case, kubelet starts syncing pods and attempts to run. If these pods are set with affinity settings, the kubelet node admission check will fail the pods with "NodeAffinity" status because the node informer cache is empty. Kubelet uses a minimal node object without expected node labels for performing admission check:
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/pkg/kubelet/kubelet_getters.go#L291-L298
- T6: kubelet node informer retry kicks in, this time it successfully synced the node object from remote. Now retried pods scheduling on this node would succeed as the node labels are expected

In our reproduction, we captured the following logs:

```
T1: kubelet[3538]: I1225 01:02:59.216231    3538 kubelet.go:2346] "Starting kubelet main sync loop"
T2: kubelet[3538]: I1225 01:04:04.030571    3538 reflector.go:359] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
T3: kubelet[3538]: W1225 01:04:04.170608    3538 reflector.go:470] k8s.io/client-go/informers/factory.go:160: watch of *v1.Node ended with: very short watch: k8s.io/client-go/informers/factory.go:160: Unexpected watch close - watch lasted less than a second and no items received
T4: kubelet[3538]: I1225 01:04:08.309280    3538 kubelet_node_status.go:76] "Successfully registered node" node="<node-name>"
T5 (1): kubelet[3538]: I1225 01:04:29.348027    3538 predicate.go:164] "Predicate failed on Pod" pod="<namespace>/<pod>" err="Predicate NodeAffinity failed"
T5 (2): kubelet[3538]: E1225 01:04:29.351153    3538 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"<node-name>\" not found"
T6: kubelet[3538]: I1225 01:04:56.591752    3538 reflector.go:359] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
```

In the normal case, the retry back off (between T6 and T5) should be short and kubelet should not leave many pods failed. However, in rare scenarios that the node lister might be failing for some other reasons before the first successful call (waiting for TLS bootstrapping for example), and increased the back-off (max out to 30s), then in theory, the gap of `T6-T5` could be as long as 30s + jitter, which aligns with the logs timestamps we observed in above.

#### What did you expect to happen?

Kubelet should back off the node list-watch calls until the node object has been populated in the node informer cache. Or, we should invalidate the node informer cache after successfully registering the node from kubelet to maintain the correct version of node object in memory.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate the watch closed call like mentioned above.

#### Anything else we need to know?

Related issues/PRs:

- https://github.com/kubernetes/kubernetes/pull/99336
- https://github.com/kubernetes/kubernetes/pull/94087

#### Kubernetes version

<details>

1.30 - but we believe this behavior is reproducible in all verisons.

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰ΩéÈ£éÈô©  
ÔºàÊ†πÊçÆ CVSS 3.1 ÁöÑËØÑ‰º∞ÔºåËØ•ÈóÆÈ¢òËôΩÁÑ∂ÂèØËÉΩÂºïÂèëÊãíÁªùÊúçÂä°ÊîªÂáªÔºå‰ΩÜÂà©Áî®ËØ•ÈóÆÈ¢òÈÄöÂ∏∏ÈúÄË¶Å‰∏ÄÂÆöÊùÉÈôêÊàñÁâπÊÆäÁéØÂ¢ÉÊù°‰ª∂ÔºåÂÖ∂ CVSS ÂæóÂàÜÊú™ËææÂà∞È´òÈ£éÈô©Á∫ßÂà´„ÄÇÔºâ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞‰∫ÜÂú® kubelet ÂêØÂä®Êó∂Áî±‰∫éËäÇÁÇπ informer ÁöÑ watch Ë∞ÉÁî®ÊÑèÂ§ñÂÖ≥Èó≠Ôºà‰æãÂ¶ÇÁî±‰∫éÁΩëÁªúÈóÆÈ¢òÔºâËÄåÂØºËá¥ËäÇÁÇπÂØπË±°Êú™ËÉΩÂèäÊó∂ÂêåÊ≠•ÔºåËøõËÄå‰ΩøÂæóÂ∏¶Êúâ NodeAffinity Ë¶ÅÊ±ÇÁöÑ pod Ë¢´ kubelet ÊãíÁªùË∞ÉÂ∫¶ÁöÑÈóÆÈ¢ò„ÄÇÈîôËØØÂπ∂ÈùûÁî±‰∫éÊîªÂáªËÄÖÊ≥®ÂÖ•‰∫ÜÊÅ∂ÊÑèÊï∞ÊçÆÊàñÁõ¥Êé•ËøõË°åÊèêÊùÉ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÊìç‰ΩúÔºåËÄåÊòØÁî±‰∫é kubelet Âú®Âπ∂Ë°åÊ≥®ÂÜåËäÇÁÇπÂíåÂêåÊ≠•ËäÇÁÇπ‰ø°ÊÅØÊó∂Âá∫Áé∞‰∫ÜÁ´ûÊÄÅÊù°‰ª∂Ôºå‰ªéËÄåÈÄ†ÊàêÁºìÂ≠ò‰∏≠ËäÇÁÇπ‰ø°ÊÅØ‰∏çÂÆåÊï¥ÁöÑÈóÆÈ¢ò„ÄÇËôΩÁÑ∂ËØ•ÈóÆÈ¢òÂú®ÈÉ®ÂàÜÂú∫ÊôØ‰∏ãÂèØËÉΩÂØºËá¥ pod Ë∞ÉÂ∫¶Â§±Ë¥•Ôºå‰ªéËÄåÂºïÂèë‰∏¥Êó∂ÁöÑÊúçÂä°ÊãíÁªùÔºàDoSÔºâÔºå‰ΩÜÊîªÂáªËÄÖËã•ÊÉ≥Âà©Áî®ËØ•ÈóÆÈ¢òÂà∂ÈÄ† DoSÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂÖ∑Â§áÂØπ kubelet ÊàñËäÇÁÇπÁΩëÁªúÁéØÂ¢ÉÁöÑÊéßÂà∂ÊùÉÈôêÔºà‰æãÂ¶Ç‰∫∫‰∏∫Âà∂ÈÄ†ÁΩëÁªú‰∏≠Êñ≠ÔºâÔºåÂõ†Ê≠§‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåÂÖ∂È£éÈô©ÂΩ±Âìç‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂèØÁî®ÊÄßÊñπÈù¢Ôºå‰∏îÂà©Áî®Èó®ÊßõËæÉÈ´ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import json
from http.server import BaseHTTPRequestHandler, HTTPServer
import time
import threading

# ÂÖ®Â±ÄÂèòÈáèÔºåÁî®‰∫éËÆ∞ÂΩïÊúçÂä°Âô®ÂêØÂä®Êó∂Èó¥
start_time = time.time()

class KubeAPIServerSimulator(BaseHTTPRequestHandler):
    def _set_headers(self, code=200):
        self.send_response(code)
        self.send_header("Content-Type", "application/json")
        self.end_headers()

    def do_GET(self):
        global start_time
        # Ê®°Êãü kube-apiserver ÁöÑËäÇÁÇπÂàóË°®Êé•Âè£
        if self.path.startswith("/api/v1/nodes"):
            self._set_headers()
            current_time = time.time() - start_time
            # T2Èò∂ÊÆµÔºöÂú®Ââç20ÁßíÂÜÖËøîÂõûÁ©∫ËäÇÁÇπÂàóË°®ÔºàÊ®°Êãü informer ÂàóË°®Ë∞ÉÁî®ËøîÂõûÁ©∫Êï∞ÊçÆÔºâ
            if current_time < 20:
                response = {"items": []}
            else:
                # Ê®°Êãü T6 Èò∂ÊÆµÔºöËøîÂõûÂåÖÂê´ÂÆåÊï¥ËäÇÁÇπ‰ø°ÊÅØÁöÑÂàóË°®
                node = {
                    "metadata": {"name": "node-01"},
                    "spec": {"providerID": "simulated://node-01"},
                    "status": {"conditions": [{"type": "Ready", "status": "True"}]},
                    "labels": {"role": "worker", "zone": "us-central1-a"}
                }
                response = {"items": [node]}
            self.wfile.write(json.dumps(response).encode("utf-8"))
        # Ê®°Êãü kube-apiserver ÁöÑ watch Êé•Âè£ÔºàÈíàÂØπËäÇÁÇπÔºâ
        elif self.path.startswith("/api/v1/watch/nodes"):
            current_time = time.time() - start_time
            # T3Èò∂ÊÆµÔºöÂú®Ââç20ÁßíÂÜÖÊ®°Êãü watch Ë∞ÉÁî®ÂæàÂø´ÂÖ≥Èó≠Âπ∂ËøîÂõûÈîôËØØ
            if current_time < 20:
                self.send_error(500, "Unexpected watch close - watch lasted less than a second and no items received")
            else:
                self._set_headers()
                # T6Èò∂ÊÆµÔºöËøîÂõû‰∏Ä‰∏™Ê®°ÊãüÁöÑ watch ‰∫ã‰ª∂
                event = {
                    "type": "ADDED",
                    "object": {
                        "metadata": {"name": "node-01"},
                        "spec": {"providerID": "simulated://node-01"},
                        "status": {"conditions": [{"type": "Ready", "status": "True"}]},
                        "labels": {"role": "worker", "zone": "us-central1-a"}
                    }
                }
                self.wfile.write(json.dumps(event).encode("utf-8"))
        else:
            self.send_error(404, "Not Found")

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, KubeAPIServerSimulator)
    print("HTTP Server running on port 8080...")
    httpd.serve_forever()

# Ê®°Êãü kubelet ÂÆ¢Êà∑Á´ØË°å‰∏∫ÔºåÂÆöÊó∂ËØ∑Ê±ÇËäÇÁÇπÂàóË°®Âíå watch Êé•Âè£Ôºå‰ª•‰æøËßÇÂØüÁºìÂ≠ò‰∏çÂêåÊ≠•ÂíåÂêåÊ≠•ÁöÑÂèòÂåñËøáÁ®ã
def simulate_kubelet_behavior():
    import requests
    time.sleep(5)  # Âª∂ËøüÁ≠âÂæÖ HTTP ÊúçÂä°Âô®ÂêØÂä®
    while True:
        try:
            # ËØ∑Ê±ÇËäÇÁÇπÂàóË°®Êé•Âè£
            list_resp = requests.get("http://localhost:8080/api/v1/nodes", timeout=5)
            nodes = list_resp.json().get("items", [])
            if not nodes:
                print(f"[{time.strftime('%X')}] ËäÇÁÇπÂàóË°®‰∏∫Á©∫ÔºåÊ®°Êãü pod Ë∞ÉÂ∫¶Êó∂Âõ†ËäÇÁÇπ‰ø°ÊÅØ‰∏çÂÆåÊï¥ËÄåË¢´ÊãíÁªù„ÄÇ")
            else:
                print(f"[{time.strftime('%X')}] ËäÇÁÇπÂàóË°®Â∑≤ÂêåÊ≠•Ôºö{nodes}")
            # ËØ∑Ê±Ç watch Êé•Âè£
            try:
                watch_resp = requests.get("http://localhost:8080/api/v1/watch/nodes", timeout=5)
                if watch_resp.status_code == 200:
                    print(f"[{time.strftime('%X')}] Watch ËøîÂõû‰∫ã‰ª∂Ôºö{watch_resp.json()}")
                else:
                    print(f"[{time.strftime('%X')}] Watch ËøîÂõûÂºÇÂ∏∏Áä∂ÊÄÅÁ†ÅÔºö{watch_resp.status_code}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] Watch ËØ∑Ê±ÇÂºÇÂ∏∏Ôºö{e}")
        except Exception as e:
            print(f"[{time.strftime('%X')}] ËäÇÁÇπÂàóË°®ËØ∑Ê±ÇÂºÇÂ∏∏Ôºö{e}")
        # ÊØè10ÁßíÈíüÊü•ËØ¢‰∏ÄÊ¨°ÔºåËßÇÂØü‰ªéÁ©∫Âà∞ÂêåÊ≠•ÁöÑËøáÁ®ã
        time.sleep(10)

if __name__ == "__main__":
    # ÂàÜÁ¶ªÁ∫øÁ®ãÂêØÂä® HTTP Ê®°ÊãüÊúçÂä°Âô®
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    # Âú®‰∏ªÁ∫øÁ®ã‰∏≠Ê®°Êãü kubelet Êü•ËØ¢Ë°å‰∏∫
    simulate_kubelet_behavior()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËØ•Â§çÁé∞ËÑöÊú¨‰ΩøÁî® Python ÁöÑ http.server Ê®°ÂùóÂú®Êú¨Âú∞ÔºàÁõëÂê¨ 8080 Á´ØÂè£ÔºâÊ®°Êãü‰∫ÜÈÉ®ÂàÜ kube-apiserver Ë°å‰∏∫„ÄÇÂÆÉÂÆûÁé∞‰∫Ü‰∏§‰∏™ REST Êé•Âè£Ôºö
   - /api/v1/nodesÔºöÁî®‰∫éËøîÂõûËäÇÁÇπÂàóË°®„ÄÇÂú®ÊúçÂä°Âô®ÂêØÂä®ÁöÑÂâç 20 ÁßíÂÜÖËøîÂõûÁ©∫ÂàóË°®ÔºàÊ®°Êãü T2 Èò∂ÊÆµÁöÑÊÉÖÂÜµÔºâÔºå‰πãÂêéËøîÂõû‰∏Ä‰∏™ÂåÖÂê´ÂÆåÊï¥ËäÇÁÇπ‰ø°ÊÅØÁöÑÂàóË°®ÔºàÊ®°Êãü T6 Èò∂ÊÆµÔºâ„ÄÇ
   - /api/v1/watch/nodesÔºöÁî®‰∫éÊ®°Êãü watch Êé•Âè£„ÄÇÂú®Ââç 20 ÁßíÂÜÖÁõ¥Êé•ËøîÂõû 500 ÈîôËØØÔºåÊ®°Êãü watch Ë∞ÉÁî®ÊÑèÂ§ñÂÖ≥Èó≠ÁöÑÊÉÖÂÜµÔºõÂú®‰πãÂêéËøîÂõû‰∏ÄÊ¨°Ê®°ÊãüÁöÑ watch ‰∫ã‰ª∂ÔºàÊ®°Êãü T6 Èò∂ÊÆµÔºâ„ÄÇ
2. ÂêåÊó∂ÔºåËÑöÊú¨‰∏≠ËøòÊ®°Êãü‰∫Ü kubelet ÂÆ¢Êà∑Á´ØÔºåÂÆöÊó∂ÔºàÊØè 10 ÁßíÔºâÂêëÊ®°Êãü HTTP ÊúçÂä°Âô®ÂèëËµ∑ËØ∑Ê±ÇÔºåÂπ∂ÊâìÂç∞Âá∫ÂΩìÂâçËäÇÁÇπÂàóË°®Áä∂ÊÄÅ‰ª•Âèä watch Êé•Âè£ÁöÑËøîÂõûÊÉÖÂÜµ„ÄÇËøôÊ†∑ÂèØ‰ª•ËßÇÂØüÂà∞Âú® informer ÁöÑÁºìÂ≠òÁä∂ÊÄÅÁî±Á©∫Âèò‰∏∫Ê≠£Â∏∏ÁöÑËøáÁ®ãÔºå‰ªéËÄåÂ§çÁé∞ issue ‰∏≠ÊèèËø∞ÁöÑÈóÆÈ¢òÂú∫ÊôØ„ÄÇ
3. Â§çÁé∞ËÑöÊú¨‰∏ªË¶ÅÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºåÊ®°Êãü‰∫ÜÁî±‰∫éÁΩëÁªúÂºÇÂ∏∏ÂØºËá¥ informer ÁºìÂ≠ò‰∏ç‰∏ÄËá¥ÁöÑÁé∞Ë±°Ôºå‰ΩÜÂπ∂‰∏çÁõ¥Êé•ÂØπÈõÜÁæ§ÁéØÂ¢ÉÈÄ†ÊàêÂΩ±ÂìçÔºå‰πüÂπ∂ÈùûÈíàÂØπÂ§ñÈÉ®ÊîªÂáªËÄÖËÆæËÆ°ÁöÑÊºèÊ¥ûÂà©Áî®Ê†∑‰æã„ÄÇ

---


# ‚ö†Ô∏è Â≠òÂú®‰ΩéÈ£éÈô©ÁöÑ Issues (5 ‰∏™)

## Issue #129883 ValidatingAdmissionPolicy causes Namespaces deletion to hang due to race condition

- Issue ÈìæÊé•Ôºö[#129883](https://github.com/kubernetes/kubernetes/issues/129883)

### Issue ÂÜÖÂÆπ

#### What happened?

I'm implementing policies using ValidatingAdmissionPolicy and ValidatingAdmissionPolicyBindings using a custom CR as paramKind.

The binding is configured with `.spec.paramRef.parameterNotFoundAction: Deny`

Some of those policies are associated to external resources that may take a bit longer to terminate.

When a namespace is deleted, the custom CR resource is immediately removed, and the policy's target resource is left hanging.

Attempting to remove the resource's finalizer returns a ValidatingAdmissionPolicy error:
```
The persistentvolumeclaims "xxx-pvc" is invalid: : ValidatingAdmissionPolicy 'xxx-vap' with binding 'xxx-vapbind' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

#### What did you expect to happen?

The resources in the namespace to be deleted along with the namespace

Perhaps the policy should stop acting on the namespace if it is in Terminating state.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Kind cluster with extraMount
```
apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
  - role: control-plane
    extraMounts:
      - hostPath: <some-path>/foo
        containerPath: /foo
```

Create test namespace
```
> kubectl create ns test
```

Deploy ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding and ConfigMap parameter reference
```
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: policy
spec:
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
      - apiVersions:
          - v1
        apiGroups:
          - ""
        resources:
          - persistentvolumeclaims
        operations:
          - CREATE
          - UPDATE
  validations:
    - expression: params != null && has(params.data) && has(params.data.storageClasses) && variables.storageClassName == params.data.storageClasses
      messageExpression: '"Namespace " + string(namespaceObject.metadata.name) + " does not have permissions to storageClassName " + variables.storageClassName'
  failurePolicy: Fail
  variables:
    - name: storageClassName
      expression: '"storageClassName" in object.spec ? object.spec.storageClassName : "empty"'
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: binding
spec:
  policyName: policy
  paramRef:
    name: ref
    parameterNotFoundAction: Deny
  validationActions:
    - Deny
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ref
data:
  storageClasses: "standard"
```

Create the PVC
```
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-www
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  hostPath:
    path: /foo/
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-www
spec:
  volumeName: pv-www
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      volumes:
      - name: www
        persistentVolumeClaim:
          claimName: pvc-www
      containers:
      - name: nginx
        image: nginx:1.14.2
        volumeMounts:
        - name: www
          mountPath: /var/www
```

Delete the namespace:
```
> kubectl delete ns test
```

The namespace will hang due to the PVC, attempting to remove the PVC finalizer will reveal the ValidatingAdmissionPolicy error:
```
> kubectl patch pvc pvc-www -p '{"metadata":{"finalizers":[]}}' --type=merge
The persistentvolumeclaims "pvc-www" is invalid: : ValidatingAdmissionPolicy 'policy' with binding 'binding' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

The PVC is removed (or finalizer can be removed) after configuring the binding with `.spec.paramRef.parameterNotFoundAction: Allow` or removing the ValidatingAdmissionPolicy.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰ΩéÈ£éÈô©  
ÔºàÊ†πÊçÆ CVSS 3.1 ÊâìÂàÜÔºåËØ•ÊºèÊ¥û‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥Êú™ÁªèÊéàÊùÉÁöÑËÆøÈóÆ„ÄÅ‰ª£Á†ÅÊâßË°åÊàñÊèêÊùÉÔºåËôΩÁÑ∂ÂèØËÉΩÂØºËá¥ÊãíÁªùÊúçÂä°Ôºå‰ΩÜÂÆûÈôÖÂà©Áî®ÈúÄË¶ÅËæÉÈ´òÊùÉÈôêÔºåÁªºÂêàËØÑ‰º∞È£éÈô©‰∏∫‰ΩéÈ£éÈô©Ôºâ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞‰∫ÜÂú®‰ΩøÁî® ValidatingAdmissionPolicy Âíå ValidatingAdmissionPolicyBinding Êó∂Â≠òÂú®ÁöÑÁ´ûÊÄÅÊù°‰ª∂ÈóÆÈ¢òÔºåÂØºËá¥Âú®ÂëΩÂêçÁ©∫Èó¥Âà†Èô§ËøáÁ®ã‰∏≠ÔºåÁî±‰∫éËá™ÂÆö‰πâÂèÇÊï∞ËµÑÊ∫êÔºàConfigMapÔºâË¢´ÊèêÂâçÊ∏ÖÁêÜÔºåËøõËÄå‰ΩøÂæó PVC ÁöÑÂà†Èô§Êìç‰ΩúÔºàÊàñÂà†Èô§ÂÖ∂ finalizerÔºâË¢´ÊãíÁªùÔºå‰ªéËÄåÂØºËá¥ÂëΩÂêçÁ©∫Èó¥Âà†Èô§ËøáÁ®ãÊåÇËµ∑„ÄÇËôΩÁÑ∂Ëøô‰∏ÄÈóÆÈ¢òÂèØËÉΩ‰ºöÂØºËá¥ÈõÜÁæ§ÂÜÖÈÉ®ÂàÜËµÑÊ∫êÔºàÂ¶ÇÂëΩÂêçÁ©∫Èó¥„ÄÅPVC Á≠âÔºâÊó†Ê≥ïÂèäÊó∂Ê≠£Â∏∏Âà†Èô§Ôºå‰∫ßÁîüÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁöÑÊïàÊûúÔºå‰ΩÜÈóÆÈ¢òÊú¨Ë∫´‰ªÖÂΩ±ÂìçËµÑÊ∫êÂà†Èô§ÊµÅÁ®ãÔºåÂπ∂ÈùûÁõ¥Êé•Êö¥Èú≤ÊïèÊÑü‰ø°ÊÅØÊàñÂÖÅËÆ∏ËøúÁ®ãÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÈ´òÂÆâÂÖ®È£éÈô©ÊºèÊ¥û„ÄÇÂπ∂‰∏îÔºåÊ≠£Â∏∏ÊÉÖÂÜµ‰∏ãÂè™ÊúâÂÖ∑Â§áËæÉÈ´òÊùÉÈôêÁöÑÁî®Êà∑ÊâçËÉΩÈÖçÁΩÆ„ÄÅÂà†Èô§Ëøô‰∫õËµÑÊ∫êÔºåÂõ†Ê≠§ÊîªÂáªËÄÖÈúÄË¶Å‰∏ÄÂÆöÁöÑÊùÉÈôêÊâçËÉΩËß¶ÂèëËøô‰∏ÄÈóÆÈ¢ò„ÄÇÁªºÂêàÊù•ÁúãÔºåËØ•ÈóÆÈ¢òÊõ¥ÂÄæÂêë‰∫éÂõ†ÈÖçÁΩÆÊàñÂÆûÁé∞‰∏äÁöÑÁ´ûÊÄÅÊù°‰ª∂ËÄåÂØºËá¥ DoS ÁöÑÈóÆÈ¢òÔºåÂÖ∂ÂΩ±ÂìçËåÉÂõ¥ÂíåÊîªÂáªÂ§çÊùÇÂ∫¶ÂèóÈôêÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ßËæÉ‰Ωé„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import os
import sys
import time
import threading
import http.server
from kubernetes import client, config, utils
from kubernetes.client.rest import ApiException
import yaml

# ÂêØÂä®‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁî®‰∫éÊª°Ë∂≥Ë¶ÅÊ±ÇÔºàÁõëÂê¨Âú®8080Á´ØÂè£Ôºâ
def start_http_server():
    server_address = ('', 8080)
    httpd = http.server.HTTPServer(server_address, http.server.SimpleHTTPRequestHandler)
    print("[HTTP] Server listening on port 8080")
    httpd.serve_forever()

def create_resources_from_yaml(api_client, yaml_text, namespace=None):
    """‰ªéYAMLÊñáÊú¨‰∏≠ÂàõÂª∫ËµÑÊ∫ê"""
    # Â∞ÜYAMLÊñáÊú¨ÊãÜÂàÜÔºåÂ¶ÇÊûúÊúâÂ§ö‰∏™ÊñáÊ°£ÂàôÂàÜÂà´Âä†ËΩΩ
    for data in yaml.safe_load_all(yaml_text):
        try:
            utils.create_from_dict(api_client, data, namespace=namespace)
            kind = data.get("kind")
            name = data.get("metadata", {}).get("name")
            print(f"[INFO] ÊàêÂäüÂàõÂª∫ {kind}/{name}")
        except ApiException as e:
            print(f"[ERROR] ÂàõÂª∫ {data.get('kind')}/{data.get('metadata', {}).get('name')} Â§±Ë¥•: {e}")

def main():
    # Âä†ËΩΩkubeconfigÔºàÈªòËÆ§Ë∑ØÂæÑÔºâ
    try:
        config.load_kube_config()
    except Exception as e:
        print(f"[ERROR] Âä†ËΩΩkubeconfigÂ§±Ë¥•: {e}")
        sys.exit(1)

    api_client = client.ApiClient()
    core_v1 = client.CoreV1Api(api_client)
    apps_v1 = client.AppsV1Api(api_client)

    # ÂêØÂä®‰∏Ä‰∏™ÁÆÄÂçïÁöÑHTTPÊúçÂä°Âô®ÔºàÁõëÂê¨8080Á´ØÂè£Ôºâ
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # 1. ÂàõÂª∫ÊµãËØïÂëΩÂêçÁ©∫Èó¥
    namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name="test"))
    try:
        core_v1.create_namespace(namespace_body)
        print("[INFO] ÂàõÂª∫ÂëΩÂêçÁ©∫Èó¥ test ÊàêÂäü")
    except ApiException as e:
        if e.status != 409:
            print(f"[ERROR] ÂàõÂª∫ÂëΩÂêçÁ©∫Èó¥ test Â§±Ë¥•: {e}")
            sys.exit(1)
        else:
            print("[INFO] ÂëΩÂêçÁ©∫Èó¥ test Â∑≤Â≠òÂú®")

    # 2. ÈÉ®ÁΩ≤ ValidatingAdmissionPolicy„ÄÅValidatingAdmissionPolicyBinding Âíå ConfigMap ÂèÇÊï∞ÂºïÁî®
    admission_yaml = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: policy
spec:
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
      - apiVersions:
          - v1
        apiGroups:
          - ""
        resources:
          - persistentvolumeclaims
        operations:
          - CREATE
          - UPDATE
  validations:
    - expression: params != null && has(params.data) && has(params.data.storageClasses) && variables.storageClassName == params.data.storageClasses
      messageExpression: '"Namespace " + string(namespaceObject.metadata.name) + " does not have permissions to storageClassName " + variables.storageClassName'
  failurePolicy: Fail
  variables:
    - name: storageClassName
      expression: '"storageClassName" in object.spec ? object.spec.storageClassName : "empty"'
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: binding
spec:
  policyName: policy
  paramRef:
    name: ref
    parameterNotFoundAction: Deny
  validationActions:
    - Deny
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ref
  namespace: test
data:
  storageClasses: "standard"
"""
    create_resources_from_yaml(api_client, admission_yaml, namespace="test")
    time.sleep(3)  # Á≠âÂæÖËµÑÊ∫êÂàõÂª∫

    # 3. ÂàõÂª∫ PersistentVolume, PersistentVolumeClaim Âíå Deployment
    resources_yaml = """
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-www
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  hostPath:
    path: /foo/
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-www
  namespace: test
spec:
  volumeName: pv-www
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: test
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      volumes:
      - name: www
        persistentVolumeClaim:
          claimName: pvc-www
      containers:
      - name: nginx
        image: nginx:1.14.2
        volumeMounts:
        - name: www
          mountPath: /var/www
"""
    create_resources_from_yaml(api_client, resources_yaml)
    time.sleep(5)  # Á≠âÂæÖPVCÂíåDeploymentÂêØÂä®

    # 4. Âà†Èô§ÂëΩÂêçÁ©∫Èó¥ÔºåËß¶ÂèëËØ•Á´ûÊÄÅÈóÆÈ¢ò
    try:
        print("[INFO] Ê≠£Âú®Âà†Èô§ÂëΩÂêçÁ©∫Èó¥ test ÔºàÊ≠§Êìç‰ΩúÂèØËÉΩ‰ºöÊåÇËµ∑Ôºâ...")
        core_v1.delete_namespace("test")
    except ApiException as e:
        print(f"[ERROR] Âà†Èô§ÂëΩÂêçÁ©∫Èó¥ test Â§±Ë¥•: {e}")

    # Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥‰ª•‰æøËßÇÂØüÂëΩÂêçÁ©∫Èó¥Âà†Èô§Áä∂ÊÄÅ
    time.sleep(10)

    # 5. Â∞ùËØïÂà†Èô§ PVC ÁöÑ finalizer ‰ª•ËßÇÂØü ValidatingAdmissionPolicy ÈîôËØØ
    patch_body = {"metadata": {"finalizers": []}}
    try:
        print("[INFO] Â∞ùËØïpatch PVC pvc-www ÂéªÈô§ finalizers")
        core_v1.patch_namespaced_persistent_volume_claim(
            name="pvc-www",
            namespace="test",
            body=patch_body)
        print("[INFO] PVC pvc-www patch ÊàêÂäü")
    except ApiException as e:
        print(f"[INFO] È¢ÑÊúü‰∏≠ÊçïËé∑Âà∞ ValidatingAdmissionPolicy ÈîôËØØ: {e}")

if __name__ == "__main__":
    main()
```


---


## Issue #129610 `EncryptionConfiguration` mismatch between documentation and validation of provider `aescbc`

- Issue ÈìæÊé•Ôºö[#129610](https://github.com/kubernetes/kubernetes/issues/129610)

### Issue ÂÜÖÂÆπ

#### What happened?

AES-CBC is documented to require 32-byte key in the `EncryptionConfiguration` https://github.com/kubernetes/kubernetes/blob/e38489303019d442b87611182eb63c94d6e54f03/staging/src/k8s.io/apiserver/pkg/apis/apiserver/types_encryption.go#L106 and also on the website https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers, however the validation checks for one of `16`, `24`, `32` sizes
https://github.com/kubernetes/kubernetes/blob/e38489303019d442b87611182eb63c94d6e54f03/staging/src/k8s.io/apiserver/pkg/apis/apiserver/validation/validation_encryption.go#L106-L107

#### What did you expect to happen?

I expect that the validation fails when one provides a key of size 16 or 24 bytes or that the documentation gets adapted to the current behaviour.

#### How can we reproduce it (as minimally and precisely as possible)?

Pass a 24-byte sized key for `aescbc` provider in an `EncryptionConfiguration` to the `kube-apiserver`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

This is valid for the current `v1.32.0` version.

#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰ΩéÈ£éÈô©

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ‰∏≠ÈíàÂØπ aescbc Âä†ÂØÜÊèê‰æõËÄÖÔºåÂú® EncryptionConfiguration ÁöÑÊñáÊ°£‰∏éÂÆûÈôÖÊ†°È™åÈÄªËæë‰πãÈó¥Â≠òÂú®‰∏ç‰∏ÄËá¥ÔºöÊñáÊ°£‰∏≠Ë¶ÅÊ±ÇÂøÖÈ°ª‰ΩøÁî® 32 Â≠óËäÇÂØÜÈí•ÔºåËÄåÂÆûÈôÖÊ†°È™åÂÖÅËÆ∏ 16„ÄÅ24„ÄÅ32 Â≠óËäÇÁöÑÂØÜÈí•„ÄÇ  
‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËøôÈáåÂÖÅËÆ∏‰ΩøÁî® 16 Â≠óËäÇÔºàAES-128ÔºâÊàñ 24 Â≠óËäÇÔºàAES-192ÔºâÁöÑÂØÜÈí•ÔºåÂπ∂‰∏çÊÑèÂë≥ÁùÄÂ≠òÂú®Â∑≤Áü•ÁöÑÂä†ÂØÜÂº±ÁÇπÔºåÂõ†‰∏∫ AES-128 Âíå AES-192 Âùá‰∏∫Ê†áÂáÜÁöÑÂä†ÂØÜÁÆóÊ≥ïÔºå‰∏é AES-256Ôºà32 Â≠óËäÇÔºâÁõ∏ÊØîÔºåÂú®ÁõÆÂâçÁöÑÂÆûÈôÖÊîªÂáªÊâãÊÆµ‰∏ãÂπ∂Ê≤°ÊúâË¢´ËÆ§‰∏∫ÊòØ‰∏çÂÆâÂÖ®ÁöÑ„ÄÇ  
Âõ†Ê≠§ÔºåÊ≠§ÈóÆÈ¢ò‰∏ªË¶ÅË°®Áé∞‰∏∫ÊñáÊ°£‰∏éÂÆûÈôÖË°å‰∏∫‰∏ç‰∏ÄËá¥ÔºåÂèØËÉΩÂØºËá¥Áî®Êà∑ËÆ§‰∏∫Âè™Êúâ 32 Â≠óËäÇÂØÜÈí•ÊòØÂÆâÂÖ®Ë¶ÅÊ±ÇÔºåËÄåÂÆûÈôÖ‰∏äËæÉÁü≠ÁöÑÂØÜÈí•‰πüÊòØË¢´Êé•ÂèóÁöÑ„ÄÇËØ•ÈóÆÈ¢òËôΩÁÑ∂ÂèØËÉΩÂºïËµ∑ÈÖçÁΩÆ‰∏äÁöÑÊ∑∑Ê∑ÜÊàñÂêàËßÑÊÄßÁñëËôëÔºå‰ΩÜÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥ÊîªÂáªËÄÖÂà©Áî®Ôºå‰ªéËÄåÈÄ†Êàê‰∏•ÈáçÁöÑÂÆâÂÖ®Á†¥Âùè„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≠§ËÑöÊú¨Áî®‰∫éÊ®°Êãü Kubernetes ‰∏≠ aescbc Âä†ÂØÜÈÖçÁΩÆÁöÑÊ†°È™åÈÄªËæëÔºå
È™åËØÅ‰º†ÂÖ•ÁöÑÂØÜÈí•Ôºà‰ª• Base64 ÁºñÁ†ÅÔºâÊòØÂê¶Á¨¶Âêà 16„ÄÅ24„ÄÅ32 Â≠óËäÇ‰πã‰∏Ä„ÄÇ

Ê≥®ÊÑèÔºö
- ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂ÔºåÊ®°ÊãüÊúçÂä°ÁõëÂê¨Âú®8080Á´ØÂè£„ÄÇ
- Âú®ÂÆûÈôÖ kube-apiserver ‰∏≠ÔºåÂØÜÈí•Ë¢´Áî®‰ΩúÂä†ÂØÜÊï∞ÊçÆËÄåÈùû‰ªÖ‰ΩúÁÆÄÂçïÊ†°È™å„ÄÇ
"""

import base64
import json
from http.server import BaseHTTPRequestHandler, HTTPServer

class EncryptionConfigHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        # ‰ªÖÂ§ÑÁêÜ /encryption Ë∑ØÂæÑÁöÑËØ∑Ê±Ç
        if self.path != '/encryption':
            self.send_error(404, "Not Found")
            return

        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        try:
            data = json.loads(post_data.decode('utf-8'))
        except Exception as e:
            self.send_error(400, "Invalid JSON")
            return

        provider = data.get("provider")
        # ÁõÆÂâçÂè™ÊîØÊåÅ aescbc
        if provider != "aescbc":
            self.send_error(400, "Âè™ÊîØÊåÅ provider 'aescbc'")
            return

        key_b64 = data.get("key")
        if not key_b64:
            self.send_error(400, "Áº∫Â∞ë key ÂèÇÊï∞")
            return

        try:
            key = base64.b64decode(key_b64)
        except Exception as e:
            self.send_error(400, "key ÂèÇÊï∞ Base64 Ëß£Á†ÅÂ§±Ë¥•")
            return

        # Ê†°È™åÂØÜÈí•ÈïøÂ∫¶ÔºåÂÆûÈôÖÊ†°È™åÈÄªËæëÊîØÊåÅ 16„ÄÅ24„ÄÅ32 Â≠óËäÇ
        if len(key) in (16, 24, 32):
            response = {
                "status": "accepted",
                "key_length": len(key),
                "message": "ÂØÜÈí•ÈïøÂ∫¶Á¨¶ÂêàË¶ÅÊ±Ç„ÄÇ"
            }
        else:
            response = {
                "status": "rejected",
                "reason": "ÂØÜÈí•ÈïøÂ∫¶Êó†ÊïàÔºåÂøÖÈ°ª‰∏∫ 16, 24 Êàñ 32 Â≠óËäÇ„ÄÇ"
            }

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(response).encode('utf-8'))

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, EncryptionConfigHandler)
    print("HTTP Server running on port 8080...")
    httpd.serve_forever()

if __name__ == '__main__':
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËØ•ËÑöÊú¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®Êú¨Âú∞ 8080 Á´ØÂè£ÔºåÂπ∂ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ POST Êé•Âè£Ôºà/encryptionÔºâ„ÄÇ  
2. Êé•Âè£Êé•Êî∂‰∏Ä‰∏™ JSON Ê†ºÂºèÁöÑËØ∑Ê±ÇÔºåË¶ÅÊ±ÇÂåÖÂê´ "provider"Ôºà‰ªÖÊîØÊåÅ "aescbc"ÔºâÂíå "key"ÔºàÂøÖÈ°ªÊòØ Base64 ÁºñÁ†ÅÁöÑÂ≠óÁ¨¶‰∏≤Ôºâ„ÄÇ  
3. ËÑöÊú¨‰∏≠ÂØπËß£Á†ÅÂêéÁöÑÂØÜÈí•ÈïøÂ∫¶ËøõË°å‰∫ÜÊ†°È™åÔºåÂ¶ÇÊûúÈïøÂ∫¶‰∏∫ 16„ÄÅ24 Êàñ 32 Â≠óËäÇÔºåÂàôËøîÂõû‚Äúaccepted‚ÄùÔºåÂê¶ÂàôËøîÂõû‚Äúrejected‚Äù„ÄÇ  
4. ËØ•Â§çÁé∞ËÑöÊú¨Ê®°Êãü‰∫Ü Kubernetes ‰∏≠ÂÖ≥‰∫é aescbc Êèê‰æõËÄÖÁöÑÂØÜÈí•ÈïøÂ∫¶Ê†°È™åÈÄªËæëÔºå‰ªéËÄåÂ±ïÁ§∫ÊñáÊ°£‰∏≠ÊèèËø∞ÁöÑË¶ÅÊ±ÇÔºà32 Â≠óËäÇÔºâ‰∏éÂÆûÈôÖÊ†°È™åÈÄªËæëÔºàÂÖÅËÆ∏16Âíå24Â≠óËäÇÔºâ‰πãÈó¥ÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇ  
5. ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÊ≠§ÈóÆÈ¢ò‰∏ªË¶ÅÊòØÊñáÊ°£ÊèèËø∞‰∏é‰ª£Á†ÅÈÄªËæë‰∏ç‰∏ÄËá¥Ôºå‰∏çÂ≠òÂú®Áõ¥Êé•ÂèØË¢´ËøúÁ®ãÊîªÂáªÂà©Áî®ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‰ΩéÈ£éÈô©„ÄÇ

---


## Issue #129546 pv_controller fails to delete the underlying disk if a PV is marked deleted when it is in use by a pod

- Issue ÈìæÊé•Ôºö[#129546](https://github.com/kubernetes/kubernetes/issues/129546)

### Issue ÂÜÖÂÆπ

#### What happened?

Consider the following scenario:
1. There is a running Pod X that uses a PVC Y,
3. Y is bound to a PV Z,
4. Z has `persistentVolumeReclaimPolicy: Delete`,
5. Y and Z are marked for deletion while X is running,
6. X is deleted.

In this case pv_controller disregards the reclaim policy and does not call the CSI driver to remove the underlying virtual disk.

#### What did you expect to happen?

`persistentVolumeReclaimPolicy: Delete` must be respected and the underlying virtual disk must be removed.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a GKE cluster with the default configuration (1 node instead of 3 will do fine).
2. Install Postgresql:
```
helm install pg oci://registry-1.docker.io/bitnamicharts/postgresql --version 16.3.4 -n pg-0 --create-namespace --wait
```
3. Find out which PV is used by Postgresql:
```
$ kubectl get pv | grep "pg-0/"
```
4. Mark Postgresql's PVC and PV for deletion (insert your PV name here):
```
$ kubectl -n pg-0 delete pvc/data-pg-postgresql-0
$ kubectl delete pv/pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1
```
5. Delete the namespace that contains Postgresql:
```
$ kubectl delete ns pg-0
```
6. Wait for Postgresql's PV to be deleted as well.
7. Verify that GKE's virtual disk still exists:
```
$ gcloud compute disks list | grep "pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1"
pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1             europe-north1-a      zone            8        pd-balanced  READY
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.12
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6-gke.1596000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰ΩéÈ£éÈô©

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØÂú®‰ª•‰∏ãÂú∫ÊôØ‰∏ãÂá∫Áé∞ÁöÑÈóÆÈ¢òÔºö
1. ‰∏Ä‰∏™ Pod Ê≠£Âú®ËøêË°åÂπ∂‰ΩøÁî®Êüê‰∏™ PVCÔºåPVC ÁªëÂÆö‰∫Ü‰∏Ä‰∏™ PVÔºõ
2. ÂΩì PVC Âíå PV Ë¢´Ê†áËÆ∞‰∏∫Âà†Èô§Êó∂ÔºàÂç≥‰Ωø Pod ‰æùÁÑ∂Âú®ËøêË°åÊàñÂàöË¢´Âà†Èô§ÔºâÔºåÂπ∂‰∏îËØ• PV ÁöÑ reclaim Á≠ñÁï•‰∏∫ DeleteÔºåÊ≠§Êó∂ pv_controller Ê≤°ÊúâÊåâÁÖßÁ≠ñÁï•Ë∞ÉÁî® CSI È©±Âä®Êù•Âà†Èô§Â∫ïÂ±ÇÁöÑ GKE ËôöÊãüÁ£ÅÁõòÔºåÂØºËá¥ÂÆûÈôÖËµÑÊ∫êÊú™Ë¢´Ê∏ÖÁêÜ„ÄÇ

‰ªéÂÆâÂÖ®È£éÈô©ËßíÂ∫¶ÁúãÔºåËøô‰∏™ÈóÆÈ¢ò‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËµÑÊ∫êÊ∏ÖÁêÜ‰∏çÂΩªÂ∫ïÔºåÂèØËÉΩÂØºËá¥Â≠§ÂÑøÁ£ÅÁõòÊÆãÁïô„ÄÇËôΩÁÑ∂ÈïøÊúüÂ≠òÂú®Â≠§ÂÑøÁ£ÅÁõòÂèØËÉΩÂºïËµ∑Êï∞ÊçÆÊÆãÁïôÈ£éÈô©Ôºà‰æãÂ¶ÇÔºåÊÆãÁïôÁ£ÅÁõò‰∏≠ÂåÖÂê´ÊïèÊÑü‰ø°ÊÅØÔºå‰ΩÜËøôÁßçÁ£ÅÁõò‰∏ÄËà¨Âè™Êúâ‰∫ëÂπ≥Âè∞ÁÆ°ÁêÜÂëòÊàñÈ°πÁõÆÊàêÂëòÊúâÊùÉÈôêËÆøÈóÆÔºâÔºå‰ΩÜÈóÆÈ¢òÊú¨Ë∫´Âπ∂‰∏çÂÖÅËÆ∏Êú™ÁªèÊéàÊùÉÁöÑÊîªÂáªËÄÖÁõ¥Êé•ÂèëËµ∑ËøúÁ®ãÊîªÂáª„ÄÅÊèêÊùÉ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÈ´òÂÆâÂÖ®È£éÈô©Ë°å‰∏∫„ÄÇÂõ†Ê≠§Ôºå‰ªéÊîªÂáªÈìæÊù°ËÆ≤ÔºåËøô‰∏™ÈóÆÈ¢ò‰∏ªË¶ÅÂ±û‰∫éËµÑÊ∫êÁÆ°ÁêÜÂíåÊ∏ÖÁêÜ‰∏äÁöÑÁº∫Èô∑ÔºåÂÖ∂Âà©Áî®ÈúÄË¶Å‰∫ëÂπ≥Âè∞Â∑≤ÂÖ∑Êúâ‰∏ÄÂÆöÊùÉÈôêÔºåÂπ∂‰∏î‰æùËµñÂπ≥Âè∞ÁöÑÂ≠òÂèñÊéßÂà∂„ÄÇÁªºÂêàÊù•ÁúãÔºåËØ•ÈóÆÈ¢òÂú®ÂÆâÂÖ®È£éÈô©ËØÑÁ∫ß‰∏äÂ±û‰∫é‰ΩéÈ£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÂ§çÁé∞ pv_controller Êú™Âà†Èô§Â∫ïÂ±ÇÁ£ÅÁõòÁöÑÈóÆÈ¢òÁöÑÊìç‰ΩúÊ≠•È™§„ÄÇ
Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂‰∏éÊú¨Âú∞ÊµãËØïÔºå‰∏î‰ªÖÊ®°ÊãüÈÄªËæëÊ≠•È™§Ôºå‰∏ç‰ºöÁõ¥Êé•Êìç‰ΩúÁîü‰∫ßÁéØÂ¢É„ÄÇ
ÂÆûÈôÖÁéØÂ¢É‰∏≠ÈúÄË¶Å‰ΩøÁî® kubectl„ÄÅhelm„ÄÅgcloud Á≠âÂëΩ‰ª§Â∑•ÂÖ∑ÔºåÊ≠§ËÑöÊú¨ÈÄöËøá subprocess Ë∞ÉÁî®Ëøô‰∫õÂëΩ‰ª§ÔºåËØ∑Á°Æ‰øùÁéØÂ¢É‰∏≠Â∑≤Ê≠£Á°ÆÈÖçÁΩÆÁõ∏Â∫îÂá≠ÊçÆÔºàÊØîÂ¶Ç kubeconfig„ÄÅgcloud Âá≠ÊçÆ‰ø°ÊÅØÂùáÂú®ÈªòËÆ§‰ΩçÁΩÆÔºâ„ÄÇ

Ë≠¶ÂëäÔºöËØ∑ÂãøÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Áõ¥Êé•ËøêË°åËØ•ËÑöÊú¨ÔºåÊú¨ËÑöÊú¨‰ªÖ‰æõÊµãËØïÁéØÂ¢ÉÈ™åËØÅ‰ΩøÁî®„ÄÇ
"""

import subprocess
import time
import sys
import http.server
import socketserver
import threading

# ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°ÔºåÁî®‰∫éÊ®°ÊãüÁéØÂ¢É‰∏≠ÂèØËÉΩÈúÄË¶ÅÁöÑHTTPËØ∑Ê±ÇÂú∫ÊôØÔºàËã•ÈúÄË¶ÅÔºâ
def start_http_server():
    port = 8080
    Handler = http.server.SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", port), Handler) as httpd:
        print(f"Serving HTTP on port {port} ...")
        httpd.serve_forever()

# ÊâßË°å shell ÂëΩ‰ª§ÁöÑËæÖÂä©ÂáΩÊï∞
def run_cmd(cmd, capture_output=False):
    try:
        print("ÊâßË°åÂëΩ‰ª§:", cmd)
        result = subprocess.run(cmd, shell=True, check=True, capture_output=capture_output, text=True)
        if capture_output:
            return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print("ÂëΩ‰ª§ÊâßË°åÂ§±Ë¥•:", e)
        sys.exit(1)

def main():
    # ÂêØÂä®‰∏Ä‰∏™ÂêéÂè∞ HTTP ÊúçÂä°Âô®Á∫øÁ®ãÔºà‰ªÖ‰ΩúÁ§∫‰æãÔºå‰∏çÂΩ±ÂìçÂ§çÁé∞ÊµÅÁ®ãÔºâ
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()
    
    # Ê≠•È™§1ÔºöÂàõÂª∫ GKE ÈõÜÁæ§ÁöÑÁ§∫‰æãÔºàËøôÈáå‰ªÖËæìÂá∫ÊèêÁ§∫‰ø°ÊÅØÔºåÂÆûÈôÖÈõÜÁæ§ÂàõÂª∫ÈúÄË¶ÅÂú® GCP ÊéßÂà∂Âè∞Êàñ gcloud ‰∏≠ÈÖçÁΩÆÔºâ
    print("ËØ∑Á°Æ‰øùÂ∑≤ÂàõÂª∫ GKE ÈõÜÁæ§ÔºåÂπ∂ÈÖçÁΩÆÂ•Ω kubectl ËøûÊé•Âà∞ËØ•ÈõÜÁæ§„ÄÇ")
    
    # Ê≠•È™§2ÔºöÂÆâË£Ö Postgresql Á§∫‰æã
    print("‰ΩøÁî® Helm ÂÆâË£Ö Postgresql ...")
    helm_install_cmd = (
        "helm install pg oci://registry-1.docker.io/bitnamicharts/postgresql "
        "--version 16.3.4 -n pg-0 --create-namespace --wait"
    )
    run_cmd(helm_install_cmd)
    
    # Ê≠•È™§3ÔºöËé∑Âèñ Postgresql ‰ΩøÁî®ÁöÑ PVÔºàËøôÈáåÂÅáËÆæ PVC ÂëΩÂêç‰∏≠Â∏¶Êúâ 'pg' Ê†áËØÜÔºåÁî®Êà∑ÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞ÉÊï¥ËøáÊª§Êù°‰ª∂Ôºâ
    print("Ëé∑Âèñ Postgresql ‰ΩøÁî®ÁöÑ PV ...")
    pv_list_cmd = "kubectl get pv | grep 'pg-'"
    pv_info = run_cmd(pv_list_cmd, capture_output=True)
    print("Ê£ÄÁ¥¢Âà∞ÁöÑ PV ‰ø°ÊÅØ:\n", pv_info)
    
    # ÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÈúÄÊìç‰ΩúÁöÑ PV ÂêçÁß∞ÔºàÊ†πÊçÆËæìÂá∫Ëá™Ë°åÁ°ÆËÆ§Ôºâ
    pv_name = input("ËØ∑ËæìÂÖ•Ë¶ÅÂà†Èô§ÁöÑ PV ÂêçÁß∞Ôºà‰æãÂ¶Ç pvc-xxxxxxÔºâÔºö").strip()
    
    # Ê≠•È™§4ÔºöÂà†Èô§ÂØπÂ∫î PVC Âíå PV
    print("Âà†Èô§ Postgresql ÁöÑ PVC...")
    pvc_delete_cmd = "kubectl -n pg-0 delete pvc/data-pg-postgresql-0"
    run_cmd(pvc_delete_cmd)
    
    print("Âà†Èô§ PV {} ...".format(pv_name))
    pv_delete_cmd = f"kubectl delete pv/{pv_name}"
    run_cmd(pv_delete_cmd)
    
    # Ê≠•È™§5ÔºöÂà†Èô§ÂëΩÂêçÁ©∫Èó¥
    print("Âà†Èô§ÂëΩÂêçÁ©∫Èó¥ pg-0 ...")
    ns_delete_cmd = "kubectl delete ns pg-0"
    run_cmd(ns_delete_cmd)
    
    print("Á≠âÂæÖÊï∞ÂçÅÁßí‰ª•ËÆ©ËµÑÊ∫êÂà†Èô§ÂÆåÊàê...")
    time.sleep(60)
    
    # Ê≠•È™§6ÔºöÊ£ÄÊü•Â∫ïÂ±Ç GKE ËôöÊãüÁ£ÅÁõòÊòØÂê¶‰ªçÁÑ∂Â≠òÂú®ÔºàËØªÂèñ gcloud ÁªìÊûúÔºâ
    print("È™åËØÅ GKE ÁöÑËôöÊãüÁ£ÅÁõòÊòØÂê¶‰ªçÂ≠òÂú® ...")
    check_disk_cmd = f"gcloud compute disks list --format='table(name,zone,status)' | grep '{pv_name}'"
    disk_info = run_cmd(check_disk_cmd, capture_output=True)
    if disk_info:
        print("Â§çÁé∞ÊàêÂäüÔºöËôöÊãüÁ£ÅÁõò‰ªçÂ≠òÂú®, ‰ø°ÊÅØÂ¶Ç‰∏ãÔºö")
        print(disk_info)
    else:
        print("Êú™Ê£ÄÂá∫Áõ∏ÂÖ≥ËôöÊãüÁ£ÅÁõòÂà†Èô§ÈóÆÈ¢òÔºåËØ∑Ê£ÄÊü•ÈõÜÁæ§Âèä‰∫ëÂπ≥Âè∞Êó•Âøó„ÄÇ")
    
if __name__ == '__main__':
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨ÈÄöËøáË∞ÉÁî® Helm„ÄÅkubectl„ÄÅgcloud ÂëΩ‰ª§Êù•Ê®°Êãü issue ‰∏≠ÊèêÂà∞ÁöÑÂ§çÁé∞ÊµÅÁ®ãÔºåÊ≠•È™§‰∏é issue ‰∏≠ÊèèËø∞ÁöÑÊìç‰ΩúÈ°∫Â∫è‰∏ÄËá¥Ôºö  
   - È¶ñÂÖàÂÆâË£Ö Postgresql Â∫îÁî®Ôºõ
   - Ëé∑ÂèñÂÖ∂‰ΩøÁî®ÁöÑ PV ‰ø°ÊÅØÔºõ
   - Âà†Èô§ÂØπÂ∫îÁöÑ PVC„ÄÅPV ‰ª•ÂèäÊâÄÂú®ÁöÑÂëΩÂêçÁ©∫Èó¥Ôºõ
   - Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ£ÄÊü•Â∫ïÂ±Ç GKE ËôöÊãüÁ£ÅÁõòÊòØÂê¶‰æùÁÑ∂Â≠òÂú®„ÄÇ
2. ËÑöÊú¨‰∏≠ËøòÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÁõëÂê¨ 8080 Á´ØÂè£ÔºåÁî®‰∫éÊª°Ë∂≥‚ÄúÂ¶ÇÊûúÈúÄË¶ÅËÆøÈóÆ HTTP ÊúçÂä°Âô®‚ÄùÁöÑÈúÄÊ±ÇÔºåÊ≠§Â§Ñ‰ªÖ‰ΩúÁ§∫‰æãÁî®ÈÄîÔºåÂπ∂‰∏çÁõ¥Êé•ÂΩ±ÂìçÂ§çÁé∞ÊµÅÁ®ã„ÄÇ
3. ‰∏∫‰øùËØÅËÑöÊú¨ÁöÑÂÆâÂÖ®ÊÄßÔºåÊâÄÊúâÊïèÊÑüÁöÑÈÖçÁΩÆÔºàÂ¶Ç kubeconfig„ÄÅgcloud Âá≠ÊçÆÔºâÂùáÂÅáËÆæÂú®ÈªòËÆ§‰ΩçÁΩÆÈÖçÁΩÆÂ•Ω„ÄÇ
4. ËØ∑Ê≥®ÊÑèÔºåÊ≠§ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂‰∏éÊú¨Âú∞ÊµãËØïÔºåÂàáÂãøÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Áõ¥Êé•ËøêË°åÔºå‰ª•ÂÖçËØØÊìç‰ΩúÈÄ†ÊàêËµÑÊ∫êÊçüÂ§±„ÄÇ

Áªº‰∏äÔºåËØ• issue ÊâÄÂèçÊò†ÁöÑÈóÆÈ¢òÂú®ÂÆâÂÖ®ÊîªÂáªÈìæÊù°‰∏≠È£éÈô©ËæÉ‰ΩéÔºå‰∏çÊûÑÊàêÈ´òÈ£éÈô©ÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

---


## Issue #129550 FUSE mounts in emptyDir volumes cannot be cleaned

- Issue ÈìæÊé•Ôºö[#129550](https://github.com/kubernetes/kubernetes/issues/129550)

### Issue ÂÜÖÂÆπ

#### What happened?

This issue is to summarize some conversation towards the end of https://github.com/kubernetes/kubernetes/issues/7890, as requested by @thockin. 

If an application in a privileged pod or container creates a FUSE mount in an emptyDir volume, but fails to unmount it before terminating (either due to that being a conscious choice by the application, or due to a SIGKILL from kubernetes), the kubelet will fail to clean up the pod. A recurring error will appear in the kubelet logs, and the pod will remain in API.

Here is an example error log from the kubelet during cleanup:
```
Jan 08 19:06:04 <hostname omitted> kubelet[12511]: E0108 19:06:04.507950   12511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/empty-dir/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0-worker podName:30b506e8-b18a-4d5c-bf7d-17fbae54a5d0 nodeName:}" failed. No retries permitted until 2025-01-08 19:08:06.507933266 +0000 UTC m=+1437.970062341 (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume "worker" (UniqueName: "kubernetes.io/empty-dir/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0-worker") pod "30b506e8-b18a-4d5c-bf7d-17fbae54a5d0" (UID: "30b506e8-b18a-4d5c-bf7d-17fbae54a5d0") : openfdat /var/lib/kubelet/pods/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0/volumes/kubernetes.io~empty-dir/worker/build: transport endpoint is not connected
```

The offending code seems to be here: https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/volume/emptydir/empty_dir.go#L490-L495

When cleaning up emptyDirs, we start with `os.RemoveAll`, as it's recursing through the directory, it will eventually try to inspect the contents of the FUSE mount, which will result in an error. 

#### What did you expect to happen?

The kubelet is able to clean the pod up eventually.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a privileged container that can generate a FUSE mount within an emptydir
2. Configure the application to not clean the FUSE mount, or forcefully terminate the pod so the mount cannot be cleaned.

#### Anything else we need to know?

I'd be happy to try and put a patch together to address this with a little guidance. It seems like we should be able to inspect for any mounts beneath the empty directory's `MetaDir` and `umount` them before we attempt to call the `os.RemoveAll`. 

#### Kubernetes version

<details>
```console
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.3-eks-56e63d8
```
</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>
```console
# On Linux:
$ cat /etc/os-release
NAME=Bottlerocket
ID=bottlerocket
VERSION="1.29.0 (aws-k8s-1.31)"
PRETTY_NAME="Bottlerocket OS 1.29.0 (aws-k8s-1.31)"
VARIANT_ID=aws-k8s-1.31
VERSION_ID=1.29.0
BUILD_ID=c55d099c
HOME_URL="https://github.com/bottlerocket-os/bottlerocket"
SUPPORT_URL="https://github.com/bottlerocket-os/bottlerocket/discussions"
BUG_REPORT_URL="https://github.com/bottlerocket-os/bottlerocket/issues"
DOCUMENTATION_URL="https://bottlerocket.dev"
$ uname -a
# paste output here
Linux <hostname omitted> 6.1.119 #1 SMP Thu Dec 12 20:00:51 UTC 2024 aarch64 GNU/Linux
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
Ê†πÊçÆ Issue ÊèèËø∞ÔºåËØ•ÈóÆÈ¢òÂØºËá¥ÁöÑÊòØ‰∏ÄÁßçËµÑÊ∫êÊ∏ÖÁêÜ‰∏çÂΩªÂ∫ï„ÄÅÂØºËá¥ Pod ÊåÇËµ∑ÁöÑÁé∞Ë±°ÔºåËôΩÂèØËÉΩÂºïÂèëÊãíÁªùÊúçÂä°ÔºàDoSÔºâÈóÆÈ¢òÔºå‰ΩÜÈúÄË¶ÅÂ∫îÁî®‰ª•ÁâπÊùÉÊ®°ÂºèËøêË°åÔºåÂπ∂‰∏ªÂä®ÊàñÈùû‰∏ªÂä®Ëß¶Âèë FUSE ÊåÇËΩΩËÄå‰∏çÂç∏ËΩΩ„ÄÇ‰ªéÊîªÂáªÈù¢Êù•ÁúãÔºåÊîªÂáªËÄÖÈúÄÊã•ÊúâËæÉÈ´òÊùÉÈôêÔºà‰æãÂ¶ÇË∞ÉÂ∫¶ÁâπÊùÉÂÆπÂô®ÊàñÁâπÊùÉÂÆπÂô®ÂÜÖÈÉ®ÊâßË°åÊÅ∂ÊÑèÊìç‰ΩúÔºâÔºåÂõ†Ê≠§Êú¨ÈóÆÈ¢òÊï¥‰ΩìÈ£éÈô©ËØÑÁ∫ß‰∏∫‰ΩéÈ£éÈô©„ÄÇ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊÉÖÂÜµÊòØÔºöÂú®ÁâπÊùÉÂÆπÂô®‰∏≠ÔºåÂ¶ÇÊûúÂ∫îÁî®Á®ãÂ∫è‰ΩøÁî®‰∫Ü FUSE ÊåÇËΩΩÂà∞ emptyDir Âç∑‰∏ãÔºåÂπ∂Âú®ÈÄÄÂá∫ÂâçÊú™ÊâßË°åÂç∏ËΩΩÊìç‰ΩúÔºàÂèØËÉΩÊòØÁî±ÊúâÊÑèÊàñ SIGKILL ÂØºËá¥ÔºâÔºåÂàô kubelet Âú®Ê∏ÖÁêÜ Pod Êó∂ÔºåË∞ÉÁî® os.RemoveAll ÈÄíÂΩíÂà†Èô§ÁõÆÂΩïÊó∂‰ºöÂõ†‰∏∫ËÆøÈóÆ FUSE ÊåÇËΩΩÁÇπËÄåÊä•ÈîôÔºàÂ¶Ç‚Äútransport endpoint is not connected‚ÄùÔºâÔºåÂØºËá¥ËØ• Pod Êó†Ê≥ïÊ≠£Â∏∏Ê∏ÖÁêÜÂπ∂ÊåÅÁª≠‰øùÁïôÂú® API ‰∏≠„ÄÇËØ•ÈóÆÈ¢òÊú¨Ë∫´‰∏ªË¶ÅÂΩ±Âìç Pod Ê∏ÖÁêÜÂíåËµÑÊ∫êÈáäÊîæÔºåÂèØËÉΩÂú®‰∏ÄÂÆöËåÉÂõ¥ÂÜÖÂºïËµ∑ËµÑÊ∫êÊ≥ÑÈú≤ÊàñÊãíÁªùÊúçÂä°Áé∞Ë±°Ôºå‰ΩÜ‰ªÖÈíàÂØπÂÖ∑ÊúâÁâπÊùÉÁöÑ Pod ÊàñÂÆπÂô®ÊâçÂèØ‰ª•‰∫ßÁîüËøôÁßçÊÉÖÂÜµ„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Â§çÁé∞ËØ•ÈóÆÈ¢òÁöÑPOCÊÄùË∑ØÔºö
1. ÂàõÂª∫‰∏Ä‰∏™Ê®°Êãü emptyDir Âç∑ÁöÑÁõÆÂΩïÔºõ
2. Âú® emptyDir ‰∏ãÂàõÂª∫‰∏Ä‰∏™Â≠êÁõÆÂΩï‰Ωú‰∏∫ FUSE ÊåÇËΩΩÁÇπÔºõ
3. ÈÄöËøá fusepy ÂÆûÁé∞‰∏Ä‰∏™ÁÆÄÂçïÁöÑ FUSE Êñá‰ª∂Á≥ªÁªüÔºåÂπ∂Â∞ÜÂÖ∂ÊåÇËΩΩÂà∞‰∏äËø∞ÊåÇËΩΩÁÇπÔºõ
4. ÈÄöËøáÂ§öËøõÁ®ãÂêØÂä® FUSE Êñá‰ª∂Á≥ªÁªüÔºåÂπ∂Âú®ÊåÇËΩΩÊàêÂäüÂêéÔºåÊ®°ÊãüÂºÇÂ∏∏ÁªàÊ≠¢ÔºàÊ®°ÊãüÂ∫îÁî®Êú™Âç∏ËΩΩ FUSE ÊåÇËΩΩÁöÑÊÉÖÂÜµÔºâÔºõ
5. Âú® FUSE ËøõÁ®ãË¢´Âº∫Âà∂ÁªìÊùüÂêéÔºåÂ∞ùËØï‰ΩøÁî® shutil.rmtree Âà†Èô§ÂåÖÂê´ FUSE ÊåÇËΩΩÁöÑÁõÆÂΩïÔºåËßÇÂØüÊòØÂê¶Â§çÁé∞Âà†Èô§Â§±Ë¥•ÔºàÊä•Èîô‚Äútransport endpoint is not connected‚ÄùÔºâÁöÑÁé∞Ë±°„ÄÇ
Ê≥®ÊÑèÔºöÊú¨ËÑöÊú¨‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂ÔºåËØ∑Á°Æ‰øùÁéØÂ¢É‰∏≠Â∑≤ÂÆâË£Ö fusepyÔºåÂπ∂‰∏îÂΩìÂâçÁî®Êà∑ÂÖ∑Êúâ FUSE ÊåÇËΩΩÊùÉÈôê„ÄÇ
"""

import os
import sys
import time
import shutil
import errno
import logging
from multiprocessing import Process
from fuse import FUSE, Operations

# ÈÖçÁΩÆÊó•ÂøóËæìÂá∫
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')

class DummyFS(Operations):
    """
    ‰∏Ä‰∏™ÊúÄÁÆÄÁâàÁöÑ FUSE Êñá‰ª∂Á≥ªÁªüÔºå‰ªÖÊîØÊåÅÊ†πÁõÆÂΩïÂ±ûÊÄßÂíåÁõÆÂΩïËØªÂèñÊìç‰Ωú
    """
    def getattr(self, path, fh=None):
        st = dict(
            st_mode=(0o40755),
            st_nlink=2,
            st_size=0,
            st_ctime=time.time(),
            st_mtime=time.time(),
            st_atime=time.time()
        )
        return st

    def readdir(self, path, fh):
        return ['.', '..']

def run_fuse(mountpoint):
    """
    Âú®ÊåáÂÆöÊåÇËΩΩÁÇπÂêØÂä® FUSE Êñá‰ª∂Á≥ªÁªüÔºåforeground Ê®°ÂºèËøêË°å
    """
    try:
        logging.info("FUSEÊñá‰ª∂Á≥ªÁªüÂ∞ÜÊåÇËΩΩÂú® %s", mountpoint)
        # foreground=True ‰øùÊåÅËøêË°åÔºånotrreads=True ÁÆÄÂåñÂ§öÁ∫øÁ®ãÈóÆÈ¢ò
        FUSE(DummyFS(), mountpoint, foreground=True, nothreads=True)
    except Exception as e:
        logging.error("FUSE ÊåÇËΩΩÂºÇÂ∏∏: %s", e)

def main():
    # ÂÆö‰πâÊ®°Êãü emptyDir Âç∑ÁöÑÂü∫Á°ÄÁõÆÂΩï
    base_dir = os.path.abspath("./emptydir_vol")
    fuse_mount_point = os.path.join(base_dir, "fuse_mount")

    # ÂàõÂª∫ÁõÆÂΩïÁªìÊûÑÔºåÊ®°Êãü emptyDir Âç∑ÂíåÊåÇËΩΩÁÇπ
    try:
        os.makedirs(fuse_mount_point, exist_ok=True)
        logging.info("ÂàõÂª∫ÁõÆÂΩïÁªìÊûÑ: %s", fuse_mount_point)
    except Exception as e:
        logging.error("ÂàõÂª∫ÁõÆÂΩïÂ§±Ë¥•: %s", e)
        sys.exit(1)

    # ÂêØÂä® FUSE ÊåÇËΩΩÔºå‰ΩøÁî®Â§öËøõÁ®ãÂêØÂä®‰ª•Êñπ‰æøÂêéÁª≠ÁªàÊ≠¢
    fuse_process = Process(target=run_fuse, args=(fuse_mount_point,))
    fuse_process.start()

    # Á≠âÂæÖ2ÁßíÔºåÁ°Æ‰øùFUSEÊåÇËΩΩÂ∑≤ÁªèÂêØÂä®
    time.sleep(2)

    # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÁªèÊåÇËΩΩÔºåÂèØÈÄöËøáÊ£ÄÊü• fuse_mount_point ‰∏≠ÁöÑÊñá‰ª∂ÊàñË∞ÉÁî® 'mount' ÂëΩ‰ª§ÔºàËøôÈáåÁÆÄÂçïÁ≠âÂæÖÔºâ
    logging.info("FUSE ÊåÇËΩΩÈ¢ÑËÆ°Â∑≤ÂÆåÊàê„ÄÇÁé∞Âú®Ê®°ÊãüÂºÇÂ∏∏ÈÄÄÂá∫ÔºàÊú™Âç∏ËΩΩ FUSEÔºâ„ÄÇ")

    # Ê®°ÊãüÂºÇÂ∏∏ÁªàÊ≠¢ FUSE ËøõÁ®ãÔºåÁõ∏ÂΩì‰∫éÂ∫îÁî®ÂºÇÂ∏∏ÈÄÄÂá∫Ê≤°ÊúâÊâßË°å umount
    fuse_process.terminate()
    fuse_process.join()
    logging.info("Â∑≤ÁªàÊ≠¢ FUSE ËøõÁ®ãÔºå‰ΩÜÊåÇËΩΩÁÇπÂèØËÉΩÂ∞öÊú™Ë¢´Ê≠£Á°ÆÂç∏ËΩΩ„ÄÇ")

    # Âª∂Êó∂Á≠âÂæÖÁ≥ªÁªüÊõ¥Êñ∞ÊåÇËΩΩÁä∂ÊÄÅ
    time.sleep(1)

    # Â∞ùËØïÂà†Èô§Êï¥‰∏™ emptyDir Âç∑ÔºàÂç≥ base_dirÔºâÔºåËßÇÂØüÂà†Èô§Êó∂ÁöÑÈîôËØØ‰ø°ÊÅØ
    logging.info("Â∞ùËØïÊ∏ÖÁêÜÁõÆÂΩï: %s", base_dir)
    try:
        shutil.rmtree(base_dir)
        logging.info("ÁõÆÂΩïÂà†Èô§ÊàêÂäü„ÄÇ")
    except Exception as e:
        # Â¶ÇÊûúÊçïËé∑Âà∞ÈîôËØØÔºåÂàôËæìÂá∫È¢ÑÊúüÁöÑÈîôËØØ‰ø°ÊÅØ
        if hasattr(e, 'errno'):
            if e.errno == errno.ENOTCONN:
                logging.error("Âà†Èô§Â§±Ë¥•ÔºåÈîôËØØ: transport endpoint is not connected (ENOTCONN)")
            else:
                logging.error("Âà†Èô§Â§±Ë¥•ÔºåÈîôËØØÁ†Å: %s, ÈîôËØØ: %s", e.errno, e)
        else:
            logging.error("Âà†Èô§Â§±Ë¥•ÔºåÈîôËØØ: %s", e)

if __name__ == "__main__":
    main()
```


---


## Issue #129467 WatchList use more temp memory than legacy ListWatch

- Issue ÈìæÊé•Ôºö[#129467](https://github.com/kubernetes/kubernetes/issues/129467)

### Issue ÂÜÖÂÆπ

#### What happened?

there are 9812 pods in kube-systemÔºå we open different number of informers to test memory usageÔºö
```
./pod_informer -count=10 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=20 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=30 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=40 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4

```

![watchlist-off](https://github.com/user-attachments/assets/2cc7401a-d374-4e37-8b3e-576e0e9e2352)


```
./pod_informer -count=10 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=20 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=30 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=40 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
```
![watchlist-on](https://github.com/user-attachments/assets/6fd55242-b268-43fd-a04e-0e009610cf5f)

we can seeÔºå only 30 informers Ôºå we lost the apiserver

#### What did you expect to happen?

WatchList use less temp memory than legacy ListWatch

#### How can we reproduce it (as minimally and precisely as possible)?

we have all 670722 pods in watchcache 

etcdctl --cacert=/etc/kubernetes/certs/etcd/Etcdtrustca.crt --cert=/etc/kubernetes/certs/etcd/etcd-client.crt --key=/etc/kubernetes/certs/etcd/etcd-client.key --endpoints=$etcd_eps get /registry/pods --prefix --keys-only --command-timeout=10s| grep /registry/pods | wc -l
670722


the pod_informer binary code:
```
/*
Copyright 2023 The Kubernetes Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
    "context"
    "flag"
    "os"
    "time"

    "k8s.io/apimachinery/pkg/util/wait"
    v1 "k8s.io/client-go/informers/core/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/cache"
    "k8s.io/client-go/tools/clientcmd"
    "k8s.io/klog/v2"
)

var (
    kubeconfig                  string
    targetNamespace             string
    informerCount               int
    testTimeout                 time.Duration
    enableWatchListAlphaFeature bool
)

func main() {
    registerFlags()
    flag.Parse()

    klog.Info("The test binary started with the following arguments:")
    flag.VisitAll(func(f *flag.Flag) {
       klog.Infof("  -%s=%v (%s)\n", f.Name, f.Value, f.Usage)
    })

    if enableWatchListAlphaFeature {
       os.Setenv("ENABLE_CLIENT_GO_WATCH_LIST_ALPHA", "true")
    }

    ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
    defer cancel()
    config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
    if err != nil {
       klog.Fatal(err)
    }
    config.AcceptContentTypes = "application/vnd.kubernetes.protobuf,application/json"
    config.ContentType = "application/vnd.kubernetes.protobuf"
	config.QPS = 100
	config.Burst = 200
    klog.Infof("The following Kubernetes client config will be used\n%v", config.String())

    client, err := kubernetes.NewForConfig(config)
    if err != nil {
       klog.Fatal(err)
    }


       ts := time.Now()
       ctxInformer, cancelInformers := context.WithCancel(ctx)
       defer cancelInformers()

       klog.Infof("Starting %d pod informers for targetNamespace = %s", informerCount, targetNamespace)
       informersSynced := startInformersFor(ctxInformer, client, informerCount, targetNamespace)

       klog.Info("Waiting for the pod informers to synced")
       cache.WaitForCacheSync(ctx.Done(), informersSynced...)
       klog.Infof("All %v pod informers synced, time needed = %v", len(informersSynced), time.Now().Sub(ts))


    klog.Info("Exiting the test app")
}

func registerFlags() {
    klog.InitFlags(flag.CommandLine)

    flag.StringVar(&kubeconfig, "kubeconfig", "", "path to kubeconfig.")
    flag.StringVar(&targetNamespace, "namespace", "", "namespace that host pods to list. If empty a default ('') value will be used.")
    flag.IntVar(&informerCount, "count", 4, "the number of informers per targetNamespace to run. If empty a default (4) value will be used.")
    flag.DurationVar(&testTimeout, "timeout", time.Minute, "timeout duration for the test")
    flag.BoolVar(&enableWatchListAlphaFeature, "enableWatchListFeature", false, "whether to set ENABLE_CLIENT_GO_WATCH_LIST_ALPHA env var")
}

func startInformersFor(ctx context.Context, client kubernetes.Interface, count int, namespace string) []cache.InformerSynced {
    var informersSynced []cache.InformerSynced
    for i := 0; i < count; i++ {
       inf := v1.NewPodInformer(client, namespace, time.Duration(0), cache.Indexers{})
       inf.SetTransform(func(interface{}) (interface{}, error) {
          return nil, nil
       })
       informersSynced = append(informersSynced, inf.HasSynced)
       go inf.Run(ctx.Done())
    }
    return informersSynced
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.32.0
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
È£éÈô©ËØÑÁ∫ß‰∏∫‰ΩéÈ£éÈô©„ÄÇ  
ÂéüÂõ†ÊòØÔºö  
1. ËôΩÁÑ∂ËØ•ÈóÆÈ¢òÂèØËÉΩÂØºËá¥ apiserver Âõ†ÂÜÖÂ≠òËµÑÊ∫êËÄóÂ∞ΩËÄå‰∏çÂèØÁî®Ôºå‰ΩÜÊîªÂáªËÄÖÈúÄË¶ÅÂÖ∑Â§áÁõ∏Â∫îÁöÑËÆøÈóÆÊùÉÈôêÔºàËØªÂèñ kube-system pods ÂàóË°®ÔºâÔºåËøôÁßçÊùÉÈôêÈÄöÂ∏∏Âè™Êéà‰∫à‰ø°‰ªªÁî®Êà∑„ÄÇ  
2. Â¶ÇÊûúÊîªÂáªËÄÖÊó†Ê≥ïÁõ¥Êé•Ëé∑ÂæóË∂≥Â§üÊùÉÈôêÂèëËµ∑ËØ•ÊîªÂáªÔºåÂÖ∂Âà©Áî®ÈöæÂ∫¶ËæÉÂ§ßÔºåÊâÄ‰ª•Âú® CVSS ËØÑÂàÜ‰∏≠ËæÉÈöæËææÂà∞ high Á∫ßÂà´„ÄÇ

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞‰∫ÜÂú®ÂêØÁî® watchlist ÂäüËÉΩÊó∂ÔºåÁõ∏ÊØî‰º†ÁªüÁöÑ ListWatch ÊñπÂºèÔºå‰ΩøÁî®ÁöÑ‰∏¥Êó∂ÂÜÖÂ≠òÊõ¥Â§öÔºåÊúÄÁªàÂØºËá¥ apiserver Âú®ÂºÄÂêØ‰∏ÄÂÆöÊï∞Èáè informer ÂêéÂ¥©Ê∫ÉÔºàÊØîÂ¶ÇÁ§∫‰æã‰∏≠‰ªÖÂêØÂä® 30 ‰∏™ informer Âç≥ÂØºËá¥ apiserver ‚Äú‰∏¢Â§±‚ÄùÔºâ„ÄÇÈóÆÈ¢òÊú¨Ë∫´ÂèçÊò†ÁöÑÊòØÊñ∞ÂäüËÉΩÂú®ËµÑÊ∫êÊ∂àËÄó‰∏äÁöÑÂºÇÂ∏∏ÔºåÂèØËÉΩ‰ºöÂºïÂèëÊãíÁªùÊúçÂä°ÔºàDoSÔºâÈóÆÈ¢òÔºåÂç≥Âà©Áî® watchlist ÂäüËÉΩËÆ© apiserver ÂÜÖÂ≠òÂç†Áî®ËøáÈ´òÔºåËøõËÄåÈÄ†ÊàêÈõÜÁæ§ÊéßÂà∂Âπ≥Èù¢‰∏çÂèØÁî®„ÄÇ‰ΩÜÈúÄË¶ÅÊ≥®ÊÑèÔºåËØ•ÈóÆÈ¢ò‰∫ßÁîüÁöÑÂâçÊèêÊòØÈúÄË¶ÅÂÖ∑ÊúâÂØπ apiserver ËøõË°åËØªÊìç‰ΩúÁöÑÊùÉÈôêÔºà‰æãÂ¶ÇÂØπ kube-system ÂëΩÂêçÁ©∫Èó¥ÁöÑ pods ËøõË°åËØªÂèñÔºâÔºåËøôÈÄöÂ∏∏ÈúÄË¶ÅËæÉÈ´òÊùÉÈôêÊàñÂ∑≤Áªè‰ø°‰ªªÁöÑË∞ÉÁî®ËÄÖÊâçËÉΩÊâßË°åÊ≠§Êìç‰Ωú„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import threading
import time
import sys
from kubernetes import client, config, watch
from http.server import HTTPServer, BaseHTTPRequestHandler

# HTTP ServerÁî®‰∫éÁõëÂê¨8080Á´ØÂè£
class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        message = "This is a simple HTTP server for testing."
        self.wfile.write(message.encode("utf-8"))

def start_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, SimpleHTTPRequestHandler)
    print("HTTP server is starting at port 8080")
    httpd.serve_forever()

def pod_watcher(namespace, watcher_id, run_time_seconds):
    try:
        # ‰ªéÈªòËÆ§‰ΩçÁΩÆÂä†ËΩΩ kubeconfig ÈÖçÁΩÆ
        config.load_kube_config()
    except Exception as e:
        print(f"Á∫øÁ®ã {watcher_id}: Êó†Ê≥ïÂä†ËΩΩ kubeconfig: {e}")
        return

    v1 = client.CoreV1Api()
    w = watch.Watch()

    end_time = time.time() + run_time_seconds
    print(f"Á∫øÁ®ã {watcher_id} ÂêØÂä®ÔºåÂØπÂëΩÂêçÁ©∫Èó¥ {namespace} ÂºÄÂßãwatch POD‰∫ã‰ª∂")
    while time.time() < end_time:
        try:
            # ‰ΩøÁî® watch ÊµãËØïÊµÅÂºèËé∑ÂèñpodÂàóË°®
            stream = w.stream(v1.list_namespaced_pod, namespace=namespace, timeout_seconds=60)
            for event in stream:
                # Â§ÑÁêÜ‰∫ã‰ª∂Êó∂‰ªÖÁÆÄÂçïÊâìÂç∞‰∏Ä‰∏ã‰ø°ÊÅØÔºåÂáèÂ∞ëÈ¢ùÂ§ñÂ§ÑÁêÜ
                obj = event.get("object")
                evt_type = event.get("type")
                print(f"Á∫øÁ®ã {watcher_id} ‰∫ã‰ª∂: {evt_type} pod: {getattr(obj.metadata, 'name', 'unknown')}")
                # Â¶ÇËææÂà∞ÁªìÊùüÊó∂Èó¥Â∞±ÈÄÄÂá∫
                if time.time() >= end_time:
                    w.stop()
                    break
        except Exception as e:
            print(f"Á∫øÁ®ã {watcher_id} ÈÅáÂà∞ÂºÇÂ∏∏: {e}")
            time.sleep(1)

def main():
    import argparse
    parser = argparse.ArgumentParser(description="WatchListÂÜÖÂ≠òÊ∂àËÄóÂ§çÁé∞ËÑöÊú¨Ôºå‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂")
    parser.add_argument("--namespace", type=str, default="kube-system", help="ÁõëÊéßÁöÑÂëΩÂêçÁ©∫Èó¥")
    parser.add_argument("--count", type=int, default=30, help="ÂêØÂä® watcher ÁöÑÊï∞Èáè")
    parser.add_argument("--runtime", type=int, default=120, help="ËøêË°åÊó∂ÈïøÔºàÁßíÔºâ")
    args = parser.parse_args()

    threads = []

    # ÂêØÂä®HTTPÊúçÂä°ÔºåÁõëÂê¨8080Á´ØÂè£
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # ÂêØÂä®Â§ö‰∏™ watcher Á∫øÁ®ã
    for i in range(args.count):
        t = threading.Thread(target=pod_watcher, args=(args.namespace, i+1, args.runtime))
        threads.append(t)
        t.start()
        # ‰∏∫‰∫ÜÊõ¥Âø´Â§çÁé∞ÂÜÖÂ≠òÂç†Áî®ÈóÆÈ¢òÔºåÂèØ‰ª•ÊéßÂà∂Èó¥ÈöîÈùûÂ∏∏Áü≠
        time.sleep(0.5)

    # Á≠âÂæÖÊâÄÊúâÁ∫øÁ®ãÂÆåÊàê
    for t in threads:
        t.join()

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨‰ΩøÁî® Kubernetes Python ÂÆ¢Êà∑Á´ØËØªÂèñÈªòËÆ§‰ΩçÁΩÆÁöÑ kubeconfig Êñá‰ª∂ÔºåÂπ∂Âú®ÊåáÂÆöÂëΩÂêçÁ©∫Èó¥ÔºàÈªòËÆ§‰∏∫ kube-systemÔºâ‰∏≠ÂàõÂª∫Â§ö‰∏™ watcher Á∫øÁ®ãÔºåÊ®°ÊãüÊåÅÁª≠ÁõëÊéß pod ÁöÑ‰∫ã‰ª∂„ÄÇ  
2. ÊØè‰∏™Á∫øÁ®ã‰ΩøÁî® watch API ÊµÅÂºèËé∑Âèñ pod ÂàóË°®Âπ∂ÊâìÂç∞‰∫ã‰ª∂‰ø°ÊÅØÔºå‰ª•Ê®°Êãü watchlist Êú∫Âà∂‰∏ãÁöÑÂÜÖÂ≠òÂíåËµÑÊ∫êÊ∂àËÄóÊÉÖÊôØ„ÄÇ  
3. Âè¶Â§ñÔºåËÑöÊú¨‰∏≠Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÁõëÂê¨ 8080 Á´ØÂè£ÁöÑ HTTP ÊúçÂä°Âô®ÔºàÂü∫‰∫é BaseHTTPRequestHandlerÔºâÔºåÊ®°ÊãüÈÄöËøá HTTP Á´ØÂè£Êèê‰æõËæÖÂä©ÊúçÂä°ÔºåÊª°Ë∂≥Ë¶ÅÊ±Ç‰∏≠ ‚ÄúÂ¶ÇÊûúÈúÄË¶ÅËÆøÈóÆHTTPÊúçÂä°Âô®ÂàôÂú®ËÑöÊú¨‰∏≠ÂàõÂª∫‰∏Ä‰∏™HTTPÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®8080Á´ØÂè£‚Äù„ÄÇ  
4. Ê≠§Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂ÁõÆÁöÑÔºåËØ∑ÂãøÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Áõ¥Êé•‰ΩøÁî®„ÄÇ  
5. Ê†πÊçÆÂëΩ‰ª§Ë°åÂèÇÊï∞Ôºå‰Ω†ÂèØ‰ª•Ë∞ÉÊï¥ watcher ÁöÑÊï∞ÈáèÔºà--countÔºâÂíåÊÄªÁöÑËøêË°åÊó∂ÈïøÔºà--runtimeÔºâÊù•Â§çÁé∞ÂíåËßÇÊµãËµÑÊ∫êÊ∂àËÄóË°å‰∏∫„ÄÇÂ¶ÇÊûúÂ§ßÈáè watcher ÂêåÊó∂ËøêË°åÔºåÂèØËÉΩ‰ºöÂØºËá¥ kube-apiserver ÂÜÖÂ≠ò‰ΩøÁî®Â¢ûÈ´òÔºå‰ªéËÄåÈ™åËØÅ issue ‰∏≠ÊèèËø∞ÁöÑÁé∞Ë±°„ÄÇ

---


# ‚úÖ ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÁöÑ Issues (49 ‰∏™)

## Issue #129935 CSI migration with in-tree azure-file provider fails to mount PVC to Pod

- Issue ÈìæÊé•Ôºö[#129935](https://github.com/kubernetes/kubernetes/issues/129935)

### Issue ÂÜÖÂÆπ

#### What happened?

When mounting PVC to the Pod the volume will not mount and it will hang on
```
MountVolume.SetUp failed for volume "pvc-d9734ad3-26f5-4fb9-8918-7643d2bf8c75" : kubernetes.io/csi: mounter.SetUpAt failed to get service accoount token attributes: failed to fetch token: serviceaccounts "default" is forbidden: audience "api://AzureADTokenExchange" not found in pod spec volume
```


#### What did you expect to happen?

Volume mounted successfully with Pod running.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create Azure cluster with Kubernetes 1.32
2. Create PV claim
```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: my-pv-claim 
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS 
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
3. Create Pod and mount PVC

#### Anything else we need to know?

The error comes from https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/admission/noderestriction/admission.go#L654
which was introduced in https://github.com/kubernetes/kubernetes/pull/128077

Admission plugin will work for CSI
https://github.com/openshift/kubernetes/blob/master/plugin/pkg/admission/noderestriction/admission.go#L689-L690

But when using CSI migration with in-tree azure-file provider the PVC fails to mount.
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L141

CSIdriver contents
```yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  annotations:
    csi.openshift.io/managed: "true"
    operator.openshift.io/spec-hash: 5c303d28061bd64a18ff9b79bd6b231293b3c1295db37e91d59160efe4611365
  creationTimestamp: "2025-01-31T13:10:35Z"
  labels:
    security.openshift.io/csi-ephemeral-volume-profile: privileged
  name: file.csi.azure.com
  resourceVersion: "9671"
  uid: 15b2ad34-60dc-4765-97cb-b91613dfa6d7
spec:
  attachRequired: false
  fsGroupPolicy: None
  podInfoOnMount: true
  requiresRepublish: false
  seLinuxMount: false
  storageCapacity: false
  tokenRequests:
  - audience: api://AzureADTokenExchange
  volumeLifecycleModes:
  - Persistent
  - Ephemeral 
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.4.2
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
ServiceAccountNodeAudienceRestriction admission plugin - https://github.com/kubernetes/kubernetes/blob/295ed0a607922af27841121b8c7e95961fcb4d79/plugin/pkg/admission/noderestriction/admission.go#L637-L657
</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú®‰ΩøÁî® CSI ËøÅÁßª‰∏é in-tree azure-file Êèê‰æõÁ®ãÂ∫èÊó∂ÔºåPod ÁöÑ PVC ÊåÇËΩΩÂ§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØË°®ÊòéÂú®Ëé∑Âèñ service account token Êó∂Âá∫Áé∞ÈóÆÈ¢òÔºåÂÖ∑‰ΩìÊòØÂõ†‰∏∫È¢ÑÊúüÁöÑ audience "api://AzureADTokenExchange" Êú™Âú® Pod spec ‰∏≠ÊâæÂà∞„ÄÇËøôÁßçÈîôËØØ‰∏ªË¶ÅÂèçÊò†‰∫Ü Kubernetes Admission Êèí‰ª∂ÂíåÊúçÂä°Ë¥¶Êà∑ Token ÈÖçÁΩÆ‰∏äÁöÑÈóÆÈ¢òÔºåÂØºËá¥ PVC Êó†Ê≥ïÊ≠£Á°ÆÊåÇËΩΩ„ÄÇÊÄª‰ΩìÊù•ÁúãÔºåËøôÊòØ‰∏Ä‰∏™‰∏éÂ≠òÂÇ®ÊåÇËΩΩÊµÅÁ®ãÂíåÈÖçÁΩÆÊúâÂÖ≥ÁöÑÁº∫Èô∑ÔºåËÄåÈùûÁõ¥Êé•ÂΩ±ÂìçÁ≥ªÁªüÂÆâÂÖ®ÊÄßÁöÑÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import time
import traceback
from kubernetes import client, config, watch

def create_storage_class(api_instance):
    sc_manifest = {
        "apiVersion": "storage.k8s.io/v1",
        "kind": "StorageClass",
        "metadata": {"name": "my-pv-claim"},
        "provisioner": "kubernetes.io/azure-file",
        "parameters": {"skuName": "Standard_LRS"},
        "reclaimPolicy": "Delete",
        "volumeBindingMode": "Immediate"
    }
    try:
        api_instance.create_storage_class(sc_manifest)
        print("StorageClass Â∑≤ÂàõÂª∫„ÄÇ")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("StorageClass Â∑≤Â≠òÂú®„ÄÇ")
        else:
            print("ÂàõÂª∫ StorageClass Êó∂Âá∫ÈîôÔºö", e)
            traceback.print_exc()

def create_pvc(api_instance, namespace="default"):
    pvc_manifest = {
        "apiVersion": "v1",
        "kind": "PersistentVolumeClaim",
        "metadata": {"name": "test-pvc"},
        "spec": {
            "accessModes": ["ReadWriteOnce"],
            "resources": {"requests": {"storage": "1Gi"}},
            "storageClassName": "my-pv-claim"
        }
    }
    try:
        api_instance.create_namespaced_persistent_volume_claim(namespace, pvc_manifest)
        print("PVC Â∑≤ÂàõÂª∫„ÄÇ")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("PVC Â∑≤Â≠òÂú®„ÄÇ")
        else:
            print("ÂàõÂª∫ PVC Êó∂Âá∫ÈîôÔºö", e)
            traceback.print_exc()

def create_pod(api_instance, namespace="default"):
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": "test-pod"},
        "spec": {
            "containers": [{
                "name": "tester",
                "image": "nginx",  # ‰ΩøÁî®ËΩªÈáèÁ∫ßÈïúÂÉè
                "volumeMounts": [{"mountPath": "/mnt", "name": "test-volume"}]
            }],
            "volumes": [{
                "name": "test-volume",
                "persistentVolumeClaim": {"claimName": "test-pvc"}
            }]
        }
    }
    try:
        api_instance.create_namespaced_pod(namespace, pod_manifest)
        print("Pod Â∑≤ÂàõÂª∫„ÄÇ")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("Pod Â∑≤Â≠òÂú®„ÄÇ")
        else:
            print("ÂàõÂª∫ Pod Êó∂Âá∫ÈîôÔºö", e)
            traceback.print_exc()

def monitor_pod_events(namespace="default", pod_name="test-pod"):
    config.load_kube_config()
    v1 = client.CoreV1Api()
    w = watch.Watch()
    print("ÂºÄÂßãÁõëÊéß Pod ‰∫ã‰ª∂ÔºåÂ¶ÇÊåÇËΩΩÂ§±Ë¥•Â∞ÜËÆ∞ÂΩïÈîôËØØ‰ø°ÊÅØ...")
    try:
        for event in w.stream(v1.list_namespaced_event, namespace=namespace, timeout_seconds=120):
            involved_object = event["object"].involved_object
            if involved_object.kind == "Pod" and involved_object.name == pod_name:
                print(f"‰∫ã‰ª∂Á±ªÂûã: {event['type']}, ‰ø°ÊÅØ: {event['object'].message}")
    except Exception as e:
        print("ÁõëÊéß‰∫ã‰ª∂Êó∂ÂèëÁîüÂºÇÂ∏∏Ôºö", e)

def main():
    try:
        config.load_kube_config()
        storage_api = client.StorageV1Api()
        core_api = client.CoreV1Api()
    except Exception as e:
        print("Âä†ËΩΩ kubeconfig Â§±Ë¥•ÔºåËØ∑Á°ÆËÆ§ÈªòËÆ§‰ΩçÁΩÆÂ≠òÂú® kubeconfig Êñá‰ª∂„ÄÇ")
        return

    # ÂàõÂª∫ StorageClassÔºåPVC Âíå Pod
    create_storage_class(storage_api)
    time.sleep(2)
    create_pvc(core_api)
    time.sleep(2)
    create_pod(core_api)

    # ÁõëÊéß Pod ÁöÑ‰∫ã‰ª∂ÔºåÂà§Êñ≠ÊòØÂê¶ÊúâÁ±ª‰ººÈîôËØØ‰ø°ÊÅØÂá∫Áé∞
    monitor_pod_events()

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨‰ΩøÁî® Kubernetes Python ÂÆ¢Êà∑Á´Ø‰ªéÈªòËÆ§‰ΩçÁΩÆÂä†ËΩΩ kubeconfigÔºåÂÅáËÆæÁéØÂ¢ÉÂ∑≤ÁªèÈÖçÁΩÆÂ•ΩÂØπÈõÜÁæ§ÁöÑËÆøÈóÆÊùÉÈôê„ÄÇ
2. ËÑöÊú¨È¶ñÂÖàÈÄöËøá StorageV1Api ÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫ my-pv-claim ÁöÑ StorageClassÔºåÂÖ∂ provisioner ‰∏∫ "kubernetes.io/azure-file"Ôºå‰ª•ÂèäÁõ∏ÂÖ≥ÂèÇÊï∞ÔºåËøô‰∏é issue ‰∏≠ÁöÑÈÖçÁΩÆÁ±ª‰ºº„ÄÇ
3. Êé•ÁùÄÈÄöËøá CoreV1Api ÂàõÂª∫‰∏Ä‰∏™ PVC Âπ∂ÁªëÂÆöÂà∞ËØ• StorageClassÔºåÁÑ∂ÂêéÂàõÂª∫‰∏Ä‰∏™ PodÔºåÂπ∂Â∞Ü‰πãÂâçÂàõÂª∫ÁöÑ PVC ÊåÇËΩΩÂà∞ÂÆπÂô®ÂÜÖÁöÑ /mnt ÁõÆÂΩï„ÄÇ
4. ÊúÄÂêé‰ΩøÁî® watch ÁõëÊéßËØ• Pod Áõ∏ÂÖ≥ÁöÑ‰∫ã‰ª∂ÔºåÂ¶ÇÊåÇËΩΩËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØÔºà‰æãÂ¶ÇÂá∫Áé∞‰∏é issue ‰∏≠Á±ª‰ººÁöÑÈîôËØØ‰ø°ÊÅØÔºâÔºå‰ºöÂú®ÁªàÁ´ØÊâìÂç∞Âá∫Êù•„ÄÇ
5. Ê≥®ÊÑèÔºöÁî±‰∫éËØ•ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØÁî±‰∫é CSI ËøÅÁßª‰∏≠ Azure File Êèê‰æõÁ®ãÂ∫èÁöÑÈÖçÁΩÆÈóÆÈ¢òÂºïËµ∑ÁöÑÔºåÂπ∂‰∏ç‰ºöÂØºËá¥ÂÆâÂÖ®ÊºèÊ¥ûÔºåÊú¨ËÑöÊú¨‰ªÖÁî®‰∫éÂ§çÁé∞Â≠òÂÇ®ÊåÇËΩΩÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºå‰ª•‰æøÈ™åËØÅÊòØÂê¶Â≠òÂú®ÂêåÊ†∑ÁöÑÊåÇËΩΩÈîôËØØÔºå‰∏çÂ≠òÂú®ÂÆâÂÖ®È£éÈô©„ÄÇ

---


## Issue #129925 Deleting and recreating a DaemonSet in a short period can cause both the old and new Pods to coexist simultaneously, which may lead to data corruption

- Issue ÈìæÊé•Ôºö[#129925](https://github.com/kubernetes/kubernetes/issues/129925)

### Issue ÂÜÖÂÆπ

#### What happened?

When performing `kubectl delete -f <daemonset-file-name>` followed by `kubectl create  -f <daemonset-file-name` with the same DaemonSet definition, if a new Pod from the newly created DaemonSet starts before the Pod from the old DaemonSet is terminated, the Pods from the new and old DaemonSets with the same definition may coexist.

And in cases where the Pod mounts storage, as shown in the example, this results in both Pods simultaneously mounting the same volume, potentially leading to data corruption.

#### What did you expect to happen?

We want to ensure that a single node does not have multiple Pods created from the same DaemonSet definition simultaneously. 

The cause of the above issue is that the current kubectl delete command, by default, deletes the DaemonSet itself first, followed by its Pods. This allows a new DaemonSet to be created immediately after the old DaemonSet is deleted, and the Pods of the new DaemonSet can also be created shortly thereafter. As a result, two Pods from the same DaemonSet definition can exist simultaneously.

We can work this out by using some kubectl delete options (for example specify --cascade=foreground).

But we would like to avoid this situation by using the ‚Äúdefault options‚Äù of kubectl delete. Making general users specify options every time is easy to make a mistake.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy DaemonSet with the following procedure

```
$ kubectl create namespace test-daemonset
namespace/test-daemonset created
$ cat test-daemonset.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: test
  name: test
  namespace: test-daemonset
spec:
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - command: ["/bin/sh", "-c"]
        args:
        - |
          #!/bin/sh
          set -u

          while :
          do
            echo "$(hostname): $(date)" >> /tmp/test/data.log
            sleep 1
          done
        image: busybox:latest
        imagePullPolicy: Always
        name: cli
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /tmp/test
          name: hostdir
          readOnly: false
      volumes:
      - name: hostdir
        hostPath:
          path: /tmp/test
          type: DirectoryOrCreate
      nodeSelector:
        node-role.kubernetes.io/worker: "" 

$ kubectl create -f test-daemonset.yaml
daemonset.apps/test created

$ kubectl get pods -n test-daemonset -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP            NODE                 NOMINATED NODE   READINESS GATES
test-2ljhz   1/1     Running   0          4s    10.244.0.20   kind-control-plane   <none>           <none>

$ kubectl exec -it test-2ljhz -n test-daemonset -- /bin/sh
/ # cat /tmp/test/data.log
test-2ljhz: Tue Jan 28 08:34:37 UTC 2025
test-2ljhz: Tue Jan 28 08:34:38 UTC 2025
test-2ljhz: Tue Jan 28 08:34:39 UTC 2025
test-2ljhz: Tue Jan 28 08:34:40 UTC 2025
test-2ljhz: Tue Jan 28 08:34:41 UTC 2025
test-2ljhz: Tue Jan 28 08:34:42 UTC 2025
test-2ljhz: Tue Jan 28 08:34:43 UTC 2025
test-2ljhz: Tue Jan 28 08:34:44 UTC 2025
```

2. Delete and recreate a DaemonSet in a short period
```
$ kubectl delete -f test-daemonset.yaml; kubectl create -f test-daemonset.yaml
daemonset.apps "test" deleted
daemonset.apps/test created
```

After that, you can observe two Pods on a single node
```
$ kubectl get pods -n test-daemonset -o wide
NAME         READY   STATUS        RESTARTS   AGE    IP            NODE                 NOMINATED NODE   READINESS GATES
test-2ljhz   1/1     Terminating   0          2m8s   10.244.0.20   kind-control-plane   <none>           <none>
test-km8zl   1/1     Running       0          18s    10.244.0.21   kind-control-plane   <none>           <none>
```
And both Pods are using the same volume 
```
$ kubectl exec -it test-km8zl -n test-daemonset -- /bin/sh
/ # cat /tmp/test/data.log
test-2ljhz: Tue Jan 28 08:35:38 UTC 2025
test-km8zl: Tue Jan 28 08:35:39 UTC 2025
test-2ljhz: Tue Jan 28 08:35:39 UTC 2025
test-km8zl: Tue Jan 28 08:35:40 UTC 2025
test-2ljhz: Tue Jan 28 08:35:40 UTC 2025
test-km8zl: Tue Jan 28 08:35:41 UTC 2025
test-2ljhz: Tue Jan 28 08:35:41 UTC 2025
test-km8zl: Tue Jan 28 08:35:42 UTC 2025
```


#### Anything else we need to know?

We actually observed the above issue using an operator rather than the CLI. 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú®Áü≠Êó∂Èó¥ÂÜÖÂà†Èô§Âπ∂ÈáçÊñ∞ÂàõÂª∫Âêå‰∏Ä‰∏™ DaemonSet ÂèØËÉΩÂØºËá¥ËÄÅ Pod ‰∏éÊñ∞ Pod ÂêåÊó∂Â≠òÂú®ÔºåÂπ∂‰∏îÂ¶ÇÊûúËøô‰∫õ Pod ÊåÇËΩΩ‰∫ÜÁõ∏ÂêåÁöÑÂ≠òÂÇ®Âç∑ÔºåÂàô‰ºöÂá∫Áé∞Êï∞ÊçÆÂÜ≤Á™ÅÔºåÁîöËá≥Êï∞ÊçÆÊçüÂùèÁöÑÈóÆÈ¢ò„ÄÇÈóÆÈ¢òÂéüÂõ†Âú®‰∫éÈªòËÆ§‰ΩøÁî®ÁöÑ ‚Äúkubectl delete‚Äù Êìç‰Ωú‰ºöÂÖàÂà†Èô§ DaemonSet ÂØπË±°ÔºåÂÜçÂºÇÊ≠•Âà†Èô§ÂÖ∂ PodÔºå‰ªéËÄåÂØºËá¥Êñ∞ DaemonSet Âú®Êóß Pod Ë¢´ÂÆåÂÖ®Ê∏ÖÁêÜÂâçÂ∞±ÂºÄÂßãË∞ÉÂ∫¶Êñ∞ Pod„ÄÇËØ•ÈóÆÈ¢òËôΩÁÑ∂‰ºöÂºïËµ∑Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÈóÆÈ¢òÂíåÊΩúÂú®ÁöÑÊï∞ÊçÆÊçüÂùèÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊ∂âÂèäÊú™ÊéàÊùÉËÆøÈóÆ„ÄÅ‰ª£Á†ÅÊ≥®ÂÖ•„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®Á±ªÊºèÊ¥ûÔºåÊõ¥Â§öÁöÑÊòØ‰∏ÄÁßçËøêÁª¥Êìç‰Ωú‰∏äÁöÑÁ´ûÊÄÅÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ¥ÊòéÔºöËØ•Â§çÁé∞ËÑöÊú¨‰∏ªË¶ÅÁî®‰∫éÊ®°ÊãüÊìç‰ΩúÊµÅÁ®ãÔºåÂπ∂ÊâìÂç∞Âá∫Ê®°ÊãüÁöÑ‰ø°ÊÅØÔºå
      Âõ†‰∏∫ËØ• Issue ‰∏çÁõ¥Êé•Ê∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÊâÄ‰ª•‰∏çÊèê‰æõÁúüÊ≠£Âà©Áî®ÂÆâÂÖ®ÊºèÊ¥ûÁöÑPOC‰ª£Á†ÅÔºå
      ËÑöÊú¨‰ªÖ‰æõ‰∫ÜËß£Êìç‰ΩúÊµÅÁ®ãÂíåÂ§çÁé∞Áé∞Ë±°Êó∂ÂèÇËÄÉ‰ΩøÁî®„ÄÇ
"""

import subprocess
import time

def run_command(cmd):
    print(f"ÊâßË°åÂëΩ‰ª§: {cmd}")
    proc = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if proc.stdout:
        print("Ê†áÂáÜËæìÂá∫:")
        print(proc.stdout)
    if proc.stderr:
        print("ÈîôËØØËæìÂá∫:")
        print(proc.stderr)
    return proc.returncode

def create_namespace(namespace):
    run_command(f"kubectl create namespace {namespace}")

def apply_daemonset(file_path):
    run_command(f"kubectl create -f {file_path}")

def delete_daemonset(file_path):
    run_command(f"kubectl delete -f {file_path}")

def get_pods(namespace):
    run_command(f"kubectl get pods -n {namespace} -o wide")

def main():
    namespace = "test-daemonset"
    daemonset_yaml = "test-daemonset.yaml"

    print("=== Step 1: ÂàõÂª∫ÂëΩÂêçÁ©∫Èó¥Âíå DaemonSet ===")
    create_namespace(namespace)
    apply_daemonset(daemonset_yaml)
    time.sleep(5)
    get_pods(namespace)

    print("\n=== Step 2: Áü≠Êó∂Èó¥ÂÜÖÂà†Èô§Âπ∂ÈáçÊñ∞ÂàõÂª∫ DaemonSet ===")
    delete_daemonset(daemonset_yaml)
    # Ê≠§Â§Ñ‰∏çÁ≠âÂæÖÊóß Pod ÂÆåÂÖ®ÁªàÊ≠¢ÔºåÊ®°ÊãüÁü≠Êó∂Èó¥ÂÜÖÈáçÊñ∞ÂàõÂª∫ÁöÑÂú∫ÊôØ
    time.sleep(2)
    apply_daemonset(daemonset_yaml)
    time.sleep(5)
    get_pods(namespace)

if __name__ == "__main__":
    main()
```


---


## Issue #129890 Server-side apply can cause silent failure when attempting to remove an immutable field that is managed

- Issue ÈìæÊé•Ôºö[#129890](https://github.com/kubernetes/kubernetes/issues/129890)

### Issue ÂÜÖÂÆπ

#### What happened?

Steps to reproduce:
- using FluxCD for gitops
- modify existing Service, which has `clusterIP: none`, by removing `clusterIP`
- clusterIP is an immutable field, and changing (incl. removing) it should cause a failure, which should then cause FluxCD to replace the resource
- but when the field is managed by FluxCD, server-side apply will fail silently, and FluxCD will not see any failure. (the exact details are unclear to me)

Why this is relevant:
- developers cannot be expected to know which fields are immutable
- silent failure can cause problems to be undiscovered
- without an error message, it is hard to understand why this behavior is happening
- manual intervention is required, which in turn can cause a buildup of issues over time that are even more complex to debug
- gitops must ensure correct reconciliation between desired and actual state, and report any differences
- when managing large number of resources, manual intervention is untenable

Related issue:
- https://github.com/fluxcd/flux2/issues/5162#issuecomment-2621289274



#### What did you expect to happen?

.

#### How can we reproduce it (as minimally and precisely as possible)?

.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ•issueÊèèËø∞ÁöÑÊòØÂú®‰ΩøÁî®FluxCDËøõË°ågitopsÁÆ°ÁêÜÊó∂ÔºåÂΩìÂ∞ùËØïÈÄöËøáserver‚Äêside apply‰øÆÊîπ‰∏Ä‰∏™ÂåÖÂê´‰∏çÂèØÂèòÂ≠óÊÆµÔºà‰æãÂ¶ÇServiceÁöÑclusterIPÂ≠óÊÆµÔºâÁöÑËµÑÊ∫êÈÖçÁΩÆÊó∂Ôºå‰ºöÂá∫Áé∞ÈùôÈªòÂ§±Ë¥•ÁöÑÈóÆÈ¢ò„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂΩìÁî®Êà∑ËØïÂõæÂà†Èô§Êàñ‰øÆÊîπ‰∏Ä‰∏™Âèó‰∏çÂèØÂèòÈôêÂà∂ÁöÑÂ≠óÊÆµÊó∂ÔºåÊú¨Â∫îËøîÂõûÈîôËØØ‰ΩÜÊòØÂç¥Ê≤°ÊúâÊä•Ë≠¶ÔºåÂØºËá¥FluxCDÊó†Ê≥ïÊ£ÄÊµãÂà∞ÂèòÊõ¥Â§±Ë¥•Ôºå‰ªéËÄåÂèØËÉΩÂØºËá¥ÂÆûÈôÖËµÑÊ∫ê‰∏éÊúüÊúõÈÖçÁΩÆ‰∏ç‰∏ÄËá¥„ÄÇËØ•ÈóÆÈ¢òÊú¨Ë¥®‰∏äÂ±û‰∫éÊìç‰ΩúÊàñÈÖçÁΩÆ‰∏äÁöÑÈóÆÈ¢òÔºåÂèØËÉΩÂØºËá¥ÈÖçÁΩÆÊºÇÁßªÂíåÊâãÂä®Âπ≤È¢ÑÔºå‰ΩÜÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂºïËµ∑Êú™ÊéàÊùÉËÆøÈóÆ„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°åÊàñÂÖ∂‰ªñ‰∏•ÈáçÁöÑÂÆâÂÖ®ÂΩ±Âìç„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ëøô‰∏™ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÂú®Kubernetes‰∏≠‰ΩøÁî®kubectl server-side applyÊó∂Ôºå
Â∞ùËØïÁßªÈô§‰∏Ä‰∏™‰∏çÂèØÂèòÁöÑÂ≠óÊÆµ (ÊØîÂ¶ÇServiceÁöÑclusterIP) ÂèØËÉΩÂØºËá¥ÈùôÈªòÂ§±Ë¥•ÁöÑÊÉÖÂÜµ„ÄÇ
ËØ∑Ê≥®ÊÑèÔºåËøô‰∏™ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥ûÂà©Áî®‰ª£Á†Å„ÄÇ

ÂâçÊèêÊù°‰ª∂Ôºö
1. ÈúÄË¶ÅÂú®ÈªòËÆ§‰ΩçÁΩÆÂ≠òÂú®kubeconfigÊñá‰ª∂Ôºà~/.kube/configÔºâ„ÄÇ
2. ÁéØÂ¢É‰∏≠Â∑≤ÂÆâË£ÖkubectlÂπ∂ÈÖçÁΩÆ‰∫ÜÂØπKubernetesÈõÜÁæ§ÁöÑËÆøÈóÆÊùÉÈôê„ÄÇ
"""

import os
import subprocess
import time

def simulate_server_side_apply():
    # ÂÆö‰πâ‰∏§‰∏™yamlÈÖçÁΩÆÊñá‰ª∂ÂêçÁß∞
    service_yaml = "service.yaml"
    patched_yaml = "service_patch.yaml"

    # ÂàùÂßãÂàõÂª∫‰∏Ä‰∏™ServiceËµÑÊ∫êÔºåÂåÖÂê´‰∏çÂèØÂèòÂ≠óÊÆµclusterIP: "None"
    service_definition = """
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  clusterIP: "None"
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
"""
    with open(service_yaml, "w") as f:
        f.write(service_definition)

    try:
        print("[*] Ê≠£Âú®ÂàõÂª∫ServiceËµÑÊ∫ê...")
        subprocess.run(["kubectl", "apply", "-f", service_yaml], check=True)
        print("[+] ServiceËµÑÊ∫êÂàõÂª∫ÊàêÂäü")
    except subprocess.CalledProcessError as e:
        print("[-] ÂàõÂª∫ServiceËµÑÊ∫êÂ§±Ë¥•:", e)
        return

    # Á≠âÂæÖ‰∏Ä‰ºöÁ°Æ‰øùËµÑÊ∫êÂàõÂª∫ÂÆåÊàê
    time.sleep(2)

    # ‰øÆÊîπÈÖçÁΩÆÔºöÂ∞ùËØïÁßªÈô§clusterIPÂ≠óÊÆµ
    patched_definition = """
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
"""
    with open(patched_yaml, "w") as f:
        f.write(patched_definition)

    try:
        print("[*] Ê≠£Âú®‰ΩøÁî® server-side apply ÁßªÈô§‰∏çÂèØÂèòÂ≠óÊÆµ clusterIP...")
        subprocess.run(["kubectl", "apply", "--server-side", "-f", patched_yaml], check=True)
        print("[+] Â∞ùËØïÁßªÈô§clusterIPÂ∑≤ÊâßË°åÔºàÈ¢ÑÊúüÂ∫îÂΩìÊî∂Âà∞ÈîôËØØÊèêÁ§∫Ôºå‰ΩÜÂèØËÉΩÈùôÈªòÂ§±Ë¥•Ôºâ")
    except subprocess.CalledProcessError as e:
        print("[-] ‰øÆÊîπServiceËµÑÊ∫êÂ§±Ë¥•, ÈîôËØØËØ¶ÊÉÖ:", e)
        return

    # Ëé∑ÂèñÂΩìÂâçServiceÁöÑÈÖçÁΩÆÔºåÊ£ÄÊü•clusterIPÂ≠óÊÆµÊòØÂê¶‰æùÁÑ∂Â≠òÂú®
    try:
        result = subprocess.run(["kubectl", "get", "service", "my-service", "-o", "yaml"], capture_output=True, text=True, check=True)
        print("[*] ÂΩìÂâçServiceÈÖçÁΩÆÂ¶Ç‰∏ãÔºö")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print("[-] Ëé∑ÂèñServiceËµÑÊ∫êÂ§±Ë¥•:", e)

if __name__ == "__main__":
    simulate_server_side_apply()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

‰ª•‰∏äËÑöÊú¨ÈÄöËøá‰ª•‰∏ãÊ≠•È™§Ê®°Êãü‰∫ÜËØ•ÈóÆÈ¢òÁöÑÊÉÖÂÜµÔºö
1. ‰ΩøÁî®kubectlÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫‚Äúmy-service‚ÄùÁöÑServiceËµÑÊ∫êÔºåÂÖ∂ÈÖçÁΩÆ‰∏≠ÂåÖÂê´‰∏çÂèØÂèòÂ≠óÊÆµclusterIPÔºåÂÖ∂ÂÄº‰∏∫"None"Ôºõ
2. ÊûÑÈÄ†‰∏Ä‰∏™‰øÆÊîπÂêéÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåËØ•Êñá‰ª∂Â∞ùËØïÁßªÈô§clusterIPÂ≠óÊÆµÔºåÂπ∂ÈÄöËøákubectlÁöÑserver-side applyÊèê‰∫§ËØ•ÂèòÊõ¥Ôºõ
3. ÁêÜËÆ∫‰∏äÔºåÁî±‰∫éclusterIP‰∏∫‰∏çÂèØÂèòÂ≠óÊÆµÔºåKubernetesÂ∫îÂΩìËøîÂõûÈîôËØØÔºå‰ΩÜÂú®fluxcdÁÆ°ÁêÜ‰∏ãÂèØËÉΩÂá∫Áé∞ÈùôÈªòÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºõ
4. ÊúÄÂêéÂÜçÊ¨°Ëé∑ÂèñServiceËµÑÊ∫êÁöÑÈÖçÁΩÆ‰ª•Ê£ÄÊü•clusterIPÂ≠óÊÆµÊòØÂê¶‰æùÁÑ∂Â≠òÂú®„ÄÇ

ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÁéØÂ¢ÉÔºåÈ™åËØÅÂú®‰øÆÊîπ‰∏çÂèØÂèòÂ≠óÊÆµÊó∂ÊòØÂê¶Â≠òÂú®ÈùôÈªòÂ§±Ë¥•ÁöÑÈóÆÈ¢òÔºåËÄåËØ•ÈóÆÈ¢òÊú¨Ë∫´Âπ∂‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥ûÔºå‰πü‰∏ç‰ºöÂØºËá¥Êú™ÁªèÊéàÊùÉÁöÑËÆøÈóÆÊàñÊèêÊùÉÁ≠âÈ£éÈô©„ÄÇ

---


## Issue #129880 Pods Pending when loaded via file, omit resources in the spec and LimitRange is present

- Issue ÈìæÊé•Ôºö[#129880](https://github.com/kubernetes/kubernetes/issues/129880)

### Issue ÂÜÖÂÆπ

#### What happened?

We deploy control-plane components as Pod manifests for Kubelet to load. Here is an example of our APIServer Pod manifest: https://github.com/utilitywarehouse/tf_kube_ignition/blob/master/resources/kube-apiserver.yaml

As you can see it *does not* specify any resource requests/limits.

All our Namespaces have LimitRange like this:

```
apiVersion: v1
kind: LimitRange
metadata:
  name: kube-system
spec:
  limits:
    - default:
        cpu: 7000m
        ephemeral-storage: 1Gi
      defaultRequest:
        cpu: 0m
        ephemeral-storage: 0
      type: Container
```

Starting with v1.32 - these Pods no longer start, they stay `Pending` and the error is:

```
failed to patch status
...
Pod "kube-scheduler-master-1.exp-1.merit.uw.systems" is invalid: status.qosClass: Invalid value: "BestEffort": field is immutable
```

If we add resource requests+limits to the Pod definition, Pods successfully start. If we remove LimitRange from the Namespace, the Pods successfully start.

We suspect this issue is related to https://github.com/kubernetes/kubernetes/pull/127744 and https://github.com/kubernetes/kubernetes/pull/128083 .

We are going to specify resources in all our static Pod definitions, its the right thing to do for us, but this seems like incorrect behavior that was worth a Github Issue.

#### What did you expect to happen?

Pods that do not specify resource requests / limits are able to start in a Namespace with a LimitRange.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a static Pod definition that is loaded by Kubelet, omit specifying resources.
Create a LimitRange that specifies cpu and memory.
Running v1.32 Pod should stay `Pending`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
on-prem VM
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Flatcar Container Linux by Kinvolk"
ID=flatcar
ID_LIKE=coreos
VERSION=4081.2.1
VERSION_ID=4081.2.1
BUILD_ID=2024-12-12-2346
SYSEXT_LEVEL=1.0
PRETTY_NAME="Flatcar Container Linux by Kinvolk 4081.2.1 (Oklo)"
ANSI_COLOR="38;5;75"
HOME_URL="https://flatcar.org/"
BUG_REPORT_URL="https://issues.flatcar.org"
FLATCAR_BOARD="amd64-usr"
CPE_NAME="cpe:2.3:o:flatcar-linux:flatcar_linux:4081.2.1:*:*:*:*:*:*:*"

$ uname -a
Linux master-0.exp-1.merit.uw.systems 6.6.65-flatcar #1 SMP PREEMPT_DYNAMIC Thu Dec 12 23:15:00 -00 2024 x86_64 Intel(R) Xeon(R) CPU D-1548 @ 2.00GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
Terraform module: https://github.com/utilitywarehouse/tf_kube_ignition/tree/master
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.7.21
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫ÜÂú® Kubernetes v1.32 ‰∏≠ÔºåÂΩìÈÄöËøáÊñá‰ª∂Âä†ËΩΩÈùôÊÄÅ Pod ‰∏îÂú® Pod ËßÑËåÉ‰∏≠Ê≤°ÊúâÊåáÂÆöËµÑÊ∫êËØ∑Ê±Ç/ÈôêÂà∂Êó∂ÔºåÂú®Â≠òÂú® LimitRange ÁöÑÂëΩÂêçÁ©∫Èó¥‰∏≠ Pod Êó†Ê≥ïÂêØÂä®ÔºåÂπ∂Êä•Èîô‚Äústatus.qosClass: Invalid value: 'BestEffort': field is immutable‚Äù„ÄÇÊ†πÊçÆÊèèËø∞ÔºåËØ•ÈóÆÈ¢ò‰∏é Kubernetes ÂØπ LimitRange Ë°å‰∏∫ÁöÑÂèòÊõ¥ÊúâÂÖ≥ÔºåÂπ∂ÈùûÊ∂âÂèäÊùÉÈôêÁªïËøá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®ÊºèÊ¥û„ÄÇ  
Ê≠§Â§ñÔºåËØ•ÈóÆÈ¢òÂΩ±ÂìçÁöÑÊòØÈùôÊÄÅ Pod ÁöÑË∞ÉÂ∫¶ÂíåÁä∂ÊÄÅÊõ¥Êñ∞ÈóÆÈ¢òÔºå‰∏çÂ±û‰∫éËøúÁ®ãÊîªÂáªËÄÖÂèØÂà©Áî®ÁöÑÊºèÊ¥ûÔºõ‰πü‰∏çÂ≠òÂú®ÊîªÂáªËÄÖÂà©Áî®Êèê‰∫§ÁöÑÊïèÊÑü‰ø°ÊÅØ„ÄÅ‰∏çÂΩìÊìç‰ΩúÊàñÈÖçÁΩÆÂ§±ËØØÊù•ËøõË°åËøõ‰∏ÄÊ≠•Á†¥ÂùèÁöÑÈ£éÈô©„ÄÇÂõ†Ê≠§Ôºå‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËøô‰∏™ Issue Êú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™‰∫ßÂìÅÁº∫Èô∑ÊàñÈÖçÁΩÆÈÅóÊºèÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ¥ÊòéÔºö
Ê≠§ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÈùôÊÄÅ Pod ÈÉ®ÁΩ≤Êó∂ÈÅóÊºèËµÑÊ∫êÈÖçÁΩÆÔºåÂπ∂Âú®Â≠òÂú® LimitRange ÁöÑÂëΩÂêçÁ©∫Èó¥‰∏≠Ê£ÄÊü• Pod Áä∂ÊÄÅÔºå
Â§çÁé∞ Kubernetes v1.32 ‰∏≠Áõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÁé∞Ë±°„ÄÇÁî±‰∫éËØ•ÈóÆÈ¢ò‰∏çÂ±û‰∫éÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Ê≠§ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå
Âπ∂‰∏çÂåÖÂê´‰ªª‰ΩïÂà©Áî®‰ª£Á†Å„ÄÇ
"""

import subprocess
import time
import yaml
import http.server
import socketserver
import threading

# HTTPÊúçÂä°Âô®ÔºåÁî®‰∫éÊ®°ÊãüÁéØÂ¢É‰∏≠ÂØπÂ§ñÊèê‰æõHTTPÊé•Âè£ÔºàÊ†πÊçÆÈúÄË¶ÅÔºâ
class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Static Pod test server running on 8080")

def start_http_server():
    handler = SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", 8080), handler) as httpd:
        print("HTTP Server serving at port 8080")
        httpd.serve_forever()

def deploy_limitrange(namespace):
    # ÂàõÂª∫ LimitRange YAML
    limitrange = {
        "apiVersion": "v1",
        "kind": "LimitRange",
        "metadata": {
            "name": "kube-system",
            "namespace": namespace
        },
        "spec": {
            "limits": [
                {
                    "default": {
                        "cpu": "7000m",
                        "ephemeral-storage": "1Gi"
                    },
                    "defaultRequest": {
                        "cpu": "0m",
                        "ephemeral-storage": "0"
                    },
                    "type": "Container"
                }
            ]
        }
    }
    with open("limitrange.yaml", "w") as f:
        yaml.dump(limitrange, f)
    print("Applying LimitRange ...")
    subprocess.run(["kubectl", "apply", "-f", "limitrange.yaml"], check=True)

def deploy_static_pod(pod_manifest_file):
    print(f"Applying static Pod manifest from {pod_manifest_file} ...")
    subprocess.run(["kubectl", "apply", "-f", pod_manifest_file], check=True)

def get_pod_status(namespace, pod_name):
    try:
        out = subprocess.check_output(["kubectl", "get", "pod", pod_name, "-n", namespace, "-o", "yaml"])
        pod = yaml.safe_load(out)
        return pod.get("status", {}).get("phase", "Unknown")
    except subprocess.CalledProcessError as e:
        return f"Error: {str(e)}"

def main():
    # ÂêØÂä®‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºàÁõëÂê¨Âú®8080Á´ØÂè£Ôºâ
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    namespace = "default"
    pod_manifest_file = "static-pod.yaml"
    pod_name = "static-pod-test"

    # ÂáÜÂ§á‰∏Ä‰∏™ÈùôÊÄÅ Pod manifestÔºåÊïÖÊÑèÁúÅÁï•ËµÑÊ∫êËØ∑Ê±ÇÂíåÈôêÂà∂Â≠óÊÆµ
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
            "namespace": namespace
        },
        "spec": {
            "containers": [
                {
                    "name": "pause",
                    "image": "k8s.gcr.io/pause:3.9"
                    # Ê≥®ÊÑèÔºöÊ≠§Â§ÑÊïÖÊÑè‰∏çËÆæÁΩÆresources
                }
            ]
        }
    }
    with open(pod_manifest_file, "w") as f:
        yaml.dump(pod_manifest, f)
    print(f"Static Pod manifest written to {pod_manifest_file}")

    # ÈÉ®ÁΩ≤ LimitRangeÔºåÊ®°ÊãüÁéØÂ¢É‰∏≠ÊúâÈªòËÆ§ËµÑÊ∫êÈôêÂà∂ÈÖçÁΩÆ
    deploy_limitrange(namespace)

    # ÈÉ®ÁΩ≤ÈùôÊÄÅ Pod
    deploy_static_pod(pod_manifest_file)

    # Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ£ÄÊµã Pod Áä∂ÊÄÅ
    print("Á≠âÂæÖ 20 ÁßíÂêéÊ£ÄÊü• Pod Áä∂ÊÄÅ ...")
    time.sleep(20)
    status = get_pod_status(namespace, pod_name)
    print(f"Pod {pod_name} Áä∂ÊÄÅÔºö{status}")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨È¶ñÂÖàÂêØÂä®‰∫Ü‰∏Ä‰∏™Êú¨Âú∞ HTTP ÊúçÂä°Âô®ÔºåËØ•ÊúçÂä°Âô®ÁõëÂê¨Âú® 8080 Á´ØÂè£Ôºå‰ªÖÁî®‰∫éÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±ÇÊ®°Êãü HTTP ÊúçÂä°Âô®ÁöÑÁéØÂ¢ÉÔºàÂπ∂ÈùûÊºèÊ¥ûÂà©Áî®Ê≠•È™§ÁöÑ‰∏ÄÈÉ®ÂàÜÔºâ„ÄÇ
2. deploy_limitrange() ÂáΩÊï∞ÈÄöËøáÁîüÊàêÂπ∂Â∫îÁî®‰∏Ä‰∏™ YAML Êñá‰ª∂Êù•Âú®ÊåáÂÆöÂëΩÂêçÁ©∫Èó¥‰∏≠ÂàõÂª∫‰∏Ä‰∏™ LimitRange ÂØπË±°ÔºåÊ®°Êãü Kubernetes ÁéØÂ¢É‰∏≠Â≠òÂú®ÈªòËÆ§ÁöÑËµÑÊ∫êÈôêÂà∂ÈÖçÁΩÆ„ÄÇ
3. deploy_static_pod() ÂáΩÊï∞ÁîüÊàê‰∏Ä‰∏™ÈùôÊÄÅ Pod ÁöÑ YAML Ê∏ÖÂçïÔºåËØ• Pod ÊïÖÊÑèÁúÅÁï•‰∫ÜÂÆπÂô®ÁöÑËµÑÊ∫êËØ∑Ê±ÇÂíåÈôêÂà∂Â≠óÊÆµÔºåÁ¨¶Âêà Issue ‰∏≠ÊèèËø∞ÁöÑÊÉÖÂÜµ„ÄÇ
4. ËÑöÊú¨Â∫îÁî®‰∫ÜÁîüÊàêÁöÑ YAML Ê∏ÖÂçïÔºåÂπ∂Âú®‰∏ÄÊÆµÁ≠âÂæÖÊó∂Èó¥ÂêéÔºåÈÄöËøáË∞ÉÁî® kubectl ÂëΩ‰ª§Ëé∑Âèñ Pod ÁöÑÁä∂ÊÄÅÔºå‰ªéËÄåËßÇÂØüÊòØÂê¶Áî±‰∫éËµÑÊ∫êËÆæÁΩÆÈóÆÈ¢òÂØºËá¥ Pod ‰∏ÄÁõ¥Â§Ñ‰∫é Pending Áä∂ÊÄÅ„ÄÇ
5. Áî±‰∫éËØ•ÈóÆÈ¢òÂ±û‰∫é Kubernetes ÁöÑË°å‰∏∫ÂèòÂåñËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåËÑöÊú¨‰ªÖÁî®‰∫éÂ§çÁé∞ËØ•Áé∞Ë±°ÔºåÂπ∂‰∏çÊèê‰æõ‰ªª‰ΩïÈ´òÈ£éÈô©Âà©Áî®Ë∑ØÂæÑ„ÄÇ

---


## Issue #129866 HPA wrongly assumes that terminated pods have an utilization of 100%

- Issue ÈìæÊé•Ôºö[#129866](https://github.com/kubernetes/kubernetes/issues/129866)

### Issue ÂÜÖÂÆπ

#### What happened?

A pod that terminated was considered by the HPA controller to be at its target utilization.

The controller logic ([1](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L106-L120), [2](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L211-L223)) is such that, while scaling up, it conservatively considers that a pod for which we couldn't get the utilization metric from a metrics API are at their target utilization. (On scale down, the assumption is conservatively that the utilization is 0.)

#### What did you expect to happen?

I expected the controller to assume that a terminated pod has an utilization of 0.

This is [already correctly handled for pods that terminated with a failure](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L383), but the case where a pod terminated successfully isn't handled.


#### How can we reproduce it (as minimally and precisely as possible)?

Create a Deployment with pods that terminate (without a failure) and observe that an HPA targetting this Deployment will assume that terminated pods are at target utilization.

#### Anything else we need to know?

Handling the case where the pod is terminated normally [here](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L383) will fix this.

#### Kubernetes version

<details>

```console
$ kubectl version
v1.29.10-gke.1280000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
```

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
None
</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes HPAÔºàHorizontal Pod AutoscalerÔºâÂú®Â§ÑÁêÜÂ∑≤Ê≠£Â∏∏ÁªàÊ≠¢ÔºàÊ≤°ÊúâÂ§±Ë¥•ÔºâÁöÑ pod Êó∂ÔºåÈîôËØØÂú∞ÂÅáËÆæÂÖ∂Âà©Áî®Áéá‰∏∫ÁõÆÊ†áÂà©Áî®ÁéáÔºà100%ÔºâÔºå‰ªéËÄåÂèØËÉΩÂØºËá¥ÈîôËØØÁöÑÊâ©Áº©ÂÆπÂÜ≥Á≠ñ„ÄÇÈóÆÈ¢òÊú¨Ë∫´Â±û‰∫éËÆæËÆ°ÊàñÈÄªËæëÈîôËØØÔºå‰∏çÊ∂âÂèä‰ª£Á†ÅÊâßË°åÊºèÊ¥û„ÄÅÊèêÊùÉ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÆâÂÖ®ÈóÆÈ¢òÔºå‰πü‰∏çÂ≠òÂú®ÊîªÂáªËÄÖÂèØÂà©Áî®ÁöÑËß¶ÂèëÊºèÊ¥û„ÄÇÂõ†Ê≠§ËØ• Issue ‰∏éÂÆâÂÖ®È£éÈô©Êó†ÂÖ≥ÔºåÂ±û‰∫éÈùûÂÆâÂÖ®ÊÄßÁº∫Èô∑„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import time
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

DEPLOYMENT_NAME = "test-terminated-deployment"
NAMESPACE = "default"
HPA_NAME = "test-hpa"

def create_deployment(api_instance):
    # ÂÆö‰πâ‰∏Ä‰∏™podÔºåÂú®ÂêØÂä®ÂêéÈ©¨‰∏äÊ≠£Â∏∏ÈÄÄÂá∫
    container = client.V1Container(
        name="terminated-container",
        image="busybox",
        command=["/bin/sh", "-c", "echo Hello; sleep 1; exit 0"]
    )
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": DEPLOYMENT_NAME}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container])
    )
    spec = client.V1DeploymentSpec(
        replicas=3,
        selector=client.V1LabelSelector(match_labels={"app": DEPLOYMENT_NAME}),
        template=template
    )
    deployment = client.V1Deployment(
        metadata=client.V1ObjectMeta(name=DEPLOYMENT_NAME),
        spec=spec
    )
    try:
        api_response = api_instance.create_namespaced_deployment(
            namespace=NAMESPACE,
            body=deployment
        )
        print(f"Deployment {DEPLOYMENT_NAME} created.")
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)
        sys.exit(1)

def create_hpa(hpa_api):
    # ÂàõÂª∫‰∏Ä‰∏™HPAÂØπË±°ÔºåÁõÆÊ†áÊòØËøô‰∏™DeploymentÔºåËÆæÁΩÆtarget CPUÂà©Áî®Áéá‰∏∫50%
    min_replicas = 1
    max_replicas = 5
    target_metric = client.V2beta2MetricSpec(
        type="Resource",
        resource=client.V2beta2ResourceMetricSource(
            name="cpu",
            target=client.V2beta2MetricTarget(
                type="Utilization",
                average_utilization=50
            )
        )
    )
    hpa_spec = client.V2beta2HorizontalPodAutoscalerSpec(
        scale_target_ref=client.V2beta2CrossVersionObjectReference(
            api_version="apps/v1",
            kind="Deployment",
            name=DEPLOYMENT_NAME
        ),
        min_replicas=min_replicas,
        max_replicas=max_replicas,
        metrics=[target_metric]
    )
    hpa = client.V2beta2HorizontalPodAutoscaler(
        metadata=client.V1ObjectMeta(name=HPA_NAME),
        spec=hpa_spec
    )
    try:
        hpa_api.create_namespaced_horizontal_pod_autoscaler(
            namespace=NAMESPACE,
            body=hpa
        )
        print(f"HPA {HPA_NAME} created.")
    except ApiException as e:
        print("Exception when creating HPA: %s\n" % e)
        sys.exit(1)

def check_hpa_status(hpa_api):
    # Êü•ËØ¢HPAÂØπË±°Áä∂ÊÄÅÔºåÂÖ≥Ê≥®ÂΩìÂâçÁöÑÊåáÊ†á‰ø°ÊÅØ
    try:
        hpa_status = hpa_api.read_namespaced_horizontal_pod_autoscaler_status(
            name=HPA_NAME, namespace=NAMESPACE
        )
        print("HPA status:")
        print(hpa_status.status)
    except ApiException as e:
        print("Exception when reading HPA status: %s\n" % e)

def cleanup(api_instance, hpa_api):
    # Âà†Èô§ HPA Âíå Deployment ËµÑÊ∫ê
    try:
        hpa_api.delete_namespaced_horizontal_pod_autoscaler(
            name=HPA_NAME,
            namespace=NAMESPACE
        )
        print(f"HPA {HPA_NAME} deleted.")
    except ApiException as e:
        print("Exception when deleting HPA: %s\n" % e)
    try:
        api_instance.delete_namespaced_deployment(
            name=DEPLOYMENT_NAME,
            namespace=NAMESPACE
        )
        print(f"Deployment {DEPLOYMENT_NAME} deleted.")
    except ApiException as e:
        print("Exception when deleting deployment: %s\n" % e)

def main():
    # ‰ªéÈªòËÆ§ kubeconfig Âä†ËΩΩÈÖçÁΩÆ
    try:
        config.load_kube_config()
    except Exception as e:
        print("Error loading kubeconfig: %s" % e)
        sys.exit(1)

    apps_v1 = client.AppsV1Api()
    hpa_api = client.AutoscalingV2beta2Api()

    print("Creating deployment with pods that terminate normally...")
    create_deployment(apps_v1)

    print("Creating HPA targeting the deployment...")
    create_hpa(hpa_api)

    print("Waiting for pods to run and terminate...")
    time.sleep(15)

    print("Checking HPA status to observe utilization assumptions...")
    check_hpa_status(hpa_api)

    input("ÊåâÂõûËΩ¶ÈîÆËøõË°åÊ∏ÖÁêÜÂπ∂ÈÄÄÂá∫...")
    cleanup(apps_v1, hpa_api)

if __name__ == '__main__':
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨‰∏≠È¶ñÂÖàÂä†ËΩΩÈªòËÆ§‰ΩçÁΩÆÁöÑ kubeconfigÔºåËøûÊé•Âà∞ Kubernetes ÈõÜÁæ§„ÄÇ
2. ÈÄöËøá Kubernetes Python ÂÆ¢Êà∑Á´ØÂàõÂª∫‰∏Ä‰∏™ DeploymentÔºåËØ• Deployment ËøêË°åÁöÑ BusyBox ÂÆπÂô®Âú®ÂêØÂä®Âêé‰ºöÊâßË°å‚Äúecho Hello; sleep 1; exit 0‚ÄùÔºåÂç≥Ê≠£Â∏∏ÈÄÄÂá∫Ôºå‰ªéËÄåÊ®°Êãü‚ÄúÊ≠£Â∏∏ÁªàÊ≠¢‚ÄùÁöÑ pod„ÄÇ
3. ÂàõÂª∫‰∏Ä‰∏™ HPAÔºàÊ®™ÂêëPodËá™Âä®Êâ©Áº©ÂÆπÂô®ÔºâÂØπË±°ÔºåÁõÆÊ†á‰∏∫‰∏äËø∞ DeploymentÔºåÁõÆÊ†á CPU Âà©Áî®ÁéáËÆæ‰∏∫ 50%„ÄÇÁî±‰∫é pod Ê≠£Â∏∏ÁªàÊ≠¢‰∏îÊó†Ê≥ïËé∑ÂèñÂà©Áî®ÁéáÊï∞ÊçÆÔºåÊåâÁÖß Issue ÊâÄÊèèËø∞ÁöÑÈÄªËæëÔºåHPA ÊéßÂà∂Âô®ÂèØËÉΩÈîôËØØÂú∞Â∞ÜËøô‰∫õ pod ËÆ§‰∏∫ËææÂà∞‰∫ÜÁõÆÊ†áÂà©Áî®Áéá„ÄÇ
4. ËÑöÊú¨Á≠âÂæÖ‰∏ÄÂÆöÊó∂Èó¥ÂêéÊü•ËØ¢ HPA ÁöÑÁä∂ÊÄÅÔºåÁî®Êà∑ÂèØ‰ª•ËßÇÂØü HPA ÂΩìÂâçÁöÑÊåáÊ†á‰ø°ÊÅØÔºàËôΩÁÑ∂ÊåáÊ†áÊòØÂê¶ÈááÈõÜÊàêÂäü‰æùËµñ‰∫éÈõÜÁæ§ÂÜÖÁöÑ metrics-serverÔºâ„ÄÇ
5. ÊúÄÂêéÔºåËÑöÊú¨Êèê‰æõÊ∏ÖÁêÜÂäüËÉΩÔºåÂà†Èô§ÂàõÂª∫ÁöÑ HPA Âíå Deployment ËµÑÊ∫êÔºå‰øùÊåÅÈõÜÁæ§ÁéØÂ¢ÉÊï¥Ê¥Å„ÄÇ

ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÊ≠§ËÑöÊú¨‰ªÖÁî®‰∫éÂ§çÁé∞ HPA Ë°å‰∏∫ÁöÑBUGÔºåÁî®‰∫éÊµãËØïÂíåÁ†îÁ©∂Ôºå‰∏çÊûÑÊàêÂÆâÂÖ®ÊîªÂáª„ÄÇÊï¥‰∏™ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØ‰∏™ÈÄªËæëÁº∫Èô∑Ôºå‰∏çÁ¨¶ÂêàÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂÆö‰πâÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇ

---


## Issue #129842 hack/local-up-cluster.sh fails with emulation version needs to be greater or equal to 1.31

- Issue ÈìæÊé•Ôºö[#129842](https://github.com/kubernetes/kubernetes/issues/129842)

### Issue ÂÜÖÂÆπ

#### What happened?

Tried to create local Kubernetes cluster using hack/local-up-cluster.sh script but the api-server fails to come up.

#### What did you expect to happen?

Successfully create a local Kubernetes cluster.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Clone the Kubernetes repository
```
https://github.com/kubernetes/kubernetes.git
```

2. Run the local cluster create script
```
./hack/local-up-cluster.sh
```
[Install the missing dependencies reported by the script]

3. Error

```
2025/01/27 20:08:25 [INFO] received CSR
2025/01/27 20:08:25 [INFO] generating key: rsa-2048
2025/01/27 20:08:25 [INFO] encoded CSR
2025/01/27 20:08:25 [INFO] signed certificate with serial number xxxxxxxxx
Waiting for apiserver to come up
!!! [0127 20:09:29] Timed out waiting for apiserver:  to answer at https://127.0.0.1:6443/healthz; tried 60 waiting 1 between each
check apiserver logs: /tmp/kube-apiserver.log
Cleaning up...
```

4. API server Log
```
$ cat /tmp/kube-apiserver.log
I0127 20:08:28.017237   63752 flags.go:64] FLAG: --v="3"
I0127 20:08:28.017240   63752 flags.go:64] FLAG: --version="false"
I0127 20:08:28.017243   63752 flags.go:64] FLAG: --vmodule=""
I0127 20:08:28.017246   63752 flags.go:64] FLAG: --watch-cache="true"
I0127 20:08:28.017248   63752 flags.go:64] FLAG: --watch-cache-sizes="[]"
W0127 20:08:28.017278   63752 registry.go:256] calling componentGlobalsRegistry.AddFlags more than once, the registry will be set by the latest flags
E0127 20:08:28.017662   63752 run.go:72] "command failed" err="emulation version needs to be greater or equal to 1.31, got 1.23.0-alpha.1.24689"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
The connection to the server 127.0.0.1:62205 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On mac
Darwin KMacBook-Pro.local 24.1.0 Darwin Kernel Version 24.1.0

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú®‰ΩøÁî® hack/local-up-cluster.sh ËÑöÊú¨ÂêØÂä®Êú¨Âú∞ Kubernetes ÈõÜÁæ§Êó∂ÔºåÁî±‰∫éÊ£ÄÊµãÂà∞ emulation ÁâàÊú¨Ëøá‰ΩéÔºàÂÆûÈôÖÁâàÊú¨ 1.23.0-alpha.1.24689ÔºåËÄåË¶ÅÊ±ÇÂ§ß‰∫éÁ≠â‰∫é 1.31ÔºâÂØºËá¥ API Server Êó†Ê≥ïÂêØÂä®„ÄÇ‰ªéÊó•ÂøóÂíåÈîôËØØ‰ø°ÊÅØÊù•ÁúãÔºåËøôÊòØ‰∏ÄÂ§ÑÁâàÊú¨‰æùËµñÊàñÈÖçÁΩÆÈóÆÈ¢òÔºåÂπ∂ÈùûÈíàÂØπÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊºèÊ¥ûÊàñÂÆâÂÖ®Êú∫Âà∂Áº∫Èô∑Ôºå‰πüÊ≤°ÊúâÊ∂âÂèäÊÅ∂ÊÑèÊìç‰Ωú„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÈ´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≠§ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÂ§çÁé∞ hack/local-up-cluster.sh ËÑöÊú¨ÂêØÂä®Êó∂Âõ† emulation ÁâàÊú¨‰∏çÂåπÈÖçÁöÑÈóÆÈ¢ò„ÄÇ
Ê≥®ÊÑèÔºöÂΩìÂâçÈóÆÈ¢òÂ±û‰∫éÁâàÊú¨ÈÖçÁΩÆÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊµãËØïÂíåÈ™åËØÅÁâàÊú¨Ê£ÄÊµãÈÄªËæë„ÄÇ
"""

import subprocess
import sys
import http.server
import socketserver
import threading

def start_http_server():
    # ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºàÊª°Ë∂≥Ë¶ÅÊ±ÇÔºöÂú®ËÑöÊú¨‰∏≠ÂàõÂª∫‰∏Ä‰∏™HTTPÊúçÂä°Âô®Ôºâ
    Handler = http.server.SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", 8080), Handler)
    print("HTTP server is serving at port 8080")
    httpd.serve_forever()

def emulate_kube_apiserver():
    # Ê®°Êãü API Server ÁöÑÂêØÂä®Êó•ÂøóËæìÂá∫
    print("I0127 20:08:28.017237   63752 flags.go:64] FLAG: --v=\"3\"")
    print("I0127 20:08:28.017240   63752 flags.go:64] FLAG: --version=\"false\"")
    print("I0127 20:08:28.017243   63752 flags.go:64] FLAG: --vmodule=\"\"")
    print("I0127 20:08:28.017246   63752 flags.go:64] FLAG: --watch-cache=\"true\"")
    print("I0127 20:08:28.017248   63752 flags.go:64] FLAG: --watch-cache-sizes=\"[]\"")
    print("W0127 20:08:28.017278   63752 registry.go:256] calling componentGlobalsRegistry.AddFlags more than once, the registry will be set by the latest flags")
    print("E0127 20:08:28.017662   63752 run.go:72] \"command failed\" err=\"emulation version needs to be greater or equal to 1.31, got 1.23.0-alpha.1.24689\"")
    print("\nCleaning up...")

def run_local_cluster():
    print("Starting local Kubernetes cluster simulation...")
    # Ê®°ÊãüÂêØÂä® API Server
    emulate_kube_apiserver()
    # Ê≠§Â§ÑÂÆûÈôÖËÑöÊú¨‰∏≠Ëøò‰ºöÂåÖÂê´ÂêØÂä®ÂêÑÁªÑ‰ª∂„ÄÅ‰æùËµñÊ£ÄÊµãÁ≠âÔºåÊú¨Â§çÁé∞‰ªÖËÅöÁÑ¶ emulation ÁâàÊú¨Ê£ÄÊµã

def main():
    # ÂêØÂä®‰∏Ä‰∏™ÂêéÂè∞Á∫øÁ®ã‰Ωú‰∏∫ HTTP ÊúçÂä°Âô®
    server_thread = threading.Thread(target=start_http_server, daemon=True)
    server_thread.start()

    # Ê®°ÊãüÊú¨Âú∞ÈõÜÁæ§ÂàõÂª∫ÊµÅÁ®ã
    run_local_cluster()

    # ‰øùÊåÅ‰∏ªÁ®ãÂ∫èËøêË°å‰ª•‰æø HTTP ÊúçÂä°Âô®ÊåÅÁª≠ÊúçÂä°
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("\nShutting down simulation...")
        sys.exit(0)

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨Â§çÁé∞ËÑöÊú¨Áî®‰∫éÊ®°Êãü hack/local-up-cluster.sh ËÑöÊú¨‰∏≠ÂêØÂä®Êú¨Âú∞ Kubernetes ÈõÜÁæ§Êó∂Âõ† emulation ÁâàÊú¨‰∏çÂåπÈÖçËÄåÂØºËá¥ API Server ÂêØÂä®Â§±Ë¥•ÁöÑÊÉÖÂΩ¢„ÄÇ  
2. ËÑöÊú¨‰∏≠È¶ñÂÖàÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁõëÂê¨Âú® 8080 Á´ØÂè£ÁöÑ HTTP ÊúçÂä°Âô®ÔºåËøôÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±ÇÂú®ËÑöÊú¨‰∏≠ÂàõÂª∫ HTTP ÊúçÂä°Âô®„ÄÇ  
3. Êé•ÁùÄÊ®°Êãü API Server ÁöÑÂêØÂä®ËøáÁ®ãÔºåÂπ∂ÊâìÂç∞Âá∫‰∏éÊó•Âøó‰∏≠ÊûÅ‰∏∫Áõ∏‰ººÁöÑËæìÂá∫ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÁâàÊú¨Ê£ÄÊµãÂ§±Ë¥•ÁöÑÈîôËØØÊèêÁ§∫„ÄÇ  
4. Áî±‰∫éËØ•ÈóÆÈ¢òÊú¨Ë∫´‰∏∫ÁâàÊú¨ÈÖçÁΩÆÈóÆÈ¢òÔºå‰∏î‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Ê≠§Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÁ≥ªÁªüÂÆâÂÖ®ÊÄß‰∫ßÁîüÂΩ±Âìç„ÄÇ

---


## Issue #129838 Service with selector does not create all required endpoints, when a POD has multiple containers with a matching named port.

- Issue ÈìæÊé•Ôºö[#129838](https://github.com/kubernetes/kubernetes/issues/129838)

### Issue ÂÜÖÂÆπ

#### What happened?

I have a Deployment which creates POD's containing two containers, each container has a named port "grpc-service" on a different port to avoid conflicts.  I want a k8s service to connect to these grpc services in a round-robin fashion and also be able to use the k8s API to retrieve the list of endpoints.  I attempt to do this by creating a simple service, with a label selector that matches the container, and names the target-port.  However when I scale the  deployment only one of the containers is matched to the service,  I was expecting both containers to be matched resulting in two endpoints for each POD instance.

The generated endpoint looks like this, when the deployment is scaled to two replica's (I've trimmed out the targetRef's to keep it brief):

```
kind: Endpoints
apiVersion: v1
metadata:
  name: grpc-test-service
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: '2025-01-27T11:55:15Z'
subsets:
  - addresses:
      - ip: 10.245.0.71
      - ip: 10.245.2.91
    ports:
      - name: grpc
        port: 8080
        protocol: TCP
```

     


#### What did you expect to happen?

I expected the resultant endpoint to look like this (i.e. giving 4 endpoints):

```
apiVersion: v1
kind: Endpoints
metadata:
  name: grpc-test-service
subsets:
- addresses:
  - ip: 10.245.0.124
  - ip: 10.245.2.121
  ports:
  - port: 8080
    name: grpc
    protocol: TCP
- addresses:
  - ip: 10.245.0.124
  - ip: 10.245.2.121
  ports:
  - port: 8081
    name: grpc
    protocol: TCP    
```

#### How can we reproduce it (as minimally and precisely as possible)?

To create my test set-up the following yaml can be used (note that for the example I'm using nginx, however in practise I have different containers, which provide the same gRPC services).

```
apiVersion: v1
kind: Service
metadata:
  name: grpc-test-service
spec:
  type: LoadBalancer
  ports:
  - port: 5000
    name: grpc
    targetPort: grpc-server
  selector:
    app: grpc-test-server
    
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config-site1
data:
  nginx.conf: |
        events {
        }
        http {
           server {
               listen 8080;
               location / {
                   return 200 "Hello world! I'm site 1";
               }
           }
        }

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config-site2
data:
  nginx.conf: |
        events {
        }
        http {
           server {
               listen 8081;
               location / {
                   return 200 "Hello world! I'm site 2";
               }
           }
        }


---

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grpc-test-server
    name: grpc-test-server
  name: grpc-test-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grpc-test-server
      name: grpc-test-server
  template:
    metadata:
      labels:
        app: grpc-test-server
        name: grpc-test-server
      name: grpc-test-server
    spec:
      containers:
      - name: grpc-server-1
        image: nginx
        imagePullPolicy: Always
        ports:
          - name: grpc-server
            containerPort: 8080
            protocol: TCP
        volumeMounts:
          - name: config-site1
            mountPath: /etc/nginx/
               
      - name: grpc-server-2
        image: nginx
        imagePullPolicy: Always
        ports:
          - name: grpc-server
            containerPort: 8081
            protocol: TCP
        volumeMounts:
          - name: config-site2
            mountPath: /etc/nginx/
      volumes:
        - name: config-site1
          configMap:
            name: nginx-config-site1
            items:
              - key: nginx.conf
                path: nginx.conf
        - name: config-site2
          configMap:
            name: nginx-config-site2
            items:
              - key: nginx.conf
                path: nginx.conf
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Warning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.5", GitCommit:"aea7bbadd2fc0cd689de94a54e5b7b758869d691", GitTreeState:"clean", BuildDate:"2021-09-15T21:10:45Z", GoVersion:"go1.16.8", Compiler:"gc
", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.0", GitCommit:"70d3cc986aa8221cd1dfb1121852688902d3bf53", GitTreeState:"clean", BuildDate:"2024-12-11T17:59:15Z", GoVersion:"go1.23.3", Compiler:"gc
", Platform:"linux/amd64"}
WARNING: version difference between client (1.21) and server (1.32) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None, an on Premise setup
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux bmk8s-hermes 6.8.0-51-generic #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec 5 13:09:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ‰∏≠ Service ‰∏é Endpoints ÁöÑË°å‰∏∫‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÔºåÂç≥ÂΩì‰∏Ä‰∏™ Pod ÂÜÖÊúâÂ§ö‰∏™ÂÆπÂô®ÈÉΩÊö¥Èú≤‰∫ÜÂêåÂêçÁ´ØÂè£Êó∂ÔºåService Âè™ÂåπÈÖçÂà∞‰∫ÜÂÖ∂‰∏≠‰∏Ä‰∏™ÂÆπÂô®ÁöÑÁ´ØÂè£ÔºåÂØºËá¥ÁîüÊàêÁöÑ Endpoints Êï∞ÈáèÂ∞ë‰∫éÈ¢ÑÊúü„ÄÇËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é Kubernetes ÊúçÂä°ÂèëÁé∞Âíå Endpoint ÁîüÊàêÁöÑ bug ÊàñËÆæËÆ°Áº∫Èô∑ÈóÆÈ¢òÔºåÂπ∂Êú™Ê∂âÂèä‰æãÂ¶ÇÂëΩ‰ª§ÊâßË°å„ÄÅÂá≠ÊçÆÊ≥ÑÈú≤„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁ≠âÂÆâÂÖ®Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåËØ•ÈóÆÈ¢òÂ±û‰∫éÂ∫îÁî®Â±ÇÊàñË∞ÉÂ∫¶ÈÄªËæëÁöÑÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂõ†Ê≠§ÂèØ‰ª•Âà§Êñ≠ËØ•ÈóÆÈ¢ò‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# ËØ•ÈóÆÈ¢òÂπ∂ÈùûÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Ê≤°ÊúâÂÆâÂÖ®ÊÄßÂ§çÁé∞ÁöÑPOC„ÄÇ
# Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÈ™åËØÅËØ• Kubernetes Ë°å‰∏∫ÔºåÂèØ‰ª•ÁÆÄÂçïËæìÂá∫ÊèêÁ§∫‰ø°ÊÅØÔºö
if __name__ == "__main__":
    print("ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes Service ‰∏é Endpoints ÁöÑÁîüÊàêÁº∫Èô∑Ôºå‰∏çÂ≠òÂú®ÂÆâÂÖ®È£éÈô©„ÄÇ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ÂàÜÊûê‰∏≠ÔºåÊàë‰ª¨Á°ÆËÆ§ËØ• Issue ‰∏ªË¶ÅÊèèËø∞ÁöÑÊòØÂú®Â§öÂÆπÂô® Pod ‰∏≠ÔºåKubernetes Service ÈÄöËøá label selector Âíå targetPort ÂêçÁß∞ÂåπÈÖçÊó∂Êú™ËÉΩËØÜÂà´ÊâÄÊúâÂÖ∑Â§áÂåπÈÖçÁ´ØÂè£ÁöÑÂÆπÂô®ÔºåÂØºËá¥ Endpoints Êï∞Èáè‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÈóÆÈ¢ò„ÄÇËøôÊòØ Kubernetes ÊúçÂä°ÂèëÁé∞ÁöÑ‰∏Ä‰∏™ bugÔºåËÄåÈùûÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ
2. Ê†πÊçÆÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåËØ•ÈóÆÈ¢òÊó¢‰∏çÊ∂âÂèäÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÂÆâÂÖ®ÈóÆÈ¢òÔºå‰πü‰∏ç‰ºöÂØºËá¥ÊãíÁªùÊúçÂä°Á≠âÈ´òÂÆâÂÖ®‰∫ã‰ª∂ÔºåÂõ†Ê≠§Êï¥‰ΩìÈ£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÂÆâÂÖ®È£éÈô©„ÄÇ
3. Áî±‰∫é‰ªÖÂú®È´òÈ£éÈô©ÈóÆÈ¢ò‰∏≠ÊâçË¶ÅÊ±ÇÊèê‰æõ‰ΩøÁî® Python ÁºñÂÜôÁöÑÂ§çÁé∞ËÑöÊú¨ÔºåÊ≠§Â§ÑÁî±‰∫éÈóÆÈ¢òÊú¨Ë∫´‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Â§çÁé∞ËÑöÊú¨Âè™ÂÅöÁÆÄÂçïËØ¥ÊòéÔºåÂπ∂‰∏çÊ∂âÂèäÂÆûÈôÖÁöÑÊºèÊ¥ûÂà©Áî®‰ª£Á†ÅÔºå‰ªÖÁî®‰∫éËØ¥ÊòéËØ•ÈóÆÈ¢òÁöÑÊÄßË¥®„ÄÇ

---


## Issue #129833 k8s deployment remove duplicate `tolerations` but not `affinity`

- Issue ÈìæÊé•Ôºö[#129833](https://github.com/kubernetes/kubernetes/issues/129833)

### Issue ÂÜÖÂÆπ

#### What happened?

If we create a deployment with duplicate toleration the pod spec doesn't reflect the duplicate values, but the same doesn't happen to duplicate node affinities,

Deployment tolerations:
```
      tolerations:
      - effect: NoSchedule
        key: "key1"
        operator: Equal
        value: "true"
      - effect: NoSchedule
        key: "key1"
        operator: Equal
        value: "true"

```

Pod tolerations:
```
  tolerations:
  - effect: NoSchedule
    key: "key1"
    operator: Equal
    value: "true"
```


Deployment Affinity == Pod affinity :
```
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "key1"
                operator: Exists
              - key: "key1"
                operator: Exists
            - matchExpressions:
              - key: "key1"
                operator: Exists
              - key: "key1"
                operator: Exists
```


#### What did you expect to happen?

Desired affinity:
```
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "key1"
                operator: Exists
```

#### How can we reproduce it (as minimally and precisely as possible)?

Create a normal deployment with placements(tolerations and affinity) 

#### Anything else we need to know?

_No response_

#### Kubernetes version
k8s.io/api v0.31.2
<details>

```console
$ kubectl version
# paste output here

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ÈÉ®ÁΩ≤ÔºàDeploymentÔºâÂú®Â§ÑÁêÜÈáçÂ§çÈÖçÁΩÆÊó∂ÁöÑ‰∏ç‰∏ÄËá¥Ë°å‰∏∫ÔºöÂØπ‰∫éÈáçÂ§çÁöÑ tolerationsÔºå‰ºöËá™Âä®ÂéªÈáçÔºåËÄåÂØπ‰∫éÈáçÂ§çÁöÑ node affinityÔºåÂàô‰∏ç‰ºöÂéªÈáç„ÄÇËØ•ÈóÆÈ¢òÂèçÊò†‰∫Ü Kubernetes Âú®Â∞ÜÈÉ®ÁΩ≤ÈÖçÁΩÆËΩ¨Êç¢‰∏∫ Pod ËßÑËåÉÊó∂Â≠òÂú®ÁöÑÈÄªËæë‰∏ç‰∏ÄËá¥Ôºå‰ΩÜÂÆÉ‰ªÖÂ±û‰∫éÂÆûÁé∞‰∏äÁöÑÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÊú™ÊéàÊùÉËÆøÈóÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊùÉÈôêÊèêÂçáÊàñÂÖ∂‰ªñÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂêåÊó∂ÔºåIssue ‰∏≠Êú™Ê∂âÂèäÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÅ‰∏çÂΩìÂá≠ËØÅÈÖçÁΩÆ„ÄÅÊàñËÄÖÂõ†‰ΩéÊùÉÈôêÁî®Êà∑ËÉΩÂ§üÂΩ±ÂìçÈ´òÊùÉÈôêÁî®Êà∑Á≠âÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Êú¨ËÑöÊú¨Áî®‰∫éÊºîÁ§∫Â¶Ç‰Ωï‰ΩøÁî® kubernetes Python ÂÆ¢Êà∑Á´ØÂàõÂª∫‰∏Ä‰∏™ DeploymentÔºå
ËØ• Deployment ‰∏≠ÂåÖÂê´ÈáçÂ§çÁöÑ tolerations ‰∏éÈáçÂ§çÁöÑ node affinity ÈÖçÁΩÆÔºå‰ªéËÄåÈáçÁé∞ Issue ‰∏≠ÊèèËø∞ÁöÑÈóÆÈ¢òË°®Áé∞„ÄÇ
Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÈõÜÁæ§ÈÄ†ÊàêÁ†¥ÂùèÔºõÂêåÊó∂ÔºåÊú¨ÈóÆÈ¢òÊú¨Ë∫´‰∏çÂ±û‰∫éÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇ
"""

import yaml
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Âä†ËΩΩ kubeconfig ÈÖçÁΩÆÔºåÂÅáËÆæÂú®ÈªòËÆ§‰ΩçÁΩÆ
config.load_kube_config()

apps_v1 = client.AppsV1Api()

# ÂÆö‰πâÂåÖÂê´ÈáçÂ§ç tolerations ÂíåÈáçÂ§ç affinity ÁöÑ Deployment ÈÖçÁΩÆ
deployment_manifest = {
    "apiVersion": "apps/v1",
    "kind": "Deployment",
    "metadata": {
        "name": "duplicate-config-demo",
        "labels": {"app": "duplicate-demo"}
    },
    "spec": {
        "replicas": 1,
        "selector": {"matchLabels": {"app": "duplicate-demo"}},
        "template": {
            "metadata": {"labels": {"app": "duplicate-demo"}},
            "spec": {
                "containers": [{
                    "name": "nginx",
                    "image": "nginx:latest",
                    "ports": [{"containerPort": 80}]
                }],
                "tolerations": [
                    {
                        "key": "key1",
                        "operator": "Equal",
                        "value": "true",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "key1",
                        "operator": "Equal",
                        "value": "true",
                        "effect": "NoSchedule"
                    }
                ],
                "affinity": {
                    "nodeAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": {
                            "nodeSelectorTerms": [
                                {
                                    "matchExpressions": [
                                        {"key": "key1", "operator": "Exists"},
                                        {"key": "key1", "operator": "Exists"}
                                    ]
                                },
                                {
                                    "matchExpressions": [
                                        {"key": "key1", "operator": "Exists"},
                                        {"key": "key1", "operator": "Exists"}
                                    ]
                                }
                            ]
                        }
                    }
                }
            }
        }
    }
}

def create_deployment():
    try:
        resp = apps_v1.create_namespaced_deployment(
            body=deployment_manifest,
            namespace="default"
        )
        print("Deployment created. Status='%s'" % str(resp.metadata.name))
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)

def get_pod_spec():
    # Á≠âÂæÖ Pod ÂàõÂª∫ÂÆåÊàêÂêéËé∑Âèñ Pod ËßÑËåÉ‰∏≠ÁöÑ tolerations Âíå affinity Ë°®Áé∞
    core_v1 = client.CoreV1Api()
    pods = []
    for _ in range(10):
        resp = core_v1.list_namespaced_pod(namespace="default", label_selector="app=duplicate-demo")
        pods = resp.items
        if pods:
            break
        time.sleep(2)
    if not pods:
        print("Êú™ÊâæÂà∞Áõ∏ÂÖ≥ Pod„ÄÇ")
        return
    pod = pods[0]
    print("Pod name:", pod.metadata.name)
    print("Pod tolerations:")
    for tol in pod.spec.tolerations or []:
        print(yaml.dump(tol))
    print("Pod affinity:")
    if pod.spec.affinity and pod.spec.affinity.node_affinity:
        print(yaml.dump(pod.spec.affinity.node_affinity))
    else:
        print("Êó† affinity ÈÖçÁΩÆ")

def delete_deployment():
    try:
        apps_v1.delete_namespaced_deployment(
            name="duplicate-config-demo",
            namespace="default",
            body=client.V1DeleteOptions(
                propagation_policy='Foreground'
            )
        )
        print("Deployment deleted.")
    except ApiException as e:
        print("Exception when deleting deployment: %s\n" % e)

if __name__ == "__main__":
    print("ÂàõÂª∫ Deployment ...")
    create_deployment()
    print("Á≠âÂæÖ Pod ÂêØÂä® ...")
    time.sleep(10)
    print("Ëé∑Âèñ Pod ÈÖçÁΩÆ ...")
    get_pod_spec()
    print("Ê∏ÖÁêÜÊµãËØï Deployment ...")
    delete_deployment()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨‰ΩøÁî® Kubernetes ÂÆòÊñπÁöÑ Python ÂÆ¢Êà∑Á´Ø‰ªéÈªòËÆ§‰ΩçÁΩÆÂä†ËΩΩ kubeconfig Âπ∂ËøûÊé•ÈõÜÁæ§„ÄÇ
2. ËÑöÊú¨ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ DeploymentÔºåËØ• Deployment Âú® Pod ËßÑËåÉ‰∏≠Êó¢ÈÖçÁΩÆ‰∫ÜÈáçÂ§çÁöÑ tolerationsÔºå‰πüÈÖçÁΩÆ‰∫ÜÈáçÂ§çÁöÑ node affinityÔºàÈÄöËøá‰∏§ÁªÑÈáçÂ§çÁöÑ matchExpressionsÔºâ„ÄÇ
3. ÂàõÂª∫ Deployment ÂêéÔºåËÑöÊú¨Á≠âÂæÖ Pod ÂêØÂä®ÔºåÂπ∂Ë∞ÉÁî® get_pod_spec() ÂáΩÊï∞Ëé∑ÂèñÂ∑≤ÁîüÊàêÁöÑ Pod ‰∏≠ÁöÑ tolerations Âíå affinity ‰ø°ÊÅØ„ÄÇÈÄöËøáÂØπÊØî Deployment ‰∏≠ÁöÑÈÖçÁΩÆÂíå Pod ‰∏≠ÂÆûÈôÖÁöÑÈÖçÁΩÆÔºåÂèØ‰ª•ËßÇÂØüÂà∞ tolerations ÈáçÂ§çÈ°πË¢´ÂéªÈáçÔºåËÄå affinity ÈÉ®ÂàÜÂàô‰øùÁïô‰∫ÜÊâÄÊúâÈÖçÁΩÆ„ÄÇ
4. ÊúÄÂêéÔºåËÑöÊú¨Âà†Èô§‰∫ÜÂàõÂª∫ÁöÑ DeploymentÔºå‰ª•‰æøÊµãËØïÂÆåÊàêÂêéÊ∏ÖÁêÜÈõÜÁæ§ËµÑÊ∫ê„ÄÇ
5. ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËØ• Issue ÊâÄÊèèËø∞ÁöÑÈóÆÈ¢òÂ±û‰∫éÈÖçÁΩÆÈÄªËæë‰∏ç‰∏ÄËá¥ÈóÆÈ¢òÔºåÊ≤°ÊúâÂºïÂÖ•ÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Êú¨Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÈ™åËØÅË°å‰∏∫Â∑ÆÂºÇÔºå‰∏çÊ∂âÂèä‰ªª‰ΩïÂÆâÂÖ®ÊºèÊ¥ûÂà©Áî®„ÄÇ

---


## Issue #129829 [EventedPLEG] EventedPLEG should detect resize completion rapidly

- Issue ÈìæÊé•Ôºö[#129829](https://github.com/kubernetes/kubernetes/issues/129829)

### Issue ÂÜÖÂÆπ

#### What would you like to be added?

EventedPLEG should detect completion of pod resizing in the runtime as rapidly as GenericPLEG.

Since v1.32 (#128518), GenericPLEG polls container resources in the runtime at resizing a pod so that it detects resize completion rapidly. This poll is actuated when GenericPLEG relists pods regularly:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/pleg/generic.go#L314-L327

EventedPLEG depends on GenericPLEG for this polling:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/pleg/evented.go#L430-L432

When EventedPLEG is enabled, GenericPLEG relists pods every five minutes:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/kubelet.go#L197

Then, it takes at most five minutes till pod resize appears to be completed in the API.


#### Why is this needed?

Pod resize should complete as quickly as if EventedPLEG is not enabled.

Considering maturity of features, I don‚Äôt think this issue blocks promotion of InPlacePodVerticalScaling. This issue should be solved for promoting EventedPLEG.


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ‰∏ªË¶ÅÊèèËø∞ÁöÑÊòØ EventedPLEG Âú®Ê£ÄÊµã Pod ÈáçÁΩÆÔºàresizeÔºâÂÆåÊàêÁä∂ÊÄÅÊó∂Â≠òÂú®Âª∂Êó∂ÈóÆÈ¢òÔºåÊåáÂá∫Áõ∏ÊØî GenericPLEGÔºåEventedPLEG Êú™ËÉΩ‚ÄúËøÖÈÄü‚ÄùÊ£ÄÊµãÂà∞ Pod resize ÂÆåÊàê„ÄÇÊï¥‰∏™ÊèèËø∞ÂÖ≥Ê≥®ÁöÑÊòØÂäüËÉΩÂíåÊÄßËÉΩ‰∏äÁöÑÊîπËøõ‰ºòÂåñÔºåËÄå‰∏çÊòØÂäüËÉΩÂÆûÁé∞‰∏≠Â≠òÂú®ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÊàñÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇIssue ‰∏≠Âπ∂Êú™ÊèêÂèä‰ªª‰ΩïÂèØËÉΩÂØºËá¥Êú™ÊéàÊùÉËÆøÈóÆ„ÄÅ‰ø°ÊÅØÊ≥ÑÊºè„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÖ∏ÂûãÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÊèèËø∞‰∏≠ÂºïÁî®ÁöÑ‰ª£Á†ÅÈìæÊé•‰πüÈÉΩÊòØÁî®‰∫éËØ¥Êòé‰∫ã‰ª∂Ê£ÄÊµã‰∏éËΩÆËØ¢ÂÆûÁé∞ÁöÑÈÄªËæëÔºåÊ≤°ÊúâÊ∂âÂèäÊïèÊÑü‰ø°ÊÅØÊö¥Èú≤„ÄÅ‰∏çÂΩìÊìç‰ΩúÊàñÊïèÊÑüÈÖçÁΩÆÈîôËØØÁöÑÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ¥ÊòéÔºö
Ê≠§ËÑöÊú¨‰ªÖ‰∏∫ÊºîÁ§∫Áî®ÈÄîÔºåÁî®‰∫éÊ®°Êãü‰∏Ä‰∏™HTTPÊúçÂä°ÔºåËØ•ÊúçÂä°Âú®8080Á´ØÂè£ÁõëÂê¨Ôºå
Áî®‰∫éËØÅÊòéÊú¨IssueÊú¨Ë¥®‰∏äÊòØÂäüËÉΩ‰ºòÂåñÈóÆÈ¢òÔºåÂπ∂‰∏çÂ≠òÂú®ÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Êó†ÈúÄÂØπÊºèÊ¥ûÈ£éÈô©ËøõË°åÂ§çÁé∞„ÄÇ
"""

import http.server
import socketserver

PORT = 8080

class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/simulate_resize':
            # Ê®°ÊãüPod resize‰∫ã‰ª∂ÁöÑÂø´ÈÄüÂìçÂ∫îÊ∂àÊÅØ
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"Ê®°Êãü: Pod resize detected rapidly.")
        else:
            self.send_response(404)
            self.end_headers()

def run_server():
    with socketserver.TCPServer(("", PORT), SimpleHTTPRequestHandler) as httpd:
        print(f"HTTPÊúçÂä°Âô®ËøêË°åÂú®Á´ØÂè£ {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("ÊúçÂä°Âô®ÂÅúÊ≠¢ËøêË°å")
            httpd.server_close()

if __name__ == "__main__":
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•Â§çÁé∞ËÑöÊú¨‰∏≠ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®Êú¨Âú∞ÁöÑ8080Á´ØÂè£„ÄÇÂΩìËÆøÈóÆË∑ØÂæÑ‰∏∫ /simulate_resize Êó∂ÔºåÊúçÂä°Âô®ËøîÂõû‰∏Ä‰∏™Áî®‰∫éÊ®°Êãü Pod resize Âø´ÈÄüÊ£ÄÊµãÂìçÂ∫îÁöÑÊèêÁ§∫‰ø°ÊÅØ„ÄÇÈúÄË¶ÅËØ¥ÊòéÁöÑÊòØÔºö
1. ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫Â¶Ç‰ΩïÊê≠Âª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑHTTPÊúçÂä°ÔºåÂπ∂‰∏çÊ∂âÂèä‰ªª‰ΩïÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ÔºåÂõ†‰∏∫Êú¨ Issue Êú¨Ë¥®‰∏∫‰∏Ä‰∏™ÊÄßËÉΩ‰ºòÂåñÈóÆÈ¢ò„ÄÇ
2. ËÑöÊú¨‰∏≠Ê≤°ÊúâÊ∂âÂèäÊïèÊÑüÂá≠ËØÅÊàñÂØπÂ§ñÊö¥Èú≤ÁöÑÊïèÊÑü‰ø°ÊÅØÔºå‰πü‰∏çÊ®°Êãü‰ªª‰ΩïÂÆâÂÖ®ÊîªÂáªÂú∫ÊôØ„ÄÇ
3. Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫ÊµÅÁ®ãÔºåÂπ∂‰∏çÁî®‰∫éÂÆûÈôÖÁöÑÂÆâÂÖ®ÊµãËØïÊàñÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

Âõ†Ê≠§ÔºåËØ• Issue ÁöÑÊèèËø∞Âπ∂‰∏çÂ≠òÂú®ÊΩúÂú®ÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ßÂà§Êñ≠‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇ

---


## Issue #129827 [EventedPLEG] EventedPLEG should be more tolerant of runtime outage

- Issue ÈìæÊé•Ôºö[#129827](https://github.com/kubernetes/kubernetes/issues/129827)

### Issue ÂÜÖÂÆπ

#### What would you like to be added?

It would be better to improve the retry mechanism of EventedPLEG. EventedPLEG should be more tolerant of the outage of runtime such as restarting the runtime.




#### Why is this needed?

I tried restarting containerd when EventedPLEG is enabeld. 
```
# time systemctl restart containerd

real	0m0.401s
user	0m0.008s
sys 	0m0.007s
```

Then, Evented PLEG fell back to GenericPLEG immediately.
```
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.152929     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = error reading from server: EOF"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.152970     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.152985     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153033     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153064     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153076     698 http2_client.go:993] "[transport] [client-transport 0xc000bc38c8] Closing: connection error: desc = \"error reading from server: EOF\"\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153156     698 logging.go:39] "[core] [Channel #9 SubChannel #10]Subchannel Connectivity change to IDLE\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153192     698 controlbuf.go:544] "[transport] [client-transport 0xc000bc38c8] loopyWriter exiting with error: connection error: desc = \"error reading from server: EOF\"\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153084     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153280     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153327     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153344     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153376     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153407     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153422     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153436     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153460     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153474     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153513     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153533     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153581     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153589     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no su
ch file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153609     698 pickfirst.go:208] "[pick-first-lb] [pick-first-lb 0xc000b404e0] Received SubConn state update: 0xc000b40900, {ConnectivityState:IDLE ConnectionError:<nil>}\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153615     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153630     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153655     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153680     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or direc
tory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153695     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153737     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153766     698 evented.go:190] "Fall back to Generic PLEG relisting since Evented PLEG is not working"
```

EventedPLEG should keep working in this case because GenericPLEG also cannot work.


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ‰∏ªË¶ÅÊèèËø∞‰∫ÜÂú®ÂÆπÂô®ËøêË°åÊó∂Ôºà‰æãÂ¶Ç containerd ÈáçÂêØÔºâÂØºËá¥ EventedPLEG Â§±ÊïàÔºå‰ªéËÄåÂõûÈÄÄÂà∞ GenericPLEG ÁöÑÈóÆÈ¢ò„ÄÇËØ•ÈóÆÈ¢òÊòØÂÖ≥‰∫éÊèêÈ´òÂÆπÈîôÊú∫Âà∂„ÄÅÂ¢ûÂº∫ÂèØÈù†ÊÄßÁöÑÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÊú™ÁªèÊéàÊùÉÁöÑËÆøÈóÆ„ÄÅ‰ª£Á†ÅÊâßË°å„ÄÅÊùÉÈôêÊèêÂçáÊàñÂÖ∂‰ªñÂÆâÂÖ®ÊîªÂáªÁöÑÈ£éÈô©„ÄÇÊï¥‰∏™ Issue ÁöÑÈáçÁÇπÂú®‰∫éÂ¶Ç‰ΩïÊîπÂñÑÁ≥ªÁªüÂú®Áü≠ÊöÇÊïÖÈöúÊúüÈó¥ÁöÑË°å‰∏∫ÔºåÂπ∂‰∏çÊöóÁ§∫Â≠òÂú®ÂÆâÂÖ®ÊºèÊ¥ûÊàñË¢´ÊÅ∂ÊÑèÂà©Áî®ÁöÑÂèØËÉΩ„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°Êãü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°ÔºåÁî®Êù•Ë°®ÊòéÂΩìÂâç Issue Âπ∂Êú™Ê∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå
Âõ†Ê≠§Ê≤°ÊúâÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ POC„ÄÇ
"""

import http.server
import socketserver

PORT = 8080

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        # ÁÆÄÂçïËøîÂõû‰ø°ÊÅØÔºåË°®ÊòéËØ•ÊúçÂä°Ê≠£Âú®ËøêË°åÔºåÊó†ÂÆûÈôÖÂÆâÂÖ®ÊºèÊ¥ûÂ§çÁé∞
        self.send_response(200)
        self.send_header("Content-type", "text/plain; charset=utf-8")
        self.end_headers()
        self.wfile.write(b"ÂΩìÂâçIssueÊú™Ê∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÊó†Â§çÁé∞POC„ÄÇ")

def run_server():
    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print(f"HTTPÊúçÂä°Âô®Â∑≤ÂêØÂä®ÔºåÁõëÂê¨Á´ØÂè£{PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("ÊúçÂä°Âô®ÂÅúÊ≠¢ËøêË°å„ÄÇ")

if __name__ == "__main__":
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™Âú® 8080 Á´ØÂè£ÁõëÂê¨ÁöÑ HTTP ÊúçÂä°Âô®ÔºåÊúçÂä°Âô®ËøîÂõû‰∏ÄË°åÁÆÄÂçïÁöÑÊñáÊú¨‰ø°ÊÅØÔºåËØ¥ÊòéÂΩìÂâç Issue Âπ∂Êú™Ê∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå‰∏çÂ≠òÂú®ÂèØË¢´Âà©Áî®ÁöÑÊºèÊ¥û„ÄÇ
2. Áî±‰∫é Issue ËÆ®ËÆ∫ÁöÑÊòØÁ≥ªÁªüÂú®ÂÆπÂô®ËøêË°åÊó∂Áü≠ÊöÇ‰∏≠Êñ≠ÊÉÖÂÜµ‰∏ãÁöÑÂÆπÈîôËÉΩÂäõÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Ê≤°ÊúâÈíàÂØπÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ËÑöÊú¨„ÄÇ
3. ËÑöÊú¨‰∏≠Ê≤°ÊúâÊ∂âÂèäÈúÄË¶ÅÂá≠ËØÅÔºàÂ¶Ç kubeconfig„ÄÅgit tokenÔºâÁöÑÊÉÖÂÜµÔºå‰πüÊ≤°ÊúâÊ∂âÂèäÂØπÂ§ñÈÉ® HTTP ÊúçÂä°ÁöÑÂ§çÊùÇ‰∫§‰∫íÔºåÁ¨¶ÂêàÈ¢òÁõÆË¶ÅÊ±Ç„ÄÇ

---


## Issue #129825 NodeUnstageVolume should be called even when stagingPath doesn't exist

- Issue ÈìæÊé•Ôºö[#129825](https://github.com/kubernetes/kubernetes/issues/129825)

### Issue ÂÜÖÂÆπ

#### What happened?

[pkg/volume/csi/csi_block.go](https://github.com/kubernetes/kubernetes/blob/release-1.32/pkg/volume/csi/csi_block.go#L458)

```go
	// Call NodeUnstageVolume
	stagingPath := m.GetStagingPath()
	if _, err := os.Stat(stagingPath); err != nil {
		if os.IsNotExist(err) {
			klog.V(4).Info(log("blockMapper.TearDownDevice stagingPath(%s) has already been deleted, skip calling NodeUnstageVolume", stagingPath))
		} else {
			return err
		}
	} else {
		err := m.unstageVolumeForBlock(ctx, csiClient, stagingPath)
		if err != nil {
			return err
		}
	}
```

If the stagingPath doesn't exist, NodeUnstageVolume won't be called.

In my opinion, NodeUnstageVolume should be called even when stagingPath doesn't exist, for the following reasons:

1. As long as CSI supports StageUnstage capability, the CSI should be able to properly handle cases where stagingPath doesn't exist
2. If NodeUnstageVolume return error but stagingPath is cleaned up, NodeUnstageVolume won't be called again in next retry
3. For filesystem PVCs, NodeUnstageVolume is called normally even when stagingPath doesn't exist
4. When a CSI driver is upgraded from non-StageUnstage to StageUnstage support, volumes mounted before the upgrade won't have stagingPath. However, these pre-upgrade volumes still need NodeUnstageVolume to be called for proper unmounting

#### What did you expect to happen?

For block pvc, NodeUnstageVolume should be called even when stagingPath doesn't exist 

#### How can we reproduce it (as minimally and precisely as possible)?

It can be intuitively seen from the code

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.32
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØ Kubernetes Âú®Â§ÑÁêÜ CSI block volume ÁöÑÂç∏ËΩΩÊµÅÁ®ã‰∏≠ÔºåÂΩì stagingPath ‰∏çÂ≠òÂú®Êó∂Ôºå‰∏çË∞ÉÁî® NodeUnstageVolume ÁöÑÈóÆÈ¢ò„ÄÇËØ•ÈóÆÈ¢ò‰∏ªË¶ÅÊ∂âÂèäÂç∏ËΩΩÈÄªËæëÁöÑÊ≠£Á°ÆÊÄßÂíå‰∏ÄËá¥ÊÄßÔºåÂèØËÉΩ‰ºöÂØºËá¥Êüê‰∫õÊÉÖÂÜµ‰∏ã volume Êó†Ê≥ïÊ≠£Â∏∏Âç∏ËΩΩÔºå‰ªéËÄåÁïô‰∏ãÊÆãÁïôÁä∂ÊÄÅÔºå‰ΩÜ‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËøôÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂºïÂèëÂëΩ‰ª§ÊâßË°å„ÄÅÊãíÁªùÊúçÂä°„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÈ´òÈ£éÈô©ÊîªÂáª„ÄÇÂõ†Ê≠§ÔºåÊ≠§ÈóÆÈ¢òÂ±û‰∫éÈÄªËæëÊàñÂäüËÉΩÁº∫Èô∑ÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°Êãü issue ‰∏≠ÊèêÂà∞ÁöÑÈÄªËæëÔºåÂç≥ÂΩì stagingPath ‰∏çÂ≠òÂú®Êó∂Ôºå‰∏çË∞ÉÁî® NodeUnstageVolume„ÄÇ
ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫ÈóÆÈ¢òÁöÑ‰ª£Á†ÅÈÄªËæëÔºå‰∏ç‰ºö‰∫ßÁîüÂÆâÂÖ®È£éÈô©Ôºå‰πü‰∏ç‰ºöÂØπÁúüÂÆûÁ≥ªÁªüÈÄ†ÊàêÂΩ±Âìç„ÄÇ
"""

import os
import logging
import tempfile
import shutil

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def node_unstage_volume(staging_path):
    """
    Ê®°Êãü NodeUnstageVolume ÁöÑË∞ÉÁî®ËøáÁ®ã
    """
    logging.info("Ë∞ÉÁî® NodeUnstageVolumeÔºåstaging_path: %s", staging_path)
    # Ê®°Êãü CSI È©±Âä®ÂØπ‰∏çÂ≠òÂú®ÁöÑË∑ØÂæÑËøõË°åÂ§ÑÁêÜÔºåÊâìÂç∞ÊàêÂäüÊó•Âøó
    return True


def simulate_unmount(staging_path):
    """
    Ê®°ÊãüÂç∏ËΩΩÈÄªËæëÔºöÂ¶ÇÊûú staging_path Â≠òÂú®ÔºåÂàôË∞ÉÁî® NodeUnstageVolumeÔºõ
    Â¶ÇÊûú staging_path ‰∏çÂ≠òÂú®ÔºåÂàô‰∏çË∞ÉÁî® NodeUnstageVolumeÔºåÂπ∂ËæìÂá∫ÂØπÂ∫îÊó•Âøó
    """
    logging.info("Ê£ÄÊµã staging_path: %s", staging_path)
    if not os.path.exists(staging_path):
        logging.info("staging_path ‰∏çÂ≠òÂú®ÔºåË∑≥ËøáË∞ÉÁî® NodeUnstageVolume")
        return False
    else:
        result = node_unstage_volume(staging_path)
        return result


def main():
    # ÂàõÂª∫‰∏Ä‰∏™‰∏¥Êó∂ÁõÆÂΩïÊ®°Êãü stagingPath ÁöÑÂ≠òÂú®ÊÉÖÂÜµ
    temp_dir = tempfile.mkdtemp(prefix="staging_")
    logging.info("ÂàõÂª∫‰∏¥Êó∂ staging_path: %s", temp_dir)
    
    # ÊÉÖÊôØ 1ÔºöstagingPath Â≠òÂú®
    logging.info("ÊÉÖÊôØ 1ÔºöstagingPath Â≠òÂú®")
    simulate_unmount(temp_dir)
    
    # Ê∏ÖÁêÜ stagingPath Ê®°ÊãüË∑ØÂæÑÂà†Èô§ÁöÑÊÉÖÂÜµ
    shutil.rmtree(temp_dir)
    logging.info("Â∑≤Âà†Èô§‰∏¥Êó∂ staging_path: %s", temp_dir)
    
    # ÊÉÖÊôØ 2ÔºöstagingPath ‰∏çÂ≠òÂú®
    logging.info("ÊÉÖÊôØ 2ÔºöstagingPath ‰∏çÂ≠òÂú®")
    simulate_unmount(temp_dir)


if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨‰∏≠È¶ñÂÖàÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Ê®°ÊãüÂáΩÊï∞ node_unstage_volume Êù•Ë°®Á§∫ÂØπ NodeUnstageVolume ÁöÑË∞ÉÁî®ÔºåËØ•ÂáΩÊï∞ÁõÆÂâç‰ªÖËÆ∞ÂΩïÊó•ÂøóÊ®°ÊãüË∞ÉÁî®ËøáÁ®ã„ÄÇ
2. simulate_unmount ÂáΩÊï∞ÊåâÁÖß issue ‰∏≠‰ª£Á†ÅÈÄªËæëÂà§Êñ≠ staging_path ÊòØÂê¶Â≠òÂú®„ÄÇÂ¶ÇÊûú‰∏çÂ≠òÂú®ÔºåÂàôËæìÂá∫Êó•ÂøóËØ¥ÊòéË∑≥Ëøá NodeUnstageVolume Ë∞ÉÁî®ÔºõÂ¶ÇÊûúÂ≠òÂú®ÔºåÂàôË∞ÉÁî® node_unstage_volume„ÄÇ
3. main ÂáΩÊï∞‰∏≠Ôºö
   - ÈÄöËøá tempfile Ê®°ÂùóÂàõÂª∫‰∫Ü‰∏Ä‰∏™‰∏¥Êó∂ÁõÆÂΩïÔºåÊ®°Êãü stagingPath Â≠òÂú®Êó∂ÁöÑÂú∫ÊôØÔºåÁÑ∂ÂêéË∞ÉÁî® simulate_unmount Â±ïÁé∞Ë∞ÉÁî®ËøáÁ®ã„ÄÇ
   - ÈöèÂêéÂà†Èô§ËØ•ÁõÆÂΩïÔºåÊ®°Êãü stagingPath Ë¢´ÊÑèÂ§ñÂà†Èô§ÁöÑÂú∫ÊôØÔºåÂÜçÊ¨°Ë∞ÉÁî® simulate_unmount Â±ïÁ§∫ÂΩìË∑ØÂæÑ‰∏çÂ≠òÂú®Êó∂ÔºåË∑≥Ëøá NodeUnstageVolume Ë∞ÉÁî®ÁöÑË°å‰∏∫„ÄÇ

Ê≠§ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºåÂ∏ÆÂä©ÁêÜËß£ issue ‰∏≠ÊèèËø∞ÁöÑ‰ª£Á†ÅÈÄªËæë‰∏éË°å‰∏∫Ôºå‰∏î‰∏çÊ∂âÂèä‰ªª‰ΩïÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇ

---


## Issue #129819 Memory manager and CPU manager state file is not updated immediately when a guaranteed pod is deleted

- Issue ÈìæÊé•Ôºö[#129819](https://github.com/kubernetes/kubernetes/issues/129819)

### Issue ÂÜÖÂÆπ

#### What happened?

For guaranteed pod, when Memory Manager/CPU Manager are enabled there are entries in /var/lib/kubelet/memory_manager_state and /var/lib/kubelet/cpu_manager_state that are created capturing the resource allocations per NUMA.

When the pod is deleted the entries in these files are not deleted till a new pod is deployed. Looking at the code, these entries are deleted only when new entries are added as part of pod deployment as part of removeStaleState() where garbage collection happens.

Is this by design?

#### What did you expect to happen?

Entries in /var/lib/kubelet memory_manager_state and cpu_manager_state files are correctly updated on pod delete.

#### How can we reproduce it (as minimally and precisely as possible)?

Deployed a guaranteed pod with CPU and memory resources specified. CPU manager and Memory manager configured

Using following options in kubelet configuration

--cpu-manager-policy=static --topology-manager-policy=restricted --topology-manager-scope=pod --pod-max-pids=4096 --memory-manager-policy=Static --reserved-memory=0:memory=1024Mi --kube-reserved=memory=462Mi --system-reserved=memory=462Mi

#### Anything else we need to know?

By adding following change in PostStopContainer() as part of container lifecycle I am seeing the expected behavior. Is it a safe change or was there a reason to not add the cleanup as part of pod deletes. The only reason cleanup is not performed as part of delete would be to reduce the lock contention updating the state files. Is that right?

diff --git a/pkg/kubelet/cm/internal_container_lifecycle.go b/pkg/kubelet/cm/internal_container_lifecycle.go
index 5e50fd1..8cc9d13 100644
--- a/pkg/kubelet/cm/internal_container_lifecycle.go
+++ b/pkg/kubelet/cm/internal_container_lifecycle.go
@@ -52,5 +52,13 @@ func (i *internalContainerLifecycleImpl) PreStartContainer(pod *v1.Pod, containe
 }

 func (i *internalContainerLifecycleImpl) PostStopContainer(containerID string) error {
+       if i.cpuManager != nil {
+               i.cpuManager.RemoveContainer(containerID)
+       }
+
+       if i.memoryManager != nil {
+               i.memoryManager.RemoveContainer(containerID)
+       }
+
        return i.topologyManager.RemoveContainer(containerID)
 }

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
On Prem Cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

$ uname -a
Linux qct-23 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ËÆ®ËÆ∫ÁöÑÊòØÂΩìÂêØÁî®‰∫Ü Memory Manager Âíå CPU Manager Êó∂ÔºåÂØπ‰∫é‰øùËØÅÂûã podÔºåkubelet Âú® pod Âà†Èô§ÂêéÊ≤°ÊúâÁ´ãÂç≥Ê∏ÖÈô§ /var/lib/kubelet/memory_manager_state Âíå /var/lib/kubelet/cpu_manager_state Êñá‰ª∂‰∏≠ÁöÑËÆ∞ÂΩïÔºåËÄåÊòØÁ≠âÂà∞Êñ∞ pod ÈÉ®ÁΩ≤Êó∂ÂÜçËøõË°åÂûÉÂúæÂõûÊî∂ÔºàremoveStaleState()Ôºâ„ÄÇËØ•ÈóÆÈ¢ò‰∏ªË¶ÅÊ∂âÂèäÁä∂ÊÄÅÊñá‰ª∂ÁöÑÊõ¥Êñ∞Êó∂Êú∫ÂíåÊ∏ÖÁêÜÊú∫Âà∂ÔºåÂèØËÉΩ‰ºöÂØºËá¥Áä∂ÊÄÅ‰ø°ÊÅØÂª∂ËøüÊõ¥Êñ∞Ôºå‰ªéËÄåÂΩ±ÂìçËµÑÊ∫êÁõëÊéßÂíåË∞ÉÂ∫¶Ôºå‰ΩÜÂπ∂Ê≤°ÊúâÊö¥Èú≤Á≥ªÁªüÊïèÊÑü‰ø°ÊÅØ„ÄÅÂ≠òÂú®‰∏çÂΩìÈÖçÁΩÆÊàñËÄÖ‰ΩøÊîªÂáªËÄÖËÉΩÂ§üËøúÁ®ãÊâßË°å‰ª£Á†Å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠â„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™ÈÄªËæëÊÄßÊàñËÆæËÆ°‰∏äÁöÑÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Ëß£ÈáäËØ¥ÊòéÔºö**

Issue ‰∏≠ÊèèËø∞ÁöÑÁé∞Ë±°Â±û‰∫é kubelet ÂØπËµÑÊ∫êÁÆ°ÁêÜÁä∂ÊÄÅÊñá‰ª∂Ê∏ÖÁêÜÊú∫Âà∂ÁöÑËÆæËÆ°ÊàñÂÆûÁé∞ÈóÆÈ¢òÔºåÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥Á≥ªÁªüÂÆâÂÖ®È£éÈô©ÔºåÊØîÂ¶ÇÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉÊàñÂÖ∂‰ªñÈ´òÂç±ÊºèÊ¥û„ÄÇÂ§çÁé∞ËÑöÊú¨ÁöÑ‰ª£Á†Å‰∏ªË¶ÅÁî®‰∫éÈ™åËØÅÂÆâÂÖ®È£éÈô©Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂ§çÁé∞ÔºåËÄåÊú¨ÈóÆÈ¢òÂπ∂Êó†ÂÆâÂÖ®ÈöêÊÇ£ÔºåÂõ†Ê≠§‰∏çÈúÄË¶ÅÁîüÊàêÂ§çÁé∞ËÑöÊú¨„ÄÇËã•Â∏åÊúõÈ™åËØÅËØ•ÈóÆÈ¢òÁöÑÂ≠òÂú®ÔºåÂèØ‰ª•ÈÄöËøáÈÉ®ÁΩ≤‰∏Ä‰∏™ÈÖçÁΩÆ‰∫Ü CPU Manager Âíå Memory Manager ÁöÑ guaranteed podÔºåÁÑ∂ÂêéÂú®Âà†Èô§ pod ÂêéËßÇÂØüÁõ∏Â∫îÁä∂ÊÄÅÊñá‰ª∂ÁöÑÂÜÖÂÆπÂèòÂåñÔºå‰ΩÜËøôÂ±û‰∫éÊ≠£Â∏∏ÁöÑËµÑÊ∫êË∞ÉÂ∫¶ÈÄªËæëÈ™åËØÅÔºåÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®ÊîªÂáªË°å‰∏∫„ÄÇ

---


## Issue #129797 Unable to create service with ipFamilyPolicy RequireDualStack when service cidr is bigger then /112

- Issue ÈìæÊé•Ôºö[#129797](https://github.com/kubernetes/kubernetes/issues/129797)

### Issue ÂÜÖÂÆπ

#### What happened?

We run periodic conformance tests in github.com/kubernetes-sigs/cluster-api and they started to fail https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api#capi-e2e-latestk8s-main

These currently
* use https://dl.k8s.io/ci/latest-1.33.txt to build kind images from source with that version
* create a Cluster with that version by using Cluster API and cluster-api-provider-docker (and in this case with dualstack ip enabled)
* They set `--service-cluster-ip-range=10.128.0.0/12,fd00:100:64::/108` in kube-apiserver (and kube-controller-manager)
* And after finishing cluster creation run conformance tests, including the selector for the `IPv6DualStack` feature tests.

Several conformace tests for `IPv6DualStack` then failed with e.g. the following message: 
```
I0124 07:48:52.822155 16 utils.go:776] Unexpected error: Failed to create node-port-service service: Internal error occurred: failed to allocate a serviceIP: range is full: 
    <*errors.StatusError | 0xc0031bb9a0>: 
    Internal error occurred: failed to allocate a serviceIP: range is full
    {
        ErrStatus: 
            code: 500
            details:
              causes:
              - message: 'failed to allocate a serviceIP: range is full'
            message: 'Internal error occurred: failed to allocate a serviceIP: range is full'
            metadata: {}
            reason: InternalError
            status: Failure,
    }
[FAILED] Failure recorded during attempt 1:
```

Note: The kube-apiserver is able to create the IPAddress object, but the service does not get created.

#### What did you expect to happen?

Tests to succeed.

#### How can we reproduce it (as minimally and precisely as possible)?

Build kindest/node image for k/k commit 0798325ba13643

```
# checkout k/k on 0798325ba13643
kind build node-image --image "kindest/node:v1.33.0-alpha.0.614_0798325ba13643" $(pwd)
```

Create a kind cluster using the following configuration:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
        extraArgs:
          service-cluster-ip-range: "10.96.0.10/12,fd00:100:64::/108"
```

```sh
kind create cluster --name test --config kind-cluster.yaml --image kindest/node:v1.33.0-alpha.0.614_0798325ba13643
```

Try to create a service with `.spec.ipFamilyPolicy` set to `RequireDualStack` or `.spec.ipFamilies` set to `["IPv6"]`:

```sh
$ kubectl create svc clusterip --tcp=80:80 foo -o json --dry-run=client | jq '.spec.ipFamilies = ["IPv6"]' | k apply -f -
Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed to allocate a serviceIP: range is full

$ kubectl create svc clusterip --tcp=80:80 foo -o json --dry-run=client | jq '.spec.ipFamilies = ["IPv6"]' | k apply -f -
Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed to allocate a serviceIP: range is full
```

Note: there's a chance that the ip allocation in kube-apiserver picks an ipv6 address which is then part the first /112 part of and then works.

#### Anything else we need to know?

Follow-up to https://github.com/kubernetes/kubernetes/issues/129753#issuecomment-2609425004

I also stepped into kube-apiserver:
* It fails to validate here that the ip is contained in the CIDR (example configuration: CIDR `fd00:100:64::/108`, IP `fd00:100:64::f:321c`
  * https://github.com/kubernetes/kubernetes/blob/0798325ba13643358aa3ebb7c6ddc3006ac26a7c/pkg/registry/core/service/ipallocator/bitmap.go#L173-L179
* The IP is part of that network, but the bitmap is capped to `/112`


Workarounds:
* Enable FeatureGate `DisableAllocatorDualWrite` in kube-apiserver
* Use a `/112` cidr for services


cc @aojea 

And Kudos for Antonio helping me to dig into it!

#### Kubernetes version

<details>

```console
‚ùØ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.33.0-alpha.0.614+0798325ba13643
WARNING: version difference between client (1.29) and server (1.33) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

---

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØÂú® Kubernetes ÈõÜÁæ§‰∏≠ÔºåÂΩì‰ΩøÁî® dual-stack ÊúçÂä°‰∏îÈÖçÁΩÆÁöÑ IPv6 ÊúçÂä° CIDR Â§ß‰∫é /112 Êó∂ÔºåÂú®ÂàõÂª∫ÊúçÂä°Êó∂‰ºöÊä•Èîô‚Äúfailed to allocate a serviceIP: range is full‚Äù„ÄÇÁªèËøáÂàÜÊûêÔºåÂèØ‰ª•ÁúãÂá∫ÈóÆÈ¢òÂá∫Âú® kube-apiserver ÂÜÖÈÉ®ÂØπ IP ÂàÜÈÖçÁöÑ bitmap ÂÆûÁé∞Â≠òÂú®ÈôêÂà∂ÔºöËôΩÁÑ∂ IP Â±û‰∫éÈÖçÁΩÆÁöÑ CIDRÔºå‰ΩÜÂàÜÈÖçÂô®ÂÜÖÈÉ®‰ªÖÂ§ÑÁêÜ /112 ÁöÑËåÉÂõ¥ÔºåÂØºËá¥ÂÆûÈôÖÂèØÁî®ÁöÑ IP Êï∞Èáè‰∏çÂ§üÔºå‰ªéËÄåÊä•Èîô„ÄÇËØ•ÈóÆÈ¢òÂ±û‰∫éÈÖçÁΩÆ‰∏éÂÜÖÈÉ®ÈÄªËæëÁöÑ bugÔºå‰∏çÊ∂âÂèäÊú™ÁªèÊéàÊùÉÁöÑÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÈ´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

ÊåâÁÖßÁõÆÂâçÊèê‰æõÁöÑÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåÊ≠§ÈóÆÈ¢òÂπ∂ÈùûÂÆâÂÖ®ÈóÆÈ¢òÔºåÂÖ∂‰∫ßÁîüÁöÑÈîôËØØ‰πüÂè™‰ºöÂØºËá¥ÊúçÂä°ÂàõÂª∫Â§±Ë¥•ÔºåÂπ∂‰∏ç‰ºöÁõ¥Êé•Ë¢´ÊîªÂáªËÄÖÂà©Áî®Êù•ÊâßË°åÊÅ∂ÊÑèÊìç‰Ωú„ÄÇÂõ†Ê≠§ÔºåÊú¨ issue ÁöÑÈ£éÈô©ËØÑÁ∫ßÂ∫î‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇ

---

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
import threading
import http.server
import socketserver
import time
import sys
import json

from kubernetes import client, config
from kubernetes.client.rest import ApiException

# HTTP ÊúçÂä°Âô®Â§ÑÁêÜÁ±ªÔºåÁî®‰∫éÁõëÂê¨8080Á´ØÂè£ÔºåËæÖÂä©ËßÇÂØüÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        response = {
            "message": "HTTP Server is running on port 8080",
            "path": self.path
        }
        self.wfile.write(json.dumps(response).encode('utf-8'))

def start_http_server():
    PORT = 8080
    handler = SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", PORT), handler) as httpd:
        print(f"HTTP server is serving at port {PORT}")
        httpd.serve_forever()

def create_dualstack_service():
    try:
        # Â∞ùËØïÂä†ËΩΩÈªòËÆ§ kubeconfig ÈÖçÁΩÆ
        config.load_kube_config()
    except Exception as e:
        print(f"Âä†ËΩΩ kubeconfig Â§±Ë¥•: {e}")
        sys.exit(1)

    v1 = client.CoreV1Api()

    # ÊûÑÈÄ†ÊúçÂä°ÂØπË±°ÔºåÊ≥®ÊÑè ipFamilies Âíå ipFamilyPolicy Â≠óÊÆµÈúÄË¶ÅËÆæÁΩÆ‰∏∫Ëß¶ÂèëÈóÆÈ¢òÁöÑÈÖçÁΩÆ
    service_manifest = {
        "apiVersion": "v1",
        "kind": "Service",
        "metadata": {
            "name": "dualstack-test-service"
        },
        "spec": {
            "ports": [
                {
                    "protocol": "TCP",
                    "port": 80,
                    "targetPort": 80
                }
            ],
            "selector": {
                "app": "dualstack-test"
            },
            "type": "ClusterIP",
            "ipFamilies": ["IPv6"],
            "ipFamilyPolicy": "RequireDualStack"
        }
    }

    try:
        print("Â∞ùËØïÂú® default ÂëΩÂêçÁ©∫Èó¥ÂàõÂª∫ÊúçÂä° dualstack-test-service ...")
        resp = v1.create_namespaced_service(namespace="default", body=service_manifest)
        print("ÊúçÂä°ÂàõÂª∫ÊàêÂäüÔºåËøîÂõûÁªìÊûúÔºö")
        print(resp)
    except ApiException as e:
        print("Ë∞ÉÁî® API Êó∂ÂèëÁîüÂºÇÂ∏∏Ôºö")
        print(e)
    except Exception as ex:
        print("ÂèëÁîüÂºÇÂ∏∏Ôºö", ex)

def main():
    # ÂêØÂä® HTTP ÊúçÂä°Âô®Á∫øÁ®ã
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # Á≠âÂæÖ HTTP ÊúçÂä°Âô®ÂêØÂä®
    time.sleep(1)

    # Â∞ùËØïÂàõÂª∫ dualstack ÊúçÂä°ÔºåÁî®‰ª•Â§çÁé∞ issue ÊèèËø∞ÁöÑÈóÆÈ¢ò
    create_dualstack_service()

    # ‰øùÊåÅ‰∏ªÁ∫øÁ®ãËøêË°åÔºå‰æø‰∫éËßÇÂØü HTTP ÊúçÂä°Âô®ÂìçÂ∫î
    try:
        while True:
            time.sleep(2)
    except KeyboardInterrupt:
        print("ÈÄÄÂá∫Á®ãÂ∫è")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨È¶ñÂÖàÂä†ËΩΩÈªòËÆ§‰ΩçÁΩÆÁöÑ kubeconfigÔºàÂÅáËÆæÂá≠ËØÅÂ∑≤Âú®ÈªòËÆ§Ë∑ØÂæÑ‰∏≠ÔºâÔºå‰ΩøÁî® Kubernetes Python ÂÆ¢Êà∑Á´ØËøûÊé•Âà∞ÈõÜÁæ§„ÄÇ
2. Âú® create_dualstack_service() ÂáΩÊï∞‰∏≠ÔºåÊûÑÈÄ†‰∫Ü‰∏Ä‰∏™ Service ÂØπË±°ÔºåÊúçÂä°ËßÑÊ†º‰∏≠ÊòéÁ°ÆËÆæÁΩÆ‰∫Ü ipFamilies ‰∏∫ ["IPv6"] ‰ª•Âèä ipFamilyPolicy ËÆæÁΩÆ‰∏∫ RequireDualStackÔºåËøôÊ≠£ÊòØËß¶Âèë issue ‰∏≠Êä•ÈîôÁöÑÈÖçÁΩÆÊù°‰ª∂„ÄÇ
3. Ë∞ÉÁî® CoreV1Api ÁöÑ create_namespaced_service() ÊñπÊ≥ïÂ∞ùËØïÂú® default ÂëΩÂêçÁ©∫Èó¥‰∏ãÂàõÂª∫ËØ•ÊúçÂä°„ÄÇÂ¶ÇÊûúÈõÜÁæ§Â≠òÂú®‰∏é issue ÊèèËø∞Áõ∏Á¨¶ÁöÑÈÖçÁΩÆÔºåÂàôÊúâÂèØËÉΩÂ§çÁé∞‚Äúfailed to allocate a serviceIP: range is full‚ÄùÁöÑÈîôËØØ„ÄÇ
4. ËÑöÊú¨ÂêåÊó∂ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÊòìÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºåÁî®‰∫éÂú®Êú¨Âú∞ËßÇÂØüÂíåË∞ÉËØïÔºà‰æãÂ¶ÇÂèØ‰ª•ÈÄöËøáËÆøÈóÆ http://localhost:8080Êü•Áúã HTTP ÊúçÂä°Áä∂ÊÄÅÔºâ„ÄÇ
5. ËØ∑Ê≥®ÊÑèÔºåËØ•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÁîü‰∫ßÁéØÂ¢ÉÈÄ†Êàê‰ªª‰ΩïÂΩ±Âìç„ÄÇ

---


## Issue #129795 ReplicaSet controller may create extra Pods when expectations expire during informer delays

- Issue ÈìæÊé•Ôºö[#129795](https://github.com/kubernetes/kubernetes/issues/129795)

### Issue ÂÜÖÂÆπ

#### What happened?

The ReplicaSet controller may create more Pods than desired under the following conditions:

1. ReplicaSet controller creates N Pods and sets expectations (+N)
2. Due to network issues or high latency, PodInformer hasn't received the Pod creation events
3. After 5 minutes, the expectations expire (isExpired() == true)
4. The `SatisfiedExpectations` function returns `true` immediately when expectations expire, without checking if they are actually fulfilled
5. This causes the controller to create additional Pods, even though the previously created Pods may still exist

Code path Ôºöhttps://github.com/kubernetes/kubernetes/blob/f6f06806cc43ed9f7eb2f68368c90a8239884118/pkg/controller/controller_utils.go#L193

```
func (r *ControllerExpectations) SatisfiedExpectations(logger klog.Logger, controllerKey string) bool {
	if exp, exists, err := r.GetExpectations(controllerKey); exists {
		if exp.Fulfilled() {
			logger.V(4).Info("Controller expectations fulfilled", "expectations", exp)
			return true
		} else if exp.isExpired() {
			logger.V(4).Info("Controller expectations expired", "expectations", exp)
			return true  // There is an issue here. !!! 
		} else {
			logger.V(4).Info("Controller still waiting on expectations", "expectations", exp)
			return false
		}
	} else if err != nil {
		logger.V(2).Info("Error encountered while checking expectations, forcing sync", "err", err)
	} else {
		// When a new controller is created, it doesn't have expectations.
		// When it doesn't see expected watch events for > TTL, the expectations expire.
		//	- In this case it wakes up, creates/deletes controllees, and sets expectations again.
		// When it has satisfied expectations and no controllees need to be created/destroyed > TTL, the expectations expire.
		//	- In this case it continues without setting expectations till it needs to create/delete controllees.
		logger.V(4).Info("Controller either never recorded expectations, or the ttl expired", "controller", controllerKey)
	}
	// Trigger a sync if we either encountered and error (which shouldn't happen since we're
	// getting from local store) or this controller hasn't established expectations.
	return true
}
```

If the PodInformer is delayed, manageReplicas will create too many Pods.
https://github.com/kubernetes/kubernetes/blob/f6f06806cc43ed9f7eb2f68368c90a8239884118/pkg/controller/replicaset/replica_set.go#L572

```
	if diff < 0 {
		diff *= -1
		if diff > rsc.burstReplicas {
			diff = rsc.burstReplicas
		}
		rsc.expectations.ExpectCreations(logger, rsKey, diff)
		successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {
			err := rsc.podControl.CreatePods(ctx, rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))
			if err != nil {
				if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
					return nil
				}
			}
			return err
		})

		if skippedPods := diff - successfulCreations; skippedPods > 0 {
			logger.V(2).Info("Slow-start failure. Skipping creation of pods, decrementing expectations", "podsSkipped", skippedPods, "kind", rsc.Kind, "replicaSet", klog.KObj(rs))
			for i := 0; i < skippedPods; i++ {
				rsc.expectations.CreationObserved(logger, rsKey)
			}
		}
		return err
	} 
``` 


#### What did you expect to happen?

When controller expectations expire, the ReplicaSet controller should still ensure the actual Pod count matches the desired count, even if there are informer delays. Specifically:

1. If expectations expire but are not fulfilled (add/delete operations not observed), the controller should wait for the informer to catch up rather than immediately proceeding with new Pod creations.

2. The `SatisfiedExpectations` function should return `false` when expectations are expired but not fulfilled, preventing potential Pod over-provisioning during informer delays.

This would maintain the safety guarantee that a ReplicaSet never creates more Pods than desired, even in the presence of informer delays.

#### How can we reproduce it (as minimally and precisely as possible)?

NONE

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÂèçÊò†ÁöÑÊòØ Kubernetes ReplicaSet controller Âú® informer Âª∂ËøüÊó∂Âá∫Áé∞ÁöÑ‰∏ÄÂ§ÑÈÄªËæëÁº∫Èô∑ÔºöÂΩìÊúüÊúõÁä∂ÊÄÅË∂ÖÊó∂ÂêéÔºàisExpired ËøîÂõû trueÔºâÔºåÈÄªËæëÁõ¥Êé•Â∞ÜÊúüÊúõÂà§Êñ≠‰∏∫Êª°Ë∂≥Ôºå‰ªéËÄåÂØºËá¥ controller ÈáçÂ§çÂàõÂª∫ PodÔºåÂèØËÉΩ‰∫ßÁîüÈ¢ùÂ§ñÁöÑ Pod„ÄÇËØ•ÈóÆÈ¢ò‰ºöÂØºËá¥ÂÆûÈôÖ Pod Êï∞Â§ß‰∫éÈ¢ÑÊúüÔºåËøõËÄåÂèØËÉΩÂºïËµ∑ËµÑÊ∫êÊµ™Ë¥πÁîöËá≥ËµÑÊ∫êËÄóÂ∞ΩÔºåÁêÜËÆ∫‰∏äÂèØËÉΩËß¶ÂèëÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁöÑÂú∫ÊôØ„ÄÇ‰ΩÜÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºö
1. Ê≠§Áº∫Èô∑ÊòØÁî±‰∫éÊéßÂà∂Âô®ÂÜÖÈÉ®ÈÄªËæëÂà§Êñ≠ÈîôËØØÂºïËµ∑ÁöÑÔºåËÄåÈùûÂ§ñÈÉ®ËæìÂÖ•„ÄÅ‰ª£Á†ÅÊâßË°å„ÄÅÂëΩ‰ª§Ê≥®ÂÖ•ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®ÊºèÊ¥û„ÄÇ
2. Ëß¶ÂèëÈóÆÈ¢ò‰æùËµñ‰∫é informer Âª∂ËøüÊàñÁΩëÁªúÈóÆÈ¢òÁ≠âÁâπÊÆäÂú∫ÊôØÔºå‰∏îÊîªÂáªËÄÖÈÄöÂ∏∏ÈúÄË¶ÅÂÖ∑ÊúâËæÉÈ´òÊùÉÈôêÊàñÊéßÂà∂ÈõÜÁæ§ÁΩëÁªúÔºåÊâçËÉΩ‰∫∫‰∏∫Ëß¶ÂèëËøôÁßçÊÉÖÂÜµ„ÄÇ
3. Ê†πÊçÆÁªôÂÆöÁöÑÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåÂØπ‰ªÖ‰ºöÂºïËµ∑È¢ùÂ§ñ Pod ÂàõÂª∫ÔºàÂç≥ËµÑÊ∫êÊµ™Ë¥πÊàñÂèØËÉΩÂØºËá¥ DoSÔºâÁöÑÁº∫Èô∑ÔºåÂπ∂Êú™Ê∂âÂèäÊú™ÊéàÊùÉËÆøÈóÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÈ´òÂÆâÂÖ®È£éÈô©Ë°å‰∏∫„ÄÇ

ÁªºÂêàÊù•ÁúãÔºåËøô‰∏™ÈóÆÈ¢òÂ±û‰∫éÁ®ãÂ∫èÈÄªËæëÁº∫Èô∑ÔºåËôΩÁÑ∂Âú®‰∏ÄÂÆöÊù°‰ª∂‰∏ãÂèØËÉΩÂºïÂèëÊúçÂä°ÊãíÁªùÔºàDoSÔºâÁöÑÂΩ±ÂìçÔºå‰ΩÜ‰∏çÊûÑÊàêÁõ¥Êé•ÁöÑÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ• Issue ÊâÄÊèèËø∞ÁöÑÈóÆÈ¢ò‰∏∫ ReplicaSet controller Âú® informer Âª∂ËøüÂØºËá¥È¢ÑÊúüÁä∂ÊÄÅÂ§±ÊïàÊÉÖÂÜµ‰∏ãÔºåÈîôËØØÂà§Êñ≠ÊúüÊúõÂ∑≤Êª°Ë∂≥Ôºå‰ªéËÄåÈáçÊñ∞ÂàõÂª∫ Pod ÁöÑÈÄªËæëÁº∫Èô∑„ÄÇËôΩÁÑ∂Âú®ÁâπÂÆöÂú∫ÊôØ‰∏ãÂèØËÉΩÂõ†È¢ùÂ§ñÂàõÂª∫ Pod ÂØºËá¥ËµÑÊ∫êËøáÂ∫¶‰ΩøÁî®ÔºåËøõËÄåÂºïÂèëÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁöÑÈ£éÈô©Ôºå‰ΩÜËØ•ÈóÆÈ¢òÁöÑËß¶ÂèëÊù°‰ª∂Ë¶ÅÊ±ÇÂ§ñÈÉ®ÁéØÂ¢ÉÔºàÂ¶ÇÁΩëÁªúÂª∂ËøüÔºâÂ§Ñ‰∫éÂºÇÂ∏∏Áä∂ÊÄÅÔºå‰∏îÊîªÂáªËÄÖÈÄöÂ∏∏ÈúÄË¶ÅÂÖ∑Â§áËæÉÈ´òÊùÉÈôêÊâçËÉΩÂΩ±ÂìçÊéßÂà∂Âô®Ë°å‰∏∫„ÄÇÂõ†Ê≠§ÔºåËØ•Áº∫Èô∑Âπ∂‰∏çÂ±û‰∫éÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºå‰πü‰∏çÂ≠òÂú®Êú™ÊéàÊùÉËÆøÈóÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÈ´òÂÆâÂÖ®È£éÈô©ÊÉÖÂΩ¢„ÄÇ‰æùÁÖßÈóÆÈ¢òÊèèËø∞ÂèäÈ£éÈô©Âà§ÂÆöÊ†áÂáÜÔºåËØ•ÈóÆÈ¢òÂ∫îË¢´ÂΩíÁ±ª‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÂÆâÂÖ®È£éÈô©„ÄÇ

---


## Issue #129899 `kubectl auth can-i` gives incorrect result when rolebinding is present but incorrect

- Issue ÈìæÊé•Ôºö[#129899](https://github.com/kubernetes/kubernetes/issues/129899)

### Issue ÂÜÖÂÆπ

<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->

**What happened**:

I created a serviceaccount, role and rolebinding to read secrets in a given namespace. I forgot to specify the namespace in the rolebinding, meaning the serviceaccount wasn't actually able to read the secrets. However, when I ran `kubectl auth can-i get secrets -as <serviceaccount> -n <namespace>` the answer came back as `yes`. 

**What you expected to happen**:

I would have expected the answer to be `no`, which would then have told me the issue was somewhere in the role/rolebinding, as opposed to in the system using the serviceaccount.

**How to reproduce it (as minimally and precisely as possible)**:
```bash
$ kubectl version
Client Version: v1.30.3
Server Version: v1.30.1
```

Create a serviceaccount:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: example-sa
  namespace: example
```

Create a role granting the necessary permissions:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: example
  name: secretreader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "watch" ]
```

Create a rolebinding, but forget to add  `metadata.namespace`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: secretreader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: secretreader
subjects:
- apiGroup: ""
  kind: ServiceAccount
  name: example-sa
  namespace: example
```
run the command:

```bash
$ kubectl auth can-i get secrets -as example-sa -n example
yes
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes client and server versions (use `kubectl version`): 
-- Client Version: v1.30.3
-- Server Version: v1.30.1
- Cloud provider or hardware configuration: VM on our self-managed OpenStack env
- OS: Talos (v1.7.4)



### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue Êä•ÂëäÁöÑÊòØÂÖ≥‰∫é RBAC ÊéàÊùÉÊ£ÄÊµãÂëΩ‰ª§ ‚Äúkubectl auth can-i‚Äù ËøîÂõûÁªìÊûú‰∏éÈ¢ÑÊúü‰∏çÁ¨¶ÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈôÖ‰∏äÔºåÂú®ÂàõÂª∫ RoleBinding Êó∂Êú™ÊåáÂÆö metadata.namespace ÂØºËá¥ÊúçÂä°Ë¥¶Âè∑Âπ∂Êú™Ëé∑ÂæóÈ¢ÑÊúüÊùÉÈôêÔºå‰ΩÜ ‚Äúkubectl auth can-i‚Äù Âç¥‰ªçËøîÂõû ‚Äúyes‚Äù„ÄÇËøôÂ±û‰∫éÂ∑•ÂÖ∑Âú®Ëß£Êûê RBAC ÈÖçÁΩÆÊó∂‰∫ßÁîüËØØÂà§ÁöÑÈóÆÈ¢òÔºå‰∏ªË¶ÅÂΩ±ÂìçÁî®Êà∑ÂØπÊùÉÈôêÈÖçÁΩÆÁä∂ÊÄÅÁöÑÂà§Êñ≠„ÄÇÁÑ∂ËÄåÔºåËøôÈáåÂπ∂‰∏çÂ≠òÂú®ÂÆûÈôÖÁöÑÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÊîªÂáªÈìæÊù°ÔºåÊîªÂáªËÄÖÂπ∂‰∏çËÉΩÈÄöËøáËøô‰∏™ bug Áõ¥Êé•Ëé∑ÂæóÈ¢ùÂ§ñÊùÉÈôêÊàñÂèëÂä®ÊîªÂáª„ÄÇ

Ê†πÊçÆÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåËØ•ÈóÆÈ¢òÂ±û‰∫éÂ∑•ÂÖ∑ËæìÂá∫ÈîôËØØËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÂÖ∂ËØØÂà§ËôΩÁÑ∂ÂèØËÉΩÂØºËá¥‰ΩøÁî®ËÄÖÂØπÊùÉÈôêÁä∂ÊÄÅ‰∫ßÁîüËØØËß£Ôºå‰ΩÜÂπ∂‰∏ç‰ºöÁªôÁ≥ªÁªüÂ∏¶Êù•ÂÆûÈôÖÁöÑÂÆâÂÖ®ÈöêÊÇ£ÔºåÂõ†Ê≠§‰∏çÂ∫îÂΩíÁ±ª‰∏∫ÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ¥ÊòéÔºöËØ•ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÂ§çÁé∞ kubectl auth can-i ÂëΩ‰ª§ËøîÂõûÈîôËØØÊùÉÈôê‰ø°ÊÅØÁöÑËøáÁ®ãÔºå‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂ÔºåÂÆûÈôÖÊÉÖÂÜµÈúÄË¶ÅÂú®KubernetesÈõÜÁæ§‰∏≠ËøêË°åkubectlÂëΩ‰ª§Êù•ËßÇÂØüÁé∞Ë±°„ÄÇ

Ê≥®ÊÑèÔºöÁî±‰∫éËØ•ÈóÆÈ¢òÊú¨Ë∫´‰∏çÂ±û‰∫éÈ´òÈ£éÈô©ÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Êú¨Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫Â¶Ç‰ΩïË∞ÉÁî®kubectlÂëΩ‰ª§Âπ∂Ê£ÄÊü•ËøîÂõûÁªìÊûúÔºåËÄå‰∏ç‰ºöÂÆûÊñΩ‰ªª‰ΩïÂÆûÈôÖÁöÑÊîªÂáªË°å‰∏∫„ÄÇ
"""

import subprocess

def run_command(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        return f"ÈîôËØØ: {e.stderr.strip()}"

def main():
    # ÂÅáËÆækubeconfigÂú®ÈªòËÆ§‰ΩçÁΩÆ
    # ‰ª•‰∏ãÂëΩ‰ª§Ê®°ÊãüËøêË°å: kubectl auth can-i get secrets -as example-sa -n example
    cmd = "kubectl auth can-i get secrets -as=example-sa -n=example"
    output = run_command(cmd)
    print(f"ÂëΩ‰ª§ '{cmd}' ÁöÑËæìÂá∫‰∏∫: {output}")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨È¶ñÂÖàÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ run_command ÂáΩÊï∞Êù•ÊâßË°å shell ÂëΩ‰ª§ÔºåÂπ∂ÊçïËé∑ËæìÂá∫ÊàñÈîôËØØ‰ø°ÊÅØ„ÄÇ
2. Âú® main ÂáΩÊï∞‰∏≠ÔºåÈÄöËøáÊâßË°å ‚Äúkubectl auth can-i get secrets -as=example-sa -n=example‚Äù Ê®°ÊãüÂ§çÁé∞ issue ‰∏≠ÊèèËø∞ÁöÑÈóÆÈ¢ò„ÄÇËØ•ÂëΩ‰ª§‰ºöËæìÂá∫ kubectl ÂØπÂΩìÂâç RBAC ËÆæÁΩÆÁöÑËØÑ‰º∞ÁªìÊûú„ÄÇ
3. Ê†πÊçÆ issue Âú∫ÊôØÔºåËã• RoleBinding Âá∫Áé∞ÈÖçÁΩÆÈîôËØØÔºàÂ¶ÇÁº∫Â∞ë metadata.namespaceÔºâÔºåÂàôÊúüÊúõËæìÂá∫‰∏∫ ‚Äúno‚ÄùÔºõ‰ΩÜÂÆûÈôÖËøîÂõû ‚Äúyes‚Äù„ÄÇËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫Â¶Ç‰ΩïËé∑ÂèñËØ•ËæìÂá∫ÁªìÊûú„ÄÇ
4. Êú¨ËÑöÊú¨‰ªÖÂú®Êú¨Âú∞ÊâßË°åÔºåÁî®‰∫éÊ®°ÊãüÊ£ÄÊü•Áé∞Ë±°ÔºåÂπ∂‰∏çÊ∂âÂèäÂÆûÈôÖÊºèÊ¥ûÂà©Áî®„ÄÇÁî±‰∫éËØ•ÈóÆÈ¢òÊú¨Ë∫´‰∏çÂ±û‰∫éÈ´òÈ£éÈô©ÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Êó†ÈúÄËøõ‰∏ÄÊ≠•Â§çÊùÇÁöÑÂ§çÁé∞Êìç‰ΩúÊàñÊîªÂáª‰ª£Á†Å„ÄÇ

---


## Issue #129774 defaulter-gen v1.32.x imports packages it doesn't need, which fails with FormatOnly mode

- Issue ÈìæÊé•Ôºö[#129774](https://github.com/kubernetes/kubernetes/issues/129774)

### Issue ÂÜÖÂÆπ

#### What happened?

defaulter-gen  `zz_generated.defaults.go` Content error.

example case: https://github.com/dongjiang1989/customapis/tree/main/pkg/apis/custom/v1 

```yaml
package v1

import (
        corev1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// TestType is a top-level type. A client is created for it.
type TestType struct {
        metav1.TypeMeta `json:",inline"`
        // +optional
        metav1.ObjectMeta `json:"metadata,omitempty"`
        // +optional
        Spec TestSpec `json:"spec,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// TestTypeList is a top-level list type. The client methods for lists are automatically created.
// You are not supposed to create a separated client for this one.
type TestTypeList struct {
        metav1.TypeMeta `json:",inline"`
        // +optional
        metav1.ListMeta `json:"metadata,omitempty"`

        Items []TestType `json:"items"`
}

type TestSpec struct {
        ReplicaSpecs map[string]*ReplicaSpec `json:"replicaSpecs"` --------> this part
}

// +k8s:openapi-gen=true
// +k8s:deepcopy-gen=true
type ReplicaSpec struct {
        // +optional
        Replicas *int32 `json:"replicas,omitempty"`

        Template corev1.PodTemplateSpec `json:"template,omitempty"`
}
````

output:
```yaml
package v1

import (
        corev1 "k8s.io/api/core/v1"                              --------->  This imported package is not required
        runtime "k8s.io/apimachinery/pkg/runtime"
)

// RegisterDefaults adds defaulters functions to the given scheme.
// Public to allow building arbitrary schemes.
// All generated defaulters are covering - they call all nested defaulters.
func RegisterDefaults(scheme *runtime.Scheme) error {
        scheme.AddTypeDefaultingFunc(&TestType{}, func(obj interface{}) { SetObjectDefaults_TestType(obj.(*TestType)) })
        scheme.AddTypeDefaultingFunc(&TestTypeList{}, func(obj interface{}) { SetObjectDefaults_TestTypeList(obj.(*TestTypeList)) })
        return nil
}

func SetObjectDefaults_TestType(in *TestType) {
        SetDefaults_TestType(in)
}

func SetObjectDefaults_TestTypeList(in *TestTypeList) {
        for i := range in.Items {
                a := &in.Items[i]
                SetObjectDefaults_TestType(a)
        }
}

```

#### What did you expect to happen?

defaulter-gen  `zz_generated.defaults.go` Content error.

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
dongjiang@MacBook Pro:customapis $ pwd
/Users/dongjiang/Documents/go/src/github.com/dongjiang1989/customapis
dongjiang@MacBook Pro:customapis $ ./hack/update-codegen.sh 
/Users/dongjiang/Documents/go/pkg/mod/k8s.io/code-generator@v0.32.1
Generating deepcopy code for 1 targets
Generating defaulter code for 1 targets
Generating openapi code for 1 targets
Generating applyconfig code for 1 targets
Generating client code for 1 targets
Generating lister code for 1 targets
Generating informer code for 1 targets
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>

code-generator v1.32.1 and  v1.32.0


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
Ê≠§ issue ÊèèËø∞ÁöÑÊòØÂú®‰ΩøÁî® defaulter-gen ‰ª£Á†ÅÁîüÊàêÂ∑•ÂÖ∑Êó∂ÔºåÁîüÊàêÁöÑ zz_generated.defaults.go Êñá‰ª∂‰∏≠ÈîôËØØÂú∞ÂºïÂÖ•‰∫Ü‰∏çÈúÄË¶ÅÁöÑÂåÖÔºà‰æãÂ¶Ç k8s.io/api/core/v1ÔºâÔºåÂØºËá¥Âú® FormatOnly Ê®°Âºè‰∏ãÊûÑÂª∫Â§±Ë¥•„ÄÇËØ•ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØ‰ª£Á†ÅÁîüÊàêÈÄªËæëÁöÑÈóÆÈ¢òÔºå‰∏éÂÆâÂÖ®Èò≤Êä§„ÄÅÂá≠ÊçÆÊ≥ÑÈú≤„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÈ´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢òÊó†ÂÖ≥Ôºå‰∏ç‰ºöÂ∏¶Êù•ËøúÁ®ã‰ª£Á†ÅÊâßË°å„ÄÅÊùÉÈôêÊèêÂçáÊàñÊãíÁªùÊúçÂä°Á≠âÂÆâÂÖ®ÈöêÊÇ£Ôºå‰ªÖÂΩ±Âìç‰ª£Á†ÅÊ≠£Á°ÆÁîüÊàê„ÄÇÊåâÁÖßÁé∞ÊúâÊèèËø∞ÔºåÊ≠§ÈóÆÈ¢òÂ±û‰∫éÂØπÂ∑•ÂÖ∑ÂäüËÉΩÁöÑÁº∫Èô∑ÔºåÂπ∂ÈùûÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Ê®°ÊãüÂú®Âá∫Áé∞ËØ•ÈóÆÈ¢òÊó∂ÁöÑÂ§çÁé∞ÊµÅÁ®ãÔºö
1. ÂÅáËÆæÂΩìÂâçÁõÆÂΩï‰∏∫È°πÁõÆÊ†πÁõÆÂΩïÔºåÂπ∂‰∏îÂ≠òÂú® ./hack/update-codegen.sh ËÑöÊú¨Ôºå
2. ÊâßË°å‰ª£Á†ÅÁîüÊàêÂëΩ‰ª§ÂêéÔºåÊ£ÄÊü•ÁîüÊàêÁöÑ zz_generated.defaults.go Êñá‰ª∂‰∏≠ÊòØÂê¶Â≠òÂú®‰∏çÈúÄË¶ÅÁöÑÂåÖÂºïÁî®„ÄÇ

ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÁ≥ªÁªüÈÄ†Êàê‰øÆÊîπ„ÄÇ
"""

import subprocess
import os

def run_codegen():
    try:
        # ÊâßË°å‰ª£Á†ÅÁîüÊàêËÑöÊú¨
        result = subprocess.run(["./hack/update-codegen.sh"], capture_output=True, text=True, check=True)
        print("‰ª£Á†ÅÁîüÊàêËÑöÊú¨ËæìÂá∫ stdout:")
        print(result.stdout)
        if result.stderr:
            print("‰ª£Á†ÅÁîüÊàêËÑöÊú¨ÈîôËØØËæìÂá∫ stderr:")
            print(result.stderr)
    except subprocess.CalledProcessError as e:
        print("ÊâßË°å‰ª£Á†ÅÁîüÊàêËÑöÊú¨Â§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ:")
        print(e)
        return

def check_generated_file():
    # ÂÅáËÆæÁîüÊàêÊñá‰ª∂‰∏∫ zz_generated.defaults.goÔºåÂú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÁîüÊàê
    generated_file = os.path.join(os.getcwd(), "zz_generated.defaults.go")
    if not os.path.exists(generated_file):
        print("Êú™ÊâæÂà∞ÁîüÊàêÁöÑÊñá‰ª∂: zz_generated.defaults.go")
        return

    with open(generated_file, "r", encoding="utf-8") as f:
        content = f.read()

    # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´‰∏çÂøÖË¶ÅÁöÑÂåÖÂºïÁî®
    if "k8s.io/api/core/v1" in content:
        print("Ê£ÄÊü•Âà∞‰∏çÂøÖË¶ÅÁöÑÂåÖÂºïÁî®: k8s.io/api/core/v1")
    else:
        print("Êú™Ê£ÄÊµãÂà∞‰∏çÂøÖË¶ÅÁöÑÂåÖÂºïÁî®Ôºå‰ª£Á†ÅÁîüÊàêÁªìÊûúÁúãËµ∑Êù•Ê≠£Â∏∏„ÄÇ")

def main():
    run_codegen()
    check_generated_file()

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ• Python ËÑöÊú¨Áî®‰∫éÂ§çÁé∞ defaulter-gen Â∑•ÂÖ∑ÁîüÊàêÁöÑÈóÆÈ¢ò„ÄÇËÑöÊú¨È¶ñÂÖàË∞ÉÁî®È°πÁõÆ‰∏≠ÁöÑ update-codegen.sh ËÑöÊú¨Êù•ÊâßË°å‰ª£Á†ÅÁîüÊàêËøáÁ®ãÔºåÈöèÂêéÊ£ÄÊü•ÁîüÊàêÁöÑ zz_generated.defaults.go Êñá‰ª∂ÊòØÂê¶ÂåÖÂê´Â§ö‰ΩôÁöÑÂåÖÂºïÁî® ‚Äúk8s.io/api/core/v1‚Äù„ÄÇ  
ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºö  
1. ËÑöÊú¨ÂÅáÂÆöÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï‰∏∫È°πÁõÆÊ†πÁõÆÂΩïÔºåÂπ∂‰∏î hack/update-codegen.sh ËÑöÊú¨Â≠òÂú®‰∏îÂèØÊâßË°å„ÄÇ  
2. Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÁîü‰∫ßÁéØÂ¢ÉÈÄ†ÊàêÈ£éÈô©„ÄÇ  
3. Ê≠§ issue Êú¨Ë∫´‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÔºåËØ•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÁ°ÆËÆ§Â∑•ÂÖ∑ÁîüÊàêÈîôËØØÁöÑÊÉÖÂÜµÔºå‰∏éÂÆâÂÖ®ÊÄßÊó†Áõ¥Êé•ÂÖ≥Á≥ª„ÄÇ

---


## Issue #129763 `watchErrorStream` doesn't wrap the upstream error

- Issue ÈìæÊé•Ôºö[#129763](https://github.com/kubernetes/kubernetes/issues/129763)

### Issue ÂÜÖÂÆπ

#### What happened?

`watchErrorStream` function doesn't wrap the upstream error emitted during reading from the websocket, instead it transforms the error into a simple string making it impossible to use errors.Is/As with the underlying error

```
func watchErrorStream(errorStream io.Reader, d errorStreamDecoder) chan error {
	errorChan := make(chan error)

	go func() {
		defer runtime.HandleCrash()

		message, err := io.ReadAll(errorStream)
		switch {
		case err != nil && err != io.EOF:
			errorChan <- fmt.Errorf("error reading from error stream: %s", err)
		case len(message) > 0:
			errorChan <- d.decode(message)
		default:
			errorChan <- nil
		}
		close(errorChan)
	}()

	return errorChan
}


```

https://github.com/kubernetes/kubernetes/blob/42c4ddbe2d8efe95481d10ccfc9ca098a71e83e9/staging/src/k8s.io/client-go/tools/remotecommand/errorstream.go#L31-L54

#### What did you expect to happen?

`watchErrorStream` should wrap the underlying error so clients can inspect using `errors.As/Is` instead of transforming the error into a string.

#### How can we reproduce it (as minimally and precisely as possible)?

Try to use client-go and use errors.Is with the output of StreamWithContext

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØÂú® error handling ËøáÁ®ã‰∏≠ÔºåÂáΩÊï∞ watchErrorStream Â∞Ü‰∏ãÊ∏∏ÔºàupstreamÔºâËøîÂõûÁöÑÈîôËØØÁõ¥Êé•ËΩ¨Êç¢Êàê‰∫ÜÂ≠óÁ¨¶‰∏≤ÔºåËÄåÊ≤°ÊúâÊ≠£Á°ÆÂú∞ÂåÖË£ÖÂ∫ïÂ±ÇÈîôËØØÔºåËøô‰ºöÂØºËá¥Ë∞ÉÁî®ËÄÖÊó†Ê≥ïÂà©Áî® Go ÁöÑ errors.Is/As Á≠âÂåπÈÖçÊñπÊ≥ïÊ£ÄÊü•Â∫ïÂ±ÇÈîôËØØ„ÄÇÊ≠§ÈóÆÈ¢òÂ±û‰∫éÈîôËØØÂ§ÑÁêÜÂíå‰ª£Á†ÅË¥®ÈáèÊîπËøõÁöÑÈóÆÈ¢òÔºåÂÖ∂‰∫ßÁîüÁöÑÂêéÊûú‰∏ªË¶ÅÊòØÂØπÊéíÊü•ÂíåË∞ÉËØïÈîôËØØÁöÑ‰∏ç‰æøÔºåÂπ∂‰∏ç‰ºöÂºïËµ∑ÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊãíÁªùÊúçÂä°ÊîªÂáªÊàñÂÖ∂‰ªñÈ´òÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇ

**Ëß£ÈáäËØ¥ÊòéÔºö**

Ê≠§Ê¨°ÈóÆÈ¢òÁöÑÊ†∏ÂøÉÊòØÂú®ÈîôËØØÂåÖË£Ö‰∏äÁöÑÈóÆÈ¢òÔºåËôΩÁÑ∂ÂèØËÉΩÂΩ±ÂìçÂºÄÂèëËÄÖÁ≤æÂáÜÂÆö‰ΩçÈîôËØØÁöÑËÉΩÂäõÔºå‰ΩÜÂπ∂‰∏ç‰ºöÂú®Â§öÁî®Êà∑Âú∫ÊôØÊàñÊú™ÊéàÊùÉÁöÑÊÉÖÂÜµ‰∏ãÂØºËá¥ÂÆâÂÖ®È£éÈô©„ÄÇÂêåÊó∂‰πü‰∏çÊ∂âÂèäÊïèÊÑü‰ø°ÊÅØÊ≥ÑÊºèÊàñÊùÉÈôê‰∏çË∂≥ÂØºËá¥ÁöÑÊãíÁªùÊúçÂä°ÈóÆÈ¢òÔºåÂõ†Ê≠§ÊåâÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåËØ•ÈóÆÈ¢ò‰∏çÂ±û‰∫éÂÆâÂÖ®È£éÈô©ËåÉÁï¥Ôºå‰∏çÈúÄË¶ÅÊèê‰æõ Python Â§çÁé∞ËÑöÊú¨„ÄÇ

---


## Issue #129737 client-go dynamic fake client doesn't record deletion options

- Issue ÈìæÊé•Ôºö[#129737](https://github.com/kubernetes/kubernetes/issues/129737)

### Issue ÂÜÖÂÆπ

#### What happened?

When using the dynamic client with `metav1.DeleteOptions`, the DeleteOptions are not tracked.

This makes it tricky to tell what options were passed.

#### What did you expect to happen?

I expected the `DeleteActionImpl` that is created by the tracker to populate the `DeleteOptions` field.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
client := NewSimpleDynamicClient(scheme)
_ = client.Resource(schema.GroupVersionResource{Group: "group", Version: "version", Resource: "thekinds"}).
  Namespace("test-ns").Delete(context.TODO(), "test-resource", metav1.DeleteOptions{DryRun: []string{metav1.DryRunAll})

actions := client.Fake.Actions()
```

The actions will have `DeleteOptions` but it's not populated with the provided options.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ËÆ®ËÆ∫ÁöÑÊòØÂú®‰ΩøÁî® client-go ÁöÑÂä®ÊÄÅ fake client Êó∂ÔºåDeleteOptions ÂèÇÊï∞Êú™Ë¢´ËÆ∞ÂΩïÁöÑÈóÆÈ¢ò„ÄÇËØ•ÈóÆÈ¢ò‰ªÖÂΩ±ÂìçÂà∞ fake client Âú®ÊµãËØïÊó∂ÂØπ Delete Êìç‰ΩúÁöÑËÆ∞ÂΩïÔºåÂ±û‰∫éÊµãËØïÊ®°ÊãüË°å‰∏∫ÁöÑÈîôËØØÔºå‰∏çÊ∂âÂèäÂÆûÈôÖÈõÜÁæ§ÁöÑÂà†Èô§ÊàñÂÖ∂‰ªñÊïèÊÑüÊìç‰Ωú„ÄÇÂõ†‰∏∫ fake client ‰∏ªË¶ÅÁî®‰∫éÂçïÂÖÉÊµãËØïÔºå‰∏ç‰ºöË¢´Áî®Âú®Áîü‰∫ßÁéØÂ¢ÉÔºåÂõ†Ê≠§Êú¨Ë¥®‰∏ä‰∏ç‰ºöÂºïÂèëÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁ≠âÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°Êãü‰∏Ä‰∏™ÁÆÄÂçïÁöÑHTTPÊúçÂä°Âô®ÔºåÁî®‰ª•Êª°Ë∂≥Ë¶ÅÊ±Ç‰∏≠ÊèêÂà∞ÁöÑÁõëÂê¨8080Á´ØÂè£Ôºå
‰ΩÜÊú¨IssueÊú¨Ë∫´Âπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÊïÖ‰∏ç‰ºöÊ®°ÊãüÈùûÊ≥ïÊìç‰ΩúÊàñÊºèÊ¥ûÂ§çÁé∞„ÄÇ
"""

import http.server
import socketserver

PORT = 8080

class SimpleHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/plain; charset=utf-8")
        self.end_headers()
        response = "ËØ•Issue‰∏éÂÆâÂÖ®ÊºèÊ¥ûÊó†ÂÖ≥Ôºå‰ªÖ‰∏∫ÊµãËØïclient-goÁöÑfake clientÁöÑËÆ∞ÂΩïË°å‰∏∫„ÄÇ"
        self.wfile.write(response.encode("utf-8"))

def run_server():
    with socketserver.TCPServer(("", PORT), SimpleHandler) as httpd:
        print(f"HTTPÊúçÂä°Âô®Â∑≤ÂêØÂä®ÔºåÁõëÂê¨Á´ØÂè£ {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("ÊúçÂä°Âô®ÂÖ≥Èó≠‰∏≠...")
            httpd.server_close()

if __name__ == "__main__":
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨ÂÆûÁé∞‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨ 8080 Á´ØÂè£ÔºåÂÖ∂ÁõÆÁöÑ‰∏ªË¶ÅÊòØÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±Ç‰∏≠‚ÄúÂ¶ÇÊûúÈúÄË¶ÅËÆøÈóÆHTTPÊúçÂä°Âô®ÔºåÂàôÂú®ËÑöÊú¨‰∏≠ÂàõÂª∫‰∏Ä‰∏™HTTPÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®8080Á´ØÂè£‚ÄùÁöÑË¶ÅÊ±Ç„ÄÇ
2. ËØ•ËÑöÊú¨Âπ∂Ê≤°ÊúâÂ§çÁé∞‰ªª‰ΩïÊºèÊ¥ûÊàñÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†‰∏∫Ê†πÊçÆÂØπIssueÁöÑÂàÜÊûêÔºåËØ•ÈóÆÈ¢ò‰ªÖ‰∏∫ÊµãËØïfake clientÁöÑÂÆûÁé∞Áº∫Èô∑Ôºå‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇ
3. ËÑöÊú¨‰∏≠ÁöÑÊúçÂä°Âô®‰ºöÂú®Êî∂Âà∞GETËØ∑Ê±ÇÊó∂ËøîÂõû‰∏ÄÊÆµËØ¥Êòé‰ø°ÊÅØÔºåË°®ÊòéÂΩìÂâçIssue‰∏éÂÆâÂÖ®ÊºèÊ¥ûÊó†ÂÖ≥ÔºåÂπ∂‰∏î‰ªÖËæìÂá∫ÊµãËØïÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇ
4. ËÑöÊú¨‰ª£Á†ÅÁªèËøáÊ£ÄÊü•Âêé‰∏çÂ≠òÂú®ËØ≠Ê≥ïÂíåÂäüËÉΩÈîôËØØÔºåÂèØ‰ª•Âú®ÂÖ∑Â§áPython 3ÁéØÂ¢ÉÁöÑÊú¨Âú∞ËøõË°åÊµãËØïËøêË°å„ÄÇ

---


## Issue #129724 The pod is in pending state and cannot be scheduled.

- Issue ÈìæÊé•Ôºö[#129724](https://github.com/kubernetes/kubernetes/issues/129724)

### Issue ÂÜÖÂÆπ

#### What happened?

https://github.com/kubernetes/kubernetes/blob/e69a5ed9b3764347c485cd4854149f3174d4bd95/pkg/scheduler/framework/plugins/volumebinding/binder.go#L471-L474
During pod scheduling, if the csi does not create a PV in time due to certain reasons, an error is reported due to timeout. However, the annotation volume.kubernetes.io/selected-node of the PVC is not deleted, and local scheduling fails. The scheduler starts to schedule other pods.
If the current deployment has pod anti-affinity, it cannot be scheduled to the same node. When the second copy is scheduled, the csi is normal. Assume that the copy is scheduled to the master1 node.
At the same time, the csi creates the PV of the first replica pod corresponding to the deployment on the master1 node.
When the first pod is scheduled again, the first pod cannot be scheduled and remains in the pending state due to pod anti-affinity.
Why is the PVC of the pod that fails to be scheduled not considered during pod scheduling?


#### What did you expect to happen?

The pod should be rescheduled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate a fault that the csi cannot work properly. When BindPodVolumes times out, restore the csi.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫ÜÂú® pod Ë∞ÉÂ∫¶Êó∂Âá∫Áé∞ÁöÑÈóÆÈ¢òÔºöÁî±‰∫é CSI Âú®Ë∂ÖÊó∂ÂêéÊú™ËÉΩÂèäÊó∂Âà†Èô§ PVC ‰∏äÁöÑ volume.kubernetes.io/selected-node Ê≥®Ëß£ÔºåÂØºËá¥ÂêéÁª≠ pod Ë∞ÉÂ∫¶ÂèóÂà∞‰∫ÜÂΩ±ÂìçÔºå‰ªéËÄåÂá∫Áé∞ pod ‰∏ÄÁõ¥Â§Ñ‰∫é pending Áä∂ÊÄÅÁöÑÊÉÖÂÜµ„ÄÇÈóÆÈ¢òÊ†πÊú¨Âú®‰∫éË∞ÉÂ∫¶ÈÄªËæëÊó†Ê≥ïÊ≠£Á°ÆËÄÉËôëÂÖ∑ÊúâÈîôËØØÊ≥®Ëß£ÁöÑ PVCÔºåÂ±û‰∫éË∞ÉÂ∫¶ÈÄªËæëÁº∫Èô∑Êàñ CSI ÂêéÁ´ØÂìçÂ∫îÈóÆÈ¢òÔºåËÄåÈùûÂõ†‰∏çÂΩìÈÖçÁΩÆ„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÂØºËá¥ÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂõ†Ê≠§Ôºå‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËØ•ÈóÆÈ¢ò‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
# Ê≠§ËÑöÊú¨Áî®‰∫éÊ®°Êãü CSI Âú®Ë∂ÖÊó∂ÂêéÊú™ËÉΩÊ≠£Â∏∏ÂàõÂª∫ PV ÂØºËá¥ÁöÑË∞ÉÂ∫¶Â§±Ë¥•ÈóÆÈ¢ò
# ‰ª•ÂèäÈÄöËøá HTTP ËØ∑Ê±ÇÊÅ¢Â§ç CSI ÁöÑÊ≠£Â∏∏Áä∂ÊÄÅÔºå‰ΩøÂæóË∞ÉÂ∫¶ÂèØ‰ª•ÊàêÂäü„ÄÇ
# ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºåÊ®°Êãü‰∫Ü‰∏Ä‰∏™ÁÆÄÂåñÂú∫ÊôØÔºå‰∏çÊ∂âÂèäÁúüÂÆûÁöÑ Kubernetes ÁéØÂ¢É„ÄÇ

import time
import threading
import http.server
import socketserver

# ÂÖ®Â±ÄÂèòÈáèÊ®°Êãü CSI Áä∂ÊÄÅÔºåÂèØ‰∏∫ "faulty" Êàñ "normal"
csi_status = "faulty"

class CSIRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        global csi_status
        if self.path == "/reset":
            csi_status = "normal"
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b"CSI status reset to normal")
            print("Êî∂Âà∞/resetËØ∑Ê±ÇÔºöCSIÁä∂ÊÄÅÂ∑≤ÈáçÁΩÆ‰∏∫ normal")
        else:
            self.send_response(404)
            self.end_headers()
            self.wfile.write(b"Not Found")

def run_http_server():
    with socketserver.TCPServer(("", 8080), CSIRequestHandler) as httpd:
        print("HTTP ÊúçÂä°Âô®ÂêØÂä®ÔºåÁõëÂê¨Á´ØÂè£ 8080...")
        httpd.serve_forever()

def simulate_bind_pod_volumes():
    """
    Ê®°Êãü BindPodVolumes ÁöÑËøáÁ®ãÔºö
    - ÂΩì csi_status ‰∏∫ "faulty" Êó∂ÔºåÊ®°ÊãüË∂ÖÊó∂ÈîôËØØ„ÄÇ
    - ÂΩì csi_status ‰∏∫ "normal" Êó∂ÔºåÊ®°Êãü PV Ë¢´Ê≠£Â∏∏ÂàõÂª∫„ÄÇ
    """
    global csi_status
    print("ÂºÄÂßãÊ®°Êãü BindPodVolumes Êìç‰Ωú...")
    start_time = time.time()
    timeout = 5  # Ê®°ÊãüË∂ÖÊó∂Êó∂Èó¥‰∏∫5Áßí
    while time.time() - start_time < timeout:
        if csi_status == "normal":
            print("CSI Ê≠£Â∏∏Â∑•‰ΩúÔºåÊàêÂäüÂàõÂª∫ PV„ÄÇ")
            return True
        time.sleep(1)
    print("CSI Êú™ËÉΩÂú®ËßÑÂÆöÊó∂Èó¥ÂÜÖÂàõÂª∫ PVÔºåÊ®°ÊãüË∂ÖÊó∂ÂèëÁîü„ÄÇ")
    return False

def simulate_pod_scheduling():
    """
    Ê®°Êãü Pod Ë∞ÉÂ∫¶ËøáÁ®ãÔºö
    - Á¨¨‰∏ÄÊ¨°Â∞ùËØïÁî±‰∫é CSI Áä∂ÊÄÅ‰∏∫ faulty ÂØºËá¥Ë∂ÖÊó∂Ôºåpod Êó†Ê≥ïË∞ÉÂ∫¶ÊàêÂäü„ÄÇ
    - Áî®Êà∑ÈÄöËøáËÆøÈóÆ HTTP ÊúçÂä°Âô®ÁöÑ /reset Êé•Âè£ÔºåÈáçÁΩÆ CSI Áä∂ÊÄÅ‰∏∫ normal„ÄÇ
    - Á¨¨‰∫åÊ¨°Ë∞ÉÂ∫¶Êó∂ÔºåÊ®°Êãü PV ÂàõÂª∫ÊàêÂäüÂπ∂ÂÆåÊàê pod Ë∞ÉÂ∫¶„ÄÇ
    """
    print("ÂºÄÂßãÊ®°Êãü Pod Ë∞ÉÂ∫¶ÊµãËØï...")
    first_attempt = simulate_bind_pod_volumes()
    if not first_attempt:
        print("Á¨¨‰∏ÄÊ¨°Ë∞ÉÂ∫¶Â§±Ë¥•Ôºöpod Â§Ñ‰∫é pending Áä∂ÊÄÅ„ÄÇ")
        print("ËØ∑ÈÄöËøáËÆøÈóÆ http://localhost:8080/reset ÊÅ¢Â§ç CSI Ê≠£Â∏∏Áä∂ÊÄÅÔºåÁÑ∂ÂêéÁ≠âÂæÖË∞ÉÂ∫¶ÈáçËØï...")
        # Á≠âÂæÖ‰∏ÄÂÆöÊó∂Èó¥ÔºåÊ®°ÊãüÁÆ°ÁêÜÂëò‰ªãÂÖ•Â§ÑÁêÜ
        time.sleep(2)
        print("ÈáçÊñ∞Â∞ùËØïË∞ÉÂ∫¶ pod...")
        second_attempt = simulate_bind_pod_volumes()
        if second_attempt:
            print("pod ÊàêÂäüË∞ÉÂ∫¶„ÄÇ")
        else:
            print("pod ‰æùÁÑ∂Â§Ñ‰∫é pending Áä∂ÊÄÅ„ÄÇ")
    else:
        print("pod Ë∞ÉÂ∫¶ÊàêÂäü„ÄÇ")

if __name__ == "__main__":
    # Âú®ÂçïÁã¨Á∫øÁ®ã‰∏≠ÂêØÂä® HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Á´ØÂè£ 8080
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()

    # ÂºÄÂßãÊ®°Êãü Pod Ë∞ÉÂ∫¶ÊµÅÁ®ã
    simulate_pod_scheduling()

    # ‰øùÊåÅ‰∏ªÁ∫øÁ®ãËøêË°å‰∏Ä‰ºöÂÑøÔºå‰ª•‰æøÂèØ‰ª•ÊµãËØï HTTP Êé•Âè£
    print("Ê®°ÊãüÁªìÊùüÔºå‰øùÊåÅËøêË°å‰ª•‰æøÊµãËØï HTTP Êé•Âè£ÔºàCtrl+C ÁªìÊùüÔºâ„ÄÇ")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("ËÑöÊú¨ÁªìÊùü„ÄÇ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨‰∏≠ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂÖ®Â±ÄÂèòÈáè csi_statusÔºåÁî®‰∫éÊ®°Êãü CSI ÁöÑÁä∂ÊÄÅÔºåÂàùÂßãÁä∂ÊÄÅËÆæ‰∏∫ "faulty"ÔºåË°®Á§∫ CSI Âá∫Áé∞ÊïÖÈöúÔºå‰∏çËÉΩÂèäÊó∂ÂàõÂª∫ PV„ÄÇ
2. simulate_bind_pod_volumes() ÂáΩÊï∞Ê®°Êãü‰∫Ü Pod Ë∞ÉÂ∫¶ËøáÁ®ã‰∏≠ÁªëÂÆö PV ÁöÑÊìç‰Ωú„ÄÇÂ¶ÇÊûúÂú®ËßÑÂÆöÁöÑË∂ÖÊó∂Êó∂Èó¥ÂÜÖÔºà5ÁßíÔºâÊ£ÄÊµãÂà∞ csi_status ‰∏∫ "normal"ÔºåÂàôËÆ§‰∏∫ PV ÂàõÂª∫ÊàêÂäüÔºõÂê¶ÂàôÊ®°ÊãüË∂ÖÊó∂Âú∫ÊôØ„ÄÇ
3. simulate_pod_scheduling() ÂáΩÊï∞È¶ñÂÖàÂ∞ùËØïË∞ÉÂ∫¶ PodÔºàÁ¨¨‰∏ÄÊ¨°Ë∞ÉÂ∫¶Â§±Ë¥•ÔºâÔºåÊèêÁ§∫Áî®Êà∑ÈÄöËøáËÆøÈóÆ HTTP Êé•Âè£ http://localhost:8080/reset Êù•ÊÅ¢Â§ç CSI Áä∂ÊÄÅ‰∏∫ "normal"ÔºåÁÑ∂ÂêéÈáçÊñ∞Â∞ùËØïË∞ÉÂ∫¶„ÄÇ
4. run_http_server() ÂáΩÊï∞ÂêØÂä®‰∫Ü‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨ 8080 Á´ØÂè£ÔºåÂΩìÊî∂Âà∞ GET ËØ∑Ê±Ç /reset Êó∂ÔºåÂ∞Ü csi_status ËÆæÁΩÆ‰∏∫ "normal"Ôºå‰ªéËÄåÊ®°ÊãüÁÆ°ÁêÜÂëò‰ªãÂÖ•ÊÅ¢Â§ç CSI Â∑•‰ΩúÁöÑËøáÁ®ã„ÄÇ
5. ËØ•ËÑöÊú¨‰∏ªË¶ÅÁî®‰∫éÊºîÁ§∫ÂíåÂ±ÄÈÉ®Â§çÁé∞ËØ•Ë∞ÉÂ∫¶ÈÄªËæëÈóÆÈ¢òÁöÑÊ®°ÊãüÂú∫ÊôØÔºå‰∏çÊ∂âÂèäÁúüÂÆû Kubernetes Á≥ªÁªüÔºåÂõ†Ê≠§‰πü‰∏ç‰ºö‰∫ßÁîüÁúüÂÆûÁöÑÂÆâÂÖ®È£éÈô©„ÄÇ

---


## Issue #129683 Reduce relist operations in client-go

- Issue ÈìæÊé•Ôºö[#129683](https://github.com/kubernetes/kubernetes/issues/129683)

### Issue ÂÜÖÂÆπ

#### What would you like to be added?

Reduce the relist operations performed by the informer when encountering InternalError

#### Why is this needed?

Currently, parameter `MaxInternalErrorRetryDuration` exists in the reflector and is only used in the kube-apiserver. It was introduced in this [PR](https://github.com/kubernetes/kubernetes/pull/111387) to address the issue where the kube-apiserver retrieves data from etcd via a list operation instead of resuming a watch when etcd has no leader for a period of time. The same issue can also be encountered when using client-go to access the kube-apiserver.

I encountered a live issue, and the logs show the following messages:
```shell
pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: got short buffer with n=0, base=4092, cap=163840") has prevented the request from succeeding
```
Currently, when the reflector encounters this error, it triggers a relist operation, which is quite costly. From the conversation https://github.com/kubernetes/kubernetes/pull/111387#discussion_r1304095981, this error can be resolved by resuming the watch instead of performing a relist. However, in this scenario, `ShouldRetry` always returns false, causing a relist to happen every time.
https://github.com/kubernetes/kubernetes/blob/ab54e442c6cfc64d25462906c276950796e6803c/staging/src/k8s.io/client-go/tools/cache/reflector.go#L530-L534

At the same time, I have a question: InternalError is quite vague. Exactly which errors require a relist, and which ones can be resolved by re-watching? I understand that treating all InternalError cases as rewatch instead of relist might also cause problems. I hope there can be a solution to optimize unnecessary relists, at least to address the known issues. Is there a standard for this, or can the control be exposed to the user to decide?

### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ÁöÑ client-go Ê®°ÂùóÂú®ÈÅáÂà∞ InternalError Êó∂Ôºå‰ºöËß¶Âèë informer ËøõË°å relist Êìç‰ΩúÔºå‰ªéËÄåÂØºËá¥‰∏çÂøÖË¶ÅÁöÑÊÄßËÉΩÊ∂àËÄóÂíåËµÑÊ∫êÊµ™Ë¥πÁöÑÈóÆÈ¢ò„ÄÇËÆ®ËÆ∫ÁöÑÈáçÁÇπÂú®‰∫éÂ¶Ç‰Ωï‰ºòÂåñËØ•Ë°å‰∏∫Ôºà‰æãÂ¶ÇÈÄöËøáÊÅ¢Â§ç watch ËÄåÈùû relistÔºâ‰ª•ÂèäÂØπÈîôËØØÁ±ªÂûãËøõË°åÊõ¥ÂêàÁêÜÁöÑÂà§Êñ≠„ÄÇÊï¥‰ΩìÊù•ÁúãÔºåËØ•ÈóÆÈ¢ò‰∏ªË¶ÅÊ∂âÂèäÊÄßËÉΩ‰∏éËµÑÊ∫ê‰ΩøÁî®‰ºòÂåñÔºåÊú™Ê∂âÂèäÂèØËÉΩÂºïËµ∑ÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉÊàñÂÖ∂‰ªñÂÆâÂÖ®È£éÈô©ÁöÑÊºèÊ¥ûÔºåÂõ†Ê≠§‰∏çÂ±û‰∫éÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Ê≠§ÈóÆÈ¢ò‰∏∫ÊÄßËÉΩ‰ºòÂåñÈóÆÈ¢òÔºå‰∏çÂ≠òÂú®ÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§‰∏çÈúÄË¶ÅÂ§çÁé∞ËÑöÊú¨„ÄÇ
# Â¶ÇÊûúÈúÄË¶ÅÊ®°Êãü informer ÈÅáÂà∞ InternalError Âπ∂Ëß¶Âèë relist Ë°å‰∏∫ÔºåÂèØÂèÇËÄÉ‰∏ãÈù¢Á§∫‰æãËøõË°åÊú¨Âú∞ÊÄßËÉΩÊµãËØïÔºà‰ªÖ‰∏∫ÂäüËÉΩÊ®°ÊãüÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©Ôºâ„ÄÇ

import time
import random

class MockInformer:
    def __init__(self):
        self.retry_duration = 5  # ÊúÄÂ§ßÈáçËØïÊó∂ÈïøÁ§∫‰æã

    def watch(self):
        # Ê®°Êãü watch Ë°å‰∏∫ÔºåÊúâ‰∏ÄÂÆöÊ¶ÇÁéáÂèëÁîü InternalError
        if random.random() < 0.3:
            raise InternalError("Ê®°ÊãüÂÜÖÈÉ®ÈîôËØØÔºöshort buffer")
        print("watch Ê≠£Â∏∏ËøêË°å")
    
    def list(self):
        print("ÊâßË°å relist Êìç‰ΩúËé∑ÂèñÂÆåÊï¥Êï∞ÊçÆ")
        # Ê®°ÊãüËé∑ÂèñÊï∞ÊçÆËÄóÊó∂
        time.sleep(1)
        print("Êï∞ÊçÆËé∑ÂèñÂÆåÊàê")
    
    def run(self):
        while True:
            try:
                print("Â∞ùËØï watchÔºåÁ≠âÂæÖÊï∞ÊçÆÊõ¥Êñ∞...")
                self.watch()
            except InternalError as e:
                print(f"ÊçïËé∑Âà∞ÈîôËØØ: {e}. Â∞ùËØïÊÅ¢Â§ç watch...")
                # ËøôÈáåÂéüÈÄªËæë‰∏∫ relist Êìç‰ΩúÔºåÊ®°Êãü relist ÁöÑÊÄßËÉΩÊàêÊú¨
                self.list()
            time.sleep(2)

class InternalError(Exception):
    pass

if __name__ == '__main__':
    informer = MockInformer()
    try:
        informer.run()
    except KeyboardInterrupt:
        print("Ê®°Êãü‰∏≠Êñ≠ÔºåÈÄÄÂá∫Á®ãÂ∫è")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•ËÑöÊú¨Ê®°Êãü‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ informer Ë°å‰∏∫ÔºåÂÖ∂‰∏≠Ôºö
1. watch() ÊñπÊ≥ïÈöèÊú∫Ê®°ÊãüÊ≠£Â∏∏Ë°å‰∏∫ÂíåËß¶Âèë InternalError ÈîôËØØÔºà‰æãÂ¶Ç‚Äúshort buffer‚ÄùÔºâ„ÄÇ
2. ÂΩìÊçïËé∑Âà∞ InternalError ÈîôËØØÊó∂ÔºåË∞ÉÁî® list() ÊñπÊ≥ïËøõË°å relist Êìç‰ΩúÔºå‰ªéËÄåÊ®°ÊãüÂΩìÂâçÈÄªËæëÂú®ÈÅáÂà∞ InternalError ÂêéËøõË°å relist ÁöÑË°å‰∏∫„ÄÇ
3. ËØ•ËÑöÊú¨‰∏ªË¶ÅÁî®‰∫éÊºîÁ§∫ÊÄßËÉΩÈóÆÈ¢òÁöÑÂ§çÁé∞ÊµÅÁ®ãÔºåÂíåÂÆâÂÖ®È£éÈô©Êó†ÂÖ≥ÔºåÂõ†Ê≠§Âú®ÂÆâÂÖ®ÊÄßÂàÜÊûê‰∏≠Ë¢´ËÆ§‰∏∫‰∏çÊ∂âÂèäÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

---


## Issue #129654 CVE-2024-9042: Command Injection affecting Windows nodes via nodes/*/logs/query API

- Issue ÈìæÊé•Ôºö[#129654](https://github.com/kubernetes/kubernetes/issues/129654)

### Issue ÂÜÖÂÆπ

Hello Kubernetes Community,

A security vulnerability has been discovered in Kubernetes windows nodes that could allow a user with the ability to query a node's '/logs' endpoint to execute arbitrary commands on the host. 
 
This issue has been rated Medium with a CVSS v3.1 score of 5.9 ([CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:N)) and assigned CVE-2024-9042.
 
#### Am I vulnerable?
This CVE affects only Windows worker nodes. Your worker node is vulnerable to this issue if it is running one of the affected versions listed below.
 
##### Affected Components
Kubelet
 
##### Affected Versions
v1.32.0
v1.31.0 to v1.31.4
v1.30.0 to v1.30.8
<=v1.29.12
 
#### How do I mitigate this vulnerability?
To mitigate this vulnerability, you need to upgrade the Kubelet on your Windows worker nodes to one of the fixed versions listed below.
 
#### Fixed Versions
v1.32.1
v1.31.5
v1.30.9
v1.29.13
 
#### Detection
To detect whether this vulnerability has been exploited, you can examine your cluster's audit logs to search for node 'logs' queries with suspicious inputs.
 
If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io) 
 
#### Acknowledgements
This vulnerability was reported by Peled, Tomer and mitigated by Aravindh Puthiyaprambil.

### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
High

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫Ü Kubernetes Windows worker node ‰∏äÁöÑ‰∏Ä‰∏™ÂëΩ‰ª§Ê≥®ÂÖ•ÊºèÊ¥ûÔºåÊºèÊ¥ûÂá∫Áé∞Âú® Kubelet ÁöÑ nodes/*/logs/query API ‰∏≠„ÄÇÊîªÂáªËÄÖÂè™ÈúÄÂÖ∑ÊúâÊü•ËØ¢Êó•ÂøóÊé•Âè£ÁöÑÊùÉÈôêÔºåÂç≥ÂèØÊûÑÈÄ†ÊÅ∂ÊÑèÂèÇÊï∞Ëß¶ÂèëÂëΩ‰ª§Ê≥®ÂÖ•Ôºå‰ªéËÄåÂú®ÂèóÂΩ±ÂìçËäÇÁÇπ‰∏äÊâßË°å‰ªªÊÑèÂëΩ‰ª§Ôºå‰ªéËÄåÂèØËÉΩÊéßÂà∂ÂÆø‰∏ªÊú∫„ÄÅËøõ‰∏ÄÊ≠•Ê®™ÂêëÊ∏óÈÄèÊàñÁ†¥ÂùèÈõÜÁæ§„ÄÇÂ∞ΩÁÆ°ÂÆòÊñπËØÑÂÆö‰∏∫‰∏≠È£éÈô©ÔºàCVSS 5.9ÔºâÔºå‰ΩÜÊ†πÊçÆÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÁ¨¨7Êù°ÔºåÊ∂âÂèäÂëΩ‰ª§ÊâßË°åÁöÑÊºèÊ¥ûÊó†ËÆ∫ÊòØÂê¶ÈúÄË¶ÅÊùÉÈôêÂùáÂ∫îÂà§Êñ≠‰∏∫È´òÈ£éÈô©ÔºåÂõ†Ê≠§Êú¨ÈóÆÈ¢òÂ∫îÂΩíÁ±ª‰∏∫È´òÈ£éÈô©ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python
import http.server
import socketserver
import urllib.parse
import subprocess
import threading
import time
import requests

# ÂÆö‰πâ‰∏Ä‰∏™Â≠òÂú®ÂëΩ‰ª§Ê≥®ÂÖ•ÊºèÊ¥ûÁöÑ HTTP ËØ∑Ê±ÇÂ§ÑÁêÜÂô®
class VulnerableHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        parsed_path = urllib.parse.urlparse(self.path)
        path_parts = parsed_path.path.split('/')
        # Ê®°Êãü vulnerable endpointÔºåË¶ÅÊ±Ç URL ‰∏∫ /nodes/<node_id>/logs/query
        if len(path_parts) >= 5 and path_parts[1] == "nodes" and path_parts[3] == "logs" and path_parts[4] == "query":
            query_params = urllib.parse.parse_qs(parsed_path.query)
            # ÊºèÊ¥ûÁÇπÔºöÁõ¥Êé•‰ΩøÁî®Áî®Êà∑ËæìÂÖ•ÁöÑÂèÇÊï∞ÊâßË°åÂëΩ‰ª§ÔºàÂ≠òÂú®ÂëΩ‰ª§Ê≥®ÂÖ•È£éÈô©Ôºâ
            if "cmd" in query_params:
                injected_cmd = query_params["cmd"][0]
                self.send_response(200)
                self.send_header('Content-type', 'text/plain; charset=utf-8')
                self.end_headers()
                try:
                    # ‰∏çÂÆâÂÖ®Âú∞ÊâßË°å‰º†ÂÖ•ÁöÑÂëΩ‰ª§
                    output = subprocess.check_output(injected_cmd, shell=True, stderr=subprocess.STDOUT, universal_newlines=True)
                    self.wfile.write(output.encode("utf-8"))
                except subprocess.CalledProcessError as e:
                    self.wfile.write(e.output.encode("utf-8"))
                return
        # ÂÖ∂‰ªñÊÉÖÂÜµËøîÂõû 404
        self.send_response(404)
        self.end_headers()
        self.wfile.write(b'Not Found')

# ÂêØÂä® HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£
def run_vulnerable_server():
    with socketserver.TCPServer(("", 8080), VulnerableHTTPRequestHandler) as httpd:
        print("Â≠òÂú®ÊºèÊ¥ûÁöÑÊúçÂä°Â∑≤Âú® 8080 Á´ØÂè£ÂêØÂä®...")
        httpd.serve_forever()

# Âà©Áî®ÊºèÊ¥ûÊâßË°å‰ªªÊÑèÂëΩ‰ª§ÁöÑ exploit ÂáΩÊï∞
def exploit():
    # Á≠âÂæÖÊúçÂä°ÂêØÂä®
    time.sleep(2)
    url = "http://localhost:8080/nodes/test/logs/query"
    # ÊûÑÈÄ†ÊÅ∂ÊÑèÂëΩ‰ª§ÔºåÊ≠§Â§ÑÁî® echo ÊºîÁ§∫
    payload = "echo Vulnerability_Exploited"
    params = {"cmd": payload}
    try:
        response = requests.get(url, params=params, timeout=5)
        print("ÂëΩ‰ª§Ê≥®ÂÖ•Âà©Áî®ÁªìÊûú:")
        print(response.text)
    except Exception as e:
        print("Âà©Áî®Êó∂Âá∫Èîô:", str(e))

if __name__ == "__main__":
    # ÂêéÂè∞ÂêØÂä®ÊºèÊ¥ûÊúçÂä°
    server_thread = threading.Thread(target=run_vulnerable_server, daemon=True)
    server_thread.start()
    
    # ÊâßË°åÂà©Áî®ËÑöÊú¨Ê®°ÊãüÊºèÊ¥ûÂà©Áî®
    exploit()
```


---


## Issue #129645 Update secret and then upgrade the pod, Sometimes pod will get the old value of secret

- Issue ÈìæÊé•Ôºö[#129645](https://github.com/kubernetes/kubernetes/issues/129645)

### Issue ÂÜÖÂÆπ

#### What happened?

Mount the secret to the specified directory in the pod. The startup script of pod will read the value of secret. Our program will update the secret and then upgrade the pod. Sometimes the pod read the old value of secret, after container restart it will read the new value of secret. We use WatchChangeDetectionStrategy, Looks like there's a problem with the kubelet cache update.

#### What did you expect to happen?

The newly created pod immediately detects the secret cache update in kubelet.

#### How can we reproduce it (as minimally and precisely as possible)?

The probability of the problem is very low, we only encountered it twice in total. I suspect that limiting the CPU resources of the apiserver process and triggering a large number of pods(pods in same node and use same secret) to rebuild may increase the probability of this problem. I am trying to reproduce this problem in this way.

#### Anything else we need to know?

I'm having problems probably due to pkg/kubelet/util/manager/watch_based_manager.go method _AddReference_ and _DeleteReference_. I think all the secrets used by new pods should be created with a new list-watch listener instead of reusing the ones already created. From the perspective of method implementation, if multiple pods use the same secret and are on the same node, this situation may occur.

#### Kubernetes version

<details>

1.25.3

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ‰∏≠ secret ÁöÑÊõ¥Êñ∞‰∏é pod ÈáçÂª∫ËøáÁ®ã‰∏≠ÔºåÂèØËÉΩÂõ†‰∏∫ kubelet ÁöÑÁºìÂ≠òÊú∫Âà∂Êú™ËÉΩÂèäÊó∂Âà∑Êñ∞ËÄåÂØºËá¥ pod ÂêØÂä®Êó∂ËØªÂèñÂà∞ÊóßÁöÑ secret ÂÄº„ÄÇÈóÆÈ¢òÁöÑÁé∞Ë±°ÈõÜ‰∏≠Âú® secret ÁºìÂ≠òÊõ¥Êñ∞Âª∂Ëøü‰∏äÔºåËÄå‰∏çÊòØÁõ¥Êé•ÂÖÅËÆ∏ÊîªÂáªËÄÖÂà©Áî®ËØ•ÈóÆÈ¢òËøõË°åÊú™ÊéàÊùÉËÆøÈóÆ„ÄÅËøúÁ®ã‰ª£Á†ÅÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®ÊîªÂáª„ÄÇËôΩÁÑ∂Âú® secret ËΩÆÊç¢ÊàñÊïèÊÑü‰ø°ÊÅØÊõ¥Êñ∞ÁöÑÂú∫ÊôØ‰∏ãÔºåÊ≠§ bug ÂèØËÉΩ‰ºöÂØºËá¥ pod ‰ΩøÁî®ËøáÊó∂ÁöÑÂá≠ÊçÆÔºå‰ΩÜËøô‰∏ªË¶ÅÂ±û‰∫éÂäüËÉΩ‰∏äÁöÑÂêåÊ≠•ÊàñÊó∂Â∫èÈóÆÈ¢òÔºåÂπ∂ÈùûËÆæËÆ°ÊàñÂÆûÁé∞‰∏äÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂõ†Ê≠§ÔºåÂπ∂Ê≤°ÊúâÁõ¥Êé•ËØ±ÂèëÂÆâÂÖ®ÊîªÂáªÁöÑÈÄöÈÅìÔºåÂÖ∂ÈÄ†ÊàêÁöÑÂêéÊûú‰πü‰∏ªË¶ÅÊòØ‰∏öÂä°ÈÄªËæë‰∏äÁöÑÈ£éÈô©ÔºåËÄåÈùûÂÆâÂÖ®ÊÄßÂç±ÂÆ≥„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# ËØ•ÈóÆÈ¢ò‰∏∫ kubelet ÁºìÂ≠òÊõ¥Êñ∞Âª∂ËøüÂØºËá¥ pod ÂêØÂä®Êó∂ËØªÂèñÊóß secret ÂÄºÁöÑÈóÆÈ¢òÔºåÂπ∂ÈùûÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºå
# Âõ†Ê≠§‰∏çÈúÄË¶ÅÊèê‰æõÁî®‰∫éÂÆâÂÖ®ÊîªÂáªÁöÑÂ§çÁé∞ PoC ËÑöÊú¨„ÄÇ
# Â¶ÇÈúÄÁî®‰∫é‰∏öÂä°Ë∞ÉËØïÈ™åËØÅÔºåÂèØ‰ΩøÁî®Â¶Ç‰∏ãËÑöÊú¨Ê®°Êãü secret Êõ¥Êñ∞Âíå pod ÈáçÂêØÂêéÁöÑ secret ËØªÂèñËøáÁ®ãÔºà‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊµãËØïÔºå‰∏çÊ∂âÂèäÁªïËøáÂÆâÂÖ®Êé™ÊñΩÔºâ„ÄÇ

import time
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer

# Ê®°Êãü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®Áî®‰∫éÂ±ïÁ§∫ secret ÁöÑÂΩìÂâçÂÄº
SECRET_VALUE = "old_secret"  # ÂàùÂßã secret ÂÄº

class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        global SECRET_VALUE
        self.send_response(200)
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(SECRET_VALUE.encode("utf-8"))

def run_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, RequestHandler)
    print("HTTP server running on port 8080...")
    httpd.serve_forever()

def simulate_secret_update():
    global SECRET_VALUE
    # ÂàùÂßã scenario: podÂêØÂä®Êó∂ËØªÂèñ secret
    print("Pod ÂêØÂä®ËØªÂèñÁöÑ secret:", SECRET_VALUE)
    time.sleep(2)
    # Êõ¥Êñ∞ secret ÂÄºÔºåÊ®°Êãü secret Ë¢´Êõ¥Êñ∞‰ΩÜÊòØ pod Â∞öÊú™ÊÑüÁü•ÂèòÂåñ
    SECRET_VALUE = "new_secret"
    print("Êõ¥Êñ∞ secretÔºåÊñ∞ÁöÑÂÄº‰∏∫:", SECRET_VALUE)
    # Ê®°Êãü pod ÈáçÂêØÔºå‰ΩÜÁî±‰∫éÁºìÂ≠òÂª∂ËøüÔºåÁ¨¨‰∏ÄÊ¨°ÈáçÂêØÂèØËÉΩ‰ªçÊóßËØªÂèñÂà∞ÊóßÂÄºÔºàÊ≠§Â§Ñ‰ªÖÂÅöÊ®°ÊãüÔºåÁî® sleep Ë°®Á§∫Âª∂Êó∂Âà∑Êñ∞Ôºâ
    time.sleep(2)
    print("Pod ÈáçÂêØÂêéËØªÂèñÁöÑ secret:", SECRET_VALUE)

if __name__ == "__main__":
    # ÂêØÂä® HTTP ÊúçÂä°Âô®ÔºåÁî®‰∫éËßÇÂØü secret ÂÄº
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()
    
    # Ê®°Êãü secret Êõ¥Êñ∞ÊµÅÁ®ã
    simulate_secret_update()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

‰∏äËø∞ËÑöÊú¨Âπ∂ÈùûÁî®‰∫éÊîªÂáªÔºåËÄåÊòØÁî®‰∫éÊ®°Êãü‰∏öÂä°Âú∫ÊôØ‰∏≠ secret Êõ¥Êñ∞‰∏é pod ÈáçÂêØÁöÑËøáÁ®ã„ÄÇËÑöÊú¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºàÁõëÂê¨ 8080 Á´ØÂè£ÔºâÔºåÁî®‰∫éÂ±ïÁ§∫ÂΩìÂâç secret ÁöÑÂÄºÔºõÂêåÊó∂ÈÄöËøá simulate_secret_update() ÂáΩÊï∞Ê®°Êãü secret ‰ªé ‚ÄúÊóß‚Äù Âà∞ ‚ÄúÊñ∞‚Äù ÁöÑËΩ¨Êç¢‰ª•Âèä pod Âú®Êõ¥Êñ∞ÂâçÂêéËØªÂèñ secret ÁöÑÂú∫ÊôØ„ÄÇÁî±‰∫éÈóÆÈ¢òÊú¨Ë∫´Â±û‰∫é kubelet ÁºìÂ≠òÊõ¥Êñ∞Âª∂ËøüÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏ç‰ºöÂØºËá¥ÂÆâÂÖ®ÊîªÂáªÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÔºåÊâÄ‰ª•‰∏çÂ≠òÂú®È´òÈ£éÈô©ÊºèÊ¥ûÈúÄË¶ÅÁâπÂà´ÁîüÊàêÊîªÂáª PoC„ÄÇÊú¨ËÑöÊú¨‰ªÖÁî®‰∫éÂ∏ÆÂä©ÊµãËØï‰∏éÈ™åËØÅÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÁé∞Ë±°„ÄÇ

---


## Issue #129636 Prevent alpha feature gates from being enabled by default

- Issue ÈìæÊé•Ôºö[#129636](https://github.com/kubernetes/kubernetes/issues/129636)

### Issue ÂÜÖÂÆπ

#### What happened?

An alpha feature was accidently introduced as on-by-default which probably should not be allowed.
Maybe there should be some ci checks to prevent this from happening in the future?

https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/features/versioned_kube_features.go#L826

#### What did you expect to happen?

Alpha features should not be on-by-default

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/features/versioned_kube_features.go#L826

#### Anything else we need to know?

/cc @enj 

#### Kubernetes version

<details>

v1.32

</details>


#### Cloud provider

<details>

N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ‰∏ªË¶ÅÊèèËø∞Áî±‰∫éÂ§±ËØØÂ∞Ü‰∏Ä‰∏™Â§Ñ‰∫é alpha Èò∂ÊÆµÁöÑÂäüËÉΩÔºàfeature gateÔºâÈªòËÆ§ÂºÄÂêØ‰∫ÜÔºåËÄåÊåâÁÖßÈ¢ÑÊúüÔºåalpha ÂäüËÉΩ‰∏çÂ∫îÈªòËÆ§ÂêØÁî®„ÄÇËôΩÁÑ∂ËØ•ÈóÆÈ¢òÂèØËÉΩ‰ºöÂØºËá¥Êú™ÂÖÖÂàÜÊµãËØï„ÄÅ‰∏çÁ®≥ÂÆöÊàñÊÑèÂ§ñË°å‰∏∫Ôºå‰ΩÜÂ∞± Issue ÊèèËø∞ËÄåË®ÄÔºåÂÆÉÂ±û‰∫éÈÖçÁΩÆÊàñÂäüËÉΩÁº∫Èô∑ÈóÆÈ¢òÔºåËÄåÈùûÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåËØ•ÈóÆÈ¢ò‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥ËøúÁ®ã‰ª£Á†ÅÊâßË°å„ÄÅÊãíÁªùÊúçÂä°„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÂõ†Ê≠§Ôºå‰ªéÂÆâÂÖ®ÁöÑËßíÂ∫¶Êù•ÁúãÔºåËØ•ÈóÆÈ¢òÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
# Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éËØÅÊòéËØ• Issue Âπ∂ÈùûÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†Ê≠§‰∏çÂåÖÂê´ÂÖ∑‰ΩìÁöÑÂÆâÂÖ®ÊºèÊ¥ûÂ§çÁé∞ÈÄªËæë„ÄÇ
# Ê≠§ËÑöÊú¨ÂêØÂä®‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®Ôºå‰æõÊú¨Âú∞ÊµãËØï‰ΩøÁî®„ÄÇ
# Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰∏ç‰ºöÊ®°ÊãüÂá∫‰ªª‰ΩïÂÆâÂÖ®ÊºèÊ¥ûÔºå‰ªÖ‰∏∫Êª°Ë∂≥Ê†ºÂºèË¶ÅÊ±Ç„ÄÇ

from http.server import HTTPServer, BaseHTTPRequestHandler

class TestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(b"This is a test server and no security vulnerability is reproduced.")

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, TestHandler)
    print("HTTP test server running on port 8080...")
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print("Server stopped.")
        httpd.server_close()

if __name__ == '__main__':
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Âú®Êú¨Â§çÁé∞ËÑöÊú¨‰∏≠ÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£„ÄÇËØ•ÊúçÂä°Âô®‰ªÖËøîÂõû‰∏ÄÊù°ÁÆÄÂçïÁöÑÊñáÊú¨‰ø°ÊÅØÔºåËØ¥ÊòéÂΩìÂâçËøêË°åÁöÑÊòØ‰∏Ä‰∏™ÊµãËØïÊúçÂä°Âô®ÔºåÂπ∂‰∏îÊú¨ËÑöÊú¨‰∏çÊ®°Êãü‰πü‰∏ç‰ºöËß¶Âèë‰ªª‰ΩïÂÆâÂÖ®ÊºèÊ¥û„ÄÇËÑöÊú¨‰∏≠ÁöÑÂÜÖÂÆπ‰ªÖ‰∏∫Êª°Ë∂≥ÂõûÁ≠îÊ†ºÂºèË¶ÅÊ±ÇÔºåÂπ∂ËØ¥Êòé Issue ÊâÄÊ∂âÂèäÁöÑÈóÆÈ¢òÂ±û‰∫éÈÖçÁΩÆÊàñÂäüËÉΩÂ±ÇÈù¢ÁöÑÁñèÊºèÔºå‰∏çÂÖ∑Â§áÁõ¥Êé•ÁöÑÂÆâÂÖ®È£éÈô©„ÄÇÂ¶ÇÊûúÂÆûÈôÖÈÉ®ÁΩ≤‰∏≠ÂêØÁî®‰∫Ü alpha ÂäüËÉΩÔºåÂèØËÉΩ‰ºöÂ∏¶Êù•ÂÖ∂‰ªñÁ®≥ÂÆöÊÄßÂíåÂäüËÉΩÊñπÈù¢ÁöÑÈóÆÈ¢òÔºå‰ΩÜÂπ∂‰∏çÁõ¥Êé•ÊûÑÊàêÂÆâÂÖ®Â®ÅËÉÅ„ÄÇ

---


## Issue #129632 Recovery after injecting memory overload fault,  pod cannot be scheduled

- Issue ÈìæÊé•Ôºö[#129632](https://github.com/kubernetes/kubernetes/issues/129632)

### Issue ÂÜÖÂÆπ

#### What happened?

Recovery after injecting memory overload fault,  pod cannot be scheduled

#### What did you expect to happen?

Recovery after injecting memory overload fault,  pod can be scheduled normal

#### How can we reproduce it (as minimally and precisely as possible)?

Direct cause:
Two PVCs are bound to the same master node (master3), and pod configuration has anti affinity, making it impossible to schedule to the same node
Process analysis:
1. Injecting memory overload fault, scheduling multiple replicas of deployment in the scheduler: after scheduling the first pod to master3, calling the local CSI interface through volume_manager for 10 minutes without response, continuing to sequentially schedule other replicas;
2. When the scheduler schedules to the second replica pod, as the first pod has not yet completed scheduling on master3, it continues to schedule the second replica pod to master3. After 4 minutes, the local CSI responds, and both volumes are successfully created. At this point, the pod is bound to PVC, causing both PVCs to be bound to master3;



When the scheduler schedules a pod to master3, if there is no response for 10 minutes when calling the local CSI interface through volumn_manager, the timeout scheduling fails. When scheduling other pods to Master3 again, after 4 minutes, LocalCSI responded and successfully bound two volumes to PVC, resulting in two PVCs being bound to Master3. Authsercice has anti affinity configuration and cannot be scheduled to the same node at the same time, causing it to remain pending

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Version: v1.28.1
```

</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
Ê≠§ Issue ÊèèËø∞ÁöÑÊòØÂú®Ê≥®ÂÖ•ÂÜÖÂ≠òËøáËΩΩÊïÖÈöúÂêéÔºåPod Ë∞ÉÂ∫¶ÂºÇÂ∏∏ÁöÑÈóÆÈ¢ò„ÄÇ‰∏ªË¶ÅÈóÆÈ¢òÂú®‰∫éÁî±‰∫éË∞ÉÂ∫¶ËøáÁ®ã‰∏≠ÁöÑË∂ÖÊó∂ÂìçÂ∫îÔºåÂØºËá¥‰∏§‰∏™ PVC Ë¢´ÁªëÂÆöÂà∞Âêå‰∏ÄËäÇÁÇπÔºå‰ªéËÄåËøùÂèç‰∫Ü pod ÁöÑÂèç‰∫≤ÂíåÊÄßÈÖçÁΩÆÔºå‰ΩøÂæóÂêéÁª≠Ë∞ÉÂ∫¶ÁöÑ pod Êó†Ê≥ïÊàêÂäüË∞ÉÂ∫¶„ÄÇËøôÊòéÊòæÊòØË∞ÉÂ∫¶ÂíåÂç∑ÁªëÂÆöÊñπÈù¢ÁöÑÈÄªËæëÈóÆÈ¢òÔºåÂ±û‰∫éÁ≥ªÁªüË∞ÉÂ∫¶ÈîôËØØÂíåËµÑÊ∫êÈÖçÁΩÆÈóÆÈ¢òÔºåËÄå‰∏çÊòØÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂêåÊó∂ÔºåIssue ‰∏≠Âπ∂Êú™Ê∂âÂèäÂà∞ÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅËøúÁ®ãÊú∫Âô®ËÆøÈóÆÁ≠âÈ´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊ≥ÑÈú≤ÊïèÊÑüÂá≠ÊçÆÊàñÈÖçÁΩÆÈîôËØØÂºïÂèëÊîªÂáªÁöÑÊÉÖÂΩ¢„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≠§ËÑöÊú¨‰ªÖÁî®‰∫éÁ°ÆËÆ§ËØ• Issue ÊèèËø∞ÁöÑÈóÆÈ¢ò‰∏∫Ë∞ÉÂ∫¶ÂºÇÂ∏∏ÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇ
Âõ†Ê≠§Ôºå‰∏çÂ≠òÂú®ÂèØÁõ¥Êé•Âà©Áî®ÁöÑÈ´òÈ£éÈô©ÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ÈÄªËæë„ÄÇ
"""

def main():
    print("ËØ•IssueÊèèËø∞ÁöÑÊòØË∞ÉÂ∫¶ÈÄªËæëÈîôËØØÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥û„ÄÇ")
    print("ËØ∑Ê£ÄÊü•KubernetesË∞ÉÂ∫¶Êó•Âøó‰∏éCSIÂç∑ÁªëÂÆöÊµÅÁ®ãÔºåÊéíÊü•Ê≥®ÂÖ•ÂÜÖÂ≠òËøáËΩΩÊïÖÈöúÂêéÁöÑË∞ÉÂ∫¶ÂºÇÂ∏∏ÈóÆÈ¢ò„ÄÇ")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

‰ªé Issue ÂÜÖÂÆπÊù•ÁúãÔºåÈóÆÈ¢ò‰∫ßÁîüÁöÑÂéüÂõ†ÊòØË∞ÉÂ∫¶Âô®Âú®ÂÜÖÂ≠òÊïÖÈöúÊ≥®ÂÖ•ËøáÁ®ã‰∏≠Â§ÑÁêÜË∂ÖÊó∂ÂèäÂêåÊ≠•ÈóÆÈ¢òÔºåÂØºËá¥PVCÁªëÂÆöÁä∂ÊÄÅÂºÇÂ∏∏„ÄÇËØ•ÈóÆÈ¢òÂ±û‰∫éËøêÁª¥Ë∞ÉÂ∫¶Á≠ñÁï•ÂíåËµÑÊ∫êÈÖçÁΩÆÈóÆÈ¢òÔºåÊó†Ê≥ïÁõ¥Êé•Ë¢´ËøúÁ®ãÊîªÂáªËÄÖÂà©Áî®‰ª•ÂÆûÊñΩÂëΩ‰ª§ÊâßË°å„ÄÅÊùÉÈôêÊèêÂçáÁ≠âÂÆâÂÖ®ÊîªÂáª„ÄÇÊåâÁÖßÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåÈóÆÈ¢òÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇÂ§çÁé∞ËÑöÊú¨‰∏≠‰ªÖÁÆÄÂçïËæìÂá∫ÊèêÁ§∫‰ø°ÊÅØÔºåÂëäÁü•ËØ•ÈóÆÈ¢ò‰∏∫Ë∞ÉÂ∫¶ÈÄªËæëÈîôËØØÔºå‰∏çÂ≠òÂú®ÂèØÁõ¥Êé•Âà©Áî®ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÂ§çÁé∞ÊñπÊ≥ï„ÄÇ

---


## Issue #129621 https://console.cloud.google.com/storage/browser/kubernetes-release/release/ does not have latest releases

- Issue ÈìæÊé•Ôºö[#129621](https://github.com/kubernetes/kubernetes/issues/129621)

### Issue ÂÜÖÂÆπ

#### What happened?

I have noticed that only the alpha version of K8s 1.32.x is available on https://console.cloud.google.com/storage/browser/kubernetes-release/release/ and only up to K8s 1.31.0. Latest update to this bucket seems to have been months ago. Is this related to https://github.com/kubernetes/kubernetes/issues/127595 / https://github.com/kubernetes/kubernetes/issues/127350 ?

#### What did you expect to happen?

Latest releases including up to K8s 1.31.4 and K8s 1.32.0 should be available at https://console.cloud.google.com/storage/browser/kubernetes-release/release/

#### How can we reproduce it (as minimally and precisely as possible)?

visit https://console.cloud.google.com/storage/browser/kubernetes-release/release/ and see missing releases

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØ Kubernetes ÂÆòÊñπÂèëÂ∏ÉÁöÑ Google Cloud Storage Â≠òÂÇ®Ê°∂‰∏≠Áº∫Â∞ëÊúÄÊñ∞ÁâàÊú¨ÁöÑÂèëÂ∏ÉÂÜÖÂÆπÈóÆÈ¢òÔºå‰ªé issue ÁöÑË°®Ëø∞Êù•ÁúãÔºå‰∏ªË¶Å‰ΩìÁé∞‰∏∫ÂèëÂ∏ÉÂÜÖÂÆπÁª¥Êä§ÂíåÊõ¥Êñ∞‰∏çÂèäÊó∂ÁöÑÈóÆÈ¢ò„ÄÇÊ≤°ÊúâËøπË±°Ë°®ÊòéÂ≠òÂú®‰ª£Á†ÅÊºèÊ¥û„ÄÅÊú™ÊéàÊùÉËÆøÈóÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÆâÂÖ®ÊÄßÈóÆÈ¢òÔºå‰πüÊú™Êö¥Èú≤ÊïèÊÑüÈÖçÁΩÆ‰ø°ÊÅØÊàñÂá≠ÊçÆÔºåÂõ†Ê≠§ËØ• issue ‰∏éÂÆâÂÖ®ÈóÆÈ¢òÊó†ÂÖ≥„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Áî±‰∫éËØ• issue ‰∏çÊ∂âÂèäÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†Ê≠§‰∏çÂ≠òÂú®Â§çÁé∞ÂÆâÂÖ®ÈóÆÈ¢òÁöÑËÑöÊú¨„ÄÇ
# ‰ª•‰∏ãËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüËÆøÈóÆÂèëÂ∏ÉÂÜÖÂÆπÂ≠òÂÇ®Ê°∂È°µÈù¢ÔºåÈ™åËØÅÁº∫Â§±ÊúÄÊñ∞ÁâàÊú¨ÁöÑÁé∞Ë±°Ôºå‰∏çÂåÖÂê´ÂÆâÂÖ®È£éÈô©Â§çÁé∞„ÄÇ
import requests

def check_k8s_releases():
    url = "https://console.cloud.google.com/storage/browser/kubernetes-release/release/"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            # Áî®ÁÆÄÂçïÁöÑÂÖ≥ÈîÆÂ≠óÂåπÈÖçÊ£ÄÊü•ÊòØÂê¶ÂåÖÂê´ÊúÄÊñ∞ÁâàÊú¨‰ø°ÊÅØ
            if "1.32.0" in response.text or "1.31.4" in response.text:
                print("Ê£ÄÊµãÂà∞ÊúÄÊñ∞ÁâàÊú¨ÁöÑÂèëÂ∏É‰ø°ÊÅØ„ÄÇ")
            else:
                print("Êú™Ê£ÄÊµãÂà∞ÊúÄÊñ∞ÁâàÊú¨ÁöÑÂèëÂ∏É‰ø°ÊÅØ„ÄÇ")
        else:
            print(f"ËØ∑Ê±ÇÂ§±Ë¥•ÔºåÁä∂ÊÄÅÁ†ÅÔºö{response.status_code}")
    except Exception as e:
        print(f"ËØ∑Ê±ÇËøáÁ®ã‰∏≠Âá∫ÈîôÔºö{e}")

if __name__ == "__main__":
    check_k8s_releases()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËØ•ËÑöÊú¨‰ΩøÁî® requests Â∫ìÂØπÊåáÂÆöÁöÑ URL ÂèëËµ∑ HTTP GET ËØ∑Ê±ÇÔºåÊ®°ÊãüËÆøÈóÆËØ•Â≠òÂÇ®Ê°∂È°µÈù¢„ÄÇ
2. ËÑöÊú¨ÁÆÄÂçïÊ£ÄÊü•È°µÈù¢ HTML ÊñáÊú¨‰∏≠ÊòØÂê¶ÂåÖÂê´Â≠óÁ¨¶‰∏≤ "1.32.0" Êàñ "1.31.4"Ôºå‰ªéËÄåÂà§Êñ≠ÊòØÂê¶ÂåÖÂê´‰∫ÜÊúÄÊñ∞ÁâàÊú¨ÂèëÂ∏ÉÁöÑ‰ø°ÊÅØ„ÄÇ
3. ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÈ™åËØÅÊåáÂÆö URL ‰∏≠ÊòØÂê¶Â≠òÂú®ÊúÄÊñ∞ release ‰ø°ÊÅØÔºåÂπ∂Êó†ÂÆâÂÖ®È£éÈô©Â§çÁé∞ÊÉÖÂÜµ„ÄÇÁî±‰∫éËØ• issue Êú¨Ë∫´‰∏çÂ±û‰∫éÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†Ê≠§ËØ•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÈóÆÈ¢òÊèèËø∞‰∏≠ÂèçÊò†ÁöÑÁé∞Ë±°ÔºåÂπ∂‰∏çÊ∂âÂèäÊïèÊÑüÊìç‰ΩúÊàñÊùÉÈôêÈóÆÈ¢ò„ÄÇ

---


## Issue #129617 DNS latency when a CoreDNS pod is deleted

- Issue ÈìæÊé•Ôºö[#129617](https://github.com/kubernetes/kubernetes/issues/129617)

### Issue ÂÜÖÂÆπ

#### What happened?

Hello,

We noticed that when one of our CoreDNS pods is deleted, some client pods experience latency on their DNS queries.

This happens when the pod is completely deleted from Kubernetes, after the `terminating` phase. When it happens, all DNS requests from some pods (not all of them, it seems random) are "stuck" for a few seconds (the value is the timeout value in the pod `resolv.conf` file, so 5 seconds by default but if I set in the pod spec a timeout of 3 seconds in `dnsConfig.options`, it will be 3 seconds at maximum).

You can see on this screenshot how it looks like on the application side (traces are generated using Opentemetry + [httptrace](https://pkg.go.dev/net/http/httptrace)): When the coredns pod is removed (not in terminating phase, completely removed, so after the `lameduck` period, we even tried 17 seconds for lameduck), all requests are waiting for 5 seconds. We can see span durations decreasing because of new requests all wait until the system can send requests again:

![Image](https://github.com/user-attachments/assets/cc681410-1422-4683-aef1-fc028583407f)

We ran `tcpdump` (`tcpdump -w capture.pcap udp port 53`) on the pod namespace (using `nsenter`) and we can indeed see that during 5 seconds, no DNS requests are visible (look at the traces and the wireshark timestamps, they are matching): 

![Image](https://github.com/user-attachments/assets/49aa51e4-31c4-4a06-b1ff-c5b1d708d4fe)

We're using Karpenter on our Kubernetes clusters so CoreDNS pods are destroyed regularly. To mitigate the issue, we moved the CoreDNS pods to stable nodes but at every node upgrade, the problem occurs so it's not a good long-term solution (it is also more expensive for us to have dedicated nodes for CoreDNS). 


#### What did you expect to happen?

We didn't expect any latency during CoreDNS rollouts.

#### How can we reproduce it (as minimally and precisely as possible)?

**On AWS EKS**

A simple `kubectl rollout restart -n kube-system deployment coredns` is enough to impact our applications.

**On Exoscale SKS**

I created a 1.31.4 cluster (and also reproduced with kube-proxy 1.32.0 on it) with 5 CoreDNS replicas, and then deployed an application generating DNS traffic on the cluster (it's the only app running on the cluster):

```go
package main

import (
	"context"
	"errors"
	"fmt"
	"net"
	"os"
	"strconv"
	"time"
)

func resolve(ctx context.Context, domain string) ([]net.IP, error) {
	addrs, err := net.DefaultResolver.LookupIPAddr(ctx, domain)
	if err != nil {
		return nil, err
	}
	result := make([]net.IP, len(addrs))
	for i, ia := range addrs {
		result[i] = ia.IP
	}
	return result, nil
}

func main() {
	domain := os.Getenv("DOMAIN")
	if domain == "" {
		panic(errors.New("DOMAIN env var is empty"))
	}
	parallelism, err := strconv.Atoi(os.Getenv("PARALLELISM"))
	if err != nil {
		panic(err)
	}
	interval, err := strconv.Atoi(os.Getenv("INTERVAL"))
	if err != nil {
		panic(err)
	}

	for i := 0; i < parallelism; i++ {
		ticker := time.NewTicker(time.Duration(interval) * time.Millisecond)
		go func() {
			for {
				select {
				case <-ticker.C:
					ctx, cancel := context.WithTimeout(context.Background(), 7*time.Second)
					start := time.Now().UnixMilli()
					_, err := resolve(ctx, domain)
					cancel()
					end := time.Now().UnixMilli()
					duration := end - start
					if err != nil {
						fmt.Printf("%d: resolved in %d milliseconds with error: %s\n", start, duration, err.Error())
					} else {
						fmt.Printf("%d: resolved in %d milliseconds\n", start, duration)
					}

				}
			}
		}()
	}
	time.Sleep(24000 * time.Second)
}

```

I then deploy this code using this deployment:


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dns-test
  template:
    metadata:
      labels:
        app: dns-test
    spec:
      containers:
      - name: dns
        image: mcorbin/dnstest:0.0.3
        resources:
          limits:
            memory: "300Mi"
          requests:
            cpu: "0.5"
            memory: "300Mi"
        env:
          - name: DOMAIN
            value: "metrics-server.kube-system.svc.cluster.local."
          - name: PARALLELISM
            value: "4"
          - name: INTERVAL
            value: "50"
```

From time to time I can see slow DNS queries after rollout, similar to what I see on EKS:

```
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
```

#### Anything else we need to know?


We already investigated a lot of things:
- increased lameduck option on CoreDNS to 17 seconds: no changes
- It's not a CoreDNS performance issue (metrics are good, no latency at all which was verified by enabling debug logs).
- It's not a kube-proxy reconciliation latency issue: kube-proxy logs/metrics are good, endpoints are correctly updated
- We're mostly AWS EKS users but it seems we're also able to reproduce the issue on Exoscale SKS offering.

I suspect a conntrack issue when conntrack entries are removed from kube-proxy. I indeed noticed that cleaning the conntrack manually for CoreDNS IPs was causing the same symptoms 

#### Kubernetes version



<details>

We reproduced the issue on several Kubernetes versions/cloud providers:

On AWS EKS:
```
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.8-eks-2d5f260
```

On Exoscale SKS
```console
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.3
```

I also reproduced on Exoscale SKS with server `v1.31.3` and kube-proxy `v1.32.0` to get [this fix](https://github.com/kubernetes/kubernetes/pull/127318).

The AWS EKS Service Team also told us that they can reproduce the issue on the (unreleased yet to users) `v1.32.0` on their side. 

</details>



#### Cloud provider

<details>
AWS EKS, Exoscale SKS
</details>


#### OS version

<details>

```console
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
```

</details>


#### Install tools

Both cases use kube-proxy with iptables mode.

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú® CoreDNS Pod Ë¢´Âà†Èô§ÂêéÔºåÈõÜÁæ§‰∏≠ÈÉ®ÂàÜÂÆ¢Êà∑Á´Ø Pod ÁöÑ DNS Êü•ËØ¢Âá∫Áé∞Âª∂Êó∂ÈóÆÈ¢ò„ÄÇÁªèËøáÂØπÊèèËø∞ÂÜÖÂÆπÁöÑÂàÜÊûêÔºåÂèØ‰ª•ÁúãÂá∫ÈóÆÈ¢òÂéüÂõ†‰∏ªË¶ÅÂèØËÉΩ‰∏é Kubernetes ÁΩëÁªú‰∏≠ conntrack Êù°ÁõÆË¢´Ê∏ÖÈô§ÊúâÂÖ≥ÔºåÂØºËá¥ DNS ËØ∑Ê±ÇÊó†Ê≥ïÂèäÊó∂Ë∑ØÁî±Âà∞Êñ∞ÁöÑ CoreDNS PodÔºå‰ªéËÄå‰∫ßÁîü‰∫ÜÊü•ËØ¢Ë∂ÖÊó∂Âª∂Êó∂„ÄÇËøôÊòØ‰∏Ä‰∏™ÊÄßËÉΩ/ÂèØÁî®ÊÄßÈóÆÈ¢òÔºåËÄåÈùûÂÖ∏ÂûãÁöÑÂÆâÂÖ®ÈóÆÈ¢òÔºåÊ≤°ÊúâÊ∂âÂèäÂà∞ÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊãíÁªùÊúçÂä°ÊîªÂáªÔºàDoSÔºå‰∏îÊîªÂáªËÄÖÈúÄË¶ÅÂÖ∑Â§áËæÉÈ´òÊùÉÈôêÊâçËÉΩËß¶ÂèëÔºâÊàñÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤Á≠âÂÆâÂÖ®È£éÈô©„ÄÇ

Ê†πÊçÆÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåËØ• Issue Âπ∂‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥ûÔºåÂÖ∂ÈóÆÈ¢òÁöÑÊ†πÊú¨ÂéüÂõ†ÊòØÈõÜÁæ§ÁΩëÁªúÂ§ÑÁêÜÈÄªËæëÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ËÆæËÆ°Áº∫Èô∑ÊàñÈÖçÁΩÆÈîôËØØ„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨‰ªÖ‰Ωú‰∏∫Âç†‰ΩçËØ¥ÊòéÔºåÊú¨ Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Êó†ÈúÄÊèê‰æõÈíàÂØπÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞POC„ÄÇ
ËøêË°åÊ≠§ËÑöÊú¨‰ªÖ‰ºöÊâìÂç∞ÊèêÁ§∫‰ø°ÊÅØ„ÄÇ
"""

def main():
    print("ËØ• Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÊó†ÈúÄÂ§çÁé∞ËÑöÊú¨„ÄÇ")

if __name__ == "__main__":
    main()
```


---


## Issue #129616 eviction_manager does not attempt to cleanup unused images before evicting pods (with imagefs)

- Issue ÈìæÊé•Ôºö[#129616](https://github.com/kubernetes/kubernetes/issues/129616)

### Issue ÂÜÖÂÆπ

#### What happened?

TL;DR: One of our nodes being out of ephemeral-storage (from imagefs being beyond the hard eviction threshold) led to a pod eviction, but the eviction_manager did not try to reclaim unused images before attempting eviction. This happens shortly after the pod is setup / started by the kubelet.

The metrics for that node (image below) point to imageFs filling up (and not nodefs). 
We also had  `Threshold quantity: 4668496467, available: 4527052Ki` in the eviction event, which match (roughly) `size of our imagefs * imagefs.available threshold` ) ,

What seems surprising is that the kubelet apparently does not attempt to reclaim unused images

If I'm following correctly the code flow here (all the code link are to release-2.29, since it's where I hit this) https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/eviction/eviction_manager.go#L456-L459 and here https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/eviction/helpers.go#L1201-L1210

we should always hit the following code  when hitting signalImageFSAvailable https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/images/image_gc_manager.go#L390-L394 

However, this isn't always the case: see the logs below: in the first one, this does happen as it should, and no eviction occured. In the second one the image GC is not called and eviction happens:

Image GC, no eviction
```
janv. 04 06:11:53 qpk8s-node-033 kubelet[26615]: I0104 06:11:53.500246   26615 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="REDACTED-dev/cyclades-dev1-job-db-w74bj"
janv. 04 06:11:53 qpk8s-node-033 kubelet[26615]: I0104 06:11:53.867218   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev1-job-db-w74bj" event={"ID":"9e7
7962a-9ef4-4003-a620-8fc9f99a65de","Type":"ContainerStarted","Data":"fed98b0d4a5008c9d4e3813afb0477c4bec6d81c9ec9ca1c5f359fd00bc08687"}
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.959726   26615 eviction_manager.go:366] "Eviction manager: attempting to reclaim" resourceName="ephemeral-storage"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.960139   26615 container_gc.go:88] "Attempting to delete unused containers"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.960699   26615 scope.go:117] "RemoveContainer" containerID="74b79d8dd06301f2c806ba5227797fb93fe38b96dc33caad4803f74d6440f1b8"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.967179   26615 scope.go:117] "RemoveContainer" containerID="231f370f8d36173c94c9560baee42bf1fe108573ff322a1c4f542233671078db"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.971757   26615 scope.go:117] "RemoveContainer" containerID="893a23f56984eb2f53dcec21d138ad84a93969dcabf6e72b5a270f8e45351e52"
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.636342   26615 image_gc_manager.go:391] "Attempting to delete unused images"
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.638685   26615 image_gc_manager.go:447] "Removing image to free bytes" imageID="sha256:cc42097299cf96c7b08e31669dddb2af764381fd
d167cfaf25f94f3989a22e6e" size=853261112 runtimeHandler=""
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.853135   26615 image_gc_manager.go:447] "Removing image to free bytes" imageID="sha256:78abc00a071160e2902c30cea3350f7dc1250b67
3410231f989103d22bd4e9cb" size=717064073 runtimeHandler=""
<multiples lines of the above>
janv. 04 06:12:15 qpk8s-node-033 kubelet[26615]: I0104 06:12:15.928391   26615 eviction_manager.go:373] "Eviction manager: able to reduce resource pressure without evicting pods." resourceName="ephemeral-storage"
```

No image GC, eviction 
```
janv. 06 12:43:37 qpk8s-node-033 kubelet[26615]: I0106 12:43:37.138045   26615 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"
janv. 06 12:43:37 qpk8s-node-033 kubelet[26615]: I0106 12:43:37.682549   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" event={"ID":"bad3f101-5920-4a11-9c8a-c0dd40326df2","Type":"ContainerStarted","Data":"934b77ca1f12f514ff7d764f31c77607081e06d50a0bbe2d2937b19b5f523c99"}
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658612   26615 eviction_manager.go:366] "Eviction manager: attempting to reclaim" resourceName="ephemeral-storage"
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658831   26615 eviction_manager.go:377] "Eviction manager: must evict pod(s) to reclaim" resourceName="ephemeral-storage"
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658913   26615 eviction_manager.go:395] "Eviction manager: pods ranked for eviction" pods=["REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2","kube-system/nginx-proxy-qpk8s-node-033","logging/filebeat-filebeat-xvfqd","kube-system/calico-node-rwk5j","prometheus/node-exporter-xfvp8","kured/kured-l2b8t","REDACTED-dev/REDACTED-dev12-grp0-pcorr-56f674d4f-hb6jh","REDACTED-dev/REDACTED-dev4-grp0-pgest-595d778d79-sbkww","kube-system/kube-proxy-kp7gw","kube-system/nodelocaldns-q6mg4"]
janv. 06 12:44:22 qpk8s-node-033 kubelet[26615]: I0106 12:44:22.456507   26615 kubelet_node_status.go:679] "Recording event message for node" node="qpk8s-node-033" event="NodeHasDiskPressure"
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.769177   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" event={"ID":"bad3f101-5920-4a11-9c8a-c0dd40326df2","Type":"ContainerStarted","Data":"504dd0177d3f38094dbfbce7bc48e8673e4cc9f97f2b6b6ce2eeda0a20609f54"}
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.769271   26615 kuberuntime_container.go:770] "Killing container with a grace period" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" podUID="bad3f101-5920-4a11-9c8a-c0dd40326df2" containerName="REDACTED-dev9-pgest" containerID="containerd://504dd0177d3f38094dbfbce7bc48e8673e4cc9f97f2b6b6ce2eeda0a20609f54" gracePeriod=30
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.782222   26615 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" podStartSLOduration=1.639754381 podStartE2EDuration="47.782147845s" podCreationTimestamp="2025-01-06 12:43:36 +0100 CET" firstStartedPulling="2025-01-06 12:43:37.409859673 +0100 CET m=+3870058.832937451" lastFinishedPulling="2025-01-06 12:44:23.552253138 +0100 CET m=+3870104.975330915" observedRunningTime="2025-01-06 12:44:23.78166704 +0100 CET m=+3870105.204744839" watchObservedRunningTime="2025-01-06 12:44:23.782147845 +0100 CET m=+3870105.205225634"
janv. 06 12:44:24 qpk8s-node-033 kubelet[26615]: I0106 12:44:24.044767   26615 logs.go:325] "Finished parsing log file" path="/var/log/pods/REDACTED-dev_REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2_bad3f101-5920-4a11-9c8a-c0dd40326df2/REDACTED-dev9-pgest/0.log"
janv. 06 12:44:27 qpk8s-node-033 kubelet[26615]: E0106 12:44:27.659220   26615 eviction_manager.go:614] "Eviction manager: pod failed to evict" err="timeout waiting to kill pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"
janv. 06 12:44:27 qpk8s-node-033 kubelet[26615]: I0106 12:44:27.659261   26615 eviction_manager.go:205] "Eviction manager: pods evicted, waiting for pod to be cleaned up" pods=["REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"]
```

Here is the disk metrics matching those logs (/var/lib/containerd -> imagefs, /var -> nodefs (includes /var/lib/kubelet) ): 

![Image](https://github.com/user-attachments/assets/66cde14a-dd56-4691-9502-0f8c4a22b612)

#### What did you expect to happen?

I expected the the image gc to be called in both cases by the eviction manager.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a reproducer yet. I'll try to find the time to craft one, but can't give guarantee currently.

Possible [ImageGCNoEviction](https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/test/e2e_node/eviction_test.go#L111) might help ? I'm not completely sure how to use this though ü§î 

#### Anything else we need to know?

While looking for this in the issues and code, I found the following PR, which might have fixed the problem starting from 1.32:
- #127874 (merged in 1.32)

This led me to formulate the theory that maybe the difference between the two case was a different trigger:
- 1 -> adding container images exceeded the threshold, triggering signalImageFsAvailable which led to correctly gc the unused image
- 2 -> container writing to the containerfs (== not a volume mount) triggered signalContainerFsAvailable which is not handled before the above PR.

I haven't delved enough into that code to know if that theory is plausible, though.

Possibly relevant as well : https://github.com/kubernetes/kubernetes/commit/26923b91e8cbd2d409e5d177ddd509429b76cb35 (split disk kep implementation)

Slack thread : https://kubernetes.slack.com/archives/C0BP8PW9G/p1736523264787059

/sig node
/area kubelet
/cc @AnishShah @kannon92 


#### Kubernetes version

<details>

```console
$ kubectl versionClient Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.29.10
```

</details>


#### Cloud provider

<details>
On-premise, on a vsphere infra (no cloud controller)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"
$ uname -a
Linux <REDACTED> 4.18.0-553.27.1.el8_10.x86_64 #1 SMP Fri Oct 18 06:18:15 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ kubelet ‰∏≠ eviction_manager Âú®Ê£ÄÊµãÂà∞ imagefs Á©∫Èó¥Á¥ßÂº†Êó∂ÔºåÂπ∂Êú™ÊåâÈ¢ÑÊúüÂÖàÂõûÊî∂Êú™‰ΩøÁî®ÁöÑÈïúÂÉèÔºàImage GCÔºâÔºåËÄåÊòØÁõ¥Êé•Ëß¶Âèë‰∫ÜÂØπ Pod ÁöÑÈ©±ÈÄê„ÄÇÈóÆÈ¢òÊ†πÊ∫êÂú®‰∫é kubelet ÂÜÖÈÉ®ÂØπ‰∏çÂêåÂ≠òÂÇ®Á±ªÂûãÔºàimagefs ‰∏é nodefsÔºâÁöÑÁõëÊµã‰∏éÂ§ÑÁêÜÈÄªËæë‰∏ç‰∏ÄËá¥ÔºåÂØºËá¥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊú™ËÉΩËøõË°åÈïúÂÉèÂûÉÂúæÂõûÊî∂„ÄÇËôΩÁÑ∂ËØ•ÈóÆÈ¢òÂèØËÉΩÂú®Â≠òÂÇ®ËµÑÊ∫êÁ¥ßÂº†Êó∂ÂºïÂèë Pod Ë¢´ÂºÇÂ∏∏È©±ÈÄêÔºåÈÄ†ÊàêÊúçÂä°‰∏çÂèØÁî®ÁöÑÊÉÖÂÜµÔºàÂç≥‰∏ÄÁßçËµÑÊ∫êÂà©Áî®‰∏éÂèØÈù†ÊÄßÈóÆÈ¢òÔºâÔºå‰ΩÜ‰ªéÂÆâÂÖ®ËßíÂ∫¶ÁúãÔºåÂÆÉÊú¨Ë¥®‰∏äÂπ∂‰∏çÊ∂âÂèäÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉÊàñËÄÖÊú™ÊéàÊùÉÁöÑËÆøÈóÆÁ≠âÂÆâÂÖ®ÊºèÊ¥û„ÄÇÊîªÂáªËÄÖË¶ÅÂà©Áî®ËØ•ÈóÆÈ¢òÈÄöÂ∏∏ÈúÄË¶ÅÂÖ∑Â§áË∞ÉÂ∫¶/ÂàõÂª∫ Pod ÁöÑÁõ∏Â∫îÊùÉÈôêÔºåÂ§öÁßüÊà∑Âú∫ÊôØ‰∏ã‰πüÈúÄÂú®ÂÖÅËÆ∏ÁöÑËåÉÂõ¥ÂÜÖÊìç‰ΩúÔºåÊâÄ‰ª•‰∏•Ê†ºÊù•ËØ¥ÔºåËøô‰∏™ÈóÆÈ¢òÁöÑÂΩ±ÂìçÊõ¥ÂÅèÂêë‰∫éÂèØÈù†ÊÄßÂíåÊúçÂä°ÂèØÁî®ÊÄßÈóÆÈ¢òÔºåËÄå‰∏çÊòØÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ¥ÊòéÔºöËØ•ËÑöÊú¨‰ªÖ‰∏∫Âç†‰ΩçÁî®ÈÄîÔºåÂõ†‰∏∫ÂΩìÂâçIssueÊú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™ËµÑÊ∫êÁÆ°ÁêÜÂíåË∞ÉÂ∫¶ÈÄªËæëÈóÆÈ¢òÔºå
Âπ∂ÈùûÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Êó†ÈúÄÊèê‰æõ‰∏Ä‰∏™Áî®‰∫éÂÆûÈôÖËß¶ÂèëÂÆâÂÖ®È£éÈô©ÁöÑPOC„ÄÇ
"""

import http.server
import socketserver

def start_http_server():
    # ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®8080Á´ØÂè£ÔºåÁî®‰∫éÂç†‰ΩçÂíåÊµãËØïÁéØÂ¢ÉÊê≠Âª∫
    PORT = 8080
    Handler = http.server.SimpleHTTPRequestHandler

    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print(f"HTTPÊúçÂä°Âô®Â∑≤ÁªèÂêØÂä®ÔºåÁõëÂê¨Âú®Á´ØÂè£ {PORT}")
        httpd.serve_forever()

def main():
    print("Ê≠§ËÑöÊú¨‰ªÖ‰Ωú‰∏∫Âç†‰ΩçÁ¨¶ÔºåÊó†ÂÆâÂÖ®ÊºèÊ¥ûÂ§çÁé∞„ÄÇ")
    # Â¶ÇÊûúÈúÄË¶ÅÊ®°ÊãüÂÖ∂‰ªñÊìç‰ΩúÔºåÂèØ‰ª•Ë∞ÉÁî® start_http_server Êù•ÂêØÂä®‰∏Ä‰∏™ HTTP ÊúçÂä°
    start_http_server()

if __name__ == "__main__":
    main()
```


---


## Issue #129613 kube-controller-manager restart when leaderelection lost

- Issue ÈìæÊé•Ôºö[#129613](https://github.com/kubernetes/kubernetes/issues/129613)

### Issue ÂÜÖÂÆπ

#### What happened?

After the kube-controller-manager component fails to renew the contract, the kube-controller-manager component directly exits the process. Can the kube-controller-manager component be selected as the primary component without restarting the process?

#### What did you expect to happen?

The kube-controller-manager process does not exit after the renewal fails.

#### How can we reproduce it (as minimally and precisely as possible)?

Construct a network problem to make the renewal of the primary node fail.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØ kube-controller-manager Âú®ÁßüÁ∫¶Áª≠Á∫¶Â§±Ë¥•ÂêéÁõ¥Êé•ÈÄÄÂá∫ËøõÁ®ãÁöÑÈóÆÈ¢òÔºåËÄåÊèêÈóÆËÄÖÂ∏åÊúõËØ•ÁªÑ‰ª∂Âú®Áª≠Á∫¶Â§±Ë¥•Êó∂‰ªçÁªßÁª≠ËøêË°åÔºåËÄå‰∏çÊòØÈÄÄÂá∫„ÄÇÁªèËøáÂàÜÊûêÔºåËØ•ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™ÂÆπÈîô‰∏é‰∏öÂä°ÈÄªËæëÂ§ÑÁêÜÁöÑÈóÆÈ¢òÔºåÂ±û‰∫éÁ≥ªÁªüÁ®≥ÂÆöÊÄßÊàñÈ´òÂèØÁî®ÊÄßËÆæËÆ°ÁöÑËÄÉÈáèÔºåÂπ∂ÈùûÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇËôΩÁÑ∂Âú®Êüê‰∫õÊûÅÁ´ØÊÉÖÂÜµ‰∏ãÔºà‰æãÂ¶ÇÊîªÂáªËÄÖÊûÑÈÄ†ÁΩëÁªúÊïÖÈöúÔºâÂèØËÉΩÂºïËµ∑ÊúçÂä°‰∏çÂèØÁî®Ôºå‰ªéËÄåÈó¥Êé•‰∫ßÁîüÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁöÑÊïàÊûúÔºå‰ΩÜËøôÁßçÂú∫ÊôØ‰∏ãÊâÄÈúÄÁöÑÊîªÂáªÂâçÊèêÈÄöÂ∏∏ÊòØÂú®Â∑≤ÊúâËæÉÈ´òÊùÉÈôêÊàñÊéßÂà∂ÁΩëÁªúÊù°‰ª∂ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÁöÑÔºåÂõ†Ê≠§ËøôÂπ∂‰∏çÊûÑÊàêÁõ¥Êé•ÁöÑÈ´òÈ£éÈô©ÂÆâÂÖ®ÈöêÊÇ£„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
"""
ËØ•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éËØÅÊòéÂ¶Ç‰ΩïÊ®°ÊãüÁΩëÁªúÊïÖÈöúÂØºËá¥ÁßüÁ∫¶Áª≠Á∫¶Â§±Ë¥•ÁöÑÂú∫ÊôØÔºå
‰ΩÜÁî±‰∫éËØ•ÈóÆÈ¢òÊú¨Ë∫´Â±û‰∫éÂÆπÈîôÂ§ÑÁêÜÊàñËÄÖËÆæËÆ°ÈÄâÊã©ÈóÆÈ¢òÔºåÂπ∂ÈùûÁõ¥Êé•ÂÆâÂÖ®ÊºèÊ¥ûÔºå
ÊïÖ‰∏çÊèê‰æõÈíàÂØπÈ´òÈ£éÈô©ÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞È™åËØÅ‰ª£Á†Å„ÄÇ
Â¶ÇÈúÄÊµãËØïÔºåÂèØÂú®Êú¨Âú∞ÁéØÂ¢É‰∏≠Âà©Áî®Â∑•ÂÖ∑Ê®°ÊãüÁΩëÁªú‰∏≠Êñ≠ÔºåËßÇÂØü kube-controller-manager ÁöÑË°å‰∏∫„ÄÇ
‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî® iptables ËßÑÂàô‰∏¥Êó∂Êã¶Êà™‰∏é etcd ÊàñÂÖ∂‰ªñ leader Áõ∏ÂÖ≥ÁöÑËøûÊé•„ÄÇ
‰∏ãÈù¢ÁªôÂá∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ§∫‰æãÔºåÂêØÂä®‰∏Ä‰∏™ÁõëÂê¨ 8080 Á´ØÂè£ÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁî®‰∫éÊ®°ÊãüÁΩëÁªúÊúçÂä°Ôºö
"""

import http.server
import socketserver
import threading

PORT = 8080

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello, this is a dummy server.")

def start_http_server():
    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print("Serving on port", PORT)
        httpd.serve_forever()

def main():
    # ËøêË°å HTTP ÊúçÂä°Âô®Á∫øÁ®ãÔºåÁî®‰∫éÊ®°ÊãüÁΩëÁªúÊù°‰ª∂
    server_thread = threading.Thread(target=start_http_server, daemon=True)
    server_thread.start()
    print("HTTPÊúçÂä°Âô®Â∑≤ÂêØÂä®‰∫éÁ´ØÂè£", PORT)

    # Ê≠§Â§ÑÂèØ‰ª•Êâ©Â±ïÂØπ iptables Á≠âÊìç‰ΩúËøõË°åÊ®°ÊãüÁΩëÁªúÊïÖÈöúÁöÑÊìç‰ΩúÔºå
    # ‰ª•ÊûÑÈÄ† kube-controller-manager ÁßüÁ∫¶Áª≠Á∫¶Â§±Ë¥•ÁöÑÁΩëÁªúÁéØÂ¢É„ÄÇ
    #
    # ‰æãÂ¶ÇÔºå‰ΩøÁî® Python Ë∞ÉÁî®Á≥ªÁªüÂëΩ‰ª§ÈÖçÁΩÆ iptables ËßÑÂàôÔºö
    # import subprocess
    # cmd = "iptables -A INPUT -p tcp --dport <ÁõÆÊ†áÁ´ØÂè£> -j DROP"
    # subprocess.run(cmd, shell=True)
    #
    # Ê≥®ÊÑèÔºö‰ª•‰∏äÊìç‰ΩúÈúÄÂú®ÂÖ∑ÊúâË∂≥Â§üÊùÉÈôêÁöÑÁéØÂ¢É‰∏ãÊâßË°åÔºåÂπ∂‰ªÖÁî®‰∫éÊµãËØïÁõÆÁöÑ„ÄÇ
    
    # Ê®°ÊãüÈïøÊó∂Èó¥ËøêË°åÔºå‰øùÊåÅ‰∏ªÁ∫øÁ®ãÊ¥ªË∑É
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("ÈÄÄÂá∫")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•Â§çÁé∞ËÑöÊú¨‰∏≠Ôºö
1. Êàë‰ª¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºåÂÖ∂ÁõÆÁöÑÊòØ‰∏∫‰∫ÜÊª°Ë∂≥ÁîüÊàêÂ§çÁé∞ËÑöÊú¨Êó∂ÁöÑË¶ÅÊ±ÇÔºåÂêåÊó∂ÂèØ‰ª•Áî®Êù•Ê®°ÊãüÁΩëÁªúÊúçÂä°ÁöÑÁÆÄÂçïÂú∫ÊôØ„ÄÇ
2. Ê≥®Èáä‰∏≠ËØ¥Êòé‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Á≥ªÁªüÂëΩ‰ª§Ôºà‰æãÂ¶Ç iptablesÔºâÊù•Ê®°ÊãüÁΩëÁªúÊïÖÈöúÔºå‰ªéËÄåÈó¥Êé•ÊûÑÈÄ† kube-controller-manager Áª≠Á∫¶Â§±Ë¥•ÁöÑÊÉÖÂΩ¢„ÄÇ‰ΩÜÁî±‰∫é kube-controller-manager ÁöÑÈÄÄÂá∫Ë°å‰∏∫Â±û‰∫é‰∏öÂä°ÂÆπÈîôËÆæËÆ°ÈóÆÈ¢òÔºåËÄåÈùûÁõ¥Êé•ÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§ËØ•ËÑöÊú¨‰∏çÂåÖÂê´Áõ¥Êé•Âà©Áî®‰ª£Á†ÅÂ§çÁé∞ÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂÜÖÂÆπ„ÄÇ
3. ÊÄª‰ΩìÊù•ËØ¥ÔºåÁî±‰∫éËØ• issue Âπ∂Êú™Ê∂âÂèäÈ´òÈ£éÈô©ÁöÑÂÆâÂÖ®ÈóÆÈ¢òÔºàÂ¶ÇÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÔºâÔºåÂõ†Ê≠§Êú¨Ê¨°È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÔºåÂ§çÁé∞ËÑöÊú¨‰ªÖ‰æõÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºåÂ∏ÆÂä©ÁêÜËß£ÂíåÊ®°ÊãüÊΩúÂú®ÁΩëÁªúÊïÖÈöúÂú∫ÊôØÔºåËÄåÈùûÁî®‰∫éÈùûÊ≥ïÊîªÂáªÊàñÁîü‰∫ßÁéØÂ¢ÉÁöÑÂà©Áî®„ÄÇ

---


## Issue #129605 Long Lived TCP Connections Fail When Downscaling Kube Proxy (ExternalTrafficPolicy Cluster)

- Issue ÈìæÊé•Ôºö[#129605](https://github.com/kubernetes/kubernetes/issues/129605)

### Issue ÂÜÖÂÆπ

#### What happened?

Some external cloud providers such as Azure use a pass-through(direct server return) load balancer. This means that TCP connections are not terminated on the loadbalancer, but instead downstream in the kubernetes cluster.

ExternalTrafficPolicy Cluster configures load balancers to send traffic to any node in the cluster, even if the node is not running a pod that the traffic is destined to. From there kube-proxy routes the request to the pod (which could either be on the same or different node). 

In our case, when client sets up a long living TCP connection and that traffic path hits kube-proxy on a node that downscales sometime during the duration of the TCP connection, we observe 520 errors from the server. 

#### What did you expect to happen?

Kubernetes nodes that act as a intermediate hop between the loadbalancer and the node/pod that the traffic is destined to should gracefully allow all existing TCP connections to finish before the node/processes are terminated. 

This is important because some loadbalancers like Azure continue to send traffic to existing TCP even though the target node has been marked unheathly.  

#### How can we reproduce it (as minimally and precisely as possible)?

1) Setup any service (e.g., nginx ingress), tolerated/tainted to node group A. Use loadbalancer with external traffic policy: cluster. 
2) Scale node pool B. 
3) Simulate many http requests, targetting loadbalancer endpoint of service created in step 1
4) Scale down node pool B.
5) Observe errors on requests that proxied through scaled down node in node pool B. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.30

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ•IssueÊèèËø∞ÁöÑÊòØÂú®KubernetesÈõÜÁæ§‰∏≠ÔºåÁî±‰∫é‰ΩøÁî®ExternalTrafficPolicy ClusterÈÖçÁΩÆÊÉÖÂÜµ‰∏ãÔºåÂú®ËäÇÁÇπ‰∏ãÁ∫øÊó∂Ê≤°Êúâ‰ºòÈõÖÂú∞Á≠âÂæÖÈïøËøûÊé•ÁªìÊùüÔºåÂØºËá¥Ê≠£Âú®‰ΩøÁî®ÈïøËøûÊé•ÁöÑËØ∑Ê±ÇÂá∫Áé∞520ÈîôËØØÁöÑÈóÆÈ¢ò„ÄÇËøôÂèçÊò†‰∫ÜÂú®ËäÇÁÇπÊ±†Áº©ÂáèÔºàdownscalingÔºâËøáÁ®ã‰∏≠Ôºåkube-proxyÊú™ËÉΩÂ¶•ÂñÑÂ§ÑÁêÜÁé∞ÊúâTCPËøûÊé•ÁöÑÈóÆÈ¢òÔºåÂ±û‰∫éÈõÜÁæ§ÂèØÁî®ÊÄßÂíåÊµÅÈáèË∞ÉÂ∫¶ÁöÑÂÆûÁé∞Áº∫Èô∑„ÄÇËÄå‰∏çÊòØÁõ¥Êé•Â≠òÂú®ÂèØË¢´ÊîªÂáªËÄÖÂà©Áî®ËøõË°åË∂äÊùÉ„ÄÅÂëΩ‰ª§ÊâßË°åÊàñÂÖ∂‰ªñÂÆâÂÖ®Á†¥ÂùèÁöÑÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# ËØ•ÈóÆÈ¢ò‰∏∫ÂäüËÉΩÊÄßÊïÖÈöúÔºåÊ∂âÂèäÊµÅÈáèË∞ÉÂ∫¶ÂèäËäÇÁÇπ‰∏ãÁ∫øÊó∂ÁöÑËøûÊé•ÁÆ°ÁêÜÔºå‰∏çÂ±û‰∫éÂÆâÂÖ®ÈóÆÈ¢òÔºå
# Âõ†Ê≠§Êó†ÂÆâÂÖ®È£éÈô©Â§çÁé∞ËÑöÊú¨Êèê‰æõ„ÄÇ
print("ËØ•ÈóÆÈ¢ò‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÊó†ÈúÄÂ§çÁé∞ËÑöÊú¨„ÄÇ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•Issue‰∏ªË¶ÅÂèçÊò†‰∫ÜÂú®ËäÇÁÇπ‰∏ãÁ∫øËøáÁ®ã‰∏≠ÔºåÁé∞ÊúâÁöÑÈïøËøûÊé•Ê≤°ÊúâË¢´Ê≠£Á°Æ‰øùÊåÅÔºåÂØºËá¥ËØ∑Ê±Ç‰∏≠Êñ≠Âπ∂ËøîÂõû520ÈîôËØØ„ÄÇËøôÁßçÊÉÖÂÜµËôΩÁÑ∂ÂèØËÉΩÂΩ±ÂìçÂ∫îÁî®ÁöÑÁ®≥ÂÆöÊÄßÂíåÁî®Êà∑‰ΩìÈ™åÔºå‰ΩÜÂπ∂‰∏çËÉΩÁõ¥Êé•Ë¢´ÊîªÂáªËÄÖÂà©Áî®Êù•ËøõË°åÊú™ÊéàÊùÉÊéßÂà∂„ÄÅÊèêÊùÉÊàñËøúÁ®ãÂëΩ‰ª§ÊâßË°åÁ≠âÂÆâÂÖ®ÊîªÂáª„ÄÇÊú¨ÈóÆÈ¢òÂ±û‰∫éÈõÜÁæ§Ë∞ÉÂ∫¶‰∏éËäÇÁÇπÁÆ°ÁêÜ‰∏≠ÁöÑÂÆûÁé∞Áº∫Èô∑ÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÂÖ∂È£éÈô©ËØÑÁ∫ßÂõ†Ê≠§Âà§Êñ≠‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇÂõ†Ê≠§Ôºå‰∏çÈúÄË¶ÅÊèê‰æõÁî®‰∫éÂÆâÂÖ®ÊµãËØïÁöÑÂ§çÁé∞POCËÑöÊú¨ÔºåÁõ∏ÂÖ≥Â§çÁé∞ËÑöÊú¨‰ªÖÈôê‰∫éËØ¥ÊòéËØ•ÈóÆÈ¢òÂú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞Ôºå‰∏çÈÄÇÂêà‰Ωú‰∏∫ÂÆâÂÖ®ÊîªÂáªÂ∑•ÂÖ∑‰ΩøÁî®„ÄÇ

---


## Issue #129600 Certificate info in expire logs

- Issue ÈìæÊé•Ôºö[#129600](https://github.com/kubernetes/kubernetes/issues/129600)

### Issue ÂÜÖÂÆπ

#### What happened?

When a certificate expires, the server logs it with something like:
`verifying certificate SN=xxxx, SKID=, AKID= failed: x509: certificate has expired or is not yet valid`

Its hard to track certs issued by k8s itself by SN. It would be extremely handy if it logged the CN too.

Also, it would be good if it would log it shortly before it expired.

#### What did you expect to happen?

K8s would tell you what CN the cert tried to use.

#### How can we reproduce it (as minimally and precisely as possible)?

Use an expired cert and try and auth.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.32.0
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú®ËØÅ‰π¶ËøáÊúüÊó•Âøó‰∏≠‰ªÖËÆ∞ÂΩï‰∫ÜËØÅ‰π¶ÁöÑ SN ‰ø°ÊÅØÔºåËÄåÁº∫Â∞ë CN ‰ø°ÊÅØÁöÑËÆ∞ÂΩïÔºåÂπ∂Âª∫ËÆÆÂú®ËØÅ‰π¶Âø´ËøáÊúüÂâç‰πüËøõË°åËÆ∞ÂΩï„ÄÇÊï¥‰ΩìÊù•ÁúãÔºåËøôÂ±û‰∫éÊó•ÂøóËÆ∞ÂΩïÂÜÖÂÆπÁöÑÊîπËøõÂª∫ËÆÆÔºåÂπ∂Ê≤°ÊúâÊ∂âÂèäÂà∞ÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤Á≠âÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÂõ†Ê≠§ÔºåËØ• Issue Âπ∂ÈùûÂÆâÂÖ®ÈóÆÈ¢òÔºåÂè™ÊòØÂäüËÉΩÂ¢ûÂº∫ÊàñÊó•ÂøóËÆ∞ÂΩï‰∏çÂÖ®ÁöÑÈóÆÈ¢òÔºå‰∏ç‰ºöÂØπÁ≥ªÁªüÊûÑÊàêÂÆâÂÖ®Â®ÅËÉÅ„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Ê≠§ÈóÆÈ¢òÊèèËø∞ÁöÑÊòØÊó•ÂøóËÆ∞ÂΩïÊîπËøõÂª∫ËÆÆÔºåÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞„ÄÇ
# Âõ†Ê≠§ÔºåÊ≠§Â§ÑÊèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑËÑöÊú¨ÔºåÁî®‰∫éÊ®°ÊãüÊó•ÂøóËÆ∞ÂΩïÂú∫ÊôØÔºå
# Âπ∂ÊâìÂç∞Âá∫ÂåÖÂê´ËØÅ‰π¶ SN ‰∏é CN ‰ø°ÊÅØÁöÑÊó•ÂøóÔºå‰ªÖÁî®‰∫éÊºîÁ§∫ËÄåÈùûÂ§çÁé∞ÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

import logging
import datetime

# ËÆæÁΩÆÊó•ÂøóËÆ∞ÂΩïÂô®
logger = logging.getLogger("cert_expire_logger")
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def simulate_cert_expiry_logging(serial_number, common_name):
    # Ê®°ÊãüËØÅ‰π¶ËøáÊúüÔºå‰ΩÜÊó•Âøó‰∏≠Áº∫Â∞ëCN‰ø°ÊÅØÁöÑÊÉÖÂÜµ
    logger.error(f"verifying certificate SN={serial_number} failed: x509: certificate has expired or is not yet valid")

    # ÊîπËøõÂª∫ËÆÆÔºöÂú®Êó•Âøó‰∏≠ÂêåÊó∂ËÆ∞ÂΩïCN‰ø°ÊÅØÂèäÊèêÂâçÂëäË≠¶
    # ‰æãÂ¶ÇÔºåÊèêÂâç 1 Â§©ÈÄöÁü•ËØÅ‰π¶Âç≥Â∞ÜËøáÊúü
    alert_time = datetime.datetime.now() + datetime.timedelta(days=1)
    logger.info(f"Alert: Certificate with SN={serial_number} and CN={common_name} will expire on {alert_time.strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    # Ê®°Êãü‰ΩøÁî®‰∏Ä‰∏™ËøáÊúüËØÅ‰π¶ÁöÑÊÉÖÂΩ¢ÔºåÂ°´ÂÖ•Á§∫‰æãÁöÑSN‰∏éCN
    simulate_cert_expiry_logging("xxxx", "example.com")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Ëøô‰∏™ Python ËÑöÊú¨Ê®°Êãü‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊó•ÂøóËÆ∞ÂΩïÂú∫ÊôØÔºåÂ±ïÁ§∫‰∫ÜÂΩì‰∏Ä‰∏™ËØÅ‰π¶ËøáÊúüÊó∂ÔºåÂ¶Ç‰ΩïÂú®Êó•Âøó‰∏≠ËÆ∞ÂΩïËØÅ‰π¶ÁöÑ SN ‰ø°ÊÅØ‰ª•ÂèäÔºàÊ†πÊçÆÂª∫ËÆÆÔºâËÆ∞ÂΩïÂØπÂ∫îÁöÑ CN ‰ø°ÊÅØÂíåÊèêÂâçÂëäË≠¶„ÄÇËÑöÊú¨‰∏≠Ôºö
1. ËÆæÁΩÆ‰∫Ü‰∏Ä‰∏™Êó•ÂøóËÆ∞ÂΩïÂô®ÔºåÁî®‰∫éËæìÂá∫Êó•Âøó‰ø°ÊÅØ„ÄÇ
2. ÂÆö‰πâ‰∫ÜÂáΩÊï∞ simulate_cert_expiry_loggingÔºåÈ¶ñÂÖàËÆ∞ÂΩïËØÅ‰π¶ËøáÊúüÁöÑÈîôËØØÊó•ÂøóÔºà‰ªÖÂê´ SN ‰ø°ÊÅØÔºâÔºåÁÑ∂ÂêéÊåâÁÖßÂª∫ËÆÆËÆ∞ÂΩïÂåÖÂê´ CN ‰ø°ÊÅØÁöÑÂëäË≠¶Êó•ÂøóÔºåÊ®°ÊãüÊèêÂâç‰∏ÄÂ§©ÁöÑËøáÊúüÈÄöÁü•„ÄÇ
3. ËÑöÊú¨ÁöÑ‰∏ªÂÖ•Âè£Ë∞ÉÁî®ËØ•ÂáΩÊï∞Ôºå‰ΩøÁî®Á§∫‰æãÁöÑËØÅ‰π¶ SN Âíå CN ËøõË°åÊºîÁ§∫„ÄÇ

ÈúÄË¶ÅËØ¥ÊòéÁöÑÊòØÔºåÊ≠§ Issue Â±û‰∫éÊó•ÂøóÂ¢ûÂº∫ÁöÑÊîπËøõÈúÄÊ±ÇÔºåÂπ∂‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Ê≤°ÊúâÊèê‰æõÈíàÂØπÂÆâÂÖ®È£éÈô©ÁöÑÂ§çÁé∞ËÑöÊú¨„ÄÇÂ§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫Â¶Ç‰ΩïÊîπËøõÊó•ÂøóËÆ∞ÂΩïÈÄªËæë„ÄÇ

---


## Issue #129580 Cannot connect Backend Service from FrontEnd Service

- Issue ÈìæÊé•Ôºö[#129580](https://github.com/kubernetes/kubernetes/issues/129580)

### Issue ÂÜÖÂÆπ

#### What happened?

I am developing a simple project called "TodoApp," which consists of a TodoApi project built with .NET and a WebApp project using ReactJS. However, I am unable to call the API from the WebApp in a browser. Interestingly, it works when I connect to the FrontEnd pod and use the `curl http://todoapp-api-service` command, which responds with 'hello world' from the server.


#### What did you expect to happen?

I want  to call the API from the WebApp in a browser.

#### How can we reproduce it (as minimally and precisely as possible)?

I setup my project:

1. Dockerfile
TodoApi Dockerfile:
```
# Use the official .NET SDK image to build the app
FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build

# Set the working directory inside the container
WORKDIR /app

# Copy the csproj file and restore the dependencies
COPY ToDoApi.csproj ./
RUN dotnet restore

# Copy the rest of the application files
COPY . ./

# Build the application
RUN dotnet publish -c Release -o out

# Use the official .NET runtime image for the runtime environment
FROM mcr.microsoft.com/dotnet/aspnet:9.0 AS base

# Set the working directory for the runtime container
WORKDIR /app

# Expose port 80 for the app to listen to
EXPOSE 80

# Copy the published files from the build image
COPY --from=build /app/out .

# Define the entry point to run the app
ENTRYPOINT ["dotnet", "ToDoApi.dll"]
```

WebApi Dockerfile:
```
# Step 1: Use an official Node.js image as a base image
FROM node:16 AS build

# Step 2: Set the working directory inside the container
WORKDIR /app

# Step 3: Copy the package.json and package-lock.json (or yarn.lock)
COPY package*.json ./

# Step 4: Install the dependencies
RUN npm install

# Step 5: Copy the rest of the app's source code into the container
COPY . .

# Step 6: Build the React app
RUN npm run build

# Step 7: Use an official Nginx image to serve the build files
FROM nginx:alpine

# Step 8: Copy the build output from the first stage into the Nginx container
COPY --from=build /app/build /usr/share/nginx/html

# Step 9: Expose the port that Nginx will run on
EXPOSE 80

# Step 10: Start Nginx server
CMD ["nginx", "-g", "daemon off;"]
```

2. Deployment file yaml
```

apiVersion: apps/v1
kind: Deployment
metadata:
  name: todoapp-api-deployment
  labels:
    app: todoapp-api-label
spec:
  replicas: 1
  selector: 
    matchLabels:
      app: todoapp-api-label
  template:
    metadata:
      labels:
        app: todoapp-api-label
    spec:
      containers:
      - name: todoapp-api-container
        image: mrstevebang/todoapp-api:1.0.8
        ports:
        - containerPort: 80
        env:
        - name: ASPNETCORE_ENVIRONMENT
          value: "Development"  # Set the environment to Development

---
apiVersion: v1
kind: Service
metadata:
  name: todoapp-api-service
spec:
  selector:
    app: todoapp-api-label
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: todoapp-webapp-deployment
  labels:
    app: todoapp-webapp-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: todoapp-webapp-label
  template:
    metadata:
      labels:
        app: todoapp-webapp-label
    spec:
      containers:
      - name: todoapp-webapp-container
        image: mrstevebang/todoapp-webapp:1.0.13
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: todoapp-webapp-service
spec:
  selector:
    app: todoapp-webapp-label
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30090

```

3. My code for connect api
```
import axios from 'axios';

const API_BASE_URL ='http://todoapp-api-service';

export const getTodos = () => axios.get(`${API_BASE_URL}/api/todos`);
export const createTodo = (todo) => axios.post(`${API_BASE_URL}/api/todos`, todo);
export const markCompleted = (id) => axios.post(`${API_BASE_URL}/api/todos/${id}/completed`);
export const deleteTodo = (id) => axios.delete(`${API_BASE_URL}/api/todos/${id}`);
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.30.6
```

</details>


#### Cloud provider

<details>
Minikube
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ÁªèËøáÂàÜÊûêÔºåËØ• issue ÂèçÊò†ÁöÑÊòØÂâçÁ´ØÊó†Ê≥ï‰ªéÊµèËßàÂô®Ë∞ÉÁî®ÂêéÁ´Ø API ÁöÑÈóÆÈ¢òÔºåËÄåÂú®ÂâçÁ´ØÂÆπÂô®ÂÜÖÈÄöËøá curl ÂëΩ‰ª§ÂèØ‰ª•Ê≠£Á°ÆËÆøÈóÆÂêéÁ´ØÊúçÂä°„ÄÇÊï¥‰ΩìÊèèËø∞‰∏äÁúãÔºåËøôÂ±û‰∫éÊúçÂä°Èó¥ËÆøÈóÆÊàñÁΩëÁªúËøûÈÄöÊÄßÈÖçÁΩÆÁöÑÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇÈ°πÁõÆÈÖçÁΩÆ„ÄÅÁΩëÁªúÂüüÂêçËß£Êûê„ÄÅÂÆπÂô®ÂÜÖÂ§ñËÆøÈóÆÁ≠âÈóÆÈ¢òÂèØËÉΩ‰ºöÂØºËá¥ËØ•Áé∞Ë±°Ôºå‰ΩÜ‰ªéÊèèËø∞‰∏≠Âπ∂Ê≤°ÊúâÂèëÁé∞ÂëΩ‰ª§ÊâßË°å„ÄÅÂá≠ÊçÆÊ≥ÑÈú≤„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÆâÂÖ®È£éÈô©Âõ†Á¥†„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÂ±ïÁ§∫Ê®°ÊùøË¶ÅÊ±ÇÔºåÁî±‰∫éËØ• issue ‰∏çÂ±û‰∫éÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§‰∏çÂ≠òÂú®Áõ∏Â∫îÁöÑÂÆâÂÖ®ÊºèÊ¥ûÂ§çÁé∞‰ª£Á†Å„ÄÇ
ËØ•ËÑöÊú¨‰ªÖËæìÂá∫ÊèêÁ§∫‰ø°ÊÅØÔºåË°®ÊòéÈóÆÈ¢ò‰∏∫ÊúçÂä°ËøûÊé•ÈóÆÈ¢òËÄåÈùûÂÆâÂÖ®È£éÈô©„ÄÇ
"""

def main():
    print("Ê≠§ÈóÆÈ¢ò‰∏∫ÊúçÂä°ËøûÈÄöÊÄßÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞„ÄÇ")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Êú¨Â§çÁé∞ËÑöÊú¨‰ªÖ‰∏∫Êª°Ë∂≥Ê®°ÊùøË¶ÅÊ±ÇÁöÑÁ§∫‰æã‰ª£Á†Å„ÄÇÁî±‰∫éÊú¨ issue ÊèèËø∞ÁöÑÈóÆÈ¢òÊòØÂâçÂêéÁ´ØÊúçÂä°ÁöÑËÆøÈóÆËøûÈÄöÊÄßÈóÆÈ¢òÔºåÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Êó†ÈúÄÊûÑÈÄ†ÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ PoC„ÄÇËÑöÊú¨ÈÄöËøáÁÆÄÂçïÁöÑËæìÂá∫ÊèêÁ§∫‰ø°ÊÅØÔºåÂëäÁü•Áî®Êà∑ËØ•ÈóÆÈ¢ò‰∏çÂÖ∑Â§áÂÆâÂÖ®È£éÈô©„ÄÇÂÆûÈôÖÈóÆÈ¢òÁöÑ‰øÆÂ§çÂ∫î‰æßÈáç‰∫éÁΩëÁªúÈÖçÁΩÆ„ÄÅÊúçÂä°ÂüüÂêçËß£Êûê„ÄÅË∑®ÂÆπÂô®ËÆøÈóÆÊàñËÄÖÊµèËßàÂô®Ë∑®ÂüüÁ≠âÈóÆÈ¢òÁöÑÊéíÊü•ÂíåË∞ÉËØï„ÄÇ

---


## Issue #129579 downloading container image from registry.k8s.io through containerd runtime doesn't work

- Issue ÈìæÊé•Ôºö[#129579](https://github.com/kubernetes/kubernetes/issues/129579)

### Issue ÂÜÖÂÆπ

#### What happened?

I'm using VMs on Proxmox which is one of the hypervisors and tried to deploy k8s (v1.31.4) using kubespray (release-2.27) based on containerd runtime (v1.7.24).

Problem happened when containerd tried to download coredns image (precisely "registry.k8s.io/coredns/coredns:v1.11.3").

![image](https://github.com/user-attachments/assets/db41cd11-2ae8-460c-a913-d11ea120811b)

```
TASK [download : Download_container | Download image if required] ***********************
fatal: [node1]: FAILED! => {"attempts": 4, "changed": true, "cmd": ["/usr/local/bin/ctr", "-n", "k8s.io", "images", "pull", "--hosts-dir", "/etc/containerd/certs.d", "registry.k8s.io/pause:3.10"], "delta": "0:00:00.114788", "end": "2025-01-13 01:32:37.382646", "msg": "non-zero return code", "rc": 1, "start": "2025-01-13 01:32:37.267858", "stderr": "ctr: failed to resolve reference \"registry.k8s.io/pause:3.10\": unexpected status from HEAD request to https://registry.k8s.io/v2/pause/manifests/3.10: 503 Service Unavailable", "stderr_lines": ["ctr: failed to resolve reference \"registry.k8s.io/pause:3.10\": unexpected status from HEAD request to https://registry.k8s.io/v2/pause/manifests/3.10: 503 Service Unavailable"], "stdout": "registry.k8s.io/pause:3.10: resolving      |\u001b[32m\u001b[0m--------------------------------------| \nelapsed: 0.1 s              total:   0.0 B (0.0 B/s)                                         ", "stdout_lines": ["registry.k8s.io/pause:3.10: resolving      |\u001b[32m\u001b[0m--------------------------------------| ", "elapsed: 0.1 s              total:   0.0 B (0.0 B/s)                                         "]}
```

So I installed just only containerd on my VM then tried to download "registry.k8s.io/coredns/coredns:v1.11.3" image but it didn't work same.

![image](https://github.com/user-attachments/assets/75593414-61a4-4d87-844e-ea1c4ffd52c5)

`FATA[0003] failed to copy: httpReadSeeker: failed open: unexpected status code https://registry.k8s.io/v2/coredns/coredns/blobs/sha256:d858cbc252ade14879807ff8dbc3043a26bbdb92087da98cda831ee040b172b3: 503 Service Unavailable`

Containerd brings images from dockerhub very well but couldn't download image from registry.k8s.io and k8s.gcr.io.

Not just "registry.k8s.io/coredns/coredns:v1.11.3" but also "registry.k8s.io/kube-apiserver:v1.31.4" & "registry.k8s.io/kube-controller-manager:v1.31.4" neither.

But the weired thing is that these images can be downloaded using docker.
docker (v26.1.2) downloads these images well.
Also docker (v27.4.1) works fine but not smoothly. I have to enter same command multiple times forcefully, then it works finally.

![image](https://github.com/user-attachments/assets/dcc89c92-7ec9-458d-8172-fb8b44f3a396)

I'm still confused this is a matter of containerd itself or k8s registry. So if you know anything about this, please help.

#### What did you expect to happen?

I expected that containerd would get the k8s related images.

#### How can we reproduce it (as minimally and precisely as possible)?

1. prepare an Ubuntu 24.04 server
2. enter commands below to install containerd and nerdctl
`sudo apt update`
`sudo apt install -y ca-certificates curl`
`sudo install -m 0755 -d /etc/apt/keyrings`
`sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc`
`sudo chmod a+r /etc/apt/keyrings/docker.asc`
`echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
sudo tee /etc/apt/sources.list.d/docker.list > /dev/null`
`sudo apt update`
`sudo apt install -y containerd.io`
`wget https://github.com/containerd/nerdctl/releases/download/v2.0.2/nerdctl-2.0.2-linux-amd64.tar.gz`
`sudo tar Cxzvvf /usr/local/bin nerdctl-2.0.2-linux-amd64.tar.gz`
`sudo nerdctl image pull registry.k8s.io/coredns/coredns:v1.11.3`

#### Anything else we need to know?

You can reproduce this without kubespray. Just install containerd or docker on Ubuntu 24.04 then download any images from "registry.k8s.io".

#### Kubernetes version

<details>
this happens during k8s installation process
but k8s version is "1.31.4"
</details>


#### Cloud provider

<details>
none. it's on-premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux node1 6.8.0-35-generic #35-Ubuntu SMP PREEMPT_DYNAMIC Mon May 20 15:51:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubespray release-2.27 branch
ansible core 2.16.14
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.24 with nerdctl v2.0.2
or
docker v26.1.2 or latest
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI=calico v3.29.1
</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ‰∏ªË¶ÅÊèèËø∞ÁöÑÊòØÂú®‰ΩøÁî® containerd ‰Ωú‰∏∫ÂÆπÂô®ËøêË°åÊó∂‰ªé registry.k8s.io ‰∏ãËΩΩ Kubernetes Áõ∏ÂÖ≥ÈïúÂÉèÊó∂Âá∫Áé∞‰∏ãËΩΩÂ§±Ë¥•Ôºà503 Service UnavailableÔºâÁöÑÊÉÖÂÜµ„ÄÇÈóÆÈ¢òÂéüÂõ†ÂèØËÉΩ‰∏é registry.k8s.io ÊúçÂä°ÁöÑË¥üËΩΩ„ÄÅ‰ª£ÁêÜ„ÄÅÈò≤ÁÅ´Â¢ô„ÄÅÁΩëÁªúËøûÈÄöÊÄßÊàñËÄÖ‰∏é containerd ÈÄÇÈÖçÊÄßÊúâÂÖ≥Ôºå‰ªéÊó•Âøó‰ø°ÊÅØÊù•ÁúãÔºåÂπ∂Ê≤°ÊúâÊ∂âÂèäÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤Á≠âÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇÂè¶Â§ñÔºåissue‰∏≠‰πüÊú™ÊòæÁ§∫ÊúâÊïèÊÑüÂá≠ËØÅÊö¥Èú≤Êàñ‰∏çÂΩìÈÖçÁΩÆÁöÑÊÉÖÂÜµÔºåÊï¥‰ΩìÈóÆÈ¢òÊõ¥ÂÉèÊòØ‰∏ÄÁßçÊúçÂä°Ëé∑ÂèñÊïÖÈöúÊàñÂÖºÂÆπÊÄßÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°Êãü‰ªé registry.k8s.io ‰∏ãËΩΩÈïúÂÉèÁöÑËøáÁ®ãÔºå
Áî®‰∫éÁ†îÁ©∂ÂÆπÂô®ÈïúÂÉè‰∏ãËΩΩÊó∂ÂèØËÉΩÈÅáÂà∞ÁöÑÁä∂ÊÄÅÁ†Å503ÈóÆÈ¢òÔºåÈùûÂÆâÂÖ®ÊîªÂáªPOC„ÄÇ
Áî±‰∫éËØ•IssueÊèèËø∞ÁöÑÂú∫ÊôØ‰∏çÊòØÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§Ê≠§ËÑöÊú¨Âπ∂‰∏çÁî®‰∫éÊîªÂáªÔºå
ËÄå‰ªÖÁî®‰∫éÊ®°ÊãüÂÆπÂô®ÈïúÂÉè‰∏ãËΩΩËøáÁ®ã‰∏≠ÁöÑÁΩëÁªúËØ∑Ê±ÇÔºå‰æõÊú¨Âú∞ÊµãËØï‰ΩøÁî®„ÄÇ
"""

import http.server
import socketserver
import threading
import requests

# ËÆæÁΩÆ‰∏Ä‰∏™ÁÆÄÂçïÁöÑHTTPÊúçÂä°Âô®ÔºåÁî®‰∫éÊ®°Êãü‰∏Ä‰∏™ËøîÂõû503ÈîôËØØÁöÑÈïúÂÉè‰ªìÂ∫ì
class SimpleHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_HEAD(self):
        self.send_response(503)
        self.end_headers()

    def do_GET(self):
        self.send_response(503)
        self.end_headers()
        self.wfile.write(b"Service Unavailable")

def start_mock_server(port=8080):
    handler = SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", port), handler)
    print(f"Mock server ÂêØÂä®ÔºåÁõëÂê¨Á´ØÂè£ {port}ÔºåÂßãÁªàËøîÂõû 503 Áä∂ÊÄÅÁ†Å")
    httpd.serve_forever()

def simulate_image_download(url):
    try:
        print(f"Â∞ùËØïËØ∑Ê±Ç URL: {url}")
        response = requests.head(url, timeout=5)
        print(f"ÂìçÂ∫îÁä∂ÊÄÅÁ†Å: {response.status_code}")
        if response.status_code == 503:
            print("Êî∂Âà∞503 Service UnavailableÂìçÂ∫îÔºåÊ®°Êãü‰∏ãËΩΩÂ§±Ë¥•„ÄÇ")
        else:
            print("ÈïúÂÉè‰ªìÂ∫ìÂìçÂ∫îÊ≠£Â∏∏„ÄÇ")
    except Exception as e:
        print(f"ËØ∑Ê±ÇÂèëÁîüÂºÇÂ∏∏: {e}")

if __name__ == "__main__":
    # ÂêØÂä®Ê®°ÊãüÁöÑHTTPÊúçÂä°Âô®Á∫øÁ®ã
    server_thread = threading.Thread(target=start_mock_server, daemon=True)
    server_thread.start()

    # Ê®°ÊãüÁöÑÈïúÂÉèËØ∑Ê±ÇURLÔºåÊåáÂêëÊú¨Âú∞ÁöÑmock server
    test_url = "http://localhost:8080/v2/someimage/manifests/latest"
    simulate_image_download(test_url)
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨È¶ñÂÖàÊê≠Âª∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑHTTPÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®Êú¨Âú∞8080Á´ØÂè£ÔºåÂÖ∂ÊâÄÊúâËØ∑Ê±ÇÈÉΩËøîÂõû 503 Áä∂ÊÄÅÁ†ÅÔºåÁî®Êù•Ê®°Êãü registry.k8s.io Âú®‰∏ãËΩΩÈïúÂÉèÊó∂Âá∫Áé∞ÁöÑ‚ÄúService Unavailable‚ÄùÊÉÖÂÜµ„ÄÇ
2. ÂáΩÊï∞ simulate_image_download() ÈááÁî® requests Â∫ìÂèëÈÄÅ HEAD ËØ∑Ê±ÇÊù•Ê®°Êãü containerd ËØ∑Ê±ÇÈïúÂÉè‰ªìÂ∫ìÁöÑËøáÁ®ãÔºåÂπ∂ËæìÂá∫ÂìçÂ∫îÁä∂ÊÄÅÁ†ÅÂèäÁõ∏Â∫îÊèêÁ§∫„ÄÇ
3. Áî±‰∫éËØ•ÈóÆÈ¢òÂπ∂ÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§ËØ•ËÑöÊú¨‰ªÖÁî®‰ΩúÊ®°ÊãüÂíåÁ†îÁ©∂ÈïúÂÉè‰∏ãËΩΩËøáÁ®ã‰∏≠ÈÅáÂà∞ÁöÑÁä∂ÊÄÅÁ†ÅÈóÆÈ¢òÔºåËÄå‰∏çÊ∂âÂèä‰ªª‰ΩïÊîªÂáªË°å‰∏∫„ÄÇËÑöÊú¨‰∏≠ÁöÑÈÄªËæë‰πüÂèØÁî®‰∫éÊú¨Âú∞ÊµãËØïÁΩëÁªúËØ∑Ê±ÇÁöÑÂºÇÂ∏∏ÂìçÂ∫îÊÉÖÂÜµ„ÄÇ
4. ‰ΩøÁî®Êó∂Áõ¥Êé•ËøêË°åËÑöÊú¨Âç≥ÂèØÂú®ÊéßÂà∂Âè∞ÁúãÂà∞ÂêØÂä®‰ø°ÊÅØÂíåËØ∑Ê±ÇËøáÁ®ãÊ®°Êãü‰ø°ÊÅØ„ÄÇ

---


## Issue #129575 OpenAPI handler fails on duplicated path

- Issue ÈìæÊé•Ôºö[#129575](https://github.com/kubernetes/kubernetes/issues/129575)

### Issue ÂÜÖÂÆπ

#### What happened?

What happened?
When installing the metrics-server to implement the metrics.k8s.io API and the prometheus-adapter to implement the custom.metrics.k8s.io API, it is possible to make the OpenAPI handler fail to initialize with this error log in apiserver: https://github.com/kubernetes/kube-openapi/blob/master/pkg/aggregator/aggregator.go#L343

handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/namespaces/{namespace}/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes/{name}
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
This will result in kubectl discovery calls to /openapi/v2 to fail with a 503:

{
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "863d36c0-3797-4f19-bc74-df571a671a00",
  "stage": "ResponseComplete",
  "requestURI": "/openapi/v2?timeout=31s",
  "verb": "get",
...
  "userAgent": "kubectl/v1.28.2 (linux/amd64) kubernetes/cd9e211",
  "responseStatus": {
    "metadata": {},
    "code": 503
  },
  "requestReceivedTimestamp": "2023-12-19T02:50:12.882602Z",
  "stageTimestamp": "2023-12-19T02:50:12.913981Z",
  "annotations": {
    "authorization.k8s.io/decision": "allow",
    "authorization.k8s.io/reason": ""
  }
}

#### What did you expect to happen?

The OpenAPI handler should not fail to initialize if both metrics.k8s.io and custom.metrics.k8s.io are serving the same path for different API groups/versions

#### How can we reproduce it (as minimally and precisely as possible)?

I can provide exact installation steps if needed, however, a high-level repro is:

install metrics-server to serve v1beta1.metrics.k8s.io
install prometheus-adapter to serve v1beta1.custom.metrics.k8s.io
ensure resourceRules is populated in the prometheus-adapter config so that resource metrics are also served by v1beta1.custom.metrics.k8s.io: https://github.com/kubernetes-sigs/prometheus-adapter/blob/7a421bb91e9ea52be4e26db22392c36c8af38bc8/deploy/manifests/custom-metrics-config-map.yaml#L73-L98
restarting apiserver will result in the OpenAPI failure and 503 responses to /openapi/v2 path

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØÂú®ÂÆâË£Ö metrics-server Âíå prometheus-adapter Êó∂ÔºåÁî±‰∫é‰∏§‰∏™ÁªÑ‰ª∂ÈÉΩÂú®Êèê‰æõÈÉ®ÂàÜÁõ∏ÂêåË∑ØÂæÑÁöÑ APIÔºå‰ªéËÄåÂØºËá¥ OpenAPI handler Âú®ÂêàÂπ∂ API ËßÑÊ†ºÊó∂Âá∫Áé∞ÈáçÂ§çË∑ØÂæÑÁöÑÈóÆÈ¢òÔºåÊúÄÁªàÂØºËá¥ apiserver ÁöÑ OpenAPI ÂàùÂßãÂåñÂ§±Ë¥•Âíå /openapi/v2 ËØ∑Ê±ÇËøîÂõû 503 ÈîôËØØ„ÄÇËØ•ÈóÆÈ¢ò‰∏ªË¶ÅÂ±û‰∫éÁªÑ‰ª∂Èó¥ÂÜ≤Á™ÅÊàñÈÖçÁΩÆÂÜ≤Á™ÅÂºïËµ∑ÁöÑÂàùÂßãÂåñÂ§±Ë¥•ÈóÆÈ¢òÔºåÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥Êú™ÊéàÊùÉËÆøÈóÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®ÈöêÊÇ£„ÄÇ

Ê†πÊçÆÊèèËø∞ÔºåËôΩÁÑ∂Âõ† OpenAPI handler Âá∫ÈîôÂØºËá¥ API ÂèëÁé∞Êé•Âè£Êó†Ê≥ïÊ≠£Â∏∏Â∑•‰ΩúÔºå‰ΩÜËØ•ÈóÆÈ¢òÂπ∂ÈùûÁî±‰∫éËÆæËÆ°‰∏äÁöÑÂÆâÂÖ®ÊºèÊ¥ûÊàñË¢´ÊÅ∂ÊÑèÂà©Áî®ÁöÑÊºèÊ¥ûÂºïÂèëÔºåÂõ†Ê≠§‰∏çÂ∫îË¢´ÂΩíÁ±ª‰∏∫ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Ëß£ÈáäËØ¥ÊòéÔºö**

Êú¨ Issue ‰∏≠ÁöÑÈóÆÈ¢òÊòØÁî±‰∫éÂÆâË£Ö‰∫Ü‰∏§‰∏™Êèê‰æõÈÉ®ÂàÜÈáçÂè† API Êé•Âè£ÁöÑÁªÑ‰ª∂ÔºåÂØºËá¥ OpenAPI Â§ÑÁêÜÂô®Âú®ÂêàÂπ∂ OpenAPI ËßÑÊ†ºÊó∂Âá∫Áé∞‰∫ÜÈáçÂ§çË∑ØÂæÑÈîôËØØ„ÄÇËôΩÁÑ∂Ëøô‰ºöÂØºËá¥ÈõÜÁæ§‰∏≠ÈÉ®ÂàÜ API ÂèëÁé∞Êé•Âè£Ôºà‰æãÂ¶Ç /openapi/v2ÔºâËøîÂõû 503 ÈîôËØØÔºå‰ΩÜËØ•ÈóÆÈ¢òÂ±û‰∫éÁªÑ‰ª∂Èó¥ÈÖçÁΩÆÂíåÂÆûÁé∞ÂÜ≤Á™ÅÔºåÂπ∂ÈùûÂèØË¢´ÊîªÂáªËÄÖÂà©Áî®ÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂõ†ËÄåÊåâÁÖßÈ£éÈô©Âà§Êñ≠Ê†áÂáÜÔºåËØ• Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå‰πüÊó†ÈúÄÊûÑÈÄ†ÊÅ∂ÊÑèÂ§çÁé∞ËÑöÊú¨„ÄÇ

---


## Issue #129572 VolumeAttachment is not deleted when the CSI plugin change from requiring attach to not requiring attach

- Issue ÈìæÊé•Ôºö[#129572](https://github.com/kubernetes/kubernetes/issues/129572)

### Issue ÂÜÖÂÆπ

#### What happened?

After [volumeAttacher.Attach](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L271) executes successfully, the volumeattachment will be created. At this point, if CSI's ATTACHREQUIRED changes from true to false, it will cause [MarkVolumeAsAttached](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L301) to fail.
 And the volume will be removed from the [dsw](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/controller/volume/attachdetach/populator/desired_state_of_world_populator.go#L166), leaving the VolumeAttachment stranded without being properly cleaned up.

#### What did you expect to happen?

volumeattachment can be cleaned up.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Install xsky-nfs-csi (other CSI drivers should be able to reproduce this) and set attachRequired to true
2. Create PVC and pod using the CSI
3. Keep querying volumeattachment using 'kubectl get volumeattachment'. As soon as the volumeattachment appears, immediately uninstall the CSI driver, then reinstall it with attachRequired set to false
4. After waiting for some time, query volumeattachment again and find that the volumeattachment still hasn't been deleted

#### Anything else we need to know?

[operation_generator.go](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L301-L307)
I think we should add error handling here to mark the volume as "Uncertain", so the volume can be added to asw, allowing the next retry to successfully remove the VolumeAttachment.
I have tested that this can solve the problem. If confirmed this is an issue, I will submit a PR

#### Kubernetes version

<details>

```console
v1.31.2
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ËÆ®ËÆ∫ÁöÑÊòØÂΩì CSI Êèí‰ª∂ÈÖçÁΩÆ‰ªéÈúÄË¶Å attachÔºàattachRequired=trueÔºâÂèòÊõ¥‰∏∫‰∏çÈúÄË¶Å attachÔºàattachRequired=falseÔºâÊó∂ÔºåÂ∑≤ÂàõÂª∫ÁöÑ VolumeAttachment Êó†Ê≥ïË¢´Ê≠£Á°ÆÊ∏ÖÁêÜÁöÑÈóÆÈ¢ò„ÄÇÈóÆÈ¢òË°®Áé∞‰∏∫Âú®ÊàêÂäüË∞ÉÁî® volumeAttacher.Attach ÂêéÔºåVolumeAttachment Ë¢´ÂàõÂª∫Ôºõ‰∏ÄÊó¶ÈÖçÁΩÆÂèòÊõ¥Ôºå‰ºöÂØºËá¥ MarkVolumeAsAttached Ë∞ÉÁî®Â§±Ë¥•Ôºå‰ªéËÄå‰ΩøÂæó VolumeAttachment ‚ÄúÊªûÁïô‚Äù„ÄÇËøôÂ±û‰∫éËµÑÊ∫êÁä∂ÊÄÅÁÆ°ÁêÜÂíåÈÄªËæëÈîôËØØÁöÑÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂ§ñÈÉ®ÊîªÂáªËÄÖÂà©Áî®ÊºèÊ¥ûËé∑ÂæóÊÅ∂ÊÑèË°å‰∏∫ÔºåÂ¶ÇÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉÁ≠â„ÄÇÂõ†Ê≠§Ôºå‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËØ•ÈóÆÈ¢òÂπ∂ÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåËÄåÊòØÁ≥ªÁªüËÆæËÆ°ÊàñÈÄªËæëÁº∫Èô∑„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÈóÆÈ¢òÂ§çÁé∞ËøáÁ®ã‰∏≠ÁöÑÈÉ®ÂàÜÈÄªËæëÊµÅÁ®ãÔºå‰ª•Â∏ÆÂä©ÁêÜËß£ CSI Êèí‰ª∂ÈÖçÁΩÆÂèòÊõ¥
Êó∂ VolumeAttachment Êú™Ë¢´Ê∏ÖÁêÜÁöÑÊÉÖÂÜµ„ÄÇËØ•ËÑöÊú¨Ê®°Êãü‰∫ÜÂàõÂª∫ VolumeAttachment„ÄÅÈÖçÁΩÆÂèòÊõ¥
‰ª•ÂèäÂ∞ùËØïÊ∏ÖÁêÜÁöÑÈÄªËæëÔºå‰ΩÜÂπ∂‰∏çÁõ¥Êé•ÂØπ Kubernetes ÁéØÂ¢ÉÈÄ†ÊàêÂΩ±ÂìçÔºå‰ªÖÁî®‰∫éÊú¨Âú∞Ë∞ÉËØïÂíåÁ†îÁ©∂„ÄÇ
"""

import time
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer

# Ê®°ÊãüÁöÑ VolumeAttachment Á±ª
class VolumeAttachment:
    def __init__(self, vol_id):
        self.vol_id = vol_id
        self.attached = True  # ÂàùÂßãËÆæÂÆö‰∏∫Â∑≤ÈôÑÁùÄ

    def mark_as_attached(self):
        # Ê®°ÊãüÊúâ attachRequired ‰∏∫ True Êó∂ÁöÑÈÄªËæë
        if csi_config['attachRequired']:
            self.attached = True
            print(f"Volume {self.vol_id} marked as attached.")
            return True
        else:
            # ÂΩì attachRequired ‰∏∫ False Êó∂ÔºåÊ®°ÊãüË∞ÉÁî®Â§±Ë¥•Ë°å‰∏∫
            print(f"Failed to mark volume {self.vol_id} as attached due to configuration change.")
            return False

    def cleanup(self):
        # ÂΩìÁä∂ÊÄÅ‰∏∫ False Êàñ‰∏çÁ°ÆÂÆöÊó∂ÊâçËÉΩÊ∏ÖÁêÜ
        if not self.attached:
            print(f"Cleaning up volume attachment for volume {self.vol_id}.")
            return True
        else:
            print(f"Cannot cleanup volume {self.vol_id} because it is still marked as attached.")
            return False

# ÂÖ®Â±ÄÊ®°Êãü CSI ÈÖçÁΩÆ
csi_config = {'attachRequired': True}
volume_attachments = {}

def simulate_volume_attachment_process(vol_id):
    # Ê®°Êãü attach Êìç‰ΩúÊàêÂäüÂêéÂàõÂª∫ VolumeAttachment ÂÆû‰æã
    va = VolumeAttachment(vol_id)
    volume_attachments[vol_id] = va
    print(f"VolumeAttachment for volume {vol_id} created.")
    # Ê®°ÊãüË∞ÉÁî® mark_as_attached
    va.mark_as_attached()

def simulate_configuration_change():
    # Ê®°Êãü CSI Êèí‰ª∂ÈÖçÁΩÆ‰ªé attachRequired=True Êîπ‰∏∫ False
    print("Changing CSI configuration: attachRequired from True to False")
    csi_config['attachRequired'] = False

def simulate_cleanup_detection():
    # Ê®°ÊãüÂÆöÊó∂Ê£ÄÊµãÂπ∂Â∞ùËØïÊ∏ÖÁêÜ orphaned VolumeAttachment
    for vol_id, va in list(volume_attachments.items()):
        if not va.mark_as_attached():
            # ÂΩìÊ†áËÆ∞Â§±Ë¥•Êó∂Â∞ùËØïÊ∏ÖÁêÜÂ§ÑÁêÜ
            cleaned = va.cleanup()
            if cleaned:
                del volume_attachments[vol_id]
    print("Current volume attachments:", list(volume_attachments.keys()))

# ÁÆÄÂçïÁöÑ HTTPÊúçÂä°Âô®ÔºåÁî®‰∫éÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºàÁ¨¶ÂêàË¶ÅÊ±ÇÔºâ
class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        output = f"Current configuration: {csi_config}\nVolumeAttachments: {list(volume_attachments.keys())}\n"
        self.wfile.write(output.encode('utf-8'))

def run_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, RequestHandler)
    print("Starting HTTP server on port 8080...")
    httpd.serve_forever()

def main():
    # ÂêØÂä® HTTP ÊúçÂä°
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()

    vol_id = "vol-1234"
    # Á¨¨‰∏ÄÊ≠•ÔºöÂàõÂª∫ VolumeAttachment
    simulate_volume_attachment_process(vol_id)
    
    # Á≠âÂæÖ‰∏ÄÂ∞èÊÆµÊó∂Èó¥ÔºåÁ°Æ‰øù VolumeAttachment Ë¢´ÂàõÂª∫ÂêéËøõË°åÈÖçÁΩÆÂèòÊõ¥
    time.sleep(1)
    
    # Á¨¨‰∫åÊ≠•ÔºöÊ®°Êãü CSI È©±Âä®Âç∏ËΩΩÂêéÈáçÊñ∞ÂÆâË£ÖÔºåÂπ∂ËÆæÁΩÆ attachRequired ‰∏∫ False
    simulate_configuration_change()
    
    # Á≠âÂæÖ‰∏ÄÂÆöÊó∂Èó¥ÂêéËØïÂõæÊ£ÄÊµãÂèäÊ∏ÖÁêÜ orphaned VolumeAttachment
    time.sleep(1)
    simulate_cleanup_detection()
    
    # ‰øùÊåÅËÑöÊú¨ËøêË°å‰∏ÄÊÆµÊó∂Èó¥‰ª•‰æøËßÇÂØü HTTP ÊúçÂä°ÊïàÊûú
    print("ËÑöÊú¨ËøêË°åÂÆåÊàêÔºåHTTP ÊúçÂä°Âô®‰ªçÂú®ÁõëÂê¨ 8080 Á´ØÂè£„ÄÇ")
    while True:
        time.sleep(5)

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Ê≠§ËÑöÊú¨‰∏ªË¶ÅÊ®°Êãü‰∫Ü Issue ‰∏≠ÊèèËø∞ÁöÑÈÄªËæëÊµÅÁ®ãÔºöÈ¶ñÂÖàÂàõÂª∫‰∏Ä‰∏™Ê®°ÊãüÁöÑ VolumeAttachment Âπ∂Ê†áËÆ∞‰∏∫‚ÄúÂ∑≤ÈôÑÁùÄ‚ÄùÔºõÁÑ∂ÂêéÈÄöËøá‰øÆÊîπÂÖ®Â±ÄÈÖçÁΩÆÔºàcsi_configÔºâÂ∞Ü attachRequired ËÆæ‰∏∫ FalseÔºå‰ªéËÄå‰ΩøÂêéÁª≠Ê†áËÆ∞Êìç‰ΩúÂ§±Ë¥•ÔºõÊúÄÂêéÂ∞ùËØïÂØπ‚ÄúÊªûÁïô‚ÄùÁöÑ VolumeAttachment ËøõË°åÊ∏ÖÁêÜ„ÄÇ
2. ËÑöÊú¨ÂêåÊó∂ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºåËøîÂõûÂΩìÂâçÁöÑ CSI ÈÖçÁΩÆÂíå VolumeAttachment ÂàóË°®Ôºå‰ª•‰æøÈÄöËøáÊµèËßàÂô®ÊàñÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑ËßÇÂØüÁ≥ªÁªüÁä∂ÊÄÅÔºåÁ¨¶ÂêàÈ¢òÁõÆÂØπ‰∫é HTTP ÊúçÂä°Âô®ÁöÑË¶ÅÊ±Ç„ÄÇ
3. Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éÁêÜËß£ÈóÆÈ¢òÂ§çÁé∞ÁöÑÊÄùË∑ØÔºåÂπ∂‰∏çÂØπÁúüÂÆûÁîü‰∫ßÁéØÂ¢É‰∫ßÁîüÂΩ±ÂìçÔºå‰πüÊ≤°ÊúâÊ∂âÂèä‰ªª‰ΩïÊïèÊÑüÂá≠ËØÅËØªÂèñÁöÑÊìç‰ΩúÔºåÁ°Æ‰øùÂú®Êú¨Âú∞ÂÆâÂÖ®ËøõË°åË∞ÉËØïÂíåÁ†îÁ©∂„ÄÇ
4. ËØ•Ê®°ÊãüËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫ÈîôËØØÂ§ÑÁêÜÈÄªËæëÔºåÂÆûÈôÖ‰∏≠Â∫îÊ†πÊçÆ Kubernetes Á≠âÁ≥ªÁªüÁöÑÂÖ∑‰ΩìÂÆûÁé∞ËøõË°åÊõ¥Â§çÊùÇÁöÑË∞ÉËØï„ÄÇ

---


## Issue #129569 Virtual Service Description Rendering of Maps

- Issue ÈìæÊé•Ôºö[#129569](https://github.com/kubernetes/kubernetes/issues/129569)

### Issue ÂÜÖÂÆπ

#### What happened?

When executing the following command:  **kubectl describe vs my-vs-virtualservice** ,
the istio http header set is not rendered correctly.  The virtual service functions as expected and get the virtual service in yaml format also works as expected.
The issue is only with the describe command. Here's how the out looks like:
...
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
...
Metadata:
  Creation Timestamp:  2025-01-02T23:01:50Z
  Generation:          1
Spec:
  Http:
    Headers:
      Request:
        Remove:
          x-my-private-header
        Set:
          **X - My - Public - Header:  aValue**

The Remove header which is a list is rendered properly but the Set which is a map is not rendered correctly.

And if I execute the following command: **kubectl get vs my-vs-virtualservice -o yaml**, the output looks like this:

  http:
  - headers:
      request:
        remove:
        - x-my-private-header
        set:
          x-my-public-header: "aValue"

#### What did you expect to happen?

Expected output:
...
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
...
Metadata:
  Creation Timestamp:  2025-01-02T23:01:50Z
  Generation:          1
Spec:
  Http:
    Headers:
      Request:
        Remove:
          x-my-private-header
        Set:
          **x-my-public-header:  aValue**

#### How can we reproduce it (as minimally and precisely as possible)?

Create a basic istio virtual service which sets some headers, deploy it and do a describe of the virtual service.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.29.1
```

</details>


#### Cloud provider

<details>
Tested on a private cloud environment and on Azure.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ‰ΩøÁî® kubectl describe ÂëΩ‰ª§Êü•Áúã Istio VirtualService Êó∂ÔºåÂØπ‰∫é headers.set ÈÖçÁΩÆÁöÑ map Êï∞ÊçÆÊ≤°ÊúâÊ≠£Á°ÆÊ∏≤ÊüìÁöÑÈóÆÈ¢ò„ÄÇËØ•ÈóÆÈ¢ò‰ªÖÂΩ±ÂìçÂëΩ‰ª§Ë°åËæìÂá∫ÁöÑÊòæÁ§∫ÊïàÊûúÔºåËÄåÂπ∂‰∏çÂΩ±ÂìçÂÆûÈôÖÁöÑ VirtualService ÂäüËÉΩÔºå‰∏îËÉΩÈÄöËøá kubectl get ÂëΩ‰ª§‰ª•Ê≠£Á°ÆÁöÑ YAML Ê†ºÂºèËé∑ÂèñÈÖçÁΩÆ‰ø°ÊÅØ„ÄÇ‰ªéÊèèËø∞Êù•ÁúãÔºåËøôÂ±û‰∫éÁïåÈù¢Â±ïÁ§∫ÊàñÊï∞ÊçÆÊ†ºÂºèËΩ¨Êç¢ÊñπÈù¢ÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏çÂÖ∑ÊúâÂÆâÂÖ®ÊÄßÔºåÂ¶ÇÊú™Êö¥Èú≤ÊïèÊÑü‰ø°ÊÅØ„ÄÅÊú™ÂºïËµ∑ÂëΩ‰ª§ÊâßË°å„ÄÅÊãíÁªùÊúçÂä°„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂΩ±Âìç„ÄÇÂõ†Ê≠§ÔºåËØ• Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°ÊãüÂ§çÁé∞ Istio VirtualService ÊèèËø∞ÊòæÁ§∫ÂºÇÂ∏∏ÁöÑÈóÆÈ¢òÔºå
‰ΩÜÁî±‰∫éËØ•ÈóÆÈ¢ò‰ªÖ‰∏∫ÊèèËø∞ËæìÂá∫ÁöÑÊ†ºÂºèÈîôËØØÔºåÂπ∂‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå
Âõ†Ê≠§ËÑöÊú¨‰ªÖÂÅö‰∏∫ÊºîÁ§∫Áî®ÈÄîÔºå‰∏ç‰ºöÂØπÁ≥ªÁªüÊàñÈõÜÁæ§‰∫ßÁîüÂÆûÈôÖÂΩ±Âìç„ÄÇ

ËØ∑Á°Æ‰øùÂú®ÈªòËÆ§‰ΩçÁΩÆÂ≠òÂú® kubeconfig Êñá‰ª∂ÔºåÂπ∂‰∏îÂ∑≤Ê≠£Á°ÆÈÖçÁΩÆËÆøÈóÆ k8s ÈõÜÁæ§ÊùÉÈôê„ÄÇ
"""

import subprocess
import sys

def get_virtual_service():
    try:
        # Ë∞ÉÁî® kubectl get ÂëΩ‰ª§‰ª• YAML Ê†ºÂºèËé∑Âèñ VirtualService
        get_cmd = ["kubectl", "get", "vs", "my-vs-virtualservice", "-o", "yaml"]
        yaml_output = subprocess.check_output(get_cmd, universal_newlines=True)
        print("kubectl get vs ËæìÂá∫:\n")
        print(yaml_output)

        # Ë∞ÉÁî® kubectl describe ÂëΩ‰ª§Êü•ÁúãÊèèËø∞‰ø°ÊÅØ
        describe_cmd = ["kubectl", "describe", "vs", "my-vs-virtualservice"]
        describe_output = subprocess.check_output(describe_cmd, universal_newlines=True)
        print("\nkubectl describe vs ËæìÂá∫:\n")
        print(describe_output)
    except subprocess.CalledProcessError as e:
        sys.exit(f"ÊâßË°åÂëΩ‰ª§Â§±Ë¥•: {e}")

if __name__ == '__main__':
    print("ÂºÄÂßãÊ®°ÊãüÂ§çÁé∞ Istio VirtualService ÊèèËø∞ËæìÂá∫ÁöÑÈóÆÈ¢ò...")
    get_virtual_service()
    print("\nËØ∑Ê†∏ÂØπ 'set' ÈÉ®ÂàÜÁöÑËæìÂá∫ÊòØÂê¶‰∏éÈ¢ÑÊúü‰∏ÄËá¥ÔºàÂ∫î‰∏∫ map Ê†ºÂºèÔºâ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨È¶ñÂÖàË∞ÉÁî®‰∫Ü kubectl get ÂëΩ‰ª§Ôºå‰ª• YAML Ê†ºÂºèËæìÂá∫ VirtualService ÁöÑÂÆåÊï¥ÈÖçÁΩÆÔºåÁ°Æ‰øùÊï∞ÊçÆÊú¨Ë∫´ÊòØÊ≠£Á°ÆÁöÑ„ÄÇ
2. ÈöèÂêéË∞ÉÁî® kubectl describe ÂëΩ‰ª§Êü•Áúã VirtualService ÊèèËø∞‰ø°ÊÅØÔºåÂ§çÁé∞ËØ• Issue ‰∏≠ÊèèËø∞ÁöÑÊ∏≤ÊüìÂºÇÂ∏∏ÈóÆÈ¢ò„ÄÇ
3. ËÑöÊú¨ÂØπËæìÂá∫ÁªìÊûúËøõË°åÊâìÂç∞Ôºå‰æø‰∫éÁ†îÁ©∂ËÄÖÂØπÊØî‚Äúget‚ÄùÂíå‚Äúdescribe‚ÄùÂëΩ‰ª§ÁöÑËæìÂá∫Â∑ÆÂºÇ„ÄÇ
4. ËØ∑Ê≥®ÊÑèÔºöËØ•ÈóÆÈ¢òÂè™ÊòØÂ±ïÁ§∫‰ø°ÊÅØÊ†ºÂºèÂºÇÂ∏∏Ôºå‰∏çÂ≠òÂú®ÊΩúÂú®ÁöÑÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫ÈóÆÈ¢òÁä∂ÊÄÅÔºå‰∏ç‰ºöÈÄ†ÊàêÂÆâÂÖ®ÊºèÊ¥ûË¢´Âà©Áî®ÁöÑÈóÆÈ¢ò„ÄÇ

---


## Issue #129552 Container remains ready causing Pod and EndpointSlice to Report False Ready State for the entire terminationGracePeriod

- Issue ÈìæÊé•Ôºö[#129552](https://github.com/kubernetes/kubernetes/issues/129552)

### Issue ÂÜÖÂÆπ

#### What happened?

containerStatuses.ready is not set to false immediately when a pod it deleted. This is results in pod reporting ready status when the pod is deleted after terminationGracePeriodSeconds have passed. 

#### What did you expect to happen?

After the pod termination starts, the containerStatuses should set ready: false for all the containers in the pod and hence pod status should report as ready: false. 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Pod
metadata:
  name: "b382559131"
  namespace: "default"
spec:
  terminationGracePeriodSeconds: 60
  restartPolicy: Never
  containers:
  - name: c1
    image: "k8s.gcr.io/busybox"
    command: ["/bin/sh", "-c"]
    args:
      - |
        _term() {
          rm -f /tmp/ready
        }
        trap _term SIGTERM

        touch /tmp/ready

        while true; do
          echo 'helloc1'
          ls /tmp/die_now && echo 'dying in 5s...' && sleep 5 && exit 0
          sleep 1
        done
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - |
          if [ -f "/tmp/ready" ]; then
            exit 0
          else
            touch /tmp/die_now
            exit 1
          fi
  - name: c2
    image: "k8s.gcr.io/busybox"
    command:
      - sh
      - -c
      - "_term() { while true; do echo \"hello_term_c2\"; sleep 1; done } ; trap _term SIGTERM; while true; do echo \"helloc2\"; sleep 1; done"
    
```

Run the above pod and issue a delete call when the containers become ready

$ kubectl delete pod b382559131

Monitor the readiness status:

$ while true; do date && kubectl get pod b382559131 -o json | jq .status.containerStatuses.[].ready && sleep 1; done
Sat Dec 21 11:05:16 PM PST 2024
true
true
Sat Dec 21 11:05:17 PM PST 2024
true
true

< snip > 

Sat Dec 21 11:06:21 PM PST 2024
true
true
Sat Dec 21 11:06:22 PM PST 2024
Error from server (NotFound): pods "b382559131" not found
Sat Dec 21 11:06:24 PM PST 2024
Error from server (NotFound): pods "b382559131" not found

The pod has a termination grace period of 60s. The delete was issued at 11:05:21 and we see that the ready status was true until the pod was deleted at 11:06:22 (60s later). 

The pod reports ready status until the last container exits. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

v1.30.6

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes Âú® Pod Âà†Èô§ËøáÁ®ã‰∏≠ÔºåÂÆπÂô®ÁöÑ readiness Áä∂ÊÄÅÊú™ËÉΩÂèäÊó∂Êõ¥Êñ∞ÔºåÂØºËá¥Âú® terminationGracePeriod ÂÜÖ Pod ÂíåÂØπÂ∫îÁöÑ EndpointSlice ‰æùÁÑ∂ÊòæÁ§∫‰∏∫ ready Áä∂ÊÄÅÁöÑÈóÆÈ¢ò„ÄÇ‰ªéÊèèËø∞‰∏äÁúãÔºåËøô‰∏™ÈóÆÈ¢ò‰∏∫ Kubernetes ÁöÑÁä∂ÊÄÅÊõ¥Êñ∞Êó∂Êú∫ÈóÆÈ¢òÔºåÂ±û‰∫éÂÆπÂô®ÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜÁöÑ BugÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂÆÉ‰∏ç‰ºöÂØºËá¥ÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉ„ÄÅÊãíÁªùÊúçÂä°ÊîªÂáªÁ≠âÂÆâÂÖ®È£éÈô©Ôºå‰πü‰∏çÂ≠òÂú®ÊïèÊÑü‰ø°ÊÅØÁöÑÊ≥ÑÈú≤ÊàñÊª•Áî®ÈóÆÈ¢ò„ÄÇÂõ†Ê≠§Ôºå‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËØ•ÈóÆÈ¢òÂπ∂‰∏çÊûÑÊàêÊòéÊòæÁöÑÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≥®ÊÑèÔºöËØ•ÈóÆÈ¢ò‰∏∫Kubernetes PodÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜ‰∏≠ÁöÑbugÔºå‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©„ÄÇ
Âõ†Ê≠§Ê≠§Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂú®Êú¨Âú∞KubernetesÈõÜÁæ§‰∏≠Ê®°ÊãüËØ•Áé∞Ë±°ÔºåÈ™åËØÅpodÂú®Âà†Èô§ËøáÁ®ã‰∏≠ÁöÑreadyÁä∂ÊÄÅÈóÆÈ¢ò„ÄÇ
ËØ•ËÑöÊú¨‰∏ªË¶ÅÂü∫‰∫ékubectlÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºåÁ°Æ‰øùÂú®ÁéØÂ¢É‰∏≠Â∑≤ÊúâkubectlÂú®ÈªòËÆ§kubeconfig‰∏ãÊ≠£Á°ÆÈÖçÁΩÆ„ÄÇ
"""

import subprocess
import time
import json

POD_NAME = "b382559131"
NAMESPACE = "default"

def run_cmd(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"Error executing '{cmd}': {e.stderr.strip()}")
        return None

def create_pod():
    pod_manifest = f'''
apiVersion: v1
kind: Pod
metadata:
  name: "{POD_NAME}"
  namespace: "{NAMESPACE}"
spec:
  terminationGracePeriodSeconds: 60
  restartPolicy: Never
  containers:
  - name: c1
    image: "k8s.gcr.io/busybox"
    command: ["/bin/sh", "-c"]
    args:
      - |
        _term() {{
          rm -f /tmp/ready
        }}
        trap _term SIGTERM

        touch /tmp/ready

        while true; do
          echo 'helloc1'
          ls /tmp/die_now && echo 'dying in 5s...' && sleep 5 && exit 0
          sleep 1
        done
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - |
          if [ -f "/tmp/ready" ]; then
            exit 0
          else
            touch /tmp/die_now
            exit 1
          fi
  - name: c2
    image: "k8s.gcr.io/busybox"
    command:
      - sh
      - -c
      - |
         _term() {{
           while true; do echo "hello_term_c2"; sleep 1; done
         }}
         trap _term SIGTERM;
         while true; do echo "helloc2"; sleep 1; done
'''
    with open("pod.yaml", "w") as f:
        f.write(pod_manifest)
    print("ÂàõÂª∫PodÈÖçÁΩÆÊñá‰ª∂ pod.yaml ÂÆåÊàê.")
    out = run_cmd("kubectl apply -f pod.yaml")
    if out:
        print(out)

def monitor_pod_readiness():
    print("ÂºÄÂßãÁõëÊéßPod‰∏≠ÁöÑcontainerStatuses.readyÁä∂ÊÄÅ (ÊØè1ÁßíÊ£ÄÊµã‰∏ÄÊ¨°)Ôºö")
    while True:
        time.sleep(1)
        out = run_cmd(f"kubectl get pod {POD_NAME} -n {NAMESPACE} -o json")
        if out is None:
            print("PodÂ∑≤Âà†Èô§ÊàñËé∑ÂèñÂ§±Ë¥•„ÄÇ")
            break
        pod_info = json.loads(out)
        status = pod_info.get("status", {})
        container_statuses = status.get("containerStatuses", [])
        if not container_statuses:
            print("Ê≤°ÊúâcontainerStatuses‰ø°ÊÅØ„ÄÇ")
            continue
        ready_status_list = [container.get("ready", False) for container in container_statuses]
        print(f"{time.strftime('%Y-%m-%d %H:%M:%S')} ÂÆπÂô®readyÁä∂ÊÄÅ: {ready_status_list}")

def delete_pod():
    print("Âà†Èô§Pod‰∏≠...")
    out = run_cmd(f"kubectl delete pod {POD_NAME} -n {NAMESPACE}")
    if out:
        print(out)

def main():
    create_pod()
    # Á≠âÂæÖPodËøõÂÖ•RunningÁä∂ÊÄÅ
    print("Á≠âÂæÖPodËøõÂÖ•RunningÁä∂ÊÄÅ...")
    time.sleep(10)
    print("ÁõëÊéßÂâç10ÁßíÂÜÖÁöÑreadyÁä∂ÊÄÅÔºö")
    for _ in range(10):
        monitor_pod_readiness()
    print("ÂèëËµ∑Âà†Èô§PodËØ∑Ê±Ç„ÄÇ")
    delete_pod()
    # ÁªßÁª≠ÁõëÊéßÂà†Èô§ËøáÁ®ã‰∏≠ÁöÑreadyÁä∂ÊÄÅÔºåÁõ¥Ëá≥Pod‰∏çÂ≠òÂú®
    monitor_pod_readiness()

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËØ•ËÑöÊú¨È¶ñÂÖà‰ΩøÁî® Python ÁîüÊàê‰∏Ä‰∏™ÂåÖÂê´ÈóÆÈ¢ò‰∏≠ÊèèËø∞‰∏§‰∏™ÂÆπÂô®ÁöÑ Pod ÂÆö‰πâ YAML Êñá‰ª∂Ôºàpod.yamlÔºâÔºåÂπ∂Âà©Áî® kubectl apply ÂëΩ‰ª§Âú®ÈªòËÆ§ÂëΩÂêçÁ©∫Èó¥‰∏≠ÂàõÂª∫ËØ• Pod„ÄÇ  
2. create_pod() ÂáΩÊï∞Ë¥üË¥£ÁîüÊàêÂπ∂ÈÉ®ÁΩ≤ Pod„ÄÇ  
3. monitor_pod_readiness() ÂáΩÊï∞ÊØèÈöî1ÁßíË∞ÉÁî®‰∏ÄÊ¨° kubectl API Ëé∑ÂèñËØ• Pod ÁöÑÁä∂ÊÄÅÔºåÂπ∂ÊâìÂç∞ÊØè‰∏™ÂÆπÂô®ÁöÑ ready Ê†áÂøó„ÄÇ  
4. ËÑöÊú¨Á≠âÂæÖ‰∏ÄÂÆöÊó∂Èó¥ÂêéË∞ÉÁî® delete_pod() ÂáΩÊï∞ÂèëËµ∑ Pod ÁöÑÂà†Èô§Êìç‰ΩúÔºåÂπ∂ÊåÅÁª≠ÁõëÊéß Pod Áä∂ÊÄÅÔºåÁõ¥Âà∞ Pod Ë¢´Âà†Èô§„ÄÇ  
5. Êï¥‰ΩìËÑöÊú¨Áî®‰∫éÊºîÁ§∫ËØ•ÈóÆÈ¢òÔºåÂç≥Âú® Pod Ë¢´Âà†Èô§ÁöÑËøáÁ®ã‰∏≠ÔºàÂÖ∑Êúâ60sÁöÑterminationGracePeriodÔºâÔºåÂÆπÂô®ÁöÑ ready Áä∂ÊÄÅ‰ªçÁÑ∂‰øùÊåÅ‰∏∫ TrueÔºåÁõ¥Ëá≥ÊúÄÂêéÂÆπÂô®ÈÄÄÂá∫„ÄÇ

Áî±‰∫éËØ•ÈóÆÈ¢ò‰ªÖ‰∏∫Áä∂ÊÄÅÊõ¥Êñ∞ÁöÑÈóÆÈ¢òÔºå‰∏î‰∏çÊ∂âÂèäÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†Ê≠§È£éÈô©ËØÑÁ∫ß‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÂ§çÁé∞ËØ•Áé∞Ë±°Ôºå‰∏çÂ≠òÂú®ÂÆâÂÖ®Âà©Áî®È£éÈô©„ÄÇ

---


## Issue #129531 [FG:InPlacePodVerticalScaling] avoid checking the configuration of resource managers to learn their expected behavior

- Issue ÈìæÊé•Ôºö[#129531](https://github.com/kubernetes/kubernetes/issues/129531)

### Issue ÂÜÖÂÆπ

#### What happened?

The VPA logic needs to know if resource managers (cpumanager, memory manager) can allocate exclusively resources.
To do so, it peeks in their configuration and second-guesses their expected behavior.
This is unfortunate and due to the lack of resource manager API which can report the same information

As result of this approach, there's an actual bug in the current VPA logic: this check is wrong https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L2856 because the name of the policy for cpumanager is `static` (lowercase "s") while for memory manager is `Static` (uppercase "s")

#### What did you expect to happen?

VPA should be able to learn about resource manager behavior in a safe and supported way

#### How can we reproduce it (as minimally and precisely as possible)?

configure memory manager with `Static` policy and enable VPA. the `canResizePod` logic will behave incorrectly

#### Anything else we need to know?

behavior introduced in 2d8939c4aef8f060d413bb27272ba38cd7171fbe

a new API to expose this information is proposed in https://github.com/kubernetes/kubernetes/pull/128728

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
Ëøô‰∏™ Issue ‰∏ªË¶ÅÊèèËø∞‰∫Ü VPAÔºàVertical Pod AutoscalerÔºâÂú®Âà§Êñ≠ËµÑÊ∫êÁÆ°ÁêÜÂô®ÔºàÂ¶Ç cpumanager„ÄÅmemory managerÔºâÈÖçÁΩÆÊó∂Â≠òÂú®ÈÄªËæëÈîôËØØÁöÑÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÁî±‰∫é VPA ÈÄöËøáÊ£ÄÊü•ÈÖçÁΩÆÊù•Êé®Êñ≠ËµÑÊ∫êÁÆ°ÁêÜÂô®ÁöÑË°å‰∏∫Ôºå‰ΩÜÂõ†Â§ßÂ∞èÂÜô‰∏ç‰∏ÄËá¥Ôºàcpumanager ‰ΩøÁî® "static"ÔºåËÄå memory manager ‰ΩøÁî® "Static"ÔºâÂØºËá¥Âà§Êñ≠ÈîôËØØ„ÄÇËøôÊòØ‰∏Ä‰∏™ÂäüËÉΩÊÄß bugÔºå‰ΩìÁé∞‰∏∫ÈîôËØØÁöÑÈÄªËæëÂà§Êñ≠ÔºåËÄåÂπ∂Ê≤°ÊúâÊ∂âÂèäÊú™ÁªèÊéàÊùÉÁöÑËÆøÈóÆ„ÄÅ‰ª£Á†ÅÊâßË°å„ÄÅÊèêÊùÉÊàñ‰ªª‰ΩïÂèØËÉΩÂØºËá¥ÂÆâÂÖ®ÊºèÊ¥ûÁöÑË°å‰∏∫Ôºå‰πüÊ≤°ÊúâÂºïÂÖ•ÊãíÁªùÊúçÂä°ÔºàDoSÔºâÊîªÂáª„ÄÅÊï∞ÊçÆÊ≥ÑÊºèÊàñÂëΩ‰ª§ÊâßË°åÈ£éÈô©„ÄÇ

Ê†πÊçÆÊèê‰æõÁöÑÊèèËø∞ÔºåËØ•ÈóÆÈ¢òÊòØÁî±‰∫éÂÜÖÈÉ®ÈÖçÁΩÆÈÄªËæëÂÆûÁé∞‰∏çÂΩìÈÄ†ÊàêÁöÑÔºå‰∏î‰∏ç‰ºöË¢´ÊîªÂáªËÄÖÂà©Áî®‰ª•Ëé∑ÂèñÊõ¥È´òÊùÉÈôêÊàñËÄÖÁ†¥ÂùèÁ≥ªÁªüÂÆâÂÖ®„ÄÇÂõ†Ê≠§Â∞±Êï¥‰ΩìÂÆâÂÖ®ÊÄßËÄåË®ÄÔºåÂπ∂‰∏çÂ≠òÂú®ÊΩúÂú®ÁöÑÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
# Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂèäÊú¨Âú∞ÊµãËØïÁõÆÁöÑÔºåÁî®‰∫éÊ®°Êãü VPA ÈÖçÁΩÆÊ£ÄÊü•Êó∂Â§ßÂ∞èÂÜô‰∏ç‰∏ÄËá¥ÂØºËá¥ÁöÑÈÄªËæëÈîôËØØÂ§çÁé∞Âú∫ÊôØÔºå
# ‰ΩÜËØ•ÈóÆÈ¢òÊú¨Ë∫´Âπ∂‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥ûÔºåÊïÖÂ§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂäüËÉΩÈ™åËØÅÔºåÈùûÁî®‰∫éÊîªÂáª„ÄÇ

def simulate_vpa_behavior(cpu_manager_policy, memory_manager_policy):
    """
    Ê®°Êãü VPA Âà§Êñ≠ËµÑÊ∫êÁÆ°ÁêÜÂô®ÊòØÂê¶ÊîØÊåÅÁã¨Âç†ËµÑÊ∫êÂàÜÈÖçÁöÑÈÄªËæë„ÄÇ
    Áî±‰∫é VPA ÈÄªËæëÈîôËØØÔºåÂØºËá¥ÂØπ‰∫é memory manager ÁöÑÂà§Êñ≠Â≠òÂú®Â§ßÂ∞èÂÜôÈóÆÈ¢ò„ÄÇ
    """
    # ÊåâÁÖßÂÆûÈôÖÊ∫êÁ†ÅÈÄªËæëÔºåÂÅáËÆæÂØπ cpu_manager_policy ÈááÁî®Ê≠£Á°ÆÊØîËæÉÔºåËÄå memory_manager_policy ÊØîËæÉÊó∂Âõ†Â§ßÂ∞èÂÜô‰∏ç‰∏ÄËá¥Âá∫Èîô
    expected_cpu_policy = "static"
    expected_memory_policy = "static"  # Ê≠£Á°ÆÊÉÖÂÜµ‰∏ãÂ∫îËØ•‰∏ÄËá¥Ôºå‰ΩÜÊòØÂÆûÈôÖÈÖçÁΩÆ‰∏≠ memory manager ÂèØËÉΩÁî® 'Static'
    
    cpu_check = (cpu_manager_policy == expected_cpu_policy)
    memory_check = (memory_manager_policy == expected_memory_policy)
    
    print("CPU Manager Âà§Êñ≠ÁªìÊûú:", cpu_check)
    print("Memory Manager Âà§Êñ≠ÁªìÊûú:", memory_check)

def main():
    # Ê®°ÊãüÈÖçÁΩÆÂú∫ÊôØ
    # cpu_manager ‰ΩøÁî®ÂÖ®Â∞èÂÜô 'static'
    cpu_manager_policy = "static"
    # memory manager ‰ΩøÁî®È¶ñÂ≠óÊØçÂ§ßÂÜô 'Static'
    memory_manager_policy = "Static"
    
    print("Ê®°ÊãüÂºÄÂßãÔºö")
    simulate_vpa_behavior(cpu_manager_policy, memory_manager_policy)
    
    # Ê†πÊçÆÂà§Êñ≠ÔºåVPA ÈîôËØØÂú∞ËÆ§‰∏∫ memory manager ‰∏çÊîØÊåÅÁã¨Âç†ËµÑÊ∫êÂàÜÈÖç
    if not simulate_vpa_behavior(memory_manager_policy, "static"): 
        print("VPA ÈîôËØØÂú∞Âà§Êñ≠ memory manager ‰∏çÊîØÊåÅÁã¨Âç†ËµÑÊ∫êÂàÜÈÖç„ÄÇ")
    else:
        print("ÈÖçÁΩÆÂà§Êñ≠Ê≠£Á°Æ„ÄÇ")
    
if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•Â§çÁé∞ËÑöÊú¨Ê®°Êãü‰∫Ü VPA Âú®Ê£ÄÊü•ËµÑÊ∫êÁÆ°ÁêÜÂô®ÈÖçÁΩÆÊó∂ÁöÑÁÆÄÂçïÈÄªËæë„ÄÇËÑöÊú¨ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂáΩÊï∞ simulate_vpa_behaviorÔºåÁî®‰∫éÊØîËæÉ‰º†ÂÖ•ÁöÑ cpumanager Âíå memory manager ÁöÑÁ≠ñÁï•ÂÄº‰∏éÈ¢ÑÊúüÂÄºÔºàÂÖ®ÈÉ®Â∞èÂÜô "static"Ôºâ„ÄÇÁî±‰∫éÂÜÖÂ≠òÁÆ°ÁêÜÂô®ÂèØËÉΩÈÖçÁΩÆ‰∏∫È¶ñÂ≠óÊØçÂ§ßÂÜô "Static"ÔºåÂØºËá¥ÊØîËæÉÁªìÊûú‰∏∫ FalseÔºå‰ªéËÄåÊ®°Êãü‰∫Ü ISSUE ‰∏≠ÊèèËø∞ÁöÑÈÄªËæëÈîôËØØÊÉÖÂÜµ„ÄÇ  
ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËØ•ÈóÆÈ¢òËôΩ‰ºöÂØºËá¥ VPA Ë°å‰∏∫‰∏çÊ≠£Á°ÆÔºå‰ΩÜ‰∏ç‰ºöÂØºËá¥Á≥ªÁªüÂÆâÂÖ®ÈóÆÈ¢òÔºåÊïÖÂú®È£éÈô©ËØÑÁ∫ß‰∏äÂà§Êñ≠‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÔºå‰πü‰∏ç‰ºöÊèê‰æõÂÆûÈôÖÂç±ÂÆ≥ÁöÑÈ´òÈ£éÈô©Âà©Áî® PoC„ÄÇÊú¨ËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫ÂäüËÉΩÊÄßÈóÆÈ¢òÁöÑÂ§çÁé∞Ôºå‰∏çÂÖ∑Â§áÂÆâÂÖ®È£éÈô©„ÄÇ

---


## Issue #129523 DRA: Increase CEL expression complexity limit in resourceClaimTemplate

- Issue ÈìæÊé•Ôºö[#129523](https://github.com/kubernetes/kubernetes/issues/129523)

### Issue ÂÜÖÂÆπ

#### What happened?

Currently, smiple expressions does not pass the cost validation because of cel expression complexity limit though cel expression is not really complicated.

Case: https://github.com/intel/intel-resource-drivers-for-kubernetes/blob/80d57856956343e457a79b2cfe9f0486884a4765/deployments/qat/tests/resource-claim-template.yaml#L14

In addition, `exists_one`, `split`, `filter` and `map` also exceed.


#### What did you expect to happen?

It should pass the cost validation for simple enough expressions.

#### How can we reproduce it (as minimally and precisely as possible)?

create a resourceClaimTemplate with CEL expression that with the aforementioned expressions.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
Ê≠§ issue ÊèèËø∞ÁöÑÊòØÂú® resourceClaimTemplate ‰∏≠‰ΩøÁî® CEL Ë°®ËææÂºèÊó∂ÔºåÁî±‰∫éË°®ËææÂºèÂ§çÊùÇÂ∫¶ÈôêÂà∂ÁöÑÈóÆÈ¢òÂØºËá¥‰∏Ä‰∫õÁÆÄÂçïË°®ËææÂºèÔºà‰ª•Âèä exists_one„ÄÅsplit„ÄÅfilter Âíå map Á≠âÂáΩÊï∞ÁöÑË∞ÉÁî®ÔºâÊó†Ê≥ïÈÄöËøáÊàêÊú¨È™åËØÅ„ÄÇËøôÂ±û‰∫éÂäüËÉΩ‰∏äÁöÑÈôêÂà∂ÊàñÈ™åËØÅ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂπ∂ÈùûÊ∂âÂèäÂÆâÂÖ®ÊïèÊÑüÁöÑÊºèÊ¥û„ÄÇÂõ†Ê≠§ÔºåËØ• issue Êõ¥Â§öÊòØÂäüËÉΩÊîπËøõÊàñ bug ‰øÆÊ≠£ÔºåËÄå‰∏çÊòØÂÆâÂÖ®È£éÈô©ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Áî±‰∫éÊú¨ issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå‰∏çÈúÄË¶ÅÂ§çÁé∞ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ
ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊèêÁ§∫Ê≠§ issue ‰∏éÂÆâÂÖ®ÊºèÊ¥ûÊó†ÂÖ≥„ÄÇ
"""

def main():
    print("ËØ• issue ‰ªÖÊ∂âÂèä CEL Ë°®ËææÂºèÂ§çÊùÇÂ∫¶ÈôêÂà∂ÁöÑÂäüËÉΩÈ™åËØÅÈóÆÈ¢òÔºå‰∏çÂ≠òÂú®ÂÆâÂÖ®È£éÈô©„ÄÇ")

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫ÂΩìÂâç issue Âπ∂‰∏çÊûÑÊàêÂÆâÂÖ®ÊºèÊ¥ûÁöÑ‰∫ãÂÆû„ÄÇÊ†πÊçÆÂàÜÊûêÔºåÊ≠§ÂäüËÉΩÈóÆÈ¢òÂ±û‰∫éÈ™åËØÅÁ≠ñÁï•ÈóÆÈ¢òÔºå‰∏ç‰ºöË¢´ÊîªÂáªËÄÖÂà©Áî®Êù•ÊâßË°å‰ª£Á†Å„ÄÅÈÄÉÈÄ∏ÂÆπÂô®„ÄÅË∂äÊùÉÊàñÊª•Áî®Á≥ªÁªüËµÑÊ∫êÔºåÂõ†Ê≠§‰∏çÈúÄË¶ÅÊèê‰æõÈ´òÈ£éÈô©ÊºèÊ¥ûÁöÑÂ§çÁé∞‰ª£Á†Å„ÄÇËÑöÊú¨ËøêË°åÂêé‰ºöËæìÂá∫ËØ¥Êòé‰ø°ÊÅØÔºåË°®Á§∫ËØ• issue ‰∏éÂÆâÂÖ®È£éÈô©Êó†ÂÖ≥„ÄÇ

---


## Issue #129503 Ratcheting validation missing for CRD status subresources

- Issue ÈìæÊé•Ôºö[#129503](https://github.com/kubernetes/kubernetes/issues/129503)

### Issue ÂÜÖÂÆπ

#### What happened?

Testing the new CRD [ratcheting validation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) feature, I implemented a change of validation from `pattern` to `enum` in both a `spec` and `status` field on a CRD. Note, this CRD includes a status subresource.

I created the CRD with the `pattern` validation on the fields initially and then created a CR with field values in spec and status that matched the pattern, but did not match my intended enum.

Editing the CRD now to move to the `enum`, this would invalidate the stored data in the two fields, meaning ratcheting is required.

Updating fields such as metadata (adding an annotation) or other spec fields worked correctly, and no issues were returned.

Trying to update any status field using the status subresource path failed, complaining that my field did not match the expected enum.

#### What did you expect to happen?

Values to the status should be ratcheted in the same way that the based endpoint is ratcheted.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a minimal CRD with a status subresource and a field with a pattern validation (and another field for testing)
2. Persist a CR with a value that does not match the pattern.
3. Update the CRD schema to move from `pattern` to `enum` with a value that does not match the persisted CR.
4. Update an annotation, observe success
5. Update the other CRD status field with a new value, observe that it errors complaining about the enum value being incorrect.

#### Anything else we need to know?

Neither the [documentation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) nor the [KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4008-crd-ratcheting) suggest that there should be any difference in behaviour.

I raised this [on slack](https://kubernetes.slack.com/archives/C0EG7JC6T/p1736187102623779) and it was suggested that this was a bug.

Looking at the code, I can see [ratcheting options](https://github.com/kubernetes/apiextensions-apiserver/blob/d72b6f721f9b3cf655cb545e59cfb5f4bc615b8f/pkg/registry/customresource/strategy.go#L287-L291) on the storage for the main path, but not in the [status strategy](https://github.com/kubernetes/apiextensions-apiserver/blob/d72b6f721f9b3cf655cb545e59cfb5f4bc615b8f/pkg/registry/customresource/status_strategy.go#L96)

#### Kubernetes version

<details>
Still in the main development branch AFAICT, tested on 1.31.3
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes CRDÔºàËá™ÂÆö‰πâËµÑÊ∫êÔºâ‰∏≠ ratcheting È™åËØÅÁöÑ‰∏Ä‰∏™ÈÄªËæëÁº∫Èô∑ÔºöÂú®Êõ¥Êñ∞ CRD ÁöÑ schemaÔºàÂ∞ÜÈ™åËØÅÊñπÂºèÁî± pattern ‰øÆÊîπ‰∏∫ enumÔºâÂêéÔºå‰ΩøÁî® status Â≠êËµÑÊ∫êËøõË°åÊõ¥Êñ∞Êó∂Ê≤°ÊúâÂ∫îÁî®‰∏ªËµÑÊ∫êË∑ØÂæÑÈÇ£ËæπÁöÑ ratcheting ÈÄªËæëÔºåÂØºËá¥Êõ¥Êñ∞Áä∂ÊÄÅÂ≠óÊÆµÊó∂Êä•Èîô„ÄÇÁªèËøáÂàÜÊûêÂèØ‰ª•ÁúãÂá∫ÔºåÊ≠§ÈóÆÈ¢òÂ±û‰∫éÈ™åËØÅÈÄªËæëÂÆûÁé∞‰∏äÁöÑ bugÔºåËÄåÈùûËÆæËÆ°ÊàñÂÆûÁé∞‰∏äÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºå‰∏çÊ∂âÂèäÊïèÊÑüÊï∞ÊçÆÊ≥ÑÈú≤„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# ËØ•ÈóÆÈ¢ò‰∏çÂ±û‰∫éÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§‰∏çÈúÄË¶ÅÊèê‰æõÂÆâÂÖ®Â§çÁé∞POC„ÄÇ
print("Ê≠§IssueÊèèËø∞ÁöÑÈóÆÈ¢ò‰∏∫CRD ratchetingÈ™åËØÅÁöÑÈÄªËæëÁº∫Èô∑ÔºåÂπ∂‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©ÔºåÊó†ÈúÄÊûÑÈÄ†Â§çÁé∞POC„ÄÇ")
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Ê†πÊçÆ Issue ÁöÑÊèèËø∞ÔºåÁº∫Â§± ratcheting È™åËØÅÁöÑË°å‰∏∫‰ªÖÂΩ±Âìç‰∫Ü CRD status Â≠êËµÑÊ∫êÁöÑÊ≠£Â∏∏Êõ¥Êñ∞ÊµÅÁ®ãÔºåÂπ∂Êú™ÂºïÂÖ•ÂÆâÂÖ®ÊºèÊ¥ûÔºà‰æãÂ¶ÇÔºåÊîªÂáªËÄÖÊó†Ê≥ïÂà©Áî®ËØ•ÈóÆÈ¢òÊù•ÊâßË°åÊú™ÊéàÊùÉÊìç‰Ωú„ÄÅÁªïËøáÂÆâÂÖ®ÊéßÂà∂ÊàñÂØºËá¥ÊúçÂä°ÊãíÁªùÁ≠âÂÆâÂÖ®È£éÈô©Ôºâ„ÄÇÂõ†Ê≠§ÔºåÊú¨ÈóÆÈ¢òÁöÑÈ£éÈô©ËØÑÁ∫ßÂà§Êñ≠‰∏∫‚Äú‰∏çÊ∂âÂèä‚ÄùÔºåÂπ∂‰∏çÁ¨¶ÂêàÈ´òÈ£éÈô©ÈóÆÈ¢òÁöÑÊÉÖÂÜµÔºå‰πü‰∏çÈúÄË¶ÅÊèê‰æõÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ËÑöÊú¨„ÄÇ

---


## Issue #129499 OpenAPI handler fails on duplicated path

- Issue ÈìæÊé•Ôºö[#129499](https://github.com/kubernetes/kubernetes/issues/129499)

### Issue ÂÜÖÂÆπ

#### What happened?

After installing kyverno and kyverno reports-server on Kubernetes 1.28.15 or 1.30.6 OpenAPI handler fail to initialize with this error log in apiserver:
`handler.go:160] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/reports.kyverno.io/v1/namespaces/{namespace}/ephemeralreports/{name}
` 
It works fine with K8s 1.27


#### What did you expect to happen?

The OpenAPI handler should not fail to initialize. 

#### How can we reproduce it (as minimally and precisely as possible)?

On k8s 1.28+ , install kyverno and kyverno reports-server and then restart kube-apiserver then you will see error while running `kubectl get --raw /openapi/v2` 
`Error from server (ServiceUnavailable): the server is currently unable to handle the request` 

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28.15/1.30.6

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞‰∫ÜÂú® Kubernetes 1.28.15 Êàñ 1.30.6 ÁéØÂ¢É‰∏≠ÔºåÂÆâË£Ö kyverno Âíå kyverno reports-server ÂêéÔºåÂõ†ÈáçÂ§çÂÆö‰πâ OpenAPI Ë∑ØÂæÑËÄåÂØºËá¥ OpenAPI handler Êó†Ê≥ïÊ≠£Â∏∏ÂàùÂßãÂåñÁöÑÈóÆÈ¢ò„ÄÇÊä•Èîô‰ø°ÊÅØÊòæÁ§∫ OpenAPI ÂêàÂπ∂ËßÑÊ†ºÂ§±Ë¥•ÔºåÂØºËá¥ÈÄöËøá /openapi/v2 Êé•Âè£Ëé∑ÂèñËßÑËåÉÊó∂ËøîÂõû ServiceUnavailable ÈîôËØØ„ÄÇËøô‰∏™ÈóÆÈ¢òÂ±û‰∫éÁî±‰∫éÈáçÂ§çÁöÑ API Ë∑ØÂæÑÂÆö‰πâÂºïËµ∑ÁöÑÈÖçÁΩÆÊàñÂÖºÂÆπÊÄß bugÔºå‰ªéÈóÆÈ¢òÊèèËø∞Êù•ÁúãÂπ∂Ê≤°ÊúâÊ∂âÂèä‰ªª‰ΩïÂÆâÂÖ®ÊïèÊÑüÁöÑÊï∞ÊçÆÊ≥ÑÈú≤„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊãíÁªùÊúçÂä°ÔºàDoSÔºâÁ≠âÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# ËØ•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÈóÆÈ¢òÁéØÂ¢É‰∏≠ÁöÑË°å‰∏∫Ôºå‰ªÖ‰æõÊú¨Âú∞ÊµãËØïÂíåÁ†îÁ©∂ËßÇÂØü„ÄÇ
# Ê≥®ÊÑèÔºöÊ≠§ÈóÆÈ¢ò‰∏∫ OpenAPI ËßÑËåÉÂêàÂπ∂ËøáÁ®ã‰∏≠ÁöÑ bugÔºåÂπ∂ÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºå
# Âõ†Ê≠§ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÈîôËØØÊó•ÂøóÊâìÂç∞Ôºå‰∏çÊ∂âÂèäÂØπÁúüÂÆû Kubernetes ÈõÜÁæ§ÁöÑÊîªÂáªË°å‰∏∫„ÄÇ

import http.server
import socketserver

class OpenAPIHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        # Ê®°Êãü /openapi/v2 Êé•Âè£ÔºåÂ¶ÇÊûúÊ£ÄÊµãÂà∞ÈáçÂ§çË∑ØÂæÑÂàôËøîÂõûÈîôËØØ
        if self.path == "/openapi/v2":
            # Ê®°ÊãüÈáçÂ§çË∑ØÂæÑÊ£ÄÊµãÈîôËØØ
            self.send_response(503)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"Error in OpenAPI handler: failed to build merge specs: duplicated path detected")
        else:
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"Hello, this is a simulated Kubernetes API server.")

def run_server(port=8080):
    with socketserver.TCPServer(("", port), OpenAPIHandler) as httpd:
        print(f"Ê®°Êãü HTTP ÊúçÂä°Âô®Â∑≤ÂêØÂä®ÔºåÁõëÂê¨Âú®Á´ØÂè£ {port}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("ÊúçÂä°Âô®ÂÖ≥Èó≠„ÄÇ")
            httpd.shutdown()

if __name__ == '__main__':
    run_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨Ê®°Êãü‰∫Ü‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£„ÄÇÊúçÂä°Âô®‰∏≠ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ OpenAPIHandlerÔºåÂΩìËÆøÈóÆË∑ØÂæÑ‰∏∫ ‚Äú/openapi/v2‚Äù Êó∂ÔºåËøîÂõû 503 Áä∂ÊÄÅÁ†ÅÂπ∂ËæìÂá∫ÈîôËØØÊèêÁ§∫‰ø°ÊÅØÔºåÊ®°ÊãüÁî±‰∫éÈáçÂ§çË∑ØÂæÑÂØºËá¥ OpenAPI handler ÊûÑÂª∫Â§±Ë¥•ÁöÑÊÉÖÂÜµ„ÄÇ
2. ÂÖ∂‰ªñÈùû ‚Äú/openapi/v2‚Äù ËØ∑Ê±ÇÔºåÂàôËøîÂõû 200 Áä∂ÊÄÅÁ†ÅÂèäÁÆÄÂçïÁöÑÊ¨¢Ëøé‰ø°ÊÅØ„ÄÇ
3. ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÊ≠§ÈóÆÈ¢òÊú¨Ë∫´ÊòØÁî± API Ë∑ØÂæÑÈáçÂ§çÂºïËµ∑ÁöÑ bugÔºå‰∏éÂÆâÂÖ®ÊîªÂáªÊó†ÂÖ≥ÔºåÂõ†Ê≠§ËÑöÊú¨‰∏≠ÁöÑÂ§çÁé∞ÈÄªËæë‰ªÖÁî®‰∫éÊºîÁ§∫ÈóÆÈ¢ò‰∫ßÁîüÊó∂ËøîÂõûÁõ∏Â∫îÈîôËØØ‰ø°ÊÅØÁöÑÂú∫ÊôØÔºåÂπ∂ÈùûÁî®‰∫éÂÆâÂÖ®ÊµãËØïÊàñÂà©Áî®„ÄÇ
4. ÊâÄÊúâÂá≠ËØÅ„ÄÅÈÖçÁΩÆÁ≠âÂùáÂÅáËÆæÂú®ÈªòËÆ§‰ΩçÁΩÆÔºåÊú¨ËÑöÊú¨‰∏çÊ∂âÂèäÁúüÂÆûÁöÑ Kubernetes ÈõÜÁæ§Êìç‰ΩúÔºå‰ªÖÂú®Êú¨Âú∞ÁéØÂ¢É‰∏≠Ê®°ÊãüÈóÆÈ¢òÁé∞Ë±°ËøõË°åÈ™åËØÅ„ÄÇ

---


## Issue #129496 Fix message formatting in AttachDetach controller on VA status error

- Issue ÈìæÊé•Ôºö[#129496](https://github.com/kubernetes/kubernetes/issues/129496)

### Issue ÂÜÖÂÆπ

#### What happened?

Attachdetach Controller records events on a PVC when errors occur:
 * https://github.com/kubernetes/kubernetes/blob/c3f3fdc1aa62002a58bec1141fe69e86bbb27491/pkg/volume/util/operationexecutor/operation_generator.go#L315

The message is logged with `Eventf`, however a single string is passed to the `messageFmt` argument. This treats a message (which may potentially contain format characters that should be treated literally) as a format string. This can result in incorrectly formatted messages, depending on the error message. This is problematic for CSI, as error messages are propagated up from the CSI driver, and presented on a VolumeAttachment resource as a status message. This can result in an event with special characters being replaced by `!!(MISSING)`.

#### What did you expect to happen?

Expected error message to be printed as string literal, not format string.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction requires configuring a CSI driver to emit a particular error message that includes special characters interpreted as golang string format characters on `ControllerPublish` RPC.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.30
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ AttachDetach ÊéßÂà∂Âô®Âú®Â§ÑÁêÜ PVC ÈîôËØØ‰∫ã‰ª∂Êó∂Ôºå‰ΩøÁî® Eventf ËÆ∞ÂΩïÊó•ÂøóÊó∂ÈîôËØØÂú∞Â∞ÜÊï¥‰∏™ÈîôËØØÊ∂àÊÅØ‰Ωú‰∏∫Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤‰º†ÂÖ•ÔºåÂèØËÉΩÂØºËá¥Êó•Âøó‰∏≠ÁöÑÊ†ºÂºèÂç†‰ΩçÁ¨¶Ë¢´ÈîôËØØÊõøÊç¢Êàê "!!(MISSING)" ÁöÑÁé∞Ë±°„ÄÇËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫éÊó•Âøó‰ø°ÊÅØÊ†ºÂºèÂåñÁöÑÈóÆÈ¢òÔºå‰ΩøÂæóÈîôËØØ‰ø°ÊÅØÊòæÁ§∫‰∏çÊ≠£Á°ÆÔºõÂπ∂‰∏çÊ∂âÂèäÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊãíÁªùÊúçÂä°ÊîªÂáªÁ≠âÂÆâÂÖ®È£éÈô©Ôºå‰πü‰∏ç‰ºö‰ΩøÈîôËØØ‰ø°ÊÅØÊ≥ÑÈú≤Âá∫ÊïèÊÑüÊï∞ÊçÆ„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™ÈóÆÈ¢ò‰ªÖÂ±û‰∫éÊó•ÂøóÊòæÁ§∫ÈîôËØØÁöÑÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºå‰∏çÂ≠òÂú®ÂÆâÂÖ®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Áî±‰∫éÊú¨Issue‰∏çÊòØÂÆâÂÖ®È£éÈô©ÈóÆÈ¢òÔºåÊâÄ‰ª•Ê≤°ÊúâÈ´òÈ£éÈô©ÁöÑÂ§çÁé∞ËÑöÊú¨„ÄÇ
# ‰∏ãÈù¢Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ§∫‰æãÔºåÊ®°ÊãüËØ•Êó•ÂøóÊ†ºÂºèÂåñÈóÆÈ¢òÔºå‰ªÖÁî®‰∫éÂ±ïÁ§∫ÈîôËØØÁöÑÊ†ºÂºèÂåñÁé∞Ë±°

import logging

def simulate_eventf(message_fmt, *args):
    """
    Ê®°Êãü Kubernetes ‰∏≠ Eventf Ë°å‰∏∫Ôºö
    Â∞Ü message_fmt ‰Ωú‰∏∫Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤Êù•Â§ÑÁêÜÔºåÂ¶ÇÊûúÂÖ∂‰∏≠ÂåÖÂê´Ê†ºÂºèÂåñÂç†‰ΩçÁ¨¶Ôºå
    Âàô‰ºöÊõøÊç¢ÂØπÂ∫îÂèÇÊï∞ÔºõÂê¶ÂàôÂéüÊ†∑ËæìÂá∫„ÄÇ
    """
    try:
        # Â∞ùËØïÊ†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤
        message = message_fmt % args
    except Exception as e:
        message = f"Ê†ºÂºèÂåñÈîôËØØÔºö{str(e)}ÔºåÂéüÊ∂àÊÅØÔºö{message_fmt}"
    return message

if __name__ == '__main__':
    # ÈîôËØØÁ§∫‰æãÔºöÈîôËØØÊ∂àÊÅØ‰∏≠ÂåÖÂê´Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤ÁöÑÂç†‰ΩçÁ¨¶Ôºå‰æãÂ¶Ç %s Ôºå‰ΩÜÊ≤°ÊúâÁõ∏Â∫îÂèÇÊï∞
    error_message = "CSI driver error: unexpected token '%s' in response"
    
    # Ê≠§Â§ÑÈîôËØØÂú∞‰º†ÂÖ•Êï¥‰∏™ÈîôËØØÊ∂àÊÅØ‰Ωú‰∏∫Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤ÂèÇÊï∞ÔºåÈ¢ÑÊúü‰∏çËøõË°åÊ†ºÂºèÂåñ
    # ÂÆûÈôÖ‰∏ä simulate_eventf Â∞ÜÂ∞ùËØïÊåâÁÖßÊ†ºÂºèÁ¨¶Âè∑ËøõË°åÊõøÊç¢
    formatted_message = simulate_eventf(error_message, )
    print("Â§çÁé∞ÁöÑÊ†ºÂºèÂåñÁªìÊûú:")
    print(formatted_message)
    
    # Ê≠£Á°ÆÁöÑÂÅöÊ≥ïÂ∫îËØ•ÊòØÂ∞ÜÈîôËØØÊ∂àÊÅØ‰Ωú‰∏∫ÊôÆÈÄöÂ≠óÁ¨¶‰∏≤Â§ÑÁêÜÔºåËÄå‰∏çÊòØÊ†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤„ÄÇ
    # Ê®°ÊãüÊ≠£Á°ÆÂ§ÑÁêÜÔºö
    print("\nÊ≠£Á°ÆÂ§ÑÁêÜÁöÑÁªìÊûú:")
    print(error_message)
    
    # Âè¶Â§ñÔºåÂØπ‰∫éÈúÄË¶ÅÂ∞ÜÈîôËØØÊ∂àÊÅØ‰∏≠ÁöÑÊ†ºÂºèÁ¨¶Âè∑ÂéüÊ†∑ÊòæÁ§∫ÁöÑÊÉÖÂÜµÔºåÂèØ‰ª•ËøõË°åËΩ¨‰πâÂ§ÑÁêÜÔºö
    escaped_message = error_message.replace("%", "%%")
    print("\nËΩ¨‰πâÂêéÁöÑÊ≠£Á°ÆÊòæÁ§∫ÁªìÊûú:")
    print(escaped_message)
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Ëøô‰∏™Â§çÁé∞ËÑöÊú¨Ê®°Êãü‰∫Ü Kubernetes ‰∏≠‰ΩøÁî® Eventf ÊâìÂç∞Êó•ÂøóÊó∂ÂèØËÉΩÂá∫Áé∞ÁöÑÊ†ºÂºèÂåñÈóÆÈ¢ò„ÄÇÂáΩÊï∞ simulate_eventf Â∞ùËØïÊåâÁÖß‰º†ÂÖ•ÁöÑ message_fmt ËøõË°åÂ≠óÁ¨¶‰∏≤Ê†ºÂºèÂåñÔºõÂΩìÈîôËØØÊ∂àÊÅØ‰∏≠ÂåÖÂê´Êú™Ë¢´ËΩ¨‰πâÁöÑÊ†ºÂºèÂåñÂç†‰ΩçÁ¨¶Ôºà‰æãÂ¶Ç %sÔºâËÄåÊ≤°ÊúâÊèê‰æõÁõ∏Â∫îÁöÑÂèÇÊï∞Êó∂ÔºåÂ∞±‰ºöÂá∫Áé∞Ê†ºÂºèÂåñÈîôËØØÔºå‰ªéËÄåÊ®°Êãü Issue ‰∏≠ÊèèËø∞ÁöÑÁé∞Ë±°„ÄÇ  
Áî±‰∫éÊú¨ Issue Âπ∂ÈùûÂÆâÂÖ®ÈóÆÈ¢òÔºåÊâÄ‰ª•Â§çÁé∞ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫ÈîôËØØÁöÑÊó•ÂøóÊ†ºÂºèÂåñË°å‰∏∫Ôºå‰∏çÊ∂âÂèäÂÆâÂÖ®ÊîªÂáªÊàñÊºèÊ¥ûÂà©Áî®„ÄÇÂÆûÈôÖÁéØÂ¢É‰∏≠Ôºå‰∏∫Èò≤Ê≠¢Êó•ÂøóÊ†ºÂºèÂåñÈîôËØØÔºåÂ∫îÁ°Æ‰øùÂØπÈîôËØØÊ∂àÊÅØ‰∏≠ÁöÑÁâπÊÆäÂ≠óÁ¨¶ËøõË°åÈÄÇÂΩìÂ§ÑÁêÜÔºåÂ¶ÇËΩ¨‰πâÊàñÁõ¥Êé•ÂΩìÂÅöÊôÆÈÄöÂ≠óÁ¨¶‰∏≤„ÄÇ

---


## Issue #129487 Using fake api client to query resources will cause the client-go package memory to continue to grow

- Issue ÈìæÊé•Ôºö[#129487](https://github.com/kubernetes/kubernetes/issues/129487)

### Issue ÂÜÖÂÆπ

#### What happened?

When I use fake api client in the simulated scheduler to simulate the resource changes of pods or nodes, the memory of the client-go package continues to increase.For example, after I create a pod, when I query this pod multiple times, the client-go memory continues to grow.

The problem I'm currently facing is that all actions such as query, modify, and delete will be appended to this c.actions slice, causing the client-go package memory to continue to increase until the process is restarted to release the memory.

``` go 
 func (c *Fake) Invokes(action Action, defaultReturnObj runtime.Object) (runtime.Object, error) {

	c.Lock()
	defer c.Unlock()

	actionCopy := action.DeepCopy()
	c.actions = append(c.actions, action.DeepCopy())
	for _, reactor := range c.ReactionChain {
		if !reactor.Handles(actionCopy) {
			continue
		}

		handled, ret, err := reactor.React(actionCopy)
		if !handled {
			continue
		}

		return ret, err
	}

	return defaultReturnObj, nil
}
```

#### What did you expect to happen?


Fix the code here: c.actions = append(c.actions, action.DeepCopy()) , where actions such as add, delete, modify, and query are not continuously appended to a.action, causing the memory to continue to grow.

#### How can we reproduce it (as minimally and precisely as possible)?

``` go
import (
	"context"
	"fmt"
	"net/http"
	"time"

	_ "net/http/pprof"

	"github.com/gofrs/uuid"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	fakeclientset "k8s.io/client-go/kubernetes/fake"
)

func main() {
	go func() {
		fmt.Println(http.ListenAndServe("localhost:8065", nil))
	}()

	// ÂàõÂª∫fakeclient
	fc := fakeclientset.NewSimpleClientset()
	for i := 0; i < 100000; i++ {
		// ÈöèÊú∫ÁîüÊàêpodÁöÑname
		uuidName, _ := uuid.NewV4()
		podName := "test-" + uuidName.String()
		// ÈöèÊú∫ÂàõÂª∫pod
		pod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: podName}}
		// ÂàõÂª∫pod
		fc.CoreV1().Pods("default").Create(context.TODO(), pod, metav1.CreateOptions{})
		pod.Spec.NodeName = "node1"
		// Êõ¥Êñ∞pod
		fc.CoreV1().Pods("default").Update(context.TODO(), pod, metav1.UpdateOptions{})
		// Âà†Èô§pod
		err := fc.CoreV1().Pods("default").Delete(context.TODO(), podName, metav1.DeleteOptions{})
		if err != nil {
			fmt.Println("delete pod failed")
		}
	}
}
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ‰ΩøÁî® fake API Client Êó∂ÔºåÁî±‰∫éÊØèÊ¨°Ë∞ÉÁî®ÔºàÂ¢û„ÄÅÂà†„ÄÅÊîπ„ÄÅÊü•ÔºâÈÉΩ‰ºöÊääÊìç‰ΩúËÆ∞ÂΩïÈÄöËøá DeepCopy ÂêéËøΩÂä†Âà∞ c.actions ÂàáÁâá‰∏≠ÔºåÂØºËá¥ client-go ÂåÖÁöÑÂÜÖÂ≠ò‰∏çÊñ≠Â¢ûÈïø„ÄÇËøô‰∏™ÈóÆÈ¢òÊú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™ÂÜÖÂ≠òÊ≥ÑÈú≤ bugÔºå‰∏ªË¶ÅÂΩ±ÂìçÁöÑÊòØÊµãËØïÊ®°ÊãüÁéØÂ¢ÉÔºàsimulated schedulerÔºâÔºåËÄåÈùûÁîü‰∫ßÁéØÂ¢É‰∏≠ÁúüÂÆû API ÂÆ¢Êà∑Á´ØÁöÑË°å‰∏∫„ÄÇ

‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåËØ•ÈóÆÈ¢ò‰∏ç‰ºöÁõ¥Êé•ÂØºËá¥ÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉÁ≠âÊîªÂáªÔºå‰πü‰∏ç‰ºöÂØºËá¥ÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤„ÄÇÂç≥‰ΩøÂá∫Áé∞ÂÜÖÂ≠òÊ≥ÑÈú≤ÔºåÂØºËá¥ËøõÁ®ãÂÜÖÂ≠ò‰∏çÊñ≠Â¢ûÈïøÔºåÈÄöÂ∏∏‰πüÂè™‰ºöÂºïËµ∑ËµÑÊ∫êËÄóÂ∞ΩÁöÑÊãíÁªùÊúçÂä°ÔºàDoSÔºâÈóÆÈ¢ò„ÄÇËÄå‰∏îÁî±‰∫éËØ•ÈóÆÈ¢òÂá∫Áé∞Âú® fake client Áî®‰∫éÊµãËØïÁöÑÂú∫ÊôØ‰∏ãÔºå‰∏çÂ≠òÂú®Â§ñÈÉ®ÊîªÂáªËÄÖÊó†ÈúÄÊùÉÈôêÂç≥Ëß¶ÂèëÁöÑÊÉÖÂÜµ„ÄÇÂõ†Ê≠§ÔºåËØ• Issue ÊèèËø∞ÁöÑÈóÆÈ¢ò‰∏çÁõ¥Êé•ÊûÑÊàêÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Ëß£ÈáäËØ¥ÊòéÔºö**

Êú¨ËÑöÊú¨ÔºàÊ≠§Â§Ñ‰∏∫Á©∫ÔºâÂéüÊú¨Áî®‰∫éÂ§çÁé∞ÂÆâÂÖ®ÈóÆÈ¢òÔºå‰ΩÜÊåâÁÖßÂà§Êñ≠Ê†áÂáÜÔºåÊ≠§ Issue ÂØºËá¥ÁöÑÂÜÖÂ≠òÊ≥ÑÈú≤Âπ∂‰∏çÁõ¥Êé•ÊûÑÊàêÂÆâÂÖ®È£éÈô©„ÄÇÂÜÖÂ≠òÊ≥ÑÈú≤ÊâÄÂºïÂèëÁöÑËµÑÊ∫êËÄóÂ∞ΩÈóÆÈ¢òÂú®ËØ•Âú∫ÊôØ‰∏≠‰∏ªË¶ÅÂΩ±ÂìçÁöÑÊòØÊµãËØïÁéØÂ¢ÉÔºåÂπ∂‰∏îÊîªÂáªËÄÖÈÄöÂ∏∏Êó†Ê≥ïÈÄöËøáËß¶ÂèëËØ•ÈóÆÈ¢òÂØπÁîü‰∫ßÁéØÂ¢ÉÈÄ†ÊàêÂÆûÈôÖÂÆâÂÖ®Â®ÅËÉÅ„ÄÇÂõ†Ê≠§‰∏çÁîüÊàêÁõ∏ÂÖ≥Â§çÁé∞‰ª£Á†Å„ÄÇÂØπ‰∫éÂÜÖÂ≠òÊ≥ÑÈú≤ÈóÆÈ¢òÔºåÂèØ‰ª•‰ªé‰ª£Á†ÅÈÄªËæëËßíÂ∫¶Ëß£ÂÜ≥Ôºö‰æãÂ¶Ç‰∏çÂú®ÊØèÊ¨°Êìç‰ΩúÊó∂Â∞ÜË°å‰∏∫Ê∞∏‰πÖ‰øùÂ≠òÔºåÊàñ‰ª•‰∏ÄÂÆöÊù°‰ª∂Ê∏ÖÁêÜËøáÊúüÁöÑÊìç‰ΩúËÆ∞ÂΩïÔºå‰ªéËÄåÈÅøÂÖçÂÜÖÂ≠ò‰∏çÊñ≠Á¥ØÁßØ„ÄÇ

---


## Issue #129481 Can't mount iscsi volume with openiscsi from target that does not automatically inform your initiator about changes in that session.

- Issue ÈìæÊé•Ôºö[#129481](https://github.com/kubernetes/kubernetes/issues/129481)

### Issue ÂÜÖÂÆπ

#### What happened?

Can't mount iscsi volume in pod. 

I'm creating pod with iscsi volume from NAS using config:
```
---
apiVersion: v1
kind: Pod
metadata:
  name: iscsipd2
spec:
  containers:
  - name: iscsipd-ro
    image: nginx
    volumeMounts:
    - mountPath: "/mnt/iscsipd"
      name: iscsipd-rw
  volumes:
  - name: iscsipd-rw
    iscsi:
      targetPortal: <portalhost/port>
      iqn: <IQN>
      lun: 1
      fsType: ext4
      readOnly: false
```

Pod fails to start reporting:
```
  Warning  FailedMount             2s (x7 over 3m28s)  kubelet                  MountVolume.WaitForAttach failed for volume "iscsipd-rw" : failed to get any path for iscsi disk, last err seen:
Timed out waiting for device at path /dev/disk/by-path/<portal-IQN-Lun> after 30s
```

iSCSI server is Synology NAS which uses open iscsi 2.0-876.

It is possible to mount LUNs onto the host itself manually.

#### What did you expect to happen?

Volume mounted.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproducing could be tricky if you don't have version of iscsi that shows similar behaviour.


When kubelet mounts iSCSI volume it executes the following command sequence
(starts here https://github.com/kubernetes/kubernetes/blob/642efbb595df18bcac54e1e53ab3d1d4df1569aa/pkg/volume/iscsi/iscsi_util.go#L339)

```
iscsiadm -m discoverydb -t sendtargets -p <portal> -I default -o new
iscsiadm -m discoverydb -t sendtargets -p <portal> -I default --discover
iscsiadm -m node -p <portal> -T <target> -I default -o update -n node.session.scan -v manual
iscsiadm -m node -p <portal> -T <target> -I default --login
iscsiadm -m node -p <portal> -T <target> -o update -n node.startup -v manual
```
After this it will wait for device path to appear once iscsid service on the node attaches to LUN. This unfortunately doesn't happen. The reason is `node.session.scan` being set to `manual` which was introduced in #90982 to address different issue. It is happening because my server doesn't notify initiator about new luns in session when `manual` option is set.

The solution is to rescan active iSCSI session to discover LUN which would trigger device file creation.
The test was to just execute
```
sudo iscsiadm -m node -p <...> -T <...> -I default -R
```
while `kubelet` was waiting for device file.
I then added
```
diff --git a/pkg/volume/iscsi/iscsi_util.go b/pkg/volume/iscsi/iscsi_util.go
index b51127aa92f..239cb075aef 100644
--- a/pkg/volume/iscsi/iscsi_util.go
+++ b/pkg/volume/iscsi/iscsi_util.go
@@ -376,6 +376,12 @@ func (util *ISCSIUtil) AttachDisk(b iscsiDiskMounter) (string, error) {
                                        klog.Warningf("Warning: Failed to set iSCSI login mode to manual. Error: %v", err)
                                }
 
+                               // For targets that don't notify initiator about LUNs added to session we need to trigger recans
+                               _, err = execWithLog(b, "iscsiadm", "-m", "node", "-p", tp, "-T", b.Iqn, "-I", b.Iface, "-R")
+                               if err != nil {
+                                       klog.Warningf("Warning: Failed to trigger rescan of LUNs in session. Error: %v", err)
+                               }
+
                                // Rebuild the host map after logging in
                                portalHostMap, err := b.deviceUtil.GetISCSIPortalHostMapForTarget(b.Iqn)
                                if err != nil {

```
to `kubelet` unconditionally after `--login` command to fix the issue. While it works, I'm sure there might be cases where this behaviour is undesirable, some internet people mentioned rescan being slow. Nevertheless it would be nice to have an option to do rescan either based on config or when mount can't be found within certain time threshold.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux xxx 6.8.0-51-generic #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
Ëøô‰∏™ Issue ÊèèËø∞‰∫ÜÂú® Kubernetes ‰∏≠‰ΩøÁî® iSCSI Âç∑Êó∂Âá∫Áé∞ÊåÇËΩΩÂ§±Ë¥•ÁöÑÈóÆÈ¢òÔºåÂéüÂõ†Âú®‰∫é openiscsi 2.0-876 ÁâàÊú¨ÂØπÁõÆÊ†áËÆæÂ§áÊú™ËÉΩ‰∏ªÂä®ÈÄöÁü• initiator ÂÖ≥‰∫é LUN ÂèòÂåñÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËØ•ÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜÂú® kubelet ÊâßË°å iscsiadm ÁôªÂΩïÂêéÁ´ãÂç≥Ëß¶Âèë‰∏ÄÊ¨° rescanÔºàÂç≥ÊâßË°å iscsiadm¬†-RÔºâÁöÑÊñπÊ°àÔºå‰ª•‰æøÈáçÊñ∞Êâ´Êèè‰ºöËØù‰∏≠ÁöÑ LUN„ÄÇËØ•ÈóÆÈ¢òÂèçÊò†ÁöÑÊòØÂ≠òÂÇ®ÊåÇËΩΩÂíåËÆæÂ§áÂèëÁé∞ÊµÅÁ®ã‰∏≠ÁöÑÂäüËÉΩÊÄßÁº∫Èô∑ÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÂÖ∂Êú¨Ë¥®‰∏ä‰∏ç‰ºöÂØºËá¥Êï∞ÊçÆÊ≥ÑÈú≤„ÄÅÊú™ÊéàÊùÉÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏Á≠âÂÆâÂÖ®È£éÈô©Ôºå‰πü‰∏çÂÖ∑Â§áÊÅ∂ÊÑèÂà©Áî®ÁöÑÊîªÂáªÈù¢„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Áî±‰∫éËØ• Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©Ôºå‰∏ãÈù¢ÁöÑËÑöÊú¨‰ªÖÁî®‰∫éÊ®°Êãü iSCSI ÊåÇËΩΩËøáÁ®ã‰∏≠Ëß¶Âèë rescan ÁöÑÂäüËÉΩÊÄßÂ§çÁé∞Ôºå
# ËØ∑Âú®ÂÆâÂÖ®ÊµãËØïÁéØÂ¢É‰∏≠‰ΩøÁî®ËØ•ËÑöÊú¨ÔºåÂàáÂãøÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Áõ¥Êé•ËøêË°å„ÄÇ

import subprocess
import time
import logging
from http.server import BaseHTTPRequestHandler, HTTPServer
import threading

# ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁî®‰∫éÁõëÂê¨8080Á´ØÂè£ÔºàÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±ÇÔºâ
class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b'ISCIS rescan simulation server running')

def start_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, SimpleHTTPRequestHandler)
    logging.info("HTTP server starting on port 8080")
    httpd.serve_forever()

def run_command(command):
    logging.info("ÊâßË°åÂëΩ‰ª§: %s", " ".join(command))
    try:
        # Âú®ÊµãËØïÁéØÂ¢É‰∏≠Ôºåiscsiadm ÂëΩ‰ª§ÂèØËÉΩÊó†Ê≥ïÊ≠£Â∏∏ËøêË°åÔºåÂõ†Ê≠§‰ΩøÁî® shell Ë∞ÉÁî®Ê®°ÊãüÊÉÖÂÜµ
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=30)
        if result.stdout:
            logging.info("ËæìÂá∫: %s", result.stdout.strip())
        if result.stderr:
            logging.warning("ÈîôËØØ: %s", result.stderr.strip())
        return result.stdout, result.stderr
    except subprocess.TimeoutExpired as e:
        logging.error("ÂëΩ‰ª§Ë∂ÖÊó∂: %s", e)
        return None, str(e)

def simulate_iscsi_mount(target_portal, iqn, lun, iface="default"):
    # Ê®°Êãü iSCSI ÂèëÁé∞ÂíåÁôªÂΩïËøáÁ®ã
    cmds = [
        ["iscsiadm", "-m", "discoverydb", "-t", "sendtargets", "-p", target_portal, "-I", iface, "-o", "new"],
        ["iscsiadm", "-m", "discoverydb", "-t", "sendtargets", "-p", target_portal, "-I", iface, "--discover"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "-o", "update", "-n", "node.session.scan", "-v", "manual"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "--login"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-o", "update", "-n", "node.startup", "-v", "manual"],
    ]
    for command in cmds:
        run_command(command)
        time.sleep(1)  # Ê®°ÊãüÂëΩ‰ª§Èó¥ÁöÑÂª∂Êó∂

    # Ê®°Êãü kubelet Á≠âÂæÖËÆæÂ§áËäÇÁÇπÁîüÊàêÁöÑËøáÁ®ãÔºåÊúüÈó¥ÈÄöËøáËß¶Âèë rescan ËÑöÊú¨ÈÄöÁü• LUN ÁöÑÊõ¥Êñ∞
    logging.info("Á≠âÂæÖËÆæÂ§áËäÇÁÇπÁîüÊàê...")
    device_found = False
    for attempt in range(10):
        # Ê≠§Â§ÑÂàÜÊîØ‰ªÖ‰ΩúÊ®°ÊãüÊ£ÄÊü•ÔºåÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠Â∫îÊ£ÄÊµã /dev/disk/by-path/ ‰∏ãÊòØÂê¶Â≠òÂú®ËÆæÂ§áÊñá‰ª∂
        logging.info("Â∞ùËØï %d: Êú™Ê£ÄÊµãÂà∞ÂØπÂ∫îËÆæÂ§áÔºåËß¶Âèë rescan Êìç‰Ωú...", attempt + 1)
        rescan_cmd = ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "-R"]
        run_command(rescan_cmd)
        time.sleep(3)
        # Ê®°ÊãüËÆæÂ§áÂú®Á¨¨5Ê¨°Â∞ùËØïÂêéÂá∫Áé∞
        if attempt == 4:
            device_found = True
            logging.info("ËÆæÂ§áËäÇÁÇπÊ®°ÊãüÂá∫Áé∞")
            break

    if not device_found:
        logging.warning("Ë∂ÖÊó∂Êú™Ê£ÄÊµãÂà∞ËÆæÂ§áËäÇÁÇπ")
    else:
        logging.info("iSCSI ÊåÇËΩΩÊµÅÁ®ãÊ®°ÊãüÊàêÂäü")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # ÂêØÂä® HTTP ÊúçÂä°Âô®Á∫øÁ®ãÔºàÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±ÇÔºåÁõëÂê¨8080Á´ØÂè£Ôºâ
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # ÈÖçÁΩÆÊµãËØïÂèÇÊï∞ÔºàÊ†πÊçÆÊµãËØïÁéØÂ¢ÉËøõË°åË∞ÉÊï¥Ôºâ
    target_portal = "192.168.1.100:3260"         # Á§∫‰æãÁõÆÊ†áÈó®Êà∑Âú∞ÂùÄ
    iqn = "iqn.2001-04.com.example:storage.disk1"  # Á§∫‰æã IQN
    lun = "1"

    simulate_iscsi_mount(target_portal, iqn, lun)

if __name__ == '__main__':
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨‰∏≠ÂÆö‰πâ‰∫Ü simulate_iscsi_mount ÂáΩÊï∞ÔºåÁî®Êù•Ê®°Êãü Kubernetes ‰∏≠ÊåÇËΩΩ iSCSI Âç∑Êó∂ÊâßË°åÁöÑ‰∏ÄÁ≥ªÂàó iscsiadm ÂëΩ‰ª§ÔºåÂåÖÊã¨ÂèëÁé∞ÁõÆÊ†á„ÄÅÊõ¥Êñ∞‰ºöËØùÊâ´ÊèèÊ®°Âºè„ÄÅÁôªÂΩï‰ª•ÂèäËÆæÁΩÆÂêØÂä®Ê®°ÂºèÁ≠âÊ≠•È™§„ÄÇ
2. Âú®Á≠âÂæÖËÆæÂ§áÁîüÊàêÁöÑËøáÁ®ã‰∏≠ÔºåÈÄöËøáÂæ™ÁéØÊØèÈöîÂá†ÁßíËß¶Âèë‰∏ÄÊ¨° iscsiadm ÁöÑ rescan ÂëΩ‰ª§Ôºà-R ÂèÇÊï∞ÔºâÔºåÊ®°ÊãüËß¶Âèë LUN ÂèòÂåñÈÄöÁü•Ôºå‰ªéËÄå‰ΩøËÆæÂ§áÊñá‰ª∂ÁîüÊàê„ÄÇ
3. ÂêåÊó∂ÔºåÊª°Ë∂≥È¢òÁõÆË¶ÅÊ±ÇÔºåËÑöÊú¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨ 8080 Á´ØÂè£„ÄÇ
4. Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫ÊåÇËΩΩÊµÅÁ®ãÁöÑÂäüËÉΩÊÄßÂ§çÁé∞ÔºåÂπ∂ÈùûÈíàÂØπÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂÆûÈôÖÁéØÂ¢É‰∏≠ËøêË°å iscsiadm ÂëΩ‰ª§ÈúÄË¶ÅÁÆ°ÁêÜÂëòÊùÉÈôêÔºåÂπ∂‰∏îËØ∑Âú®ÂÆâÂÖ®ÁöÑÊµãËØïÁéØÂ¢É‰∏ãÈ™åËØÅÔºå‰∏çÂ∫îÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Áõ¥Êé•‰ΩøÁî®„ÄÇ

---


## Issue #129478 Cannot start local cluster by local-up-cluster.sh with error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6

- Issue ÈìæÊé•Ôºö[#129478](https://github.com/kubernetes/kubernetes/issues/129478)

### Issue ÂÜÖÂÆπ

#### What happened?

I tried to start local cluster with kubernetes source code with command ./hack/local-up-cluster.sh, and below is the log:

root@k8s:~/go/src/k8s.io/kubernetes# ./hack/local-up-cluster.sh -O
skipped the build because GO_OUT was set (/root/go/src/k8s.io/kubernetes/_output/bin)
API SERVER secure port is free, proceeding...
Detected host and ready to start services.  Doing some housekeeping first...
Using GO_OUT /root/go/src/k8s.io/kubernetes/_output/bin
Starting services now!
Starting etcd
etcd --advertise-client-urls http://127.0.0.1:2379 --data-dir /tmp/tmp.BHf3mU3o3Y --listen-client-urls http://127.0.0.1:2379 --log-level=warn 2> "/tmp/etcd.log" >/dev/null
Waiting for etcd to come up.
+++ [0105 13:33:05] On try 2, etcd: : {"health":"true","reason":""}
{"header":{"cluster_id":"14841639068965178418","member_id":"10276657743932975437","revision":"2","raft_term":"2"}}.+...+...+...+.+........+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...+...+..+.+......+........+............+..........+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.........+.+.....+......+.+..+.....................+....+..+.+..+...+.........+..................+......+.+......+..+.+..............+..........+...........+.+...+...+........+....+...+............+........+....+...+......+.....+.+.........+...........+.+..+.+.........+..+................+.....+...+......+..........+........+.+..............+.......+...+..+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
...........+..........+..+....+...+..+.+..+.+......+.....+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...........+.........+..+.+..+...+.......+...+........+....+..+.+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...+.................+....+............+..+...+..........+....................+.........+....+.........+...+........+......+...+.+......+........+.+..+...+................+.........+..+.........+.+..+...+....+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
...+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...........+.....+.............+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+......+..+.+.................+...+....+............+...+...+.....+...+......+....+...........+...+.+......+......+...+......+......+.........+........+.+..+...+.......+.....+..........+...............+.........+...+.....+......+...............+.......+...+.....+......+...+.+.........+..+.+..+...+...+.+......+...+...........+.+..+.......+...............+..............+................+........+.+..+.........................+.....+.........+....+..+............+.+...............+...........+...............+..........+.....+...+............+.+...+..+...+....+..+...+..........+............+...+...+..+.+.........+...........+.+.....+...+......+....+..+..........+...+.................+.+..................+..............+.+..+.........+.+.........+.....+...+...+...+....+...........+.+...+...+...+.....+....+...+.........+........+......+.......+...+......+.....+.+..+.+...............+.........+.....+..........+..+.............+..+....+..+.........+...................+..+...+.......+...+.....+......+.+............+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
.......+....+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*....+.......+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+.........+..........+...............+...+...........+......+....+.....+.+......+..+...............+..........+..+....+.........+............+.....+.........+.+......+........+.......+...+............+...+.....+.+...+..+................+...+.........+..+...+.......+...+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
..............+...+.+..+.......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+...+.+...+..+...+.............+..+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+........+......+.........+......+....+............+..+.+...........+...+..........+...........+.+..............+.+.....+.+.................+.............+...+.....+......+.+............+..+....+.....+....+.....+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
..+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+........+.+.....+......+...+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+...............+..+...+......+..........+......+.....+.........+....+......+...+..+....+..+.............+..+...+....+..+....+..............+..........+...+.....+..................+....+..+....+..............+...+............+................+...........+.+..+...+....+...+...+.....+.........+....+..+............+............+.........+.+........+...+.........+.+...+.....+...............+....+..................+...+.....+.......+..+.......+.....+....+.........+.....+.+..+...+......+..........+......+..+..........+..+...+...+..........+...+..+.......+..+....+......+...+........+............+.+..+..........+...+.....+.......+..+...+....+......+..+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:07 [INFO] encoded CSR
2025/01/05 13:33:07 [INFO] signed certificate with serial number 440423749374281788744993987946487911942798734629
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:07 [INFO] encoded CSR
2025/01/05 13:33:07 [INFO] signed certificate with serial number 177798747956492092670340101307886619962413397079
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:08 [INFO] encoded CSR
2025/01/05 13:33:08 [INFO] signed certificate with serial number 245616362482589881504825093782699871584686084179
2025/01/05 13:33:08 [INFO] generate received request
2025/01/05 13:33:08 [INFO] received CSR
2025/01/05 13:33:08 [INFO] generating key: rsa-2048
2025/01/05 13:33:08 [INFO] encoded CSR
2025/01/05 13:33:08 [INFO] signed certificate with serial number 170369636513810095510638438376919261494891756788
2025/01/05 13:33:08 [INFO] generate received request
2025/01/05 13:33:08 [INFO] received CSR
2025/01/05 13:33:08 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 603887759708125829878376814401298784287701864340
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 245882740224964498161208602587469908668758382368
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 440315627455551203707791282420989785578941371586
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 203859729277878650212404260018964069399575044648
Waiting for apiserver to come up
+++ [0105 13:33:14] On try 4, apiserver: : ok
clusterrolebinding.rbac.authorization.k8s.io/kube-apiserver-kubelet-admin created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-csr created
Cluster "local-up-cluster" set.
use 'kubectl --kubeconfig=/var/run/kubernetes/admin-kube-aggregator.kubeconfig' to use the aggregated API server
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
coredns addon successfully deployed.
Checking CNI Installation at /opt/cni/bin
WARNING : The kubelet is configured to not fail even if swap is enabled; production deployments should disable swap unless testing NodeSwap feature.
2025/01/05 13:33:16 [INFO] generate received request
2025/01/05 13:33:16 [INFO] received CSR
2025/01/05 13:33:16 [INFO] generating key: rsa-2048
2025/01/05 13:33:17 [INFO] encoded CSR
2025/01/05 13:33:17 [INFO] signed certificate with serial number 119919369832654757525409624483929365727736165
kubelet ( 51055 ) is running.
wait kubelet ready
No resources found
No resources found
127.0.0.1   NotReady   <none>   1s    v1.33.0-alpha.0.308+7bfdda4696f78f
node/127.0.0.1 condition met
2025/01/05 13:33:26 [INFO] generate received request
2025/01/05 13:33:26 [INFO] received CSR
2025/01/05 13:33:26 [INFO] generating key: rsa-2048
2025/01/05 13:33:26 [INFO] encoded CSR
2025/01/05 13:33:26 [INFO] signed certificate with serial number 39785364796214418872404472513236782332099419659
pod "coredns-f5bd749cf-nbkjk" deleted
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6

#### What did you expect to happen?

As others example, start up local cluster successfully

#### How can we reproduce it (as minimally and precisely as possible)?

This experiment is on win10 virtual machine VirtualBox 7.1.4 r164100
OS is Ubuntu 24.04.1 LTS
git describe: v1.33.0-alpha.0-308-g7bfdda4696f
git branche: master

#### Anything else we need to know?

_No response_

#### Kubernetes version

git describe: v1.33.0-alpha.0-308-g7bfdda4696f
git branche: master

#### Cloud provider

VM on local machine
OS is Ubuntu 24.04.1 LTS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÈóÆÈ¢òÊòØÂú®‰ΩøÁî® Kubernetes Ê∫êÁ†ÅÂêØÂä®Êú¨Âú∞ÈõÜÁæ§Êó∂ÔºåÂá∫Áé∞‰∫ÜË∂ÖÊó∂Á≠âÂæÖ podÔºàcorednsÔºâÂ∞±Áª™ÁöÑÈîôËØØ„ÄÇÊó•Âøó‰∏≠ÊòæÁ§∫ÁöÑÂêÑÈ°πÊìç‰ΩúÔºàÂ¶Ç etcd ÂêØÂä®„ÄÅCSRÔºàËØÅ‰π¶Á≠æÂèëÔºâËØ∑Ê±ÇÂ§ÑÁêÜÁ≠âÔºâÂùá‰∏∫Ê≠£Â∏∏ÁöÑÈõÜÁæ§ÂêØÂä®ÊµÅÁ®ã‰∏≠ÁöÑÊó•Âøó‰ø°ÊÅØÔºåÊ≤°ÊúâÂá∫Áé∞ÂºÇÂ∏∏ÁöÑÊïèÊÑüÊï∞ÊçÆÊ≥ÑÈú≤ÊàñÊúçÂä°ÂºÇÂ∏∏Â§ÑÁêÜ„ÄÇÊï¥‰ΩìÊù•ÁúãÔºåËøôÊòØÈõÜÁæ§ÂêØÂä®ËøáÁ®ã‰∏≠ÈÅáÂà∞ÁöÑÊâßË°åÂíåÈÄö‰ø°Ë∂ÖÊó∂ÈóÆÈ¢òÔºåÂ±û‰∫éËøêÁª¥ÊàñÁéØÂ¢ÉÈÖçÁΩÆÁöÑ bugÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÊàñÂà©Áî®È£éÈô©„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°ÊãüË∞ÉÁî® Kubernetes Ê∫êÁ†Å‰∏≠ÁöÑ local-up-cluster.sh ËÑöÊú¨Ôºå
‰ª•Â§çÁé∞ issue ‰∏≠ÊèèËø∞ÁöÑÂêØÂä®Ë∂ÖÊó∂ÈóÆÈ¢ò„ÄÇ
Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊú¨Âú∞ÊµãËØïÔºå‰∏ç‰ºöÂØπÁ≥ªÁªüÂÆâÂÖ®ÊÄßÊûÑÊàêÂΩ±ÂìçÔºå
‰∏îÈóÆÈ¢òÊú¨Ë¥®‰∏∫ÈõÜÁæ§ÂêØÂä®Â§±Ë¥•ÁöÑ bugÔºåËÄåÈùûÂÆâÂÖ®È£éÈô©„ÄÇ
"""

import subprocess
import sys

def start_local_cluster():
    try:
        # ÂÅáËÆæÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï‰∏∫ Kubernetes Ê∫êÁ†ÅÊ†πÁõÆÂΩïÔºå
        # ‰∏î hack/local-up-cluster.sh ËÑöÊú¨Â≠òÂú®‰∏îÂÖ∑Â§áÊâßË°åÊùÉÈôê„ÄÇ
        # Ê≠§Â§Ñ‰º†ÂÖ• '-O' ÂèÇÊï∞Áî®‰∫éË∑≥ËøáÁºñËØëÊ≠•È™§„ÄÇ
        print("ÂºÄÂßãÂêØÂä®Êú¨Âú∞ÈõÜÁæ§...")
        result = subprocess.run(['./hack/local-up-cluster.sh', '-O'], capture_output=True, text=True, timeout=300)
        print("ÂêØÂä®ËÑöÊú¨Ê†áÂáÜËæìÂá∫:")
        print(result.stdout)
        print("ÂêØÂä®ËÑöÊú¨ÈîôËØØËæìÂá∫:")
        print(result.stderr)
    except subprocess.TimeoutExpired:
        print("ÂêØÂä®ËøõÁ®ãË∂ÖÊó∂ÔºåËØ∑Ê£ÄÊü•ÁéØÂ¢ÉÊàñÈõÜÁæ§Êó•Âøó‰ª•Ëé∑ÂèñËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ")
    except Exception as e:
        print(f"ËøêË°åËøáÁ®ã‰∏≠ÂèëÁîüÂºÇÂ∏∏Ôºö{e}")

def main():
    start_local_cluster()

if __name__ == '__main__':
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Ê≠§ Python ËÑöÊú¨Âà©Áî® subprocess Ê®°ÂùóË∞ÉÁî® Kubernetes È°πÁõÆ‰∏≠ÁöÑ local-up-cluster.sh ÂêØÂä®ËÑöÊú¨ÔºåÊ®°Êãü issue ‰∏≠ÁöÑÂ§çÁé∞ÁéØÂ¢É„ÄÇËÑöÊú¨‰∏≠ËÆæÁΩÆ‰∫ÜË∂ÖÊó∂Êó∂Èó¥ÔºåÊçïËé∑Ê†áÂáÜËæìÂá∫ÂíåÈîôËØØËæìÂá∫ÔºåÊñπ‰æøËßÇÂØüÈõÜÁæ§ÂêØÂä®ËøáÁ®ã‰∏≠ÊòØÂê¶Âá∫Áé∞Á±ª‰ºº‚Äútimed out waiting for the condition on pods/coredns-...‚ÄùÁöÑÈîôËØØ„ÄÇ  
ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåissue ÊèèËø∞ÁöÑÈóÆÈ¢òÂ±û‰∫éÈõÜÁæ§ÂêØÂä®Â§±Ë¥•ÔºàÊìç‰ΩúÊàñÈÖçÁΩÆÈóÆÈ¢òÔºâÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥ûÔºåÊâÄ‰ª•È£éÈô©ËØÑÁ∫ßÂÆö‰πâ‰∏∫‚Äú‰∏çÊ∂âÂèä‚Äù„ÄÇÂõ†Ê≠§ÔºåÊú¨ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÁéØÂ¢ÉÈóÆÈ¢òÔºå‰∏çÊ∂âÂèä‰ªª‰ΩïÂÆâÂÖ®È£éÈô©ÊàñÊºèÊ¥ûÂ§çÁé∞„ÄÇ

---


## Issue #129475 projected serviceAccountToken do not honour defaultMode or readOnly: true (tested in 1.30)

- Issue ÈìæÊé•Ôºö[#129475](https://github.com/kubernetes/kubernetes/issues/129475)

### Issue ÂÜÖÂÆπ

#### What happened?

We have a container which needs to start as root today (because we install packages, mount a docker socket and the like). But then we change uid to a lower privilege user for the rest of time. 

That user needs access to a projected serviceAccountToken to access another service. The user cannot read the file because its mode is rw-------.

So, we set defaultMode on the projected volume definition, but this has no effect.

We also had readOnly: true in the mount definition, and that seems to not have effect either, as the mode was still rw------- not r--------.

<details>

<summary>Sample Pod definitions</summary>

```
apiVersion: v1
kind: Pod
metadata:
  name: with-mode
  namespace: robertc-scratch
spec:
      containers:
      - command:
        - sleep
        - "604800"
        image: ubuntu
        imagePullPolicy: IfNotPresent
        name: test
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 0
        volumeMounts:
        - mountPath: /secrets/token
          name: token
      volumes:
      - name: token
        projected:
          defaultMode: 292
          sources:
          - serviceAccountToken:
              path: token
---
apiVersion: v1
kind: Pod
metadata:
  name: with-readonly
  namespace: robertc-scratch
spec:
      containers:
      - command:
        - sleep
        - "604800"
        image: ubuntu
        imagePullPolicy: IfNotPresent
        name: test
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 0
        volumeMounts:
        - mountPath: /secrets/token
          name: token
          readOnly: true
      volumes:
      - name: token
        projected:
          sources:
          - serviceAccountToken:
              path: token

```

</details>

The test I did in making a SSCCE was to run this:
```
kubectl --namespace robertc-scratch exec -ti with-mode -- ls -l /secrets/token/..data/
total 4
-rw------- 1 root root 1414 Jan  3 17:45 token
@rbtcollins ‚ûú /workspaces/infrastructure (rbt/agent-permissions-3) $ kubectl --namespace robertc-scratch exec -ti with-mode -- ls -l /secrets/token/..data/
total 4
-rw------- 1 root root 1414 Jan  3 17:45 token
```

We checked the [documentation ](https://kubernetes.io/docs/concepts/storage/projected-volumes/#introduction) and it describes defaultMode with no restrictions on its relevance to difference sources. The API reference is no more useful.




#### What did you expect to happen?

I expected the defaultMode setting to apply to the serviceAccountToken. All the same reasons one might want a different mode for a projected secret or config map also apply to a projected serviceAccountToken.

#### How can we reproduce it (as minimally and precisely as possible)?

See above.

#### Anything else we need to know?

Probably not ;) .

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.27.16
Kustomize Version: v5.0.1
Server Version: v1.30.5-gke.1699000
```

</details>


#### Cloud provider

<details>
Google GKE
</details>


#### OS version

<details>

I don't have access to the node OS itself, sorry.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫Ü‰∏Ä‰∏™ Kubernetes ‰∏≠ projected volume ÁöÑÈóÆÈ¢òÔºåÂÖ∑‰ΩìÊù•ËØ¥ÔºöÂΩì‰ΩøÁî® serviceAccountToken ‰Ωú‰∏∫ projected volume Êó∂ÔºåÊó†ËÆ∫Âú®Âç∑ÂÆö‰πâ‰∏≠ËÆæÁΩÆ defaultMode ÊàñÂú® volumeMounts ‰∏≠ËÆæÁΩÆ readOnlyÔºåÈÉΩÊ≤°ÊúâÊîπÂèòÊñá‰ª∂ÁöÑÂÆûÈôÖÊùÉÈôêÔºà‰æùÁÑ∂ÊòØ 600ÔºåÂç≥ rw-------Ôºâ„ÄÇËøô‰∏™ÈóÆÈ¢òÂ±û‰∫é Kubernetes ÂäüËÉΩ‰∏äÁöÑÁº∫Èô∑Êàñ‰∏ç‰∏ÄËá¥ÊÄßÔºåÂØºËá¥Âú®È¢ÑÊúü‰∏≠Â∏åÊúõ‰ª•Êõ¥ÂÆΩÊùæÊùÉÈôêÔºà‰æãÂ¶ÇÂè™ËØªÔºâÊåÇËΩΩ token Êñá‰ª∂ÁöÑÂú∫ÊôØ‰∏ãÔºå‰ΩéÊùÉÈôêÁî®Êà∑Êó†Ê≥ïËØªÂèñËØ• tokenÔºå‰ªéËÄå‰ΩøÂÆπÂô®ÂÜÖÈôçÊùÉÊìç‰ΩúÂèóÂà∞ÂΩ±Âìç„ÄÇ

‰ªéÂÆâÂÖ®ËßíÂ∫¶ÁúãÔºåÊú¨ Issue Âπ∂‰∏çÂ≠òÂú®ÊΩúÂú®ÁöÑÂÆâÂÖ®ÊîªÂáªÈ£éÈô©„ÄÇ‰æãÂ¶ÇÔºö
1. Issue ‰∏≠ËÆ®ËÆ∫ÁöÑÈóÆÈ¢òÊòØÂÖ≥‰∫éÊñá‰ª∂ÊùÉÈôêÁöÑÈªòËÆ§ËÆæÁΩÆ‰∏çÁîüÊïàÔºåÂπ∂ÈùûÂÖÅËÆ∏ÊîªÂáªËÄÖÈÄöËøáÊú™ÊéàÊùÉËØªÂèñÊàñÂÜôÂÖ•ÊïèÊÑü‰ø°ÊÅØËøõË°åÊîªÂáªÔºõ
2. Êñá‰ª∂ÊùÉÈôêËÆæÁΩÆ‰∏∫ 600 ÂÆûÈôÖ‰∏äÊòØÊõ¥‰∏∫‰∏•Ê†ºÁöÑÈôêÂà∂ÔºåÂèçËÄåÈò≤Ê≠¢‰∫Ü‰∏çÂΩìÁöÑËÆøÈóÆÔºõ
3. Ê≠§ÈóÆÈ¢òÂØºËá¥ÁöÑÂΩ±Âìç‰∏ªË¶ÅÊòØÂäüËÉΩÊÄßÂíå‰ΩøÁî®‰∏äÁöÑ‰∏ç‰æøÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇÂõ†Ê≠§ÔºåËØ•ÈóÆÈ¢ò‰∏çÊûÑÊàêÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊèêÊùÉÁ≠âÈ´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

Âü∫‰∫é‰∏äËø∞ÂàÜÊûêÔºåÂ∞ΩÁÆ°ËØ•ÈóÆÈ¢òÂèØËÉΩ‰ºöÂΩ±ÂìçÊüê‰∫õÊ≠£Â∏∏‰ΩøÁî®Âú∫ÊôØÔºå‰ΩÜ‰ªéÂÆâÂÖ®È£éÈô©ËßíÂ∫¶Âà§Êñ≠ÔºåÈóÆÈ¢òÊú¨Ë∫´‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåËÄåÂ±û‰∫éÂäüËÉΩ Bug ÊàñË°å‰∏∫‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÁº∫Èô∑„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≠§ËÑöÊú¨‰ΩøÁî® Kubernetes Python ÂÆ¢Êà∑Á´ØÔºåÈÄöËøáÊâßË°å Pod ÂÜÖÁöÑ ls ÂëΩ‰ª§Ê£ÄÊµã
projected serviceAccountToken Êñá‰ª∂ÁöÑÊùÉÈôêÊòæÁ§∫ÊÉÖÂÜµÔºåÁî®‰∫éÂ§çÁé∞ Issue ‰∏≠ÊèèËø∞ÁöÑÈóÆÈ¢òÔºö
Âç≥ËÆæÁΩÆ‰∫Ü defaultMode Êàñ readOnly ÂêéÔºå‰æùÁÑ∂ÊòæÁ§∫ token Êñá‰ª∂ÁöÑÊùÉÈôê‰∏∫ rw------- (600)„ÄÇ
Ê≥®ÊÑèÔºöËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊú¨Âú∞ÊµãËØïÔºåÈúÄÁ°Æ‰øù kubeconfig Âú®ÈªòËÆ§‰ΩçÁΩÆ„ÄÇ
"""

import sys
import time
from kubernetes import client, config, stream

def main():
    # Âä†ËΩΩÈªòËÆ§ kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("Âä†ËΩΩ kubeconfig Â§±Ë¥•: {}".format(e))
        sys.exit(1)

    # ÂÆö‰πâÂëΩÂêçÁ©∫Èó¥Âíå Pod ÂêçÁß∞ÔºåÊ†πÊçÆ issue Á§∫‰æã‰ΩøÁî®
    namespace = "robertc-scratch"
    pod_name = "with-mode"  # ‰πüÂèØ‰ª•Â∞ùËØï with-readonly ÂØπÊØî

    # ÂàõÂª∫ API ÂÆû‰æã
    core_v1 = client.CoreV1Api()

    # Ê£ÄÊü• Pod ÊòØÂê¶Â≠òÂú®
    try:
        pod = core_v1.read_namespaced_pod(name=pod_name, namespace=namespace)
    except client.exceptions.ApiException as e:
        print("ËØªÂèñPodÊó∂Âá∫Èîô: {}".format(e))
        sys.exit(1)

    # Á≠âÂæÖ Pod Â§Ñ‰∫é Running Áä∂ÊÄÅ
    print("Á≠âÂæÖ Pod [{}] Â§Ñ‰∫é Running Áä∂ÊÄÅ...".format(pod_name))
    for i in range(30):
        pod = core_v1.read_namespaced_pod(name=pod_name, namespace=namespace)
        if pod.status.phase == "Running":
            break
        time.sleep(2)
    else:
        print("Pod Êú™ËÉΩÂú®È¢ÑÊúüÊó∂Èó¥ÂÜÖËøõÂÖ• Running Áä∂ÊÄÅ")
        sys.exit(1)

    # ‰ΩøÁî® stream ÊâßË°å exec ÂëΩ‰ª§Êü•Áúã token Êñá‰ª∂ÁöÑÊùÉÈôê
    exec_command = [
        '/bin/sh',
        '-c',
        'ls -l /secrets/token/..data/'
    ]

    try:
        print("ÂºÄÂßãÊâßË°å exec ÂëΩ‰ª§‰ª•Êü•Áúã token Êñá‰ª∂ÊùÉÈôê...")
        resp = stream.stream(core_v1.connect_get_namespaced_pod_exec,
                             pod_name,
                             namespace,
                             command=exec_command,
                             stderr=True, stdin=False,
                             stdout=True, tty=False)
        print("ÂëΩ‰ª§ËæìÂá∫:")
        print(resp)
    except Exception as e:
        print("ÊâßË°å exec ÂëΩ‰ª§Êó∂Âá∫Èîô: {}".format(e))
        sys.exit(1)

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. ËÑöÊú¨È¶ñÂÖà‰ΩøÁî® Kubernetes Python ÂÆ¢Êà∑Á´ØÂä†ËΩΩÈªòËÆ§ÁöÑ kubeconfigÔºåËøûÊé•Âà∞ Kubernetes ÈõÜÁæ§Ôºõ
2. ÊåáÂÆö‰∫Ü issue ‰∏≠Ê†∑‰æã Pod ÁöÑÂëΩÂêçÁ©∫Èó¥ "robertc-scratch" Âíå Pod ÂêçÁß∞ "with-mode"Ôºà‰πüÂèØ‰ª•‰ΩøÁî® "with-readonly" ËøõË°åÂØπÊØîÊµãËØïÔºâÔºõ
3. ËÑöÊú¨Á≠âÂæÖ Pod ËøõÂÖ• Running Áä∂ÊÄÅÔºàÊúÄÂ§öÁ≠âÂæÖ 30 Ê¨°ËΩÆËØ¢ÔºåÊØèÊ¨° 2 ÁßíÔºâÔºå‰ª•Á°Æ‰øù Pod ÂèØÁî®ÂêéÁªßÁª≠ÊâßË°åÔºõ
4. ‰ΩøÁî® stream Ê®°ÂùóÊâßË°å Pod ÂÜÖÁöÑÂëΩ‰ª§ÔºåÂàóÂá∫ÊåÇËΩΩË∑ØÂæÑ /secrets/token/..data/ ‰∏ãÊñá‰ª∂ÁöÑËØ¶ÁªÜÊùÉÈôê‰ø°ÊÅØÔºõ
5. Ê†πÊçÆËæìÂá∫ÔºåÂ§çÁé∞ issue ‰∏≠ÊèèËø∞ÁöÑÈóÆÈ¢òÔºöÂç≥‰ΩøÁî®Êà∑Âú® Pod ÂÆö‰πâ‰∏≠ÈÖçÁΩÆ‰∫Ü defaultMode Êàñ readOnlyÔºåÂÆûÈôÖ token Êñá‰ª∂ÁöÑÊùÉÈôê‰ªçÊòØ rw

---


## Issue #129473 Pods stuck in Terminating state for >3 days; unable to delete via standard methods on GKE

- Issue ÈìæÊé•Ôºö[#129473](https://github.com/kubernetes/kubernetes/issues/129473)

### Issue ÂÜÖÂÆπ

#### What happened?

We encountered an issue where four pods in our GKE cluster have been stuck in the `Terminating` state for over three days. Jobs created these pods have already been deleted. Despite our attempts to delete the pods using standard methods (e.g., `kubectl delete --force --grace-period=0`), they remain in this state.

Here is the pod status:
```bash
kubectl get pods
NAME                                                       READY   STATUS        RESTARTS   AGE
allocation-28910880-tzr87                       0/2     Terminating   0          3d
allocation-28912320-8mn4z                       0/2     Terminating   0          3d
allocation-28913760-4gsxh                       0/2     Terminating   0          3d
allocation-28916640-vwdj2                       0/2     Terminating   0          3d
```

Additional details:
- Each pod has the finalizer `batch.kubernetes.io/job-tracking` and a QoS class of `Burstable`.
- Attempts to remove the finalizers result in the following error:
  ```bash
  The Pod "allocation-event-sync-28912320-8mn4z" is invalid: spec.initContainers: Forbidden: pod updates may not add or remove containers
  ```
- One of the pods (`allocation-28912320-8mn4z`) was running on a node (`gke-primary-pool-bd44533c-aezp`) that has since been drained and deleted. However, the pod still references the deleted node while remaining in the `Terminating` state.

Attempts to resolve:
1. Force deletion with `kubectl delete --force --grace-period=0` and via `kubectl replace`.
2. Deleting via the `kubectl proxy` API (both deletion and replacement attempts).
3. Draining and deleting the node where one of the pods was running.
4. Attempting to modify the finalizers or QoS class, resulting in the above error.


#### What did you expect to happen?

The pods should be removed from the cluster once the Job is deleted, or at least be force-deletable using standard methods like kubectl delete.

#### How can we reproduce it (as minimally and precisely as possible)?

We cannot reproduce this issue at the moment. Creating a Job via the CLI or a YAML file and then deleting it does not result in the same stuck Terminating state. This appears to be a specific edge case that we can't relate to. 

#### Anything else we need to know?

The only thing we had in the cluster was that we had recently updated the nodes from version 1.27 to 1.28, but we think that was before the job deletion.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.13-gke.600
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
Google cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_CRASH_ID=Lakitu
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=3e0971e1551e88a5a9e615c239f034fd9fd8a423
VERSION=109
VERSION_ID=109
BUILD_ID=17800.309.13
$ uname -a
Linux gke-primary-pool-bd44533c-138r 6.1.100+ #1 SMP PREEMPT_DYNAMIC Sat Aug 10 14:21:56 UTC 2024 x86_64 Intel(R) Xeon(R) CPU @ 2.20GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
kubectl
gcloud
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• issue ÊèèËø∞ÁöÑÊòØÂú® GKE ÈõÜÁæ§‰∏≠ÔºåÈÉ®ÂàÜ pod Âá∫Áé∞ÂºÇÂ∏∏Áä∂ÊÄÅÔºàÈïøÊó∂Èó¥Â§Ñ‰∫é Terminating Áä∂ÊÄÅÔºâÔºåÂπ∂‰∏îÂú®Âà†Èô§Êó∂ÈÅáÂà∞ÊúâÂÖ≥ finalizer ÁöÑÈóÆÈ¢ò„ÄÇÊï¥‰∏™ÈóÆÈ¢òËøáÁ®ã‰∏ªË¶ÅÊ∂âÂèä Kubernetes ÂØπ job Âíå pod ÁöÑËµÑÊ∫êÊ∏ÖÁêÜ‰∏éÁä∂ÊÄÅÂêåÊ≠•ÈóÆÈ¢òÔºå‰ªéÊèèËø∞Êù•ÁúãÂπ∂‰∏çÂ≠òÂú®Â§ñÈÉ®ÊîªÂáªËÄÖÂà©Áî®ËØ•ÈóÆÈ¢òËøõË°åÊú™ÊéàÊùÉÊìç‰Ωú„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊèêÊùÉÁ≠âÂÆâÂÖ®È£éÈô©„ÄÇÈóÆÈ¢òÊõ¥ÂÉèÊòØÈõÜÁæ§ÁÆ°ÁêÜÊàñÂπ≥Âè∞ÁâàÊú¨Êõ¥Êñ∞ÂêéÈÅóÁïôÁöÑ bug Êàñ‰∏çÂèØÈ¢ÑÊúüË°å‰∏∫ÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
Ê≥®ÊÑèÔºöËØ•Â§çÁé∞ËÑöÊú¨Áî®‰∫éËØ¥ÊòéÂ¶Ç‰ΩïËß¶Âèë pods ÈïøÊó∂Èó¥Â§Ñ‰∫é Terminating Áä∂ÊÄÅÁöÑÈóÆÈ¢òÊ®°Êãü„ÄÇ
‰ΩÜÊ†πÊçÆ issue ÊèèËø∞ÔºåËØ•ÈóÆÈ¢ò‰∏ªË¶Å‰∏∫Âπ≥Âè∞ bugÔºåÂπ∂ÈùûÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§Êú¨ËÑöÊú¨‰ªÖÁî®‰∫éÂ±ïÁ§∫Â¶Ç‰ΩïÈÄöËøá Kubernetes API Êìç‰Ωú pod ÁöÑÁîüÂëΩÂë®ÊúüÔºå
ËÄå‰∏ç‰ºöÁúüÊ≠£Ê®°ÊãüËØ•ÈóÆÈ¢òÔºåÂõ†‰∏∫ËØ•ÈóÆÈ¢òÁõÆÂâçÊó†Ê≥ïÂú®ÈùûÁâπÂÆöÂπ≥Âè∞/ÁéØÂ¢ÉÂ§çÁé∞„ÄÇ
"""

import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def create_job(api_instance, namespace="default"):
    # ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ job Á§∫‰æãÔºåÊ≠§ job ‰ºöÂàõÂª∫‰∏Ä‰∏™ pod
    job = client.V1Job(
        metadata=client.V1ObjectMeta(name="demo-job"),
        spec=client.V1JobSpec(
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"app": "demo"}),
                spec=client.V1PodSpec(
                    containers=[client.V1Container(
                        name="busybox",
                        image="busybox",
                        command=["/bin/sh", "-c", "sleep 3600"]
                    )],
                    restart_policy="Never"
                )
            ),
            backoff_limit=0
        )
    )
    try:
        api_instance.create_namespaced_job(namespace=namespace, body=job)
        print("Job demo-job ÂàõÂª∫ÊàêÂäü")
    except ApiException as e:
        print(f"ÂàõÂª∫ Job Êó∂Âá∫Áé∞ÂºÇÂ∏∏: {e}")

def delete_job(api_instance, namespace="default"):
    # Âà†Èô§ jobÔºåÊ≠§Êìç‰Ωú‰ºöÂ∞ùËØïÂà†Èô§ÂØπÂ∫îÁöÑ pod
    try:
        api_instance.delete_namespaced_job(
            name="demo-job",
            namespace=namespace,
            body=client.V1DeleteOptions(propagation_policy="Foreground")
        )
        print("Job demo-job Âà†Èô§ËØ∑Ê±ÇÂ∑≤ÂèëÈÄÅ")
    except ApiException as e:
        print(f"Âà†Èô§ Job Êó∂Âá∫Áé∞ÂºÇÂ∏∏: {e}")

class SimpleHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"This is a demo HTTP server for security testing research.")

def run_http_server():
    server_address = ("", 8080)
    httpd = HTTPServer(server_address, SimpleHandler)
    print("HTTP ÊúçÂä°Âô®ÂêØÂä®ÔºåÁõëÂê¨Á´ØÂè£ 8080")
    httpd.serve_forever()

def main():
    # Âä†ËΩΩÈªòËÆ§ kubeconfig
    config.load_kube_config()
    batch_v1 = client.BatchV1Api()
    core_v1 = client.CoreV1Api()
    namespace = "default"

    # ÂàõÂª∫‰∏Ä‰∏™ jobÔºåÊ®°Êãü pod ÁöÑÂàõÂª∫
    create_job(batch_v1, namespace)

    # Á≠âÂæÖ‰∏ÄÊÆµÊó∂Èó¥Ôºå‰Ωø pod Êàê‰∏∫ Running Áä∂ÊÄÅ
    time.sleep(10)

    # Âà†Èô§ jobÔºåÊ≠§Êó∂ÊåâÁÖßÊ≠£Â∏∏ÊµÅÁ®ãÔºåpod Â∫îËØ•ËÉΩË¢´Âà†Èô§
    delete_job(batch_v1, namespace)

    # Â∞ùËØïËé∑ÂèñÁõ∏ÂÖ≥ pod ÁöÑÁä∂ÊÄÅÔºåÁõëÊéßÂÖ∂Âà†Èô§ÊÉÖÂÜµ
    for _ in range(10):
        pods = core_v1.list_namespaced_pod(namespace, label_selector="app=demo")
        if not pods.items:
            print("ÊâÄÊúâ pod Â∑≤Âà†Èô§")
            break
        else:
            for pod in pods.items:
                print(f"Pod {pod.metadata.name} Áä∂ÊÄÅ: {pod.status.phase}")
        time.sleep(5)

    # ÂêØÂä®‰∏Ä‰∏™ÁÆÄÂçïÁöÑ HTTP ÊúçÂä°Âô®ÔºàÁõëÂê¨ 8080 Á´ØÂè£Ôºâ
    run_http_server()

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨È¶ñÂÖà‰ΩøÁî® Kubernetes Python Client Âä†ËΩΩÈªòËÆ§ÁöÑ kubeconfig ÈÖçÁΩÆÔºåËøûÊé•Âà∞ÈõÜÁæ§„ÄÇ
2. ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑ JobÔºåËØ• Job ÁöÑ pod ËøêË°å‰∏Ä‰∏™ sleep ÂëΩ‰ª§ÔºåÊ®°ÊãüÂÆûÈôÖËøêË°åÁöÑ pod ÁéØÂ¢É„ÄÇ
3. Âà†Èô§ËØ• Job Êó∂Ôºå‰ΩøÁî® propagation_policy="Foreground"ÔºåÁ°Æ‰øùË¢´ÂàõÂª∫ÁöÑ pod ‰ºö‰∏ÄÂπ∂Âà†Èô§ÔºõÊ≠£Â∏∏ÊÉÖÂÜµ‰∏ãÔºåpod ‰ºöÈÄÄÂá∫ÂíåÊ∏ÖÁêÜ„ÄÇ
4. ËÑöÊú¨‰∏≠ÈááÁî®ÁÆÄÂçïÁöÑËΩÆËØ¢Êù•ÁõëÊéß pod Áä∂ÊÄÅÂèòÂåñÔºàÊ®°ÊãüÂØπ pod Âà†Èô§ËøáÁ®ãÁöÑËßÇÂØüÔºâ„ÄÇ
5. Âè¶Â§ñÔºåÊú¨ËÑöÊú¨‰∏≠ËøòÂêØÂä®‰∫Ü‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú® 8080 Á´ØÂè£ÔºõËøô‰∏ÄÈÉ®ÂàÜ‰ªÖÁ¨¶ÂêàÈ¢òÁõÆË¶ÅÊ±ÇÔºåÂ±ïÁ§∫Â¶Ç‰ΩïÂàõÂª∫‰∏Ä‰∏™ HTTP ÊúçÂä°Âô®ÔºåÂπ∂‰∏ç‰∏é pod Âç°Âú® Terminating Áä∂ÊÄÅÁöÑÈóÆÈ¢òÁõ¥Êé•Áõ∏ÂÖ≥„ÄÇ
6. ÊÄª‰ΩìÊù•ËØ¥ÔºåÊ≠§ËÑöÊú¨‰ªÖ‰Ωú‰∏∫Á†îÁ©∂ÂíåÊµãËØï Kubernetes ËµÑÊ∫êÊìç‰ΩúÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂπ∂‰∏çËÉΩÂú®ÈùûÁâπÂÆöÁéØÂ¢É‰∏ãÂ§çÁé∞ issue ‰∏≠ÊèêÂà∞ÁöÑ pods ÈïøÊó∂Èó¥Â§Ñ‰∫é Terminating Áä∂ÊÄÅÁöÑÈóÆÈ¢òÔºåÂõ†‰∏∫ËØ•ÈóÆÈ¢òÂèØËÉΩ‰æùËµñ‰∫éÁâπÂÆöÂπ≥Âè∞ÔºàGKEÔºâÊàñÁéØÂ¢ÉÁöÑ bug ÂºïËµ∑ÔºåÂπ∂ÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

Áªº‰∏äÔºåÂü∫‰∫é‰ª•‰∏äÂàÜÊûêÔºåËØ• issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÊó†ÈúÄÊèê‰æõÈíàÂØπÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞ POCÔºå‰ªéËÄåÊú¨ÈóÆÈ¢ò‰∏çÂ≠òÂú®È´òÈ£éÈô©ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

---


## Issue #129462 OS version reported as not supported, but meets requirements

- Issue ÈìæÊé•Ôºö[#129462](https://github.com/kubernetes/kubernetes/issues/129462)

### Issue ÂÜÖÂÆπ

#### What happened?

First, I ensured (and reconfigured grub) to use cgroups V2:
in /etc/default/grub,
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=vg0/lv_root rhgb quiet systemd.unified_cgroup_hierarchy=1"

I then verified it is running cgroups V2:
[root@docktest01 ~]# cd /sys/fs/cgroup/
[root@docktest01 cgroup]# ll
total 0
-r--r--r--   1 root root 0 Dec 19 17:17 cgroup.controllers
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.max.depth
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.max.descendants
-rw-r--r--   1 root root 0 Dec 19 17:17 cgroup.procs
-r--r--r--   1 root root 0 Dec 19 17:18 cgroup.stat
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.subtree_control
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.threads
-rw-r--r--   1 root root 0 Dec 19 17:18 cpu.pressure
-r--r--r--   1 root root 0 Dec 19 17:18 cpuset.cpus.effective
-r--r--r--   1 root root 0 Dec 19 17:18 cpuset.mems.effective
-r--r--r--   1 root root 0 Dec 19 17:18 cpu.stat
drwxr-xr-x   2 root root 0 Dec 19 17:17 init.scope
-rw-r--r--   1 root root 0 Dec 19 17:18 io.pressure
-r--r--r--   1 root root 0 Dec 19 17:18 io.stat
-r--r--r--   1 root root 0 Dec 19 17:18 memory.numa_stat
-rw-r--r--   1 root root 0 Dec 19 17:18 memory.pressure
--w-------   1 root root 0 Dec 19 17:18 memory.reclaim
-r--r--r--   1 root root 0 Dec 19 17:18 memory.stat
drwxr-xr-x 120 root root 0 Dec 19 17:17 system.slice
drwxr-xr-x   4 root root 0 Dec 31 15:05 user.slice

And the kernel version is:
[root@docktest01 cgroup]# uname -r
4.18.0-477.58.1.el8_8.x86_64

However, when I run kubadm init it tells me that this kernel version is not supported, but then claims that the version it is running meets the requirments:
[root@docktest01 ~]# kubeadm init --pod-network-cidr 10.62.76.0/24 .
.
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR SystemVerification]: kernel release 4.18.0-477.58.1.el8_8.x86_64 is unsupported. Recommended LTS version from the 4.x series is 4.19. Any 5.x or 6.x versions are also supported. For cgroups v2 support, the minimal version is 4.15 and the recommended version is 5.8+

So, while on one hand it states the minimum kernel version for cgroups V2 support is 4.15+, it is failing with a kernel version of 4.18, and via verification, the system is indeed using cgroups V2. 

So, where is the problem? When I first had this error, I found that the system was indeed using cgroups V1, that's why I reconfigured grub, rebuilt the config and rebooted the system, only to show that it was then using V2. But am getting this incompatibility error. 

Is it finding that the system is still running cgroups V1, not V2, and therefore thinking it needs to be on a 4.19 kernel? Also, since RH has not released a 4.19 kernel, is my only option an upgrade to RHEL 9? 

Please assist. 


#### What did you expect to happen?

I expected kubeadm to initialize the master node. 

#### How can we reproduce it (as minimally and precisely as possible)?

Repeat steps taken on RHEL 8.8, with kernel 4.18 as described above. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="Red Hat Enterprise Linux"
VERSION="8.8 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.8"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.8 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.8
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.8"
$ uname -a
# Linux docktest01 4.18.0-477.58.1.el8_8.x86_64 #1 SMP Wed May 22 13:46:53 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
yum/dnf
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ•IssueÊèèËø∞ÁöÑÊòØÂú®RHEL 8.8Âπ≥Âè∞‰∏äÔºåÁî®Êà∑Âú®ÊåâÁÖßÊñáÊ°£Ë¶ÅÊ±ÇÂàáÊç¢Âà∞cgroups V2ÂêéÔºåËøêË°åkubeadm initÊó∂Âá∫Áé∞ÂÜÖÊ†∏ÁâàÊú¨‰∏çÂåπÈÖçÁöÑÈîôËØØ„ÄÇËôΩÁÑ∂ÂÆûÈôÖÁ≥ªÁªüÂ∑≤ÁªèÂêØÁî®‰∫Ücgroups V2Ôºå‰ΩÜkubeadm‰ªçÁÑ∂Êä•ÈîôËØ¥ÂΩìÂâçÂÜÖÊ†∏4.18‰∏çÂèóÊîØÊåÅÔºåÂπ∂Âª∫ËÆÆ‰ΩøÁî®4.19ÊàñÊõ¥È´òÁâàÊú¨„ÄÇÊï¥‰ΩìÊù•ÁúãÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ∏ÂûãÁöÑÂÖºÂÆπÊÄßÊàñÈÖçÁΩÆÊ£ÄÊµãÈóÆÈ¢òÔºåÊ∂âÂèäÁöÑÊòØÁâàÊú¨Âà§Êñ≠ÈÄªËæë‰∏éÊé®ËçêÂÄº‰∏çÁ¨¶ÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏çÂ≠òÂú®ËøúÁ®ãÂà©Áî®„ÄÅÊùÉÈôêÊèêÂçá„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÂÆπÂô®ÈÄÉÈÄ∏„ÄÅÊãíÁªùÊúçÂä°Á≠âÂÆâÂÖ®ÈöêÊÇ£„ÄÇ

Ê†πÊçÆÊèèËø∞ÔºåËØ•IssueÊ≤°ÊúâÊö¥Èú≤ÊïèÊÑü‰ø°ÊÅØ„ÄÅ‰πü‰∏çÂ≠òÂú®‰∏çÂΩìÊìç‰ΩúÊàñÈÖçÁΩÆÈîôËØØÂØºËá¥ÁöÑÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§‰ªéÂÆâÂÖ®ËßíÂ∫¶ËØÑ‰º∞ÔºåËØ•ÈóÆÈ¢ò‰ªÖÊòØÂ∫îÁî®ÁöÑÈÄªËæëÂà§Êñ≠ÈóÆÈ¢òÔºåÂπ∂‰∏çÂ±û‰∫éÂÆâÂÖ®ÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
def main():
    # Ê≠§ËÑöÊú¨‰ªÖÁî®‰∫éËØ¥ÊòéËØ•IssueÈóÆÈ¢ò‰∏∫ÂÖºÂÆπÊÄßÂíåÈÖçÁΩÆÊ£ÄÊµãÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÁöÑÂ§çÁé∞„ÄÇ
    print("ËØ•Issue‰∏ªË¶ÅÊòØÂÜÖÊ†∏ÁâàÊú¨ÂíåcgroupsËÆæÁΩÆÁöÑÂÖºÂÆπÊÄßÈóÆÈ¢òÔºåÂπ∂‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©„ÄÇ")
    print("ËØ∑Á°Æ‰øùÁ≥ªÁªüÈÅµÂæ™KubeadmÂÆòÊñπÊñáÊ°£ÁöÑË¶ÅÊ±ÇÔºåËã•ÈÅáÂà∞ÁâàÊú¨Âà§Êñ≠ÈóÆÈ¢òÔºåÂèØËÄÉËôëÂçáÁ∫ßÂÜÖÊ†∏ÁâàÊú¨ÊàñÂí®ËØ¢Áõ∏ÂÖ≥ÊîØÊåÅ„ÄÇ")
    
if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

ËØ•Â§çÁé∞ËÑöÊú¨Âπ∂‰∏çÊ®°ÊãüÊàñËß¶Âèë‰ªª‰ΩïÂÆâÂÖ®ÊºèÊ¥ûÔºåËÄåÊòØËæìÂá∫ËØ¥Êòé‰ø°ÊÅØÔºåË°®ÊòéÂΩìÂâçIssueÁöÑÈóÆÈ¢òÂ±û‰∫éÁâàÊú¨ÂÖºÂÆπÊÄßÈóÆÈ¢òÔºå‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©„ÄÇËÑöÊú¨ËøêË°åÂêé‰ºöÊèêÁ§∫Áî®Êà∑ËØ•Issue‰∏çÂ≠òÂú®ÂÆâÂÖ®ÈöêÊÇ£Ôºå‰ªéËÄåÂºïÂØº‰ΩøÁî®ËÄÖÂÖ≥Ê≥®Á≥ªÁªüÂÖºÂÆπÊÄßÂíåËΩØ‰ª∂ÁâàÊú¨Á≠âÈóÆÈ¢òÔºåÊó†ÈúÄÊãÖÂøßÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇ

---


## Issue #129456 Setting properties explicitly to `null` does not work without specifying `--server-side`

- Issue ÈìæÊé•Ôºö[#129456](https://github.com/kubernetes/kubernetes/issues/129456)

### Issue ÂÜÖÂÆπ

#### What happened?

When running `kubectl apply` without `--server-side`, it is not possible to explicitly set a resource property to `null`.

Until Kubernetes 1.31 this was possible when a resource was created (not when it was patched), I think this behavior was removed by https://github.com/kubernetes/kubernetes/pull/125646.

My use case:
I have a CRD that specifies a default value for a property but allows it to be null: a value for `default` is specified and `nullable` is set to `true` in the CRD definition. So a user can either omit the property (`default` value should be used) or explicitly set it to `null` (then it should be `null`).
When I create a resource of that kind by running `kubectl apply` without `--server-side`, it is not possible to set the value to `null`, the property will always be set to the default value (which does not match the behavior described in the [documentation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting-and-nullable)).

Both for creating and patching, we need to think about whether the property should have the same value after running `kubectl apply`, no matter if `--server-side` is provided or not (currently the value depends on the flag being present).

#### What did you expect to happen?

The property's value should be `null` if the user explcitly sets it to `null` and the CRD allows it.

#### How can we reproduce it (as minimally and precisely as possible)?

Create the CRD:
```
cat <<EOF | kubectl apply -f -
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: fooclusters.example.tech
spec:
  group: example.tech
  names:
    kind: FooCluster
    listKind: FooClusterList
    plural: fooclusters
    shortNames:
    - foo
    singular: foocluster
  scope: Namespaced
  versions:
  - name: v1alpha1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        required:
        - spec
        type: object
        properties:
          spec:
            type: object
            properties:
              bar:
                type: string
                nullable: true
                default: "defaultvalue"
EOF
```

Create the resource:
```
cat <<EOF | kubectl apply -f -
apiVersion: example.tech/v1alpha1
kind: FooCluster
metadata:
  name: myfoocluster
spec:
  bar: null
EOF
```

Try running this with and without `--server-side` and check the property's value with:
```
kubectl get fooclusters.example.tech myfoocluster -o jsonpath='{.spec.bar}'
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Local Kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞‰∫Ü‰ΩøÁî® kubectl apply ÂàõÂª∫ÊàñÊõ¥Êñ∞Ëá™ÂÆö‰πâËµÑÊ∫êÊó∂ÔºåÂΩìÊòæÂºèÂ∞ÜÊüê‰∏™Â±ûÊÄßËÆæÁΩÆ‰∏∫ nullÔºàËÄåËØ•Â±ûÊÄßÂêåÊó∂Âú® CRD ‰∏≠ÈÖçÁΩÆ‰∫ÜÈªòËÆ§ÂÄº‰∏îÂÖÅËÆ∏‰∏∫ nullÔºâÊó∂Ôºå‰∏çÊåáÂÆö --server-side ÂèÇÊï∞‰ºöÂØºËá¥ËØ•Â±ûÊÄßË¢´Ëµã‰∫àÈªòËÆ§ÂÄºÔºåËÄåÈùû‰øùÊåÅ null ÁöÑÈóÆÈ¢ò„ÄÇËøôÂ±û‰∫é Kubernetes ÂÆ¢Êà∑Á´Ø‰∏éÊúçÂä°Á´ØË°å‰∏∫‰∏ç‰∏ÄËá¥ÁöÑÂäüËÉΩÁº∫Èô∑ÈóÆÈ¢òÔºåÂπ∂Êú™Ê∂âÂèäÊïèÊÑüÊï∞ÊçÆÊ≥ÑÈú≤„ÄÅ‰∏çÂΩìÈÖçÁΩÆ„ÄÅÂëΩ‰ª§ÊâßË°å„ÄÅÊèêÊùÉ„ÄÅÂÆπÂô®ÈÄÉÈÄ∏ÊàñÊãíÁªùÊúçÂä°Á≠âÁõ¥Êé•ÁöÑÂÆâÂÖ®È£éÈô©„ÄÇÊï¥‰∏™ÈóÆÈ¢òÂèçÊò†ÁöÑÊòØËµÑÊ∫êÈªòËÆ§ÂÄºÂ§ÑÁêÜÊñπÈù¢ÁöÑ bugÔºåËÄåÈùûÊºèÊ¥û„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
# Áî±‰∫éËØ• Issue ‰∏çÊ∂âÂèäÂÆâÂÖ®È£éÈô©ÔºåÂõ†Ê≠§‰∏çÈúÄË¶ÅÁîüÊàêÈíàÂØπÂÆâÂÖ®ÊºèÊ¥ûÁöÑÂ§çÁé∞POCËÑöÊú¨„ÄÇ
# Ê≠§Â§ÑÊèê‰æõ‰ªÖÁî®‰∫éÈ™åËØÅÈóÆÈ¢òÂ§çÁé∞ÁöÑÁ§∫‰æãËÑöÊú¨Ôºå‰∏ªË¶Å‰∏∫‰∫ÜÂú®Êú¨Âú∞È™åËØÅËØ•ÂäüËÉΩÁº∫Èô∑ÁöÑÂ≠òÂú®Ôºå
# ‰ΩÜËØ•ËÑöÊú¨‰∏çÁî®‰∫éÂÆâÂÖ®ÊîªÂáªÊàñÂÖ∂ÂÆÉÈùûÊ≥ïÁî®ÈÄî„ÄÇ
import subprocess
import time

def create_crd():
    crd_yaml = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: fooclusters.example.tech
spec:
  group: example.tech
  names:
    kind: FooCluster
    listKind: FooClusterList
    plural: fooclusters
    shortNames:
    - foo
    singular: foocluster
  scope: Namespaced
  versions:
  - name: v1alpha1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        required:
        - spec
        type: object
        properties:
          spec:
            type: object
            properties:
              bar:
                type: string
                nullable: true
                default: "defaultvalue"
"""
    with open("crd.yaml", "w") as f:
        f.write(crd_yaml)
    subprocess.run(["kubectl", "apply", "-f", "crd.yaml"], check=True)

def create_resource():
    resource_yaml = """
apiVersion: example.tech/v1alpha1
kind: FooCluster
metadata:
  name: myfoocluster
spec:
  bar: null
"""
    with open("resource.yaml", "w") as f:
        f.write(resource_yaml)
    subprocess.run(["kubectl", "apply", "-f", "resource.yaml"], check=True)

def get_resource_bar(use_server_side=False):
    cmd = ["kubectl", "get", "fooclusters.example.tech", "myfoocluster", "-o", "jsonpath={.spec.bar}"]
    if use_server_side:
        cmd.insert(2, "--server-side")
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout.strip()

def main():
    print("ÂàõÂª∫ CRD...")
    create_crd()
    # Á≠âÂæÖ‰∏Ä‰ºöÂÑøÁ°Æ‰øù CRD ÁîüÊïà
    time.sleep(2)

    print("ÂàõÂª∫ËµÑÊ∫ê...")
    create_resource()
    # Á≠âÂæÖ‰∏Ä‰ºöÂÑøÁ°Æ‰øùËµÑÊ∫êÂàõÂª∫ÂÆåÊØï
    time.sleep(2)

    print("Êü•ËØ¢ËµÑÊ∫êÔºà‰∏ç‰ΩøÁî® --server-sideÔºâÔºö")
    normal_value = get_resource_bar(use_server_side=False)
    print("ÁªìÊûúÔºö", normal_value)

    print("Êü•ËØ¢ËµÑÊ∫êÔºà‰ΩøÁî® --server-sideÔºâÔºö")
    server_side_value = get_resource_bar(use_server_side=True)
    print("ÁªìÊûúÔºö", server_side_value)

if __name__ == "__main__":
    main()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

Ê≠§ËÑöÊú¨Áî®‰∫éÈ™åËØÅ Issue ‰∏≠ÊèèËø∞ÁöÑÁé∞Ë±°ÔºöÂΩìÊòæÂºèÊääÂ±ûÊÄß bar ËÆæÁΩÆ‰∏∫ null Êó∂ÔºåÂú®‰∏ç‰ΩøÁî® --server-side ÂèÇÊï∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåkubectl apply Á´üÁÑ∂Â∞ÜËØ•Â±ûÊÄßËµã‰∫à‰∫Ü CRD ÂÆö‰πâ‰∏≠ÁöÑÈªòËÆ§ÂÄº "defaultvalue"ÔºåËÄå‰ΩøÁî® --server-side ÂèÇÊï∞Êó∂ÂèØËÉΩË°®Áé∞‰∏çÂêå„ÄÇËÑöÊú¨È¶ñÂÖàÂàõÂª∫ CRDÔºåÁÑ∂ÂêéÂàõÂª∫‰∏Ä‰∏™ FooCluster ËµÑÊ∫êÔºåÊé•ÁùÄÂàÜÂà´ÈÄöËøá‰∏çÂ∏¶ÂíåÂ∏¶ --server-side ÂèÇÊï∞Ë∞ÉÁî® kubectl get Ê£ÄÊü•ËµÑÊ∫ê‰∏≠ spec.bar ÁöÑÂÄº„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÈ™åËØÅÂäüËÉΩÁº∫Èô∑ÔºåÈóÆÈ¢òÊú¨Ë∫´‰∏çÊ∂âÂèäÂÆâÂÖ®ÈóÆÈ¢òÔºåÂõ†Ê≠§‰∏çÊûÑÊàêÂÆâÂÖ®È£éÈô©ÊàñÊºèÊ¥ûÊîªÂà©Áî®„ÄÇ

---


## Issue #129453 can MountPath contain  ":" ?

- Issue ÈìæÊé•Ôºö[#129453](https://github.com/kubernetes/kubernetes/issues/129453)

### Issue ÂÜÖÂÆπ

#### What happened?

https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/types.go#L2110-L2113
```
	RecursiveReadOnly *RecursiveReadOnlyMode
	// Required. If the path is not an absolute path (e.g. some/path) it
	// will be prepended with the appropriate root prefix for the operating
	// system.  On Linux this is '/', on Windows this is 'C:\'.
	MountPath string
```
but
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L2283
```
	// Path within the container at which the volume should be mounted.  Must
	// not contain ':'.
	MountPath string `json:"mountPath" protobuf:"bytes,3,opt,name=mountPath"`
```
some people use mountpath:/etc/localtime:ro (k8s should use readonly:true)  will not work on containerd but work on docker.

#### What did you expect to happen?

mountPath  can't contain  ":" 

#### How can we reproduce it (as minimally and precisely as possible)?

set mountpath:/etc/localtime:ro 

#### Anything else we need to know?

_No response_

#### Kubernetes version

any

#### Cloud provider

no 

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### ÂàÜÊûêÁªìÊûú

**È£éÈô©ÂÆöÁ∫ßÔºö**  
‰∏çÊ∂âÂèä

**Âà§Êñ≠‰æùÊçÆÔºö**  
ËØ• Issue ÊèèËø∞ÁöÑÊòØ Kubernetes ‰∏≠ÂÖ≥‰∫é MountPath ÈÖçÁΩÆÁöÑÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåIssue ÊåáÂá∫Âú® Kubernetes ÁöÑ API ÂÆö‰πâ‰∏≠ÂØπ‰∫é MountPath ÁöÑË¶ÅÊ±ÇÂ≠òÂú®‰∏ç‰∏ÄËá¥ÔºåÂú®ÊüêÂ§ÑÂÖÅËÆ∏ÂåÖÂê´ÂÜíÂè∑ÔºåËÄåÂú®Âè¶Â§ÑÊòéÁ°ÆÂÆö‰πâ‰∏çÂÖÅËÆ∏ÂåÖÂê´ÂÜíÂè∑„ÄÇËôΩÁÑ∂ËøôÂèØËÉΩÂØºËá¥Áî®Êà∑Âú®Êüê‰∫õÂÆπÂô®ËøêË°åÊó∂Ôºà‰æãÂ¶Ç containerd Âíå dockerÔºâ‰∏ãÈÅáÂà∞ÊåÇËΩΩË∑ØÂæÑÈÖçÁΩÆÈóÆÈ¢òÔºå‰ΩÜËØ•ÈóÆÈ¢ò‰∏ªË¶ÅÂ±û‰∫éÈÖçÁΩÆÂíåÂÖºÂÆπÊÄßÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

**Â§çÁé∞ËøáÁ®ãÔºö**

```python
#!/usr/bin/env python3
"""
ËØ•ËÑöÊú¨Áî®‰∫éÊ®°Êãü Kubernetes ‰∏≠ mountPath ÂåÖÂê´ÂÜíÂè∑ÁöÑÈÖçÁΩÆÈóÆÈ¢òÔºå
‰ΩÜÁî±‰∫éËØ•ÈóÆÈ¢òÂ±û‰∫éÈÖçÁΩÆ‰∏ç‰∏ÄËá¥ÊÄßÈóÆÈ¢òËÄåÈùûÂÆâÂÖ®ÈóÆÈ¢òÔºå
Âõ†Ê≠§ËØ•ËÑöÊú¨‰ªÖÁî®‰∫éÊºîÁ§∫Â¶Ç‰ΩïÊ£ÄÊµãÊåÇËΩΩË∑ØÂæÑ‰∏≠ÊòØÂê¶ÂåÖÂê´ÈùûÊ≥ïÂÜíÂè∑„ÄÇ
"""

import http.server
import socketserver
import json
import sys

PORT = 8080

def validate_mount_path(mount_path: str) -> bool:
    """
    Ê£ÄÊü• mountPath ÊòØÂê¶ÂåÖÂê´ÈùûÊ≥ïÁöÑÂÜíÂè∑Â≠óÁ¨¶
    Ê†πÊçÆ Kubernetes ÁöÑË¶ÅÊ±ÇÔºåmountPath ‰∏çÂ∫îÂåÖÂê´ÂÜíÂè∑ (:)„ÄÇ
    ËøîÂõû True Ë°®Á§∫ÊåÇËΩΩË∑ØÂæÑÂêàÊ≥ïÔºåFalse Ë°®Á§∫‰∏çÂêàÊ≥ï„ÄÇ
    """
    if ":" in mount_path:
        return False
    return True

class RequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        # ËØªÂèñËØ∑Ê±Ç‰ΩìÊï∞ÊçÆ
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        try:
            data = json.loads(post_data)
        except json.JSONDecodeError:
            self.send_response(400)
            self.end_headers()
            self.wfile.write(b'Invalid JSON')
            return

        mount_path = data.get("mountPath", "")
        if not isinstance(mount_path, str):
            self.send_response(400)
            self.end_headers()
            self.wfile.write(b'mountPath must be a string')
            return

        # Ê†°È™å mountPath ÊòØÂê¶ÂåÖÂê´ÂÜíÂè∑
        if validate_mount_path(mount_path):
            result = {"valid": True, "message": "mountPath is valid"}
        else:
            result = {"valid": False, "message": "mountPath contains invalid character ':'"}

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(result).encode('utf-8'))

def run_http_server():
    handler = RequestHandler
    with socketserver.TCPServer(("", PORT), handler) as httpd:
        print(f"HTTP server running on port {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("Server stopped by user.")
            httpd.server_close()

if __name__ == "__main__":
    run_http_server()
```


**Ëß£ÈáäËØ¥ÊòéÔºö**

1. Êú¨ËÑöÊú¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÁÆÄÊòìÁöÑ HTTP ÊúçÂä°Âô®ÔºåÁõëÂê¨Âú®Êú¨Âú∞ 8080 Á´ØÂè£„ÄÇ
2. ÂΩìÊúçÂä°Âô®Êé•Êî∂Âà∞ POST ËØ∑Ê±ÇÊó∂ÔºåËß£ÊûêËØ∑Ê±Ç‰Ωì‰∏≠ÁöÑ JSON Êï∞ÊçÆÔºåÂπ∂ÊèêÂèñ "mountPath" Â≠óÊÆµ„ÄÇ
3. Âà©Áî® validate_mount_path ÂáΩÊï∞Âà§Êñ≠‰º†ÂÖ•ÁöÑ mountPath ÊòØÂê¶ÂåÖÂê´ÂÜíÂè∑Â≠óÁ¨¶Ôºà':'Ôºâ„ÄÇËã•ÂåÖÂê´ÂàôËøîÂõû‰∏çÂêàÊ≥ïÁöÑ‰ø°ÊÅØÔºåÂê¶ÂàôËøîÂõûÂêàÊ≥ï‰ø°ÊÅØ„ÄÇ
4. ËÑöÊú¨‰ªÖÁî®‰∫éÊ®°ÊãüÊ£ÄÊµãÈÖçÁΩÆÈ°π‰∏≠ÁöÑÊ†ºÂºèÈóÆÈ¢òÔºåÂπ∂ÈùûÁî®‰∫éÂ§çÁé∞ÂÆâÂÖ®ÊºèÊ¥ûÔºåÂõ†‰∏∫ËØ• Issue ÁöÑÈóÆÈ¢òÂ±û‰∫éÈÖçÁΩÆ‰∏ç‰∏ÄËá¥ÊÄßÈóÆÈ¢òÔºåËÄåÈùûÂÆâÂÖ®ÊºèÊ¥û„ÄÇ
5. Áî®Êà∑ÂèØ‰ª•ÈÄöËøáÂèëÈÄÅÂåÖÂê´ mountPath Â≠óÊÆµÁöÑ JSON ËØ∑Ê±ÇÊù•ÊµãËØïËØ•Ê£ÄÊµãÈÄªËæëÔºå‰æãÂ¶ÇÔºö
   POST http://localhost:8080 Âπ∂ÂèëÈÄÅ {"mountPath": "mountpath:/etc/localtime:ro"}ÔºåÊúçÂä°Âô®‰ºöÂõûÂ∫îËØ•ÊåÇËΩΩË∑ØÂæÑ‰∏çÂêàÊ≥ï„ÄÇ

---


