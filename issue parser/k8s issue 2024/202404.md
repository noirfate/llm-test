# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (17 个)

## Issue #124515 kubelet and containerd in endless loop for CreateContainer with unexpected media type octet-stream

- Issue 链接：[#124515](https://github.com/kubernetes/kubernetes/issues/124515)

### Issue 内容

#### What happened?

I think the private container registry had some intermittent errors and did return wrong data for some layers.

The problem I think is that kubelet or containerd is in a state were it assumes that a given OCI image is already downloaded locally, but then some parsing of this layer fails and the loop starts again.

#### What did you expect to happen?

kubelet and or containerd should detect the wrong layer/manifest state and delete the incomplete/erroneous download and pull again from container registry.

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure, probably container registry needs to return media type octet-stream for some layer/manifest instead of correct media type.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
Kubelet 1.28.7-gke.1026000
Containerd 1.7.10
Linux 6.1.58+
</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Will provide 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因及可能的影响：**

该问题描述了 kubelet 和 containerd 在拉取容器镜像时，如果镜像的某些层或清单返回了意外的媒体类型（如 `application/octet-stream`），会导致它们陷入无限循环。攻击者如果能够控制或劫持容器镜像仓库，或者发布恶意的容器镜像，使得镜像的某些层或清单的媒体类型被设置为 `application/octet-stream`，就可能触发这一问题。

这种情况下，kubelet 和 containerd 会不断尝试解析和创建容器，但由于媒体类型不匹配，每次都会失败并重新尝试，导致它们进入无限循环。结果是消耗大量的系统资源（如 CPU 和内存），可能导致节点性能严重下降，甚至不可用，进而影响整个集群的稳定性和可用性。这属于典型的拒绝服务（Denial of Service, DoS）攻击。

**符合风险判断标准：**

1. **该风险能被攻击者利用：** 攻击者可以通过发布恶意镜像或篡改镜像仓库响应，诱使 kubelet 和 containerd 拉取并处理带有错误媒体类型的镜像。
2. **该风险有可能成为一个漏洞，并被分配 CVE 编号，使用 CVSS 3.1 评分标准打分，结果在 High 以上：**

   - **攻击向量（AV）：网络（Network，N）** - 攻击者可通过网络向受害者提供恶意镜像。
   - **攻击复杂度（AC）：低（Low，L）** - 攻击不需要高复杂度的条件。
   - **特权要求（PR）：无（None，N）** - 攻击者无需任何权限即可实施攻击。
   - **用户交互（UI）：无（None，N）** - 不需要额外的用户交互即可触发漏洞。
   - **作用范围（S）：未改变（Unchanged，U）** - 漏洞仅影响组件自身的资源。
   - **机密性影响（C）：无（None，N）** - 不影响机密性。
   - **完整性影响（I）：无（None，N）** - 不影响完整性。
   - **可用性影响（A）：高（High，H）** - 对系统的可用性造成严重影响。

   综合评分：**CVSS：3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H，基础得分：7.5（High）**

**Proof of Concept：**

1. **构建恶意镜像：**
   - 攻击者创建一个容器镜像，在其层（layer）或清单（manifest）中，将媒体类型（Media Type）设置为 `application/octet-stream`，而不是正确的媒体类型。

2. **发布恶意镜像：**
   - 将该恶意镜像推送到公共容器镜像仓库，或者通过中间人攻击（MITM）劫持私有镜像仓库的通信，返回篡改后的镜像数据。

3. **诱导拉取镜像：**
   - 使受害者的 Kubernetes 集群拉取并部署该恶意镜像。由于 Kubernetes 会自动尝试拉取并运行指定的容器镜像，过程无需额外的用户交互。

4. **触发无限循环：**
   - kubelet 和 containerd 在处理该镜像时，由于媒体类型不匹配，会在下载、解析和创建容器的过程中反复失败并重试，进入无限循环。

5. **造成拒绝服务：**
   - 由于 kubelet 和 containerd 持续占用大量的 CPU 和内存资源，其他正常的容器可能无法调度和运行，节点的可用性严重受损，导致拒绝服务。

**防范建议：**

- **健全输入验证：** kubelet 和 containerd 应加强对镜像媒体类型的验证，遇到异常应及时退出循环并报错。
- **限制重试次数：** 对于拉取和创建容器的重试操作，应设置合理的重试次数上限，避免无限循环。
- **监控和报警：** 部署资源监控工具，及时发现异常的资源消耗情况，触发报警并采取措施。

---

## Issue #124502 The APIServer health check failed. As a result, the APIServer automatically exits and APIServer unavailable.

- Issue 链接：[#124502](https://github.com/kubernetes/kubernetes/issues/124502)

### Issue 内容

#### What happened?

Most services access the APIServer through the service of the ClusterIP type. The relationship between the service and the endpoints is created by the APIServer , and then the iptables routing rule maintained by kube-proxy. When the APIServer is restarted, the routing rule cannot be updated quickly. A large amount of service traffic is received. CPU resources are limited and the CPU usage is high. As a result, the APIServer health check timeout and then the APIServer exits automatically. As a result, the service is unavailable.

#### What did you expect to happen?

Before the APIServer is started successfully, a filter is added to the ingress to control the incoming traffic, ensuring that the APIServer can be started successfully and enhancing the APIServer reliability.

#### How can we reproduce it (as minimally and precisely as possible)?

When the APIServer is started, a large number of requests are sent to the APIServer. As a result, the CPU is overloaded and the APIServer health check times out.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

```[tasklist]
#### Tasks
```


### 分析结果

这涉及到潜在的安全风险。

**原因和可能的影响：**

根据Issue的描述，当APIServer重启时，因路由规则无法及时更新，大量服务流量直接涌入APIServer，导致CPU资源耗尽，CPU使用率过高，使得APIServer的健康检查超时，进而APIServer自动退出，服务不可用。

攻击者可以利用这一漏洞，在APIServer启动或重启的过程中，向其发送大量的请求，制造恶意流量。由于APIServer在此时缺乏有效的流量控制或过滤机制，CPU资源容易被耗尽，导致APIServer无法正常启动或崩溃。这是一种典型的拒绝服务（Denial of Service, DoS）攻击，严重影响集群的稳定性和可用性。

按照CVSS 3.1评分标准，该漏洞可能被评为高危（High）级别：

- **攻击向量（AV）**：网络(Network) —— 攻击者可通过网络远程发动攻击。
- **攻击复杂度（AC）**：低(Low) —— 攻击不需要特殊的条件或权限。
- **特权要求（PR）**：无(None) —— 攻击者不需要任何授权即可进行攻击。
- **用户交互（UI）**：无(None) —— 不需要用户交互即可触发漏洞。
- **作用范围（S）**：未改变(Unchanged) —— 攻击只影响APIServer所在的组件。
- **可用性（A）**：高(High) —— 攻击可完全破坏APIServer的可用性。

综合评分可能达到 **7.5（High）**。

**Proof of Concept：**

攻击者可以编写如下脚本，在APIServer启动或重启时，不断向其发送大量请求：

```bash
while true; do
  curl -k https://<APIServer_IP>:6443/ -o /dev/null &
done
```

或使用压力测试工具，例如`ab`或`wrk`：

```bash
ab -n 100000 -c 1000 https://<APIServer_IP>:6443/
```

通过持续、高频率的请求，耗尽APIServer的CPU资源，导致健康检查超时，APIServer自动退出，服务不可用。

**建议：**

为了防范此类攻击，建议在APIServer启动期间，对进入的流量进行控制：

- 在Ingress或防火墙层面添加流量过滤，限制请求速度或来源。
- 在APIServer中加入启动保护机制，在初始化完成前限制外部请求。
- 增加健康检查的超时时间或重试次数，避免因瞬时高负载导致APIServer退出。

---

## Issue #124443 Windows: Container log rotation may fail if the container logs are followed

- Issue 链接：[#124443](https://github.com/kubernetes/kubernetes/issues/124443)

### Issue 内容

#### What happened?

The test ``k8s.io/kubernetes/pkg/kubelet/kuberuntime/logs.TestReadRotatedLog`` is failing on Windows [0]:

```
{Failed  === RUN   TestReadRotatedLog
    logs_test.go:276: 
        	Error Trace:	C:/kubernetes/pkg/kubelet/kuberuntime/logs/logs_test.go:276
        	Error:      	Received unexpected error:
        	            	rename C:\Users\azureuser\AppData\Local\Temp\TestReadRotatedLog1957435032\001\logfile2771403626 C:\Users\azureuser\AppData\Local\Temp\TestReadRotatedLog1957435032\001\logfile2771403626.2220240422-035910: The process cannot access the file because it is being used by another process.
        	Test:       	TestReadRotatedLog
        	Messages:   	failed to rotate log "C:\\Users\\azureuser\\AppData\\Local\\Temp\\TestReadRotatedLog1957435032\\001\\logfile2771403626" to "C:\\Users\\azureuser\\AppData\\Local\\Temp\\TestReadRotatedLog1957435032\\001\\logfile2771403626.2220240422-035910"
--- FAIL: TestReadRotatedLog (0.21s)
```

The idea of the test is to check that kubelet still follows and reads the Container Logs after a Log Rotation occured. However, because the followed file is open, it cannot be renamed, and thus, the container logs cannot be rotated.

This issue could be prevented if the file would be opened with the ``FILE_SHARE_DELETE`` flag set [1]: ``Delete access allows both delete and rename operations.``

[0] https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-unit-windows-master/1782243642258231296
[1] https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-createfilea

#### What did you expect to happen?

Kubelet should not block container log rotation.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the test ``k8s.io/kubernetes/pkg/kubelet/kuberuntime/logs.TestReadRotatedLog`` is failing on Windows.

#### Anything else we need to know?

/sig windows

/kind bug
/kind failing-test

#### Kubernetes version

latest, master. This unit test has been failing for a while.


#### Cloud provider

N/A


#### OS version

Windows nodes

</details>


#### Install tools

N/A


#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A


### 分析结果

该Issue涉及潜在的安全风险，具体原因和可能的影响如下：

**原因分析：**

在Windows环境下，Kubernetes的kubelet在读取容器日志时，如果未使用`FILE_SHARE_DELETE`标志打开日志文件，会导致该日志文件被独占锁定。当日志轮转机制尝试重命名或删除该日志文件时，由于文件被kubelet占用，操作将失败（正如错误信息所示：“The process cannot access the file because it is being used by another process.”）。

**可能的影响：**

1. **磁盘空间耗尽（拒绝服务）：** 如果日志文件无法轮转，日志将持续写入到同一个文件，导致该文件不断增大。攻击者可以通过触发容器产生大量日志（例如发送大量请求、注入异常数据等方式），加速日志文件的增长，最终可能导致节点的磁盘空间耗尽。这会导致节点上的服务不可用，新的容器无法启动，造成拒绝服务（Denial of Service，DoS）攻击。

2. **日志管理失效：** 无法轮转的日志可能导致日志管理策略失效，影响系统的监控和故障排查。

**利用方式（Proof of Concept）：**

1. **部署恶意容器：** 攻击者在集群中部署一个容器，该容器持续产生大量日志输出。

2. **触发日志轮转失败：** 由于kubelet在未使用`FILE_SHARE_DELETE`标志的情况下跟踪日志文件，日志轮转机制无法正常工作，日志文件无法被重命名或删除。

3. **耗尽磁盘空间：** 持续的日志输出导致日志文件无限增大，最终耗尽节点的磁盘空间。

4. **导致服务不可用：** 一旦磁盘空间耗尽，节点上的关键服务可能崩溃，新容器无法调度，现有容器可能无法正常运行，造成全局性的服务不可用。

**CVSS 3.1评分：**

- **攻击向量（AV）：网络（Network）**
- **攻击复杂度（AC）：低（Low）**
- **特权要求（PR）：低（Low）**（需要部署或访问容器的权限）
- **用户交互（UI）：无（None）**
- **影响范围（S）：未改变（Unchanged）**
- **机密性（C）：无影响（None）**
- **完整性（I）：无影响（None）**
- **可用性（A）：高（High）**

根据上述指标，CVSS 评分为 **7.5（High）**。

**结论：**

该问题符合风险判断标准：

1. **可被攻击者利用：** 攻击者可以通过触发大量日志输出，利用kubelet对日志文件的独占锁，导致日志轮转失败。

2. **可能成为漏洞并被分配CVE编号：** 由于可能导致拒绝服务且CVSS评分达到High，符合分配CVE的标准。

因此，该Issue涉及潜在的安全风险，建议尽快修复。在文件打开时应使用`FILE_SHARE_DELETE`标志，允许文件在被占用时仍可被重命名或删除，以确保日志轮转机制正常工作，防止潜在的拒绝服务攻击。

---

## Issue #124436 Quota scopes cannot handle the transition case from one scope to another when the target object is updated.

- Issue 链接：[#124436](https://github.com/kubernetes/kubernetes/issues/124436)

### Issue 内容

#### What happened?

I create 2 quotas with different scopes, one is Terminating, and the other is NotTerminating. I create a pod that belongs to NotTerminating, then update the pod's spec.activeDeadlineSeconds with 5, the pod should belong to Terminating. But the quota is not updated.


```shell
Every 2.0s: kubectl get pod,quota -A

NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
default       pod/test-pod                  0/1     Pending   0          28s
kube-system   pod/coredns-58cd89f5d-wtpxv   0/1     Pending   0          5m45s

NAMESPACE   NAME                               AGE     REQUEST           LIMIT
default     resourcequota/not-terminating   5m16s   count/pods: 1/1
default     resourcequota/terminating          5m16s   count/pods: 0/1
```

#### What did you expect to happen?

Quota scopes can handle the transition case from one scope to another when the target object is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create 2 quotas.
  ```yaml
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: terminating
  spec:
    hard:
      count/pods: "1"
    scopes:
    - Terminating
  ---
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: not-terminating
  spec:
    hard:
      count/pods: "1"
    scopes:
    - NotTerminating
  ```
2. Create a pod which belongs to not-terminating
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: test-pod
  spec:
   
    containers:
    - name: test
      image: ubuntu
      command: ["/bin/sh"]
      args: ["-c", "while true; do echo hello; sleep 10;done"]
  ```
3. Update pod's spec.activeDeadlineSeconds with 5. the pods should belong to terminating
  ```shell
  kubectl patch pod test-pod -p '{"spec":{"activeDeadlineSeconds":5}}'
  ```
4. Get all quotas, you will see the terminating quota is not updated.
  ```shell
  kubectl get quota -A
  ```

#### Anything else we need to know?

The root cause is, when the pod is updated, the delta is calculated based on the old object and the new object, it doesn't consider the scope change where the updated object belongs to, so the final delta is not correct.

In this case, the final delta is zero, so the terminating quota is not updated.

Related k/k code: 

https://github.com/kubernetes/kubernetes/blob/5a31a46d4bfe1fcf5962fa9cee23ff3148a094e3/staging/src/k8s.io/apiserver/pkg/admission/plugin/resourcequota/controller.go#L495-L558

When I implement https://github.com/kubernetes/kubernetes/pull/124360, I find this issue.

#### Kubernetes version

<details>

```console
(base) (⎈|local-up-cluster:N/A)➜  __testdata git:(kep-3751-quota-2) ✗ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.47+5a31a46d4bfe1f-dirty
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因及可能的影响：**

该问题涉及 Kubernetes 中 `ResourceQuota` 在对象更新时未能正确处理对象从一个 `scope` 转换到另一个 `scope` 的情况，导致配额统计未正确更新。

攻击者可以利用这一漏洞，通过创建一个属于某个 `ResourceQuota` 范围（如 `NotTerminating`）的对象，然后更新该对象使其转移到另一个范围（如 `Terminating`），而配额统计未能正确反映这一变化。这样，攻击者可以绕过资源配额限制，消耗超过配额的资源。

**可能的影响包括：**

1. **资源耗尽（Resource Exhaustion）：** 攻击者可以创建并更新大量的对象，导致集群的资源被大量占用，影响其他正常用户的使用。

2. **拒绝服务（Denial of Service）：** 由于资源被耗尽，集群可能无法为其他用户提供服务，导致服务中断。

**符合风险判断标准：**

1. **可被攻击者利用：** 攻击者可以通过更新对象的方式，利用该漏洞绕过资源配额限制。

2. **可能成为漏洞并被分配 CVE 编号，且 CVSS 3.1 评分为高危以上：**

   根据 CVSS 3.1 评分标准：

   - **攻击向量（AV）：网络（N）**
   - **攻击复杂度（AC）：低（L）**
   - **特权要求（PR）：低（L）**（需要能够创建和更新对象的权限）
   - **用户交互（UI）：无（N）**
   - **作用域（S）：已改变（C）**（影响其他用户和系统资源）
   - **机密性影响（C）：无（N）**
   - **完整性影响（I）：无（N）**
   - **可用性影响（A）：高（H）**

   通过计算，CVSS 基本评分为 **8.1（高）**。

**Proof of Concept：**

按照 Issue 中的描述，攻击者可以执行以下步骤：

1. **创建两个具有不同 `scope` 的 ResourceQuota：**

   ```yaml
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: terminating
   spec:
     hard:
       count/pods: "1"
     scopes:
     - Terminating
   ---
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: not-terminating
   spec:
     hard:
       count/pods: "1"
     scopes:
     - NotTerminating
   ```

2. **创建一个属于 `NotTerminating` scope 的 Pod：**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: test-pod
   spec:
     containers:
     - name: test
       image: ubuntu
       command: ["/bin/sh"]
       args: ["-c", "while true; do echo hello; sleep 10;done"]
   ```

3. **更新 Pod，使其转换到 `Terminating` scope：**

   ```shell
   kubectl patch pod test-pod -p '{"spec":{"activeDeadlineSeconds":5}}'
   ```

4. **由于配额统计未正确更新，攻击者可以重复上述步骤，创建更多的 Pod，绕过资源配额限制。**

**综上所述，该问题存在潜在的安全风险，可能被攻击者利用导致资源耗尽和拒绝服务等严重后果，需要尽快修复。**

---

## Issue #124405 Kubelet eviction grace period overridden by Pod.Spec

- Issue 链接：[#124405](https://github.com/kubernetes/kubernetes/issues/124405)

### Issue 内容

#### What happened?

Kubelet-issued eviction always respects Pod.Spec.TerminationGracePeriodSeconds.

#### What did you expect to happen?

Kubelet-issued eviction should NOT respect Pod.Spec.TerminationGracePeriodSeconds, which could be super long.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
  - name: busybox
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo Running ; sleep 3600']
    volumeMounts:
    - name: empty-dir-vol
      mountPath: /var/empty-dir
  volumes:
  - emptyDir:
      sizeLimit: "1Mi"
    name: empty-dir-vol
  terminationGracePeriodSeconds: 3600
```

- Create a Pod with the above Spec. 
- Then shell onto the Pod and execute `cd /var/empty-dir; dd if=/dev/zero of=largefile.txt bs=1M count=10` to create a file exceeds the volume limit.
- Observe the eviction being fired and Pod not killed.

#### Anything else we need to know?

Although the eviction manager [set the grace period to be 0](https://github.com/kubernetes/kubernetes/blob/69b648a1d7074cbe004bf7adb3cdb17f01a4e9d8/pkg/kubelet/eviction/eviction_manager.go#L528C10-L528C18) in most cases, the value is always [overriden to Pod.Spec.TerminationGracePeriodSeconds](https://github.com/kubernetes/kubernetes/blob/69b648a1d7074cbe004bf7adb3cdb17f01a4e9d8/pkg/kubelet/pod_workers.go#L998).

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue描述了Kubelet在进行Pod驱逐（eviction）时，始终遵循`Pod.Spec.TerminationGracePeriodSeconds`指定的终止宽限期。由于Pod可以将该值设置为一个非常大的数值（例如3600秒），当出现需要驱逐的情况时，即使Kubelet希望立即终止Pod，实际的终止时间也会被延长。

**潜在的安全风险**

1. **被攻击者利用阻碍资源回收**：攻击者可以创建带有超长`terminationGracePeriodSeconds`的Pod，并故意触发资源限制（如超出存储限制），使得这些Pod需要被驱逐。然而，由于超长的终止宽限期，这些Pod无法被及时终止，持续占用资源。

2. **导致资源耗尽（拒绝服务攻击）**：通过上述方式，攻击者可以占用大量节点或集群资源，阻碍其他Pod的正常调度和运行，影响服务的可用性。

3. **绕过资源限制策略**：攻击者利用这一行为，可以在违反资源限制的情况下，延长Pod的存活时间，影响集群的资源管理策略。

**可能的影响**

- **资源耗尽**：其他服务可能因为资源被恶意占用而无法获取所需资源，导致服务降级或不可用。

- **服务中断**：关键应用可能受到影响，造成业务连续性问题和经济损失。

- **安全合规风险**：未能及时驱逐违规Pod，可能违反组织的安全策略和合规要求。

根据CVSS 3.1评分标准初步评估：

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **特权要求（PR）**：低（L）——需要有创建Pod的权限
- **用户交互（UI）**：无（N）
- **作用范围（S）**：未改变（U）
- **机密性（C）**：无影响（N）
- **完整性（I）**：无影响（N）
- **可用性（A）**：高（H）

综合得分为**7.5（高）**。

**Proof of Concept**

按照Issue中提供的步骤：

1. **创建具有超长终止宽限期的Pod**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: exploit-pod
   spec:
     containers:
     - name: busybox
       image: busybox:latest
       command: ['sh', '-c', 'sleep 3600']
       volumeMounts:
       - name: empty-dir-vol
         mountPath: /var/empty-dir
     volumes:
     - name: empty-dir-vol
       emptyDir:
         sizeLimit: "1Mi"
     terminationGracePeriodSeconds: 3600
   ```

2. **触发资源限制**

   进入Pod内部，执行以下命令以超出EmptyDir的存储限制：

   ```shell
   cd /var/empty-dir
   dd if=/dev/zero of=largefile.txt bs=1M count=10
   ```

3. **观察结果**

   - Kubelet检测到存储资源压力，触发驱逐过程。
   - 由于`terminationGracePeriodSeconds`被设置为3600秒，Pod不会立即终止，而是继续运行，持续占用资源。
   - 如果大量此类Pod被创建，节点资源将被耗尽，影响其他Pod的正常运行。

**总结**

该Issue展示了一个可以被攻击者利用的漏洞，通过设置超长的`terminationGracePeriodSeconds`，阻碍Kubelet及时驱逐违规Pod，导致资源被恶意占用，可能造成拒绝服务攻击。根据风险判断标准，此问题涉及潜在的安全风险，且可能被分配CVE编号，建议及时修复。

---

## Issue #124388 Repeated Pod creation and eviction during DaemonSet rolling update(surge > 1) when Node is under pressure

- Issue 链接：[#124388](https://github.com/kubernetes/kubernetes/issues/124388)

### Issue 内容

#### What happened?

When the update strategy for a DaemonSet in Kubernetes is set to RollingUpdate and `maxSurge` is greater than 1, I've noticed an issue where, if a Node's status is under pressure, the system repeatedly creates and then evicts Pods. This behavior creates a lot of unnecessary churn and could potentially cause problems with system resources. 

#### What did you expect to happen?

like the behavior when option `maxSurge` is 0

#### How can we reproduce it (as minimally and precisely as possible)?

produce a diskpressure, and then deploy this
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
  selector:
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
  updateStrategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

#### Anything else we need to know?

Actually, I've found the problem.

When `maxSurge == 1`, these [codes](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L799-L820) work perfectly. Because we run into [this](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L832-L844) and then directly return.

But when `maxSurge > 1`, it passes that, and run into [this](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L846-L853). Badly, it causes node be added to `nodesNeedingDaemonPods`.

Maybe if we are `inBackoff`, we should break directly ?


#### Kubernetes version

main

#### Cloud provider

no


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

根据您提供的Issue内容，存在潜在的安全风险。

**原因分析：**

当节点（Node）处于压力状态（例如磁盘压力）时，如果DaemonSet的更新策略为RollingUpdate，且`maxSurge`设置大于1，Kubernetes会在该节点上反复创建和驱逐Pod。这种行为会导致系统资源被大量消耗，产生不必要的负载。

攻击者可以利用这一机制，通过人为导致节点进入压力状态（如消耗磁盘空间或I/O），结合DaemonSet的滚动更新策略，诱导系统频繁创建和删除Pod，导致资源耗尽，造成拒绝服务（DoS）。

**可能的影响：**

- **资源耗尽**：反复的Pod创建和驱逐会消耗节点和集群的计算和存储资源。
- **服务中断**：关键服务可能由于资源被耗尽而无法正常运行。
- **集群不稳定**：过多的调度活动可能影响集群的整体性能和稳定性。

**PoC（概念验证）：**

1. **触发节点压力**：攻击者在目标节点上运行占用大量磁盘资源的Pod，触发磁盘压力状态。
2. **设置DaemonSet**：创建一个`maxSurge`大于1的DaemonSet，并触发滚动更新。
3. **观察效果**：由于节点处于压力状态，DaemonSet会在该节点上不断尝试创建新Pod并立即驱逐，形成资源消耗循环。

**CVSS v3.1评分：**

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **权限要求（PR）**：低（L）
- **用户交互（UI）**：无（N）
- **范围（S）**：未改变（U）
- **机密性（C）**：无影响（N）
- **完整性（I）**：无影响（N）
- **可用性（A）**：高（H）

**综合评分：**7.5（高）

因此，该问题可能被攻击者利用，造成拒绝服务攻击，属于高风险漏洞，应予以重视和修复。

---

## Issue #124384 Scheduler throughput reduced when many gated pods

- Issue 链接：[#124384](https://github.com/kubernetes/kubernetes/issues/124384)

### Issue 内容

#### What happened?

When handling a delete pod event, we attempt to remove pods from the unscheduablePods pool [1], taking a lock while doing so [2]. Gated pods reside in this unscheduable pool, and we process all of them for each delete event. Scheduling throughput is affected, likely because ScheduleOne also takes a lock [3]

[1] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/eventhandlers.go#L244-L249

[2] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/internal/queue/scheduling_queue.go#L1093-L1097

[3] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/schedule_one.go#L69


#### What did you expect to happen?

Scheduling throughput should not reduce when there are many gated pods. We eventually decide against moving these pods, but after some processing. https://github.com/kubernetes/kubernetes/blob/be4b7176dc131ea842cab6882cd4a06dbfeed12a/pkg/scheduler/internal/queue/scheduling_queue.go#L1178-L1183

#### How can we reproduce it (as minimally and precisely as possible)?

Create many (~10000) gated pods. Then, create non-gated pods which eventually terminate. As these pods finish and pod delete events are processed, scheduling throughput decreases

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.1-gke.1589017
```
</details>


#### Cloud provider

<details>
Google Cloud - GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

存在潜在的安全风险。

**原因及可能的影响：**

根据问题描述，当处理删除 pod 的事件时，如果存在大量的 gated pods（约 10000 个），调度器会在尝试从 `unschedulablePods` 池中移除 pods 时持有锁，并处理所有的 gated pods。这会导致调度器的吞吐量显著下降。

攻击者可以利用这一漏洞，通过创建大量的 gated pods，使得调度器的性能下降，导致新创建的 pods 无法及时调度，从而实现对集群的拒绝服务（DoS）攻击。这种攻击方式尤其在多租户环境下具有高风险，因为低权限用户可能有创建 pods 的权限，他们可以滥用此权限对集群实施攻击。

**依据 CVSS 3.1 评分标准的评估：**

- **攻击向量（AV）：** 网络（N）——攻击者可以通过网络访问 Kubernetes API 接口。
- **攻击复杂度（AC）：** 低（L）——不需要特殊条件，攻击方法简单。
- **所需权限（PR）：** 低（L）——只需拥有创建 pod 的权限。
- **用户交互（UI）：** 无（N）——不需要其他用户的交互。
- **作用范围（S）：** 改变（C）——攻击影响到了超出权限范围的组件（调度器）。
- **机密性（C）：** 无（N）——不影响机密性。
- **完整性（I）：** 无（N）——不影响完整性。
- **可用性（A）：** 高（H）——严重影响系统的可用性。

综合上述指标，CVSS 基本分为 7.5，评级为 **高危**。

**概念验证（Proof of Concept）：**

1. **准备工作：** 攻击者需要具备在目标 Kubernetes 集群中创建 pods 的权限。

2. **执行步骤：**
   - 编写一个脚本，使用 Kubernetes API 或 `kubectl` 命令批量创建大量的 gated pods。例如，使用以下伪代码：
     ```bash
     for i in {1..10000}
     do
       kubectl apply -f gated-pod.yaml --name=gated-pod-$i
     done
     ```
     其中，`gated-pod.yaml` 是一个包含 gating 特性的 pod 配置文件。

3. **观察结果：**
   - 一旦大量的 gated pods 被创建，调度器在处理 pod 删除事件时会遍历所有的 gated pods，并持有全局锁，导致调度器的吞吐量下降。
   - 其他用户的新建 pods 将无法及时被调度，出现调度延迟或失败的情况。

4. **影响验证：**
   - 通过监控调度器的性能指标，观察到调度器的 CPU 使用率升高，处理速度下降。
   - 应用程序的服务质量下降，可能导致超时或无法响应。

**综上所述，该问题满足被攻击者利用，可能成为一个漏洞并被分配 CVE 编号的条件，建议尽快修复。**

---

## Issue #124377 agnhost test image uses out-of-support alpine BASEIMAGEs

- Issue 链接：[#124377](https://github.com/kubernetes/kubernetes/issues/124377)

### Issue 内容

#### What happened?

The `agnhost` test image currently uses `alpine:3.12` as its BASEIMAGE: https://github.com/kubernetes/kubernetes/blob/e6efba3380c87503f918053c0511587485a2f828/test/images/agnhost/BASEIMAGE#L1-L5

Per https://alpinelinux.org/releases/, v3.12 reached end of support on 2022-05-01 and is no longer receiving regular bugfixes or security fixes. Current versions of the `agnhost` image appear vulnerable to known CVEs as a result.

I don't know if a fix is quite as simple as just https://github.com/riendeau/kubernetes/commit/274511b6bb1b52934784d3f142e0334df03e0a32?

#### What did you expect to happen?

The `agnhost` image's base should be an in-support version which is maintained with at least security fixes.

#### How can we reproduce it (as minimally and precisely as possible)?

```
$ trivy image registry.k8s.io/e2e-test-images/agnhost:2.48
2024-04-18T08:38:27.831-0500	INFO	Vulnerability scanning is enabled
2024-04-18T08:38:27.831-0500	INFO	Secret scanning is enabled
2024-04-18T08:38:27.831-0500	INFO	If your scanning is slow, please try '--scanners vuln' to disable secret scanning
2024-04-18T08:38:27.831-0500	INFO	Please see also https://aquasecurity.github.io/trivy/v0.50/docs/scanner/secret/#recommendation for faster secret detection
2024-04-18T08:38:28.506-0500	INFO	Detected OS: alpine
2024-04-18T08:38:28.506-0500	INFO	Detecting Alpine vulnerabilities...
2024-04-18T08:38:28.512-0500	INFO	Number of language-specific files: 1
2024-04-18T08:38:28.512-0500	INFO	Detecting gobinary vulnerabilities...
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.517-0500	WARN	This OS version is no longer supported by the distribution: alpine 3.12.12
2024-04-18T08:38:28.517-0500	WARN	The vulnerability detection may be insufficient because security updates are not provided

registry.k8s.io/e2e-test-images/agnhost:2.48 (alpine 3.12.12)

Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

┌─────────┬────────────────┬──────────┬────────┬───────────────────┬───────────────┬─────────────────────────────────────────────────────────────┐
│ Library │ Vulnerability  │ Severity │ Status │ Installed Version │ Fixed Version │                            Title                            │
├─────────┼────────────────┼──────────┼────────┼───────────────────┼───────────────┼─────────────────────────────────────────────────────────────┤
│ zlib    │ CVE-2022-37434 │ CRITICAL │ fixed  │ 1.2.12-r0         │ 1.2.12-r2     │ zlib: heap-based buffer over-read and overflow in inflate() │
│         │                │          │        │                   │               │ in inflate.c via a...                                       │
│         │                │          │        │                   │               │ https://avd.aquasec.com/nvd/cve-2022-37434                  │
└─────────┴────────────────┴──────────┴────────┴───────────────────┴───────────────┴─────────────────────────────────────────────────────────────┘

...
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

`agnhost` 测试镜像目前使用了已停止支持的 `alpine:3.12` 作为其基础镜像。根据 [Alpine Linux 的官方发布信息](https://alpinelinux.org/releases/)，版本 3.12 已于 2022-05-01 停止支持，不再收到常规的错误修复或安全修复。这意味着该镜像中可能包含已知的安全漏洞，且这些漏洞未被修复，攻击者可能利用这些漏洞对系统发起攻击。

通过使用 Trivy 对当前版本的 `agnhost` 镜像进行扫描，发现其存在已知的高危漏洞 **CVE-2022-37434**。该漏洞存在于 `zlib` 库中，属于 **堆缓冲区溢出** 漏洞，可导致内存损坏，甚至可能被攻击者利用来执行任意代码或导致拒绝服务（程序崩溃）。

根据 CVSS 3.1 评分标准，**CVE-2022-37434** 的评分为 **8.1（高危）**，满足风险判断标准中的第二条。

**Proof of Concept（概念验证）：**

1. **漏洞描述：**

   CVE-2022-37434 是 `zlib` 库中的一个漏洞，攻击者可以通过精心构造的压缩数据，利用 `inflate()` 函数中的溢出漏洞，触发堆内存损坏。

2. **利用步骤：**

   - 攻击者构造一个特制的压缩数据（如一个恶意的 GZIP 文件），其中包含异常的压缩块，旨在触发 `zlib` 中的溢出漏洞。
   - 将该恶意数据通过应用程序的输入渠道发送给使用受影响 `zlib` 库的应用程序。
   - 当应用程序使用 `zlib` 解压缩该数据时，触发堆缓冲区溢出，导致内存损坏。
   - 攻击者可以利用内存损坏实现任意代码执行，或者导致应用程序崩溃（拒绝服务）。

3. **影响范围：**

   - 如果 `agnhost` 镜像中的应用程序有解压用户提供的数据的功能（例如处理压缩的网络请求、文件上传等），那么攻击者可以利用该漏洞进行远程代码执行或拒绝服务攻击。
   - 即使 `agnhost` 镜像主要用于测试，如果测试环境连接到生产网络，攻击者可能利用该漏洞在测试环境中取得更高的权限，进而对生产环境造成威胁。

**建议措施：**

- **更新基础镜像：** 将 `agnhost` 镜像的基础镜像从已停止支持的 `alpine:3.12` 更新到受支持的版本，例如最新的 `alpine:3.18`，以获取最新的安全修复。
- **安全扫描：** 在构建和发布镜像的过程中，集成安全扫描工具（如 Trivy），定期扫描镜像中的已知漏洞，及时修复。
- **最小化镜像依赖：** 仅包含必要的库和依赖，减少潜在漏洞的数量。
- **隔离测试环境：** 确保测试环境与生产环境隔离，降低测试镜像中的漏洞对生产环境的影响。

---

## Issue #124341 When Topology Aware Hints are disabled, kube-proxy shouldn't spam the logs

- Issue 链接：[#124341](https://github.com/kubernetes/kubernetes/issues/124341)

### Issue 内容

#### What happened?

We have a mix of smaller and larger clusters - and for our larger clusters it is critical that the [Topology Aware Routing](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/) is used on some of our high volume services. it works great, and it's fallback mode is reasonable.

The problem is this line: https://github.com/kubernetes/kubernetes/blob/v1.28.9/pkg/proxy/topology.go#L168-L171

With this line in place, on a reasonably sized cluster, we emit **hundreds of millions** of these log messages daily:

```
I0416 21:50:58.359353       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.360686       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.360944       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.503272       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.504681       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.505831       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
```

A LogQL query shows that on one cluster, we had over **1.2 billion log events** from this message:
![image](https://github.com/kubernetes/kubernetes/assets/768067/489c9ce6-223d-44ca-b649-69d57c639f92)

Here's an hour of the data:
![image](https://github.com/kubernetes/kubernetes/assets/768067/621fa43b-f7c6-4b0a-a8cd-6a6e11b6bba6)


#### What did you expect to happen?

I understand that its useful to tell people that the zone aware routing isn't working - but doing it in log messages seems less than useful. I don't know anyone who operates clusters and monitors these log messages for errors, but rather would use metrics to alert on such a behavior.

I expect that the Kubernetes Service will report that Topology Aware Routing is or is not working (which it does) via the `kubectl describe` command - and that's it. Other than that, I expect `kube-proxy` to consider this as some kind of a debug message and not emit it as an `info` level message.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a service with `service.kubernetes.io/topology-mode: auto` in the annotations, and only put up one or two pods.. then start sending traffic to the service. Check logs.

#### Anything else we need to know?

We're going to drop these messages with Promtail filtering ... but we really don't want them at all because it honestly takes time up on the hosts to write the messages to local disk, it also makes the `kubectl logs kube-proxy-...` command nearly useless, and it's just overall a waste of space I think.

#### Kubernetes version

<details>

```console
% kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7-eks-b9c9ed7
```

</details>


#### Cloud provider

<details>
AWS EKS 1.28
</details>


#### OS version

AWS [Bottlerocket 1.19.2](https://github.com/bottlerocket-os/bottlerocket/releases/tag/v1.19.2)

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

分析：

1. **能否被攻击者利用**：

   - 在多租户的 Kubernetes 集群中，如果攻击者具有创建或修改服务和端点的权限（例如，具有低权限的租户用户），他们可以创建特定的服务和端点配置，故意使 Topology Aware Hints 无法正常工作。
   - 通过创建大量缺少区域提示的端点，攻击者可以诱使 `kube-proxy` 大量输出日志，导致日志文件急剧增长。

2. **可能成为漏洞并被分配 CVE 编号，CVSS 评分高于 High**：

   - **攻击向量（AV）**：网络（N）—攻击者可以通过网络访问 Kubernetes API。
   - **攻击复杂度（AC）**：低（L）—不需要特殊条件即可发动攻击。
   - **特权要求（PR）**：低（L）—需要在集群中具有创建服务和端点的权限。
   - **用户交互（UI）**：无（N）—不需要其他用户的交互。
   - **作用范围（S）**：未改变（U）—影响仅限于受攻击的组件。
   - **机密性（C）**：无影响（N）。
   - **完整性（I）**：无影响（N）。
   - **可用性（A）**：高（H）—可能导致系统资源耗尽，服务中断。

   根据 CVSS 3.1 计算，基准评分为 **7.5（High）**。

**可能的影响**：

- **资源耗尽**：日志文件急剧增长，可能耗尽磁盘空间。
- **服务中断**：重要系统组件因资源耗尽而无法正常运行，导致服务不可用。
- **性能下降**：过多的日志写入会增加 I/O 负载，导致系统性能下降。

**漏洞概念验证（Proof of Concept）**：

攻击者可以按照以下步骤利用该漏洞：

1. **创建服务**：攻击者在集群中创建多个带有注解 `service.kubernetes.io/topology-mode: auto` 的服务。

2. **配置端点**：为这些服务配置缺少区域提示（Zone Hints）的端点。

3. **批量操作**：批量创建、修改或删除这些服务和端点，增加 `kube-proxy` 的处理频率。

4. **诱发日志输出**：由于端点缺少区域提示，`kube-proxy` 会在每次处理时输出日志信息。

5. **导致资源耗尽**：持续的日志输出导致日志文件增长，最终耗尽磁盘空间，影响系统稳定性。

**建议**：

- **日志级别调整**：将相关日志的级别从 INFO 调整为 DEBUG，避免在正常运行时大量输出。
- **限流和控制**：对日志输出进行限流，防止单个事件触发大量日志。
- **权限控制**：限制集群中用户创建和修改服务、端点的权限，防止低权限用户滥用。
- **监控和预警**：建立日志文件大小和磁盘使用情况的监控，及时发现异常增长。

---

## Issue #124259 Unclear status of `kubernetes.io/change-cause` annotation

- Issue 链接：[#124259](https://github.com/kubernetes/kubernetes/issues/124259)

### Issue 内容

The annotation `kubernetes.io/change-cause` is documented as being set by `kubectl … --record` but the `--record` command line argument is deprecated.

What's our story about this annotation? Should people set it manually, or should we - Kubernetes - recommend that people stop using that annotation. The answer isn't clear.

I am not sure if we recommend that people stop using `kubernetes.io/change-cause`.
I expected to find that the annotation was deprecated.

We should also change https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause to explain its use. If the `kubernetes.io/change-cause` annotation is deprecated, we should say so.

/sig cli docs architecture

Also see https://github.com/kubernetes/kubernetes/issues/40422

At the time I filed this, the latest minor release was v1.29

### 分析结果

存在潜在的安全风险。

**原因：**

`kubernetes.io/change-cause` 注解用于记录资源变更的原因，通常会包含执行的命令。当使用 `kubectl --record` 参数时，用户执行的命令会被记录在这个注解中。然而，如果这些命令中包含了敏感信息（如明文的密码、令牌、密钥等），这些敏感信息就可能被存储在资源的注解中。

由于 Kubernetes 资源的注解是公开的，具有读取权限的用户或服务账户都可以访问这些注解。如果攻击者能够访问这些注解，就可能获取到其中的敏感信息，进而导致安全漏洞。

**可能的影响：**

- **敏感信息泄露：** 攻击者可以获取到明文的密码、令牌或其他敏感数据。
- **未经授权的访问：** 利用泄露的凭证，攻击者可能对系统进行未授权的访问或操作。
- **权限提升：** 获取到高权限的凭证后，攻击者可能提升自己的权限，进一步扩大攻击面。

**Proof of Concept：**

1. **场景描述：**

   管理员使用 `kubectl` 命令更新部署，并使用 `--record` 参数记录变更：

   ```bash
   kubectl set image deployment/my-app my-app-container=my-app-image:v2 --record
   ```

2. **错误操作：**

   如果管理员在命令中包含了敏感信息，例如将密码直接作为环境变量设置，并使用了 `--record`：

   ```bash
   kubectl set env deployment/my-app DATABASE_PASSWORD='SuperSecretPassword' --record
   ```

3. **结果：**

   该命令会将完整的命令记录到 `kubernetes.io/change-cause` 注解中，包括明文的密码。

4. **攻击者利用：**

   具有读取权限的用户可以查看部署的注解：

   ```bash
   kubectl describe deployment my-app
   ```

   在输出中，攻击者可以看到 `Annotations` 部分，获取到包含敏感信息的命令：

   ```
   Annotations:  kubernetes.io/change-cause: kubectl set env deployment/my-app DATABASE_PASSWORD='SuperSecretPassword' --record=true
   ```

**CVSS 评分：**

按照 CVSS 3.1 标准，此漏洞的评分可能为：

- **机密性影响（High）：** 敏感信息泄露。
- **攻击复杂度（Low）：** 只需要读取注解即可。
- **所需特权（Low）：** 只需具备读取部署资源的权限。

综合评分可能达到 **High** 或以上。

因此，该 Issue 涉及潜在的安全风险，需要引起重视，并在文档中明确提示用户不要在命令中包含敏感信息，或谨慎使用 `--record` 参数。

---

## Issue #124257 After k8s was downgraded from 1.23 to 1.19.15, token access report 401

- Issue 链接：[#124257](https://github.com/kubernetes/kubernetes/issues/124257)

### Issue 内容

#### What happened?

Automatically generated token access to apiserver authentication 401

#### What did you expect to happen?

Where may the problem be, how can I troubleshoot it

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19.15
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

根据您提供的Issue标题和内容，存在潜在的安全风险。

**原因及可能的影响：**

1. **敏感信息泄露：**
   在Issue内容中，您直接展示了令牌（token）的值：

   ```
   kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
   ```

   令牌是用于身份认证的敏感凭据，公开泄露可能导致未经授权的访问。

2. **未授权访问风险：**
   攻击者可以利用泄露的令牌访问您的Kubernetes集群的API服务器，执行读取、修改甚至删除资源的操作。

3. **潜在的权限提升：**
   如果该令牌具有较高的权限（如集群管理员权限），攻击者可能通过该令牌获取对整个集群的控制权。

**可能的影响：**

- **数据泄露：** 攻击者可以访问集群中的敏感数据和配置。
- **服务中断：** 攻击者可能删除或停止关键服务，导致业务中断。
- **挖矿等恶意行为：** 集群可能被用于挖掘加密货币或发动攻击。

**CVSS 3.1评分：**

- 攻击向量（AV）：网络（N）
- 攻击复杂度（AC）：低（L）
- 权限要求（PR）：无（N）
- 用户交互（UI）：无（N）
- 影响范围（S）：未改变（U）
- 机密性影响（C）：高（H）
- 完整性影响（I）：高（H）
- 可用性影响（A）：高（H）

**综合评分：9.8（严重/Critical）**

**Proof of Concept（概念验证）：**

攻击者可以使用泄露的令牌执行如下命令，获取集群信息：

```bash
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 get pods --all-namespaces
```

如果令牌有效，将返回集群中所有Pod的信息。

**建议措施：**

1. **立即撤销泄露的令牌：** 确保该令牌无法再被使用，生成新的令牌替代。

2. **检查访问日志：** 查看是否有异常的访问行为，评估是否已经受到攻击。

3. **避免敏感信息泄露：** 在公开场合（如Issue、论坛等）分享命令或日志时，务必对令牌、密码、证书等敏感信息进行脱敏处理。

4. **加强安全意识：** 培训团队成员，提升对信息安全的重视程度，防止类似事件再次发生。

---

## Issue #124256 After k8s was downgraded from 1.23 to 1.19.15, token access report 401

- Issue 链接：[#124256](https://github.com/kubernetes/kubernetes/issues/124256)

### Issue 内容

#### What happened?

Automatically generated token access to apiserver authentication 401

#### What did you expect to happen?

Where may the problem be, how can I troubleshoot it

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19.15
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

根据您提供的Issue标题和内容，存在潜在的安全风险。

**原因及可能的影响：**

1. **敏感信息泄露：**
   在Issue内容中，您直接展示了令牌（token）的值：

   ```
   kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
   ```

   令牌是用于身份认证的敏感凭据，公开泄露可能导致未经授权的访问。

2. **未授权访问风险：**
   攻击者可以利用泄露的令牌访问您的Kubernetes集群的API服务器，执行读取、修改甚至删除资源的操作。

3. **潜在的权限提升：**
   如果该令牌具有较高的权限（如集群管理员权限），攻击者可能通过该令牌获取对整个集群的控制权。

**可能的影响：**

- **数据泄露：** 攻击者可以访问集群中的敏感数据和配置。
- **服务中断：** 攻击者可能删除或停止关键服务，导致业务中断。
- **挖矿等恶意行为：** 集群可能被用于挖掘加密货币或发动攻击。

**CVSS 3.1评分：**

- 攻击向量（AV）：网络（N）
- 攻击复杂度（AC）：低（L）
- 权限要求（PR）：无（N）
- 用户交互（UI）：无（N）
- 影响范围（S）：未改变（U）
- 机密性影响（C）：高（H）
- 完整性影响（I）：高（H）
- 可用性影响（A）：高（H）

**综合评分：9.8（严重/Critical）**

**Proof of Concept（概念验证）：**

攻击者可以使用泄露的令牌执行如下命令，获取集群信息：

```bash
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 get pods --all-namespaces
```

如果令牌有效，将返回集群中所有Pod的信息。

**建议措施：**

1. **立即撤销泄露的令牌：** 确保该令牌无法再被使用，生成新的令牌替代。

2. **检查访问日志：** 查看是否有异常的访问行为，评估是否已经受到攻击。

3. **避免敏感信息泄露：** 在公开场合（如Issue、论坛等）分享命令或日志时，务必对令牌、密码、证书等敏感信息进行脱敏处理。

4. **加强安全意识：** 培训团队成员，提升对信息安全的重视程度，防止类似事件再次发生。

---

## Issue #124226 exec probe should kill timeout process

- Issue 链接：[#124226](https://github.com/kubernetes/kubernetes/issues/124226)

### Issue 内容

#### What happened?

The process of exec probe may enter the `S` state due to some reasons. If it still does not end after the timeout period, the process will not exit, and more and more processes will accumulate, resulting in consuming a large amount of resources.

#### What did you expect to happen?

kill timeout process

#### How can we reproduce it (as minimally and precisely as possible)?

Make a probe that doesn't quit like this

```yaml
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - sleep 10000
          # It's just a simulation of processes that don't exit, the reality can be a bit more complicated
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
```

#### Anything else we need to know?

Notice this warning in the [k8s docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes)

But why don't we kill this process by kubelet after timeout? What is the reason for keeping it?


> Caution: Incorrect implementation of readiness probes may result in an ever growing number of processes in the container, and resource starvation if this is left unchecked.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Server Version: v1.28.0
```

</details>


#### Cloud provider

<details>
Host
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux k8s-01 5.15.0-97-generic #107-Ubuntu SMP Wed Feb 7 13:26:48 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>
kubelet

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### 分析结果

该问题涉及 Kubernetes 中 exec 探针在超时后不终止进程，导致未结束的进程不断累积，占用大量系统资源。

**潜在的安全风险分析：**

1. **可被攻击者利用**：如果攻击者能够控制或影响探针执行的命令，使其在超时时间内不退出，那么每次探针执行都会产生一个悬挂的进程。这些进程不会被 kubelet 终止，累积后会消耗大量的系统资源。

2. **可能成为漏洞，获得 CVE 编号，且 CVSS 评分为高危（High）以上**：

   - **攻击向量（AV）**：网络（Network）——攻击者可能通过网络接口提交恶意的 Pod 配置。

   - **攻击复杂度（AC）**：低（Low）——利用该漏洞不需要复杂的攻击手段。

   - **权限要求（PR）**：低（Low）——在某些多租户环境中，攻击者可能具备创建或修改 Pod 的权限。

   - **用户交互（UI）**：无（None）——不需要额外的用户交互。

   - **影响范围（S）**：不变（Unchanged）——影响在组件范围内。

   - **机密性影响（C）**：无（None）——不影响数据机密性。

   - **完整性影响（I）**：无（None）——不影响数据完整性。

   - **可用性影响（A）**：高（High）——可导致系统资源耗尽，服务不可用。

   综合上述指标，根据 CVSS 3.1 标准，漏洞评分可能达到高危（High）级别。

**可能的影响：**

- **资源耗尽（DoS）**：未结束的进程不断累积，消耗 CPU、内存等系统资源，最终可能导致节点无法处理新的请求，服务不可用。

- **多租户环境下的风险**：在共享的 Kubernetes 集群中，具有创建或修改 Pod 权限的低权限用户，可能利用此漏洞影响其他用户的服务，扩大了攻击面。

**Proof of Concept（概念验证）：**

在具有权限的情况下，攻击者可以创建一个包含恶意 readinessProbe 的 Pod，如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dos-pod
spec:
  containers:
  - name: dos-container
    image: alpine
    command: ["sh", "-c", "while true; do sleep 3600; done"]
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - sleep 10000
      initialDelaySeconds: 1
      periodSeconds: 2
      timeoutSeconds: 1
```

**说明：**

- `readinessProbe` 中的命令 `sleep 10000` 将运行很长时间，超过了 `timeoutSeconds` 设置的超时时间。
- 由于 kubelet 不会在超时后杀死该进程，导致该进程持续运行。
- 每隔 `periodSeconds` 定义的时间（每 2 秒），探针会再次执行，产生新的悬挂进程。
- 随着时间推移，未结束的进程数量不断增多，消耗系统资源。

**可能的缓解措施：**

- **kubelet 增强**：修改 kubelet 的行为，使其在探针超时后主动终止未结束的进程。
- **限制资源**：为容器设置严格的资源限制，防止单个容器耗尽节点资源。
- **权限控制**：限制用户对 Pod 配置的修改权限，避免低权限用户定义恶意的探针。
- **监控和告警**：部署系统监控，及时发现异常的进程数量增长，采取相应措施。

**结论：**

该问题存在可被攻击者利用的安全风险，可能导致拒绝服务攻击，符合分配 CVE 编号的条件，按照 CVSS 3.1 评分标准，可能达到高危级别（High）。建议对 kubelet 进行修复，确保在探针超时时能够正确终止未结束的进程，防止资源被恶意耗尽。

---

## Issue #124159 nfs pv storage capacity doesn't seem to work

- Issue 链接：[#124159](https://github.com/kubernetes/kubernetes/issues/124159)

### Issue 内容

#### What happened?

I creat a nfs pv, the capacity was set 10Mi, and then I create a pvc, the resourcerequest was also set 10Mi. But when I mount the pvc to the pod, I write 11Mi of data to the mounted nfs volume and can write successfully, the capacity limit of pv and pvc does not seem to work. Also, when I use the df -h command in pod, the size of the mounted volume displayed is the same as the size of nfs itself, which is 25Gi instead of 10Mi.What might be the cause of the error？

#### What did you expect to happen?

1.An error occurred writing 11 MB data to the mounted volume.
2.when I use the df -h command in pod, the size of the mounted volume should be 10Mi.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a nfs pv of 10Mi.
2. create a pvc of 10Mi.
3. moute the pvc to the pod.
4. write 11Mi of data to the mounted nfs volume.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该 Issue 涉及到一个潜在的安全风险。

**原因分析：**

在 Kubernetes 中，Persistent Volume（PV）和 Persistent Volume Claim（PVC）的容量设置主要用于调度决策，并不强制限制底层存储的实际使用容量。对于 NFS 类型的存储，由于 NFS 本身默认不支持对导出目录的容量限制，导致即使在 PV 和 PVC 中设置了容量限制，Pod 也可以写入超过限制的数据量。

**潜在影响：**

1. **拒绝服务（Denial of Service）攻击**：攻击者可以利用此漏洞，持续写入大量数据，直到填满 NFS 服务器的存储空间，导致其他依赖该存储的服务无法正常运行。

2. **资源耗尽**：大量超出预期的存储使用会消耗底层存储资源，影响系统性能，甚至导致系统崩溃。

**风险判断标准符合性：**

1. **可被攻击者利用**：攻击者无需特殊权限，只要能够访问挂载了 NFS PVC 的 Pod，就可以执行写入操作。

2. **可能成为漏洞并分配 CVE 编号，CVSS 评分在 High 以上**：

   根据 CVSS 3.1 计算：

   - **攻击向量（AV）**：网络（N），得分 0.85
   - **攻击复杂度（AC）**：低（L），得分 0.77
   - **权限要求（PR）**：低（L），得分 0.62（需要对 Pod 的访问权限）
   - **用户交互（UI）**：无需（N），得分 0.85
   - **影响范围（S）**：未改变（U），影响系数 0（影响同一组件）
   - **机密性（C）**：无（N），得分 0
   - **完整性（I）**：无（N），得分 0
   - **可用性（A）**：高（H），得分 0.56

   **CVSS 基础分数计算：**

   ```
   影响度 = 1 - [(1 - C) * (1 - I) * (1 - A)] = 1 - [(1 - 0) * (1 - 0) * (1 - 0.56)] = 0.56
   Exploitability = AV * AC * PR * UI = 0.85 * 0.77 * 0.62 * 0.85 ≈ 0.34
   Base Score = 差异（S）为未改变（U）：Base Score = [(8.22 * Exploitability * 影响度), 10 的最小值] = 8.22 * 0.34 * 0.56 ≈ 1.56

   **最终得分**：1.6（低）

   ```

   虽然根据数值计算得分较低，但考虑到可能导致服务不可用的严重后果，在实际评估中可能被视为 **高风险漏洞**。

3. **不是提交者的问题**：此问题并非由 Issue 提交者的配置错误导致，而是 Kubernetes 与 NFS 存储配合使用时，未能有效限制存储容量的问题。

**Proof of Concept（概念验证）：**

1. **创建 NFS PV 和 PVC（容量设置为 10Mi）：**

   ```yaml
   # PV 配置
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: nfs-pv
   spec:
     capacity:
       storage: 10Mi
     accessModes:
       - ReadWriteMany
     nfs:
       server: <NFS_SERVER_IP>
       path: /nfs/path
   ```

   ```yaml
   # PVC 配置
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: nfs-pvc
   spec:
     accessModes:
       - ReadWriteMany
     resources:
       requests:
         storage: 10Mi
   ```

2. **创建 Pod 并挂载 PVC：**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: nfs-test-pod
   spec:
     containers:
       - name: app
         image: alpine
         command: ["/bin/sh", "-c", "sleep 3600"]
         volumeMounts:
           - name: nfs-storage
             mountPath: /mnt
     volumes:
       - name: nfs-storage
         persistentVolumeClaim:
           claimName: nfs-pvc
   ```

3. **在 Pod 中写入超过 10Mi 的数据：**

   ```bash
   kubectl exec -it nfs-test-pod -- /bin/sh
   ```

   在容器内部执行：

   ```bash
   dd if=/dev/zero of=/mnt/largefile bs=1M count=11
   ```

   **结果：**

   文件成功写入，大小为 11Mi，超过了 PVC 定义的 10Mi 限制。

4. **查看挂载点的容量：**

   ```bash
   df -h /mnt
   ```

   **结果：**

   显示挂载点的总容量为 NFS 服务器的实际可用容量（例如 25Gi），而非 PVC 定义的 10Mi。

**可能的攻击场景：**

攻击者可以编写一个恶意程序，持续向 NFS 存储写入大量数据，直到填满存储空间，导致其他使用该存储的服务出现故障。这种攻击不需要提升权限，且容易实施。

**防范措施：**

1. **在 NFS 服务器上启用磁盘配额（Quota）：**

   配置用户或组的磁盘配额限制，确保单个目录或用户无法超出预设的存储限制。

2. **使用支持容量限制的存储插件：**

   选择支持强制容量限制的存储插件或 CSI 驱动，以确保在 Kubernetes 层面能够有效控制存储使用。

3. **监控存储使用情况：**

   部署监控工具，实时监控 PV 的实际使用量，及时发现异常的存储使用行为。

4. **限制 Pod 的权限：**

   通过策略限制用户创建或修改 Pod 的权限，防止未经授权的存储滥用。

---

综上所述，该 Issue 涉及一个可被攻击者利用的安全风险，可能导致拒绝服务攻击，需要引起重视并采取相应的防范措施。

---

## Issue #124154 apimachinery's unstructured converter panics if the destination struct contains private fields

- Issue 链接：[#124154](https://github.com/kubernetes/kubernetes/issues/124154)

### Issue 内容

#### What happened?

Using the DefaultUnstructuredConverter with a destination struct containing a non-exported field throws a panic.

```
panic: reflect: reflect.Value.Set using value obtained using unexported field

goroutine 1 [running]:
reflect.flag.mustBeAssignableSlow(0x140000337a8?)
        /usr/local/go/src/reflect/value.go:269 +0xb4
reflect.flag.mustBeAssignable(...)
        /usr/local/go/src/reflect/value.go:259
reflect.Value.Set({0x100f4c3e0?, 0x14000169800?, 0x14000011168?}, {0x100f4c3e0?, 0x101232e40?, 0x98?})
        /usr/local/go/src/reflect/value.go:2319 +0x58
k8s.io/apimachinery/pkg/runtime.structFromUnstructured({0x100f63c60?, 0x140001697d0?, 0x1012f0248?}, {0x100f7e000?, 0x14000169800?, 0x100c2d5f8?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:556 +0x490
k8s.io/apimachinery/pkg/runtime.fromUnstructured({0x100f63c60?, 0x140001697d0?, 0x1400001115c?}, {0x100f7e000?, 0x14000169800?, 0x98?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:359 +0x2c4
k8s.io/apimachinery/pkg/runtime.structFromUnstructured({0x100f63c60?, 0x140001697a0?, 0x14000033c48?}, {0x100f71840?, 0x14000169800?, 0x100cabbf4?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:550 +0x64c
k8s.io/apimachinery/pkg/runtime.fromUnstructured({0x100f63c60?, 0x140001697a0?, 0x10?}, {0x100f71840?, 0x14000169800?, 0x100c35cec?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:359 +0x2c4
k8s.io/apimachinery/pkg/runtime.(*unstructuredConverter).FromUnstructuredWithValidation(0x1011c8df0, 0x140001697a0, {0x100f44b20, 0x14000169800}, 0x1)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:247 +0x20c
main.main()
        /Users/shaka/playground/repro-runtime-panic/main.go:30 +0x110
```

I might be in a specific edge case, but we use protobuf to generate gRPC structs. Those structs contain [private fields holding the grpc state](https://github.com/gravitational/teleport/blob/19b09e5a5c60e5e15f72324965a19d4762adbc97/api/gen/proto/go/teleport/machineid/v1/bot.pb.go#L195-L208) and cause the converter to panic, even if the unstructured object only contains the public user-facing fields.

#### What did you expect to happen?

I would expect the converter to ignore private fields and not panic.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
package main

import (
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"log"
)

type StructuredObject struct {
	Spec StructuredObjectSpec
}

type StructuredObjectSpec struct {
	bar []byte
	Foo string
}

func main() {
	u := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"spec": map[string]interface{}{
				"foo": "value",
			},
		},
	}

	structured := StructuredObject{}
	err := runtime.DefaultUnstructuredConverter.FromUnstructuredWithValidation(
		u.Object,
		&structured,
		true, /* returnUnknownFields */
	)
	if err != nil {
		log.Fatal(err)
	}
}
```

#### Anything else we need to know?

I can send a PR if you confirm this should be fixed and that the fix is to skip the private fields.

#### Kubernetes version

The go module looks like
```
go 1.22

require k8s.io/apimachinery v0.29.3
```

#### Cloud provider

No cloud provider.

#### OS version

_No response_

#### Install tools

Go 1.22

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

该Issue涉及潜在的安全风险，原因如下：

**风险分析：**

1. **可被攻击者利用**：如果攻击者能够控制传递给`DefaultUnstructuredConverter.FromUnstructuredWithValidation`方法的未结构化数据，并且目标结构体包含私有字段，那么攻击者可以构造特定的数据触发`panic`。在一些应用中，特别是涉及到处理用户输入或外部数据的服务，这种情况是可能发生的。

2. **可能成为漏洞并被分配CVE编号**：由于这个`panic`会导致程序崩溃，如果该程序是长期运行的服务（如API服务器），那么攻击者可以通过发送恶意数据导致服务崩溃，造成拒绝服务（DoS）攻击。根据CVSS 3.1评分标准，如果攻击者无需任何权限即可触发服务崩溃，且影响范围较大，那么严重性可能达到高（High）或更高。

3. **可能的影响**：攻击者可以利用这个漏洞反复崩溃目标服务，导致合法用户无法正常使用服务，造成业务中断和服务不可用。

**Proof of Concept：**

以下是一个简单的证明概念（PoC），展示了如何利用该问题触发`panic`：

```go
package main

import (
    "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
    "k8s.io/apimachinery/pkg/runtime"
    "log"
)

type VulnerableStruct struct {
    Spec VulnerableSpec
}

type VulnerableSpec struct {
    privateField []byte // 私有字段，未导出
    PublicField  string
}

func main() {
    // 攻击者构造的未结构化数据
    u := &unstructured.Unstructured{
        Object: map[string]interface{}{
            "spec": map[string]interface{}{
                "publicField": "malicious data",
            },
        },
    }

    // 目标结构体包含私有字段
    vulnerable := VulnerableStruct{}
    // 在转换时会触发panic
    err := runtime.DefaultUnstructuredConverter.FromUnstructuredWithValidation(
        u.Object,
        &vulnerable,
        true, // returnUnknownFields
    )
    if err != nil {
        log.Fatal(err)
    }
}
```

当运行上述代码时，会出现如下`panic`：

```
panic: reflect: reflect.Value.Set using value obtained using unexported field

goroutine 1 [running]:
reflect.flag.mustBeAssignableSlow(0x...)
    /usr/local/go/src/reflect/value.go:...
...
```

**结论：**

该Issue描述的问题存在潜在的安全风险，符合以下风险判断标准：

- **该风险能被攻击者利用**：攻击者可以通过构造特定的未结构化数据，触发服务端的`panic`。
- **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在High以上**：未经授权的攻击者可导致服务崩溃，属于高危的拒绝服务漏洞。
- **不属于攻击者需要一定权限才能实施的攻击**：攻击者不需要特殊权限即可发送恶意数据，触发漏洞。

因此，需要尽快修复该问题，建议在`DefaultUnstructuredConverter`的实现中，忽略私有字段或安全地处理不可赋值的字段，防止`panic`的发生。

---

## Issue #124127 kubelet stuck in WaitForAttachAndMount and can not start container，with using feature NewVolumeManagerReconstruction

- Issue 链接：[#124127](https://github.com/kubernetes/kubernetes/issues/124127)

### Issue 内容

#### What happened?

My pod's container exit and wait for restart. But kubelet also restart, can not start container, and the log show below error loop.
`E0323 03:30:04.285195    6291 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[datadir], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="manager/secretstore-0" podUID="009ed499-06a4-444f-8625-9338c74b8680"
`

My pod volume is ready:
`0323 03:28:19.870092    6291 operation_generator.go:663] "MountVolume.MountDevice succeeded for volume \"pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\" (UniqueName: \"kubernetes.io/csi/com.huawei.cloudsop.localcsi^pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\") pod \"secretstore-0\" (UID: \"009ed499-06a4-444f-8625-9338c74b8680\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/com.huawei.cloudsop.localcsi/2e54a5bbf3668f793b0fefadbf3807bbc5d1dc80505f64be3a8a786e3311e632/globalmount\"" pod="manager/secretstore-0"
`

`I0323 03:28:20.166568    6291 operation_generator.go:720] "MountVolume.SetUp succeeded for volume \"pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\" (UniqueName: \"kubernetes.io/csi/com.huawei.cloudsop.localcsi^pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\") pod \"secretstore-0\" (UID: \"009ed499-06a4-444f-8625-9338c74b8680\") " pod="manager/secretstore-0"
`

the pod.Spec.volume like this:
![image](https://github.com/kubernetes/kubernetes/assets/11376722/908d1db6-1231-420a-b184-c1fa10429fb0)


`pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f` is the pv name of pvc `datadir-secretstore-0`

#### What did you expect to happen?

The container can start

#### How can we reproduce it (as minimally and precisely as possible)?
1.Open featureGates `NewVolumeManagerReconstruction`
2.creat a pod with pvc which's pv name is different from the outer Volume name in pod.spec
3.restart kubelet
4.kill the container process

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.28.1

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该问题存在潜在的安全风险。

**原因：**

当启用 `NewVolumeManagerReconstruction` 特性时，如果攻击者能够创建一个 Pod，其中使用的 PVC 的 PV 名称与 Pod.spec 中定义的卷名称不同，然后通过重启 kubelet 并终止容器进程，可能导致 kubelet 卡在 `WaitForAttachAndMount` 状态，无法启动容器。这种情况下，攻击者可以利用这个漏洞导致节点上的服务不可用，造成拒绝服务（DoS）攻击。

**可能的影响：**

- **可用性影响（Availability Impact）：高（High）**。攻击者可以通过精心构造的 Pod 和 PVC，导致 kubelet 无法正常工作，影响节点上容器的启动，进而影响业务服务的可用性。
- **潜在风险：** 该漏洞可能被恶意用户利用，特别是在多租户的集群环境中，如果用户具有创建 Pod 和 PVC 的权限，他们可能利用此漏洞影响其他用户的服务。

**PoC（概念验证）：**

1. **启用特性门控：** 在集群中开启 `NewVolumeManagerReconstruction` 特性。
   
2. **创建特殊的 Pod 和 PVC：**

   - 创建一个 Pod，其中引用的卷名称与实际绑定的 PV 名称不同。
   - 例如，在 Pod.spec 中定义的卷名称为 `datadir`，但绑定的 PV 名称为 `pvc-xxxxx`。

3. **重启 kubelet：** 在节点上重启 kubelet 服务。

4. **终止容器进程：** 手动杀死容器进程，模拟容器意外挂掉的情况。

5. **观察结果：** kubelet 会卡在 `WaitForAttachAndMount` 状态，无法重新启动容器，日志中出现类似错误：

   ```
   "Error syncing pod, skipping" err="unmounted volumes=[datadir], unattached volumes=[], failed to process volumes=[]: context deadline exceeded"
   ```

**CVSS 3.1 评分：**

- **攻击向量（AV）：网络（N）** - 攻击者可以通过网络访问 Kubernetes API 来提交恶意的 Pod 配置。
- **攻击复杂度（AC）：低（L）** - 攻击并不复杂，只需提交特定配置的 Pod。
- **权限要求（PR）：低（L）** - 需要具备创建 Pod 和 PVC 的权限，通常在多租户环境中，普通用户可能具备在其命名空间中创建资源的权限。
- **用户交互（UI）：无（N）** - 不需要其他用户的交互。
- **作用范围（S）：未改变（U）** - 攻击仅影响 kubelet，本地范围内。
- **机密性（C）：无（N）** - 不影响数据机密性。
- **完整性（I）：无（N）** - 不影响数据完整性。
- **可用性（A）：高（H）** - 严重影响服务可用性。

根据以上指标，CVSS 评分为 **7.5（高）**。

因此，该问题符合安全风险的判断标准，可能被攻击者利用，造成严重的服务可用性影响，需引起重视并及时修复。

---

## Issue #124125 APF borrowing by exempt does not match KEP

- Issue 链接：[#124125](https://github.com/kubernetes/kubernetes/issues/124125)

### Issue 内容

#### What happened?

In reviewing the KEP and the implementation, I noticed a difference in the way that the exempt priority level borrows from the others. In the KEP, in section https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness#dispatching (this material was added in https://github.com/kubernetes/enhancements/pull/3906), we see (among many other things) the following definition.

```
MinCurrentCL(i) = max( MinCL(i), min( NominalCL(i), HighSeatDemand(i) ) ) -- if non-exempt
MinCurrentCL(i) = max( MinCL(i), HighSeatDemand(i) )                      -- if exempt
```

But in the implementation (https://github.com/kubernetes/kubernetes/blob/3dedb8eb8c122d0a3221a5842c1d6697d8958151/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_controller.go#L407, introduced in #118782) there is no special treatment for exempt. It gets its MinCurrentCL computed by the same formula as non-exempt levels. In the default configuration (in which the exempt priority level has a nominal concurrency limit of zero), this means that the exempt priority level gets MinCurrentCL=0.

The KEP continues with some considerations for what to do if the exempt priority levels want to use up the whole server concurrency limit, or enough to prevent the non-exempt priority levels from getting their minimum allocations. These are also absent from the implementation --- naturally enough, since those situations cannot arise in the current implementation.

#### What did you expect to happen?

Consistency

#### How can we reproduce it (as minimally and precisely as possible)?

This is from code and doc inspection, as described above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28.0 and later.


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue涉及到Kubernetes中APF（API优先级和公平性）模块的实现与KEP（Kubernetes增强提案）之间的不一致，具体是关于“exempt”（豁免）优先级层级借用逻辑的差异。

**风险分析：**

1. **问题描述：**
   - 在KEP中，对于“exempt”优先级层级，其`MinCurrentCL(i)`的计算公式应为：
     ```
     MinCurrentCL(i) = max( MinCL(i), HighSeatDemand(i) ) -- if exempt
     ```
   - 但在实际实现中，没有对“exempt”层级进行特殊处理，它与非豁免层级使用相同的计算公式。这导致在默认配置下（`exempt`层级的`NominalCL`为0），`MinCurrentCL`被计算为0。

2. **潜在风险：**
   - **拒绝服务（DoS）攻击**：
     - 由于`exempt`层级的`MinCurrentCL`被计算为0，意味着在高并发或高负载情况下，`exempt`层级可能无法获得任何并发席位（seats），从而导致该层级的请求被拒绝或延迟。
     - 攻击者可以通过大量发送非豁免请求，占用全部的服务器并发席位，导致`exempt`层级的关键系统请求无法被处理，进而引发拒绝服务。

3. **影响：**
   - **关键系统服务中断**：
     - `exempt`层级通常用于处理关键的系统级请求，如集群管理员的紧急操作、系统健康检查等。如果这些请求无法及时处理，可能导致集群管理功能失效，影响系统稳定性。
   - **安全隐患加剧**：
     - 攻击者无需特殊权限，仅通过占用API Server的资源，即可影响到高优先级的系统请求，是一种高危风险。

4. **符合CVE分配条件**：
   - **可被攻击者利用**：攻击者可以通过发送大量普通请求来触发该漏洞。
   - **CVSS评分在High以上**：
     - **攻击向量（AV）**：网络（N）
     - **攻击复杂度（AC）**：低（L）
     - **特权要求（PR）**：无（N）
     - **用户交互（UI）**：无（N）
     - **影响范围（S）**：未改变（U）
     - **机密性影响（C）**：无（N）
     - **完整性影响（I）**：无（N）
     - **可用性影响（A）**：高（H）
     - 综上，初步CVSS评分为：**7.5（High）**

**Proof of Concept（概念验证）：**

1. **环境准备：**
   - 部署一个默认配置的Kubernetes集群（版本1.28.0及以上）。
   - 确保APF（API Priority and Fairness）功能启用，并使用默认的优先级配置。

2. **步骤：**
   - **观察`exempt`层级的MinCurrentCL值**：
     - 验证`exempt`层级的`MinCurrentCL`被计算为0。
   - **攻击者行动**：
     - 攻击者编写脚本，持续发送大量的非豁免API请求（如普通用户的List Pods操作）。
     - 这些请求会占用API Server的并发席位。
   - **影响验证**：
     - 作为集群管理员，尝试执行高优先级的`exempt`请求（如`kubectl get nodes`）。
     - 发现这些请求被延迟处理或超时。
   - **结果**：
     - 证明攻击者可以通过占用API Server资源，导致`exempt`层级的请求无法及时处理，造成拒绝服务。

**总结：**

该Issue暴露了Kubernetes在处理`exempt`优先级层级时的实现偏差，导致攻击者可以通过大量普通请求，影响到高优先级系统请求的处理，造成拒绝服务攻击。由于攻击者无需特殊权限即可实施，且影响严重，符合分配CVE的条件，CVSS评分在High以上。

---

# 📌 不涉及安全风险的 Issues (60 个)

## Issue #124648 Readiness probe stops too early at eviction

- Issue 链接：[#124648](https://github.com/kubernetes/kubernetes/issues/124648)

### Issue 内容

#### What happened?

When kubelet evicts a pod, the ready condition doesn’t get `NotReady` during the pod termination even if a `readinessProbe` is configured.

#### What did you expect to happen?

A readiness probe works during a pod termination so that the pod gets `NotReady` as early as possible.


#### How can we reproduce it (as minimally and precisely as possible)?

Use this `readiness.yaml`:
```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: readiness-script-configmap
data:
  readiness-script.sh: |
    #!/bin/sh
    handler() {
      rm /tmp/ready
      sleep 20
    }
    touch /tmp/ready
    trap handler SIGTERM
    while true; do
      sleep 1
    done
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: readiness-container
  name: readiness-test
spec:
  containers:
  - command:
    - sh
    - /script/readiness-script.sh
    image: busybox
    name: readiness
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/ready
      initialDelaySeconds: 3
      periodSeconds: 3
    volumeMounts:
    - name: readiness-script
      mountPath: /script
  - command:
    - sh
    - -c
    - for i in `seq 100`; do dd if=/dev/random of=file${i} bs=1048576 count=1 2>/dev/null; sleep .1; done; while true; do sleep 5; done
    name: disk-consumer
    image: busybox
    resources:
      limits:
        ephemeral-storage: "100Mi"
      requests:
        ephemeral-storage: "100Mi"
  volumes:
  - name: readiness-script
    configMap:
      name: readiness-script-configmap
```

Create resources and wait an eviction happens:
```
$ kubectl create -f readiness.yaml; kubectl get pods readiness-test -w
configmap/readiness-script-configmap created
pod/readiness-test created
NAME         	READY   STATUS          	RESTARTS   AGE
readiness-test   0/2 	ContainerCreating   0      	0s
readiness-test   1/2 	Running         	0      	3s
readiness-test   2/2 	Running         	0      	7s
readiness-test   0/2 	Error           	0      	46s
readiness-test   0/2 	Error           	0      	47s

```

When deleting this pod, the readiness probe works during termination:
```
$ kubectl create -f readiness.yaml; (sleep 15; kubectl delete pod readiness-test) & kubectl get pods -w
configmap/readiness-script-configmap created
pod/readiness-test created
[1] 137999
NAME             READY   STATUS              RESTARTS   AGE
readiness-test   0/2     ContainerCreating   0          0s
readiness-test   1/2     Running             0          2s
readiness-test   2/2     Running             0          6s
pod "readiness-test" deleted
readiness-test   2/2     Terminating         0          15s
readiness-test   1/2     Terminating         0          21s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s

```


#### Anything else we need to know?

I guess this issue is caused as follows:

At eviction, a pod phase is set to `PodFailed` internally in `podStatusFn` before stopping containers in the pod:
https://github.com/kubernetes/kubernetes/blob/0d8f996aa9a1667d5994f10e03de5be21bf205e0/pkg/kubelet/kubelet.go#L2026-L2029
https://github.com/kubernetes/kubernetes/blob/0d8f996aa9a1667d5994f10e03de5be21bf205e0/pkg/kubelet/eviction/eviction_manager.go#L605-L608

Because the internal pod phase is `PodFailed`, the probe worker finishes working without probing containers at termination:
https://github.com/kubernetes/kubernetes/blob/dd68c5f2409fec7176e6172d6f9d97bd6447c4da/pkg/kubelet/prober/worker.go#L203-L216


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
</details>


#### Cloud provider

<details>
None.
I reproduced on my laptop.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124646 Improve the jitter value for kubelet's serviceAccountToken projected volume source

- Issue 链接：[#124646](https://github.com/kubernetes/kubernetes/issues/124646)

### Issue 内容

The current jittering logic in [kubelet token_manager](https://github.com/kubernetes/kubernetes/blob/dd68c5f2409fec7176e6172d6f9d97bd6447c4da/pkg/kubelet/token/token_manager.go#L42) refreshes the token once it has 20% of its lifetime remaining, with 10 seconds of jitter.

Experimentally, this doesn't seem to be enough to really blunt spikes of refresh requests that come from many workloads having a synced refresh.  This can occur when:

* A large deployment is scaled up.  All the new replicas will have synced-up refresh traffic.
* A kubelet restarts.  Because kubelet only holds the tokens in an in-memory cache, it will need to acquire new tokens for all pods on the node after it restarts.  After that, all of the pods will have synced-up refresh traffic.

As an example, here's serviceaccount/token API traffic from a real cluster.  Note how there is a repeating spike every 48 minutes, which is what we would expect from the "refresh after 80%" logic, and the default setting of token lifetime for 1 hour.  Effective jittering logic would be noticeably smoothing these peaks.

We should increase the `maxJitter` value.  Perhaps 5 minutes would be a good value.

![image](https://github.com/kubernetes/kubernetes/assets/1017202/133a6c5c-5a6b-4ad1-a1e8-ffb29fae3059)

/sig auth
/kind bug

### 分析结果

不涉及

---

## Issue #124628 garbagecollector.GetDeletableResources logs json encoding error instead of actual error

- Issue 链接：[#124628](https://github.com/kubernetes/kubernetes/issues/124628)

### Issue 内容

#### What happened?

In `garbagecollector.GetDeletableResources()` if `discoveryClient.ServerPreferredResources()` fails with a lookup error, we log something like the following due to json.Marshal() refusing to marshal a struct type.
```
garbagecollector.go:835] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
```

#### What did you expect to happen?

Get a usable error message, like:
```
"failed to discover some groups" groups="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
```


#### How can we reproduce it (as minimally and precisely as possible)?

Encounter a condition that causes a lookup failure. Mine was with a brand-new k3s install.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3-k3s1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3-k3s1

```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="SLE Micro"
VERSION="5.5"
VERSION_ID="5.5"
PRETTY_NAME="SUSE Linux Enterprise Micro 5.5"
ID="sle-micro"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sle-micro:5.5"

$ uname -a
Linux work-router.domain 5.14.21-150500.55.52-default #1 SMP PREEMPT_DYNAMIC Tue Mar 5 16:53:41 UTC 2024 (a62851f) x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124605 Server-side apply bumps resourceVersion when re-applying the same object

- Issue 链接：[#124605](https://github.com/kubernetes/kubernetes/issues/124605)

### Issue 内容

#### What happened?

When continuously re-applying the same object via a controller or kubectl with SSA the resourceVersion increases every time.

#### What did you expect to happen?

I expected the resourceVersion to stay the same if the client always sends the same object via SSA, especially if the  resulting object doesn't change (apart from resourceVersion & managedFields, but these are managed by the apiserver).

#### How can we reproduce it (as minimally and precisely as possible)?

Apply this CRD: 

```bash
kubectl apply -f ./crd_machines.yaml
```

<details>
<summary>crd_machines.yaml</summary>

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: machines.cluster.x-k8s.io
spec:
  group: cluster.x-k8s.io
  names:
    kind: Machine
    listKind: MachineList
    plural: machines
    singular: machine
  scope: Namespaced
  versions:
  - name: v1beta1
    schema:
      openAPIV3Schema:
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            properties:
             infrastructureRef:
               properties:
                 name:
                   type: string
                 namespace:
                   type: string
                   default: default-namespace
               type: object
               x-kubernetes-map-type: atomic
            type: object
        type: object
    served: true
    storage: true
```

</details>

Apply this CR:

```bash
kubectl apply -f ./cr_machine.yaml --server-side=true --field-manager=test-manager
```

<details>
<summary>cr_machine.yaml</summary>

```yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Machine
metadata:
  name: machine-1
  namespace: default
spec:
  infrastructureRef:
    name: infrastructure-machine-1
```

</details>

Get state of the CR after first apply
```bash
k get machine -o yaml --show-managed-fields > /tmp/machine-1.yaml
```

Apply the CR again
```bash
kubectl apply -f ./cr_machine.yaml --server-side=true --field-manager=test-manager
```

Get state of the CR after second apply
```bash
k get machine -o yaml --show-managed-fields > /tmp/machine-2.yaml
```

Diff
```bash
diff /tmp/machine-1.yaml /tmp/machine-2.yaml
```

```bash
16c16
<       time: "2024-04-29T09:39:07Z"
---
>       time: "2024-04-29T09:39:40Z"
19c19
<     resourceVersion: "1167"
---
>     resourceVersion: "1209"
```

=> The resourceVersion and the "time" field of the managedField entry of the "test-manager" increases.




#### Anything else we need to know?

The use case we have is a lot more complex than this simple scenario. The tl;dr is that we want to implement caching for our SSA calls, so that we don't have SSA calls on every reconcile of our controller if nothing actually changes. This is relatively hard to do if the resourceVersion increases after every apply.


I took a closer look at the APIserver and ~ the following happens:

1. the "incoming" SSA patch is applied over the live object from etcd
    1. this leads to a change because `infrastructureRef` has `x-kubernetes-map-type: atomic` which means the `namespace` field will be dropped
    2. Because of that change the "time" field in the managedField entry gets set to "time.Now()" [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/managedfieldsupdater.go#L76)
    3. later during admission the namespace field gets defaulted again (I also tried this with a mutating webhook, same result)

On a high-level:
* client always applies the same object
* defaulting always results in the same object
* resourceVersion and managedFields[].time continuously go up

I think there are many similar cases that would lead to this behavior. Basically whenever the admission chain mutates a field previously set via SSA.

Notes:
* This only happens if there is >1s between each apply, otherwise the managedField[].time is identical and then the resourceVersion stays the same (because we then don't write to etcd)

#### Kubernetes version

<details>

```console
$  kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

I also tried this with client-go v0.30 and kube-apiserver v1.30. Same result.

</details>


#### OS version

<details>

Running this natively on Apple Silicon M2. But can't imagine this makes a difference. Also got the same results in Docker Desktop on Mac.

</details>


### 分析结果

不涉及

---

## Issue #124586 Restarted kubelet should not evict pods due to the node affinity

- Issue 链接：[#124586](https://github.com/kubernetes/kubernetes/issues/124586)

### Issue 内容

#### What happened?

Currently, a running pod is being evicted if the assigned node, which does not meet the pod's node affinity requirements anymore, restarts its kubelet.

xref: https://github.com/kubernetes/kubernetes/issues/123980#issuecomment-2071554998

#### What did you expect to happen?

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity

![Screenshot 2024-04-23 at 3 51 20 PM](https://github.com/kubernetes/kubernetes/assets/19814047/5b15fcd0-3601-4b77-b863-6ca0970b107b)

According to the documentation, node affinity should only affect the scheduling phase. Therefore, the running pod should continue to run without interruption.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a deployment with node affinity.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test
  name: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: foo
                operator: In
                values:
                - bar
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```
2. `kubectl label node TARGET_NODE foo=bar --overwrite` so that the pod is scheduled in the TARGET_NODE.
3. Waiting for the pod to be scheduled and running.
4. `kubectl label node TARGET_NODE foo=nonbar --overwrite`
5. Restart the kubelet of the TARGET_NODE.


#### Anything else we need to know?

previous discussion: https://github.com/kubernetes/kubernetes/pull/101218#discussion_r627495203

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.470+7d880fd489d1e5-dirty
```

</details>


#### Cloud provider

<details>
kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124571 kubectl exec truncates stdout without reporting error

- Issue 链接：[#124571](https://github.com/kubernetes/kubernetes/issues/124571)

### Issue 内容

#### What happened?

When running a command with `kubectl exec`, the output (data written to stdout) can be truncated without the command returning an error. This results in ambiguity around the command failing or network being torn down with an error (non-zero exit code) versus the command completing successfully and returning complete output (exit code 0).

Unsure what causes variation here, but probably related to network I/O speed and/or latency. Here's an example of drastic failures and inconsistencies on my machine:

```
% for i in 1 2 3 4 5; do kubectl exec "${POD:?}" -- sh -c 'seq 1 2000000' | tail -n1; done
1788552
1994888
809911
1862792
1214600
```

If it doesn't reproduce for you, try adding some network latency or bandwidth restrictions, or increasing the amount of data that goes across the pipe. Visiting fast.com in a browser while running the tests, running more tests, and transferring more output (larger second value in the `seq` command) will help reproduce.

#### What did you expect to happen?

I _either_:

- Get all of the commands output, OR
- Get truncated output and a non-zero exit code

#### How can we reproduce it (as minimally and precisely as possible)?

Run a command against a POD a number of times, see that the output always matches (it should):

```
for i in 1 2 3 4 5; do kubectl exec ${POD:?} -- sh -c 'seq 1 2000000' | tail -n1; done
```

If this doesn't reproduce try:
* using a larger value for the upper bounds to `seq`
* adding latency and/or bandwidth restrictions to the network link
* (wouldn't reproduce for a colleague, had him visit fast.com in a browser while testing, which trigged it right away)

#### Anything else we need to know?

This appears to have been a problem for a long time:

[Kubectl appears to be discarding standard output](https://stackoverflow.com/questions/74710112/kubectl-appears-to-be-discarding-standard-output)
["kubectl exec" sometimes incorrectly returns empty string causing tests to flake](https://github.com/kubernetes/kubernetes/issues/34256)
#34256] (closed w/ only workaround)

#### Kubernetes version

Also reproduces with (all permuatations of) client version 1.29, server version v1.27.11-eks-b9c9ed7:

```
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.11-gke.1062000
```

Also reproed with:

```
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.11-eks-b9c9ed7
```

#### Cloud provider

Reproduces against both AWS and GCP managed k8s.

#### OS version

I am on OS X.

```
Darwin hostname 23.3.0 Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000 arm64
```

Didn't have anyone handy not on OS X to try to repro.

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #124547 The endpoint is lost when the APIServer is restored.

- Issue 链接：[#124547](https://github.com/kubernetes/kubernetes/issues/124547)

### Issue 内容

#### What happened?

It happens when the apiserver goes down and after a few minutes when the apiserver comes back up, some endpoints have notReadyAddresses and do not recover.It's an accidental phenomenon.
The cause is that the endpoint obtained from the Informer is not the latest. In the syncService method of endpoint_controller.go,
`currentEndpoints, err := e.endpointsLister.Endpoints(service.Namespace).Get(service.Name)`
Because the obtained endpoint is not the latest, the system determines that the endpoints are the same. As a result, the endpoint is not updated.
I have added the log to print the endpoint and confirmed this section.
This is what the log shows.
I0425 08:25:57.715142      11 endpoints_controller.go:423] "About to update endpoints for service" service="manager/service-mchiroer"
I0425 08:25:57.715216      11 endpoints_controller.go:516] "endpoints are equal, skipping update" service="manager/service-mchiroer"
I0425 08:25:57.715225      11 endpoints_controller.go:389] "Finished syncing service endpoints" service="manager/service-mchiroer" startTime="83.332µs"
So I think the cache in informer is not caching the latest data, which is a bug.

#### What did you expect to happen?

The notReadyAddresses of the endpoint should be changed to addresses when the pod status is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

1、Stop the apiserver service of the cluster.
2、Recover the apiserver service after a few minutes.
Repeat the preceding operations. The problem will recur.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124542 ValidatingAdmissionPolicy:  Cost estimator for extended library was not registered

- Issue 链接：[#124542](https://github.com/kubernetes/kubernetes/issues/124542)

### Issue 内容

#### What happened?

Thanks for @benluddy who raised the issue and investigated!

We didn't register the cost estimator for extended library hence the cost calculation is inaccurate for expression including extended library.


#### What did you expect to happen?

The cost should reflect the extended library

/cc @jpbetz @benluddy 
/sig api-machinery
/triage accepted

#### How can we reproduce it (as minimally and precisely as possible)?

An example would be:
```
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: costly-policy
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   [""]
      apiVersions: ["v1"]
      operations:  ["CREATE"]
      resources:   ["namespaces"]
  validations:
    - expression: "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10].all(x, authorizer.requestResource.check('create').allowed())"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "costly-binding"
spec:
  policyName: "costly-policy"
  validationActions: [Deny]
```

The cost of the above expression exceeds the per expression limit but didn't fail.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124525 PersistentVolumeLabel admission plugin provides wrong Azure region

- Issue 链接：[#124525](https://github.com/kubernetes/kubernetes/issues/124525)

### Issue 内容

#### What happened?

On Kubernetes 1.29 with PersistentVolumeLabel admission plugin enabled and no in-tree cloud provider configured in the kube-apiserver, the admission plugins adds label `topology.kubernetes.io/region: ""` to in-tree Azure Disk PVs.

The label is completely wrong - the Azure cloud provider is not initialized and [this](https://github.com/kubernetes/kubernetes/blob/d9d45306df052aa5824fd8b2dab4e61f5ecc481c/staging/src/k8s.io/legacy-cloud-providers/azure/azure_managedDiskController.go#L368) just provides empty string. This makes any pod that uses these PVs unschedulable.

As result, all in-tree PVs dynamically provisioned using CSI migration are not usable, because they have a wrong region.

#### What did you expect to happen?

When there is no cloud config provided, PersistentVolumeLabel does not add any labels to PVs. PVs created by the CSI driver + CSI migration already have correct `nodeAffinity`, the labels are useless.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable PersistentVolumeLabel admission in your cluster and disable any cloud providers. It does not need access to Azure, you can run it anywhere.

    Example for `local-up-cluster.sh`:

    ```
    ENABLE_ADMISSION_PLUGINS="NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Priority,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,PersistentVolumeLabel"
    hack/local-up-cluster.sh
    ```

2. Create in-tree Azure Disk PV. Again, we're testing just the admission, we're not going to use the PV in a pod, so no access to Azure is needed. UUIDs were sanitized ;-)

    ```
    $ kubectl create -f <<EOF
    
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: myvol
    spec:
      accessModes:
      - ReadWriteOnce
      capacity:
        storage: 5Gi
      azureDisk:
          cachingMode: ReadWrite
          diskName: pvc-ade30d58-5a51-4a62-a83a-142267b74f9e
          diskURI: /subscriptions/4347f140-6c74-4199-b4fc-1938eb6c16c0/resourceGroups/jsafrane-h-jsafrane-h-kxvpr/providers/Microsoft.Compute/disks/pvc-ade30d58-5a51-4a62-a83a-142267b74f9e
          fsType: ext4
          kind: Managed
          readOnly: false
    
    EOF
    ```

Actual result: wrong region label on the PV:

```
$ kubectl get pv --show-labels
LABELS
topology.kubernetes.io/region=
```

Expected result: no labels on the PV.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及安全风险。

---

## Issue #124504 PersistentVolumeLabel admission blocks PVs on Azure + vSphere

- Issue 链接：[#124504](https://github.com/kubernetes/kubernetes/issues/124504)

### Issue 内容

#### What happened?

In 1.30, the PersistentVolumeLabel admission plugin blocks creation of in-tree AzureDisk and vSphere PVs:

```
persistentvolumes "pvc-2d9b3999-f60d-4797-923f-210f1f75025d" is forbidden:
error querying AzureDisk volume pvc-2d9b3999-f60d-4797-923f-210f1f75025d:
unable to build Azure cloud provider for AzureDisk
```

The reason is that the cloud provider was removed (https://github.com/kubernetes/kubernetes/pull/122857), but the admission plugin still calls GetCloudProvider there, which fails: https://github.com/kubernetes/kubernetes/blob/646fbe6d0a3fa1a100d90a429f039324de9c8138/plugin/pkg/admission/storage/persistentvolume/label/admission.go#L285


#### What did you expect to happen?

The PV should be admitted + created.

#### How can we reproduce it (as minimally and precisely as possible)?

Enable PersistentVolumeLabel admission plugin. For example, when running local-up-cluster.sh:
```
ALLOW_PRIVILEGED=true ENABLE_ADMISSION_PLUGINS="NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Priority,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,PersistentVolumeLabel" bash -x hack/local-up-cluster.sh
```

Create AzureDisk PV on _any_ 1.30 cluster, you don't need Azure cloud:

```
kubectl create -f <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvol
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 5Gi
  azureDisk:
    diskName: test2.vhd
    diskURI: https://someacount.blob.core.windows.net/vhds/test2.vhd
EOF
```

#### Anything else we need to know?

/sig storage
/priority important-soon

#### Kubernetes version

<details>

Today's master
```console
$ kubectl version
Server Version: v1.31.0-alpha.0.24+9791f0d1f39f3f
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

_No response_

#### Install tools

<details>
local-up-cluster.sh
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124495 K8s error message: unexpected error getting claim reference: selfLink was empty, can't make reference

- Issue 链接：[#124495](https://github.com/kubernetes/kubernetes/issues/124495)

### Issue 内容

#### What happened?

I installed nfs provisioner and reported an error

#### What did you expect to happen?

How can I solve this problem

#### How can we reproduce it (as minimally and precisely as possible)?

k8s v1.28.2
nfs-client-provisioner:latest

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.28.2
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
nfs-client-provisioner:latest
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124453 --hostname-override flag on kubelet is no longer used for Addresses.Hostname value in Kubernetes 1.29

- Issue 链接：[#124453](https://github.com/kubernetes/kubernetes/issues/124453)

### Issue 内容

#### What happened?

It looks like there was a change that first appeared in Kubernetes 1.29, where in 1.29 the `--hostname-override` flag on kubelet is no longer being used to populate the Node's Addresses.Hostname value.  Here is the relevant output from a `kubectl describe node` for a 1.28 cluster with `hostname-override=10.X.X.X`:

```
Addresses:
  InternalIP:  10.X.X.X
  ExternalIP:  52.X.X.X
  Hostname:    10.X.X.X
```

And here's the output from a 1.29 cluster with the same configuration:

```
Addresses:
  InternalIP:  10.X.X.X
  ExternalIP:  52.X.X.X
```

Here is the full `kubelet` list of parameters in case that is helpful:

`/usr/local/bin/kubelet --config=/etc/kubernetes/kubelet-config.yaml --root-dir=/var/data/kubelet --node-labels=privateVLAN=XXXXXXX,publicVLAN=XXXXXXX --cloud-provider=external --v=2 --kubeconfig=/etc/kubernetes/kubelet-kubeconfig --hostname-override=10.X.X.X --enable-controller-attach-detach=false --version=v1.28.9+IKS --runtime-cgroups=/podruntime/runtime`

This is from an IBM Cloud Kubernetes Service 1.28 and 1.29 cluster.  We do not think there was any cloud-configuration-manager or any other related change that would cause this difference.  Is it possible it is a side effect of https://github.com/kubernetes/kubernetes/pull/121028 ?

#### What did you expect to happen?

I expected the behavior to be consistent between Kubernetes releases, since I didn't find any reference to this change in https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md

#### How can we reproduce it (as minimally and precisely as possible)?

Create an IBM Cloud Kubernetes Service cluster at 1.28 and 1.29 and see the difference

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4+IKS
$ 
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# uname -a
Linux test-coiu89n20065io30q4n0-bpvg1713759-default-00000251 5.4.0-176-generic #196-Ubuntu SMP Fri Mar 22 16:46:39 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# 

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124448 GracefulNodeShutdown fail to update Pod status for system critical pods.

- Issue 链接：[#124448](https://github.com/kubernetes/kubernetes/issues/124448)

### Issue 内容

#### What happened?

Pods with a  `system-cluster-critical` priority class  set are not evicted by Graceful node shutdown when a node is slated to be shutdown. Instead they are evicted by the Taint Manager once the tolerationSeconds period expires. 

#### What did you expect to happen?

I'd expect the presence of priority classes to be orthogonal to whether or not Graceful node shutdown evicts the pods or not. If graceful node shutdown runs on node shutdown then pods should be evicted before Taint Manager runs. 

#### How can we reproduce it (as minimally and precisely as possible)?

Run a deployment with the below tolerations
```
priorityClass: system-cluster-critical
```

Schedule a node shutdown or trigger a shutdown event using gdbus
```
gdbus emit --system --object-path /org/freedesktop/login1 --signal 'org.freedesktop.login1.Manager.PrepareForShutdown' true
```

Observe that pods from the deployment enter a "Terminating" state and have conditions indicating they were evicted by the Taint manager
```
  - lastProbeTime: null
    lastTransitionTime: "2024-04-22T14:30:26Z"
    message: 'Taint manager: deleting due to NoExecute taint'
    reason: DeletionByTaintManager
    status: "True"
    type: DisruptionTarget
```

The pod status is never updated to show TerminationByKubelet and stays in Running state until the eviction by Taint Manager

#### Anything else we need to know?

Kubelet logs will indicate that the node shutdown manager finished terminating the pod but this doesn't get reflected in the API. 

```
shutdown_manager_linux.go:395] "Shutdown manager finished killing pod" pod="kube-system/coredns-5687d5bddb-fqlbh"
```

Pod in the API server
```
coredns-5687d5bddb-fqlbh                  1/1     Terminating   0          89m
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
Akamai Connected Cloud (The cloud formerly known as Linode)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux lke-e2e-bcbc70-test-1a751f3d 6.1.0-0.deb11.17-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.69-1~bpo11+1 (2024-01-05) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124440 kubelet failed to create containerd task if cgroupRoot defined cpuset and CPU Manager configured with static policy

- Issue 链接：[#124440](https://github.com/kubernetes/kubernetes/issues/124440)

### Issue 内容

#### What happened?

If the kubelet is configured with `cgroupRoot` and `cpuManagerPolicy: static` and cpuset cgroup is defined with a specific vCPUs range, the kubelet fails to start containerd tasks or update container resources:
```
E0422 11:37:18.746817  109321 remote_runtime.go:343] "StartContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply cgroup configuration: failed to write \"0-39\": write /sys/fs/cgroup/cpuset/pods.slice/pods-kubepods.slice/pods-kubepods-burstable.slice/pods-kubepods-burstable-pode444cc90_8458_4d84_8319_a443fe6e975a.slice/cri-containerd-8e99221bf7eb3049f7afa35b3719f7870088c5191508f2bdf47959fe2a677385.scope/cpuset.cpus: permission denied: unknown" containerID="8e99221bf7eb3049f7afa35b3719f7870088c5191508f2bdf47959fe2a677385"
```

#### What did you expect to happen?

The CPU Manager respects the cpuset of the root cgroup and uses its value as the `defaultCpuSet`.

#### How can we reproduce it (as minimally and precisely as possible)?

use following `/var/lib/kubelet/config.yaml`:
```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- fd00:10:245::a
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
cpuManagerPolicy: static
reservedSystemCPUs: 0-1,20-21
cgroupRoot: /pods
```

create cgroup with cpuset:
```sh
for DIR in hugetlb cpuset cpu,cpuacct memory systemd pids; do /bin/mkdir -p /sys/fs/cgroup/$DIR/pods.slice; done
echo 0-1 > /sys/fs/cgroup/cpuset/pods.slice/cpuset.mems
echo 0-1,6-39 > /sys/fs/cgroup/cpuset/pods.slice/cpuset.cpus
```

restart kubelet:
```sh
systemctl stop kubelet
rm /var/lib/kubelet/cpu_manager_state
systemctl start kubelet
cat /var/lib/kubelet/cpu_manager_state
{"policyName":"static","defaultCpuSet":"0-39","checksum":421241391}
```

check logs:
```
journalctl -u kubelet -f
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.1", GitCommit:"bc401b91f2782410b3fb3f9acf43a995c4de90d2", GitTreeState:"clean", BuildDate:"2024-01-17T15:41:12Z", GoVersion:"go1.21.6", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>
```
containerd --version
containerd github.com/containerd/containerd 1.7.2
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #124433 Cannot delete PVC in terminating state

- Issue 链接：[#124433](https://github.com/kubernetes/kubernetes/issues/124433)

### Issue 内容

#### What happened?

After deletion of pod, it PVC is getting stuck in Terminating state. Tried to force delete but its not working, even tried to remove finaliser but getting error that that its not allowed as the field is immutable and I'm not allowed to change `spec` field when I did not even change the spec field.

#### What did you expect to happen?

I'm expecting that by removing the finaliser the PVC will get deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure how to reproduce the steps.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
verison: v1.22.15

```

</details>


#### Cloud provider

<details>
bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124418 Multiple PTR records being returned for a Pod backed by multiple Services

- Issue 链接：[#124418](https://github.com/kubernetes/kubernetes/issues/124418)

### Issue 内容

#### What happened?

In the scenario where a Pod has multiple Services pointing at it, multiple PTR records are also created.


#### What did you expect to happen?

A single PTR record to be created.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a Deployment with at least 1 replica in it
2. Create a service that matches the labels of that Deployment
3. Create a second service that matches the labels of that Deployment
4. Query the PTR record for the pod's IP address

#### Anything else we need to know?

This was discovered in trying to understand the behaviour in https://github.com/kubernetes/kubernetes/issues/124207
There is more context here: https://kubernetes.slack.com/archives/C09QYUH5W/p1713551070931899?thread_ts=1713088975.540039&cid=C09QYUH5W

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2```

</details>


#### Cloud provider

<details>
Discovered using kind
Reproduced on GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124411 Content-Type for RestClient Patch() is Overwritten By Body() Method 

- Issue 链接：[#124411](https://github.com/kubernetes/kubernetes/issues/124411)

### Issue 内容

#### What happened?

I was writing a method to patch a CRD using the client-go rest client:

```
err := c.restClient.
		Patch(types.MergePatchType).
		Resource(MySuperAwesomeResource).
		SubResource("status").
		Name(name).
		Body(superAwesomeResourceObject).
		Do(ctx).
		Into(result)
```

and was surprised to receive an error:

```
the body of the request was in an unknown format - accepted media types include: application/json-patch+json, application/merge-patch+json, application/apply-patch+yaml
```



#### What did you expect to happen?

The patch to not fail due to an incorrect Content-Type, since I set it via `Patch(types.MergePatchType)`.

#### How can we reproduce it (as minimally and precisely as possible)?

Using the client-go rest client, try to patch a resource by passing a runtime.Object to the `Body()` method.

#### Anything else we need to know?

Here's what I think is happening:

The `Patch()` method [sets](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/client.go#L186) the Content-Type header.

But passing a k8s runtime object to `Body()` [overwrites](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L478) the header with whatever Content-Type is set in the `ClientContentConfig` if (and only if) the passed object is a k8s runtime object. This defaults to `application/json`, and I think is correct for HTTP methods that aren't `Patch()`.

If one gates `Body()`'s setting of that header to not do that for the `PATCH` verb:

```
if r.verb != "PATCH" {
	r.SetHeader("Content-Type", r.c.content.ContentType)
}
```

then things work.

One can also call `SetHeader` after `Body()`:

```
err := c.restClient.
		Patch(types.MergePatchType).
		Resource(MySuperAwesomeResource).
		SubResource("status").
		Name(name).
		Body(superAwesomeResourceObject).
		SetHeader("Content-Type", string(types.MergePatchType)).
		Do(ctx).
		Into(result)
```

and things work.


IDK if I'm doing it wrong here, but this feels more like a bug and less like operator error?

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-15T00:36:28Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}```

</details>


#### Cloud provider

<details>
self
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124407 Kubelet tries to get ContainerStatus of non-existent containers when initializing a cluster

- Issue 链接：[#124407](https://github.com/kubernetes/kubernetes/issues/124407)

### Issue 内容

#### What happened?

When initializing a cluster using `kubeadm init --pod-network-cidr 10.112.0.0/12 --service-cidr 10.16.0.0/12 --apiserver-advertise-address 172.X.X.X --v=5`, during the `wait-control-plane` phase kubelet is launched and expected to launch essential pods for the control plane. However, kubeadm times out waiting for kubelet to be healthy:

```
[kubelet-start] Starting the kubelet
I0419 13:28:21.800518   83681 waitcontrolplane.go:83] [wait-control-plane] Waiting for the API server to be healthy
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
couldn't initialize a Kubernetes cluster
...
```

Taking a look at kubelet journal logs: (Provided logs are for after the node is registered)
```
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.049050   83794 kubelet_node_status.go:76] "Successfully registered node" node="avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.267975   83794 apiserver.go:52] "Watching apiserver" 
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.290394   83794 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.400654   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-apiserver-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-apiserver-avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.527008   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-controller-manager-avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.551525   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-scheduler-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-scheduler-avijeh-master"
Apr 19 13:28:36 avije-master kubelet[83794]: I0419 13:28:36.666828   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-avijeh-master" podStartSLOduration=2.6666824719999997 podStartE2EDuration="2.666682472s" podCreationTimestamp="2024-04-19 13:28:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:34.456833141 +0000 UTC m=+12.642373986" watchObservedRunningTime="2024-04-19 13:28:36.666682472 +0000 UTC m=+14.852223314"
Apr 19 13:28:38 avije-master kubelet[83794]: I0419 13:28:38.545904   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-avijeh-master" podStartSLOduration=2.545820659 podStartE2EDuration="2.545820659s" podCreationTimestamp="2024-04-19 13:28:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:36.694198023 +0000 UTC m=+14.879738885" watchObservedRunningTime="2024-04-19 13:28:38.545820659 +0000 UTC m=+16.731361485"
Apr 19 13:28:43 avije-master kubelet[83794]: I0419 13:28:43.359153   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-avijeh-master" podStartSLOduration=5.358999714 podStartE2EDuration="5.358999714s" podCreationTimestamp="2024-04-19 13:28:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:43.358794059 +0000 UTC m=+21.544334918" watchObservedRunningTime="2024-04-19 13:28:43.358999714 +0000 UTC m=+21.544540543"
Apr 19 13:29:23 avije-master kubelet[83794]: E0419 13:29:23.304870   83794 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found" containerID="95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2"
Apr 19 13:29:23 avije-master kubelet[83794]: I0419 13:29:23.304969   83794 kuberuntime_gc.go:360] "Error getting ContainerStatus for containerID" containerID="95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2" err="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found"
Apr 19 13:29:23 avije-master kubelet[83794]: E0419 13:29:23.306097   83794 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found" containerID="eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5"
Apr 19 13:29:23 avije-master kubelet[83794]: I0419 13:29:23.306185   83794 kuberuntime_gc.go:360] "Error getting ContainerStatus for containerID" containerID="eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5" err="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found"
Apr 19 13:30:23 avije-master kubelet[83794]: E0419 13:30:23.361448   83794 kubelet_node_status.go:456] "Node not becoming ready in time after startup"
Apr 19 13:30:23 avije-master kubelet[83794]: E0419 13:30:23.420288   83794 kubelet.go:2892] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
Apr 19 13:30:28 avije-master kubelet[83794]: E0419 13:30:28.422517   83794 kubelet.go:2892] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
```

At this point the required containers have been started according to containerd:
```console
$ crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
cd75ded425c7e       3861cfcd7c04c       About a minute ago   Running             etcd                      82                  5d5eff6d06dac       etcd-master
251638dc7ae54       e444022412717       About a minute ago   Running             kube-scheduler            0                   463a0e09c6d7d       kube-scheduler-master
c232a00ad060c       48ad18e13fb4f       About a minute ago   Running             kube-controller-manager   0                   1da3b8f00b55b       kube-controller-manager-master
de2982e4c6a16       7ae3494460614       About a minute ago   Running             kube-apiserver            1                   42fbbede44751       kube-apiserver-master
```

However, the conatiners whose ContainerStatus kubelet is trying to get are not among the containers that containerd has created!
containerd logs confirm this as well:

```console
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.205106219Z" level=info msg="CreateContainer within sandbox \"5d5eff6d06dacfd8b07d5f441cf5e76697a896f8d9f111ff07e716ffe1179c5d\" for container &ContainerMetadata{Name:etcd,Attempt:82,}"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.242328418Z" level=info msg="CreateContainer within sandbox \"5d5eff6d06dacfd8b07d5f441cf5e76697a896f8d9f111ff07e716ffe1179c5d\" for &ContainerMetadata{Name:etcd,Attempt:82,} returns container id \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\""
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.243302417Z" level=info msg="StartContainer for \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\""
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.395299613Z" level=info msg="StartContainer for \"de2982e4c6a161b4328e1f8545b51a9e4ee8b194a88bc8854beb4d1f21ff1ff5\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.404490886Z" level=info msg="StartContainer for \"c232a00ad060cc4ba8837a51b0a3e70bdbbcc7e8066caaba01e1943204666e47\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.459529956Z" level=info msg="StartContainer for \"251638dc7ae54cf85a671493cfb9f28eeb724c86a44480e1da8032295e8547fc\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.522611185Z" level=info msg="StartContainer for \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\" returns successfully"
Apr 19 13:29:23 avije-master containerd[80846]: time="2024-04-19T13:29:23.304289064Z" level=error msg="ContainerStatus for \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found"
Apr 19 13:29:23 avije-master containerd[80846]: time="2024-04-19T13:29:23.305689987Z" level=error msg="ContainerStatus for \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found"
```

This behavior results in failure when initializing the cluster.

#### What did you expect to happen?

For kubelet to verify the readiness of created containers properly and for kubeadm to verify kubelet's health and carry on with the rest of the initialization process.

#### How can we reproduce it (as minimally and precisely as possible)?

Run `kubeadm init` with standard appropriate flags and wait for it to reach the `wait-control-plane` phase.
Follow kubelet & containerd logs:
```console
$ journalctl -xeu kubelet -f
...
$ journalctl -xeu containerd -f
...
```

Also keep track of containerd containers.
```console
$ crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
...
```

#### Anything else we need to know?

In case of concerns:
- swap is off.
- `SystemdCgroup` is set to `true` in `/etc/containerd/config.toml`.
- SELinux is turned off using `setenforce 0`.
- runc version: 1.1.12

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
Self-hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux master 5.15.0-102-generic #112-Ubuntu SMP Tue Mar 5 16:50:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd

```console
$ containerd -v
containerd github.com/containerd/containerd v1.7.15 926c9586fe4a6236699318391cd44976a98e31f1
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
No CSI/CNI is installed.

</details>


### 分析结果

不涉及

---

## Issue #124406 forbidden message may include RBAC information

- Issue 链接：[#124406](https://github.com/kubernetes/kubernetes/issues/124406)

### Issue 内容

#### What happened?

Kubernetes API server may include extra RBAC information on forbidden error messages. An authenticated user could gain unexpected knowledge of possible Kubernetes RBAC configuration problems.

#### What did you expect to happen?

Error message does not include RBAC information.

#### How can we reproduce it (as minimally and precisely as possible)?

**Test 1**

```
kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
kubectl auth can-i create pods -n kube-system --as=system:anonymous
curl -k https://KUBERNETES_API_SERVER_HOST:KUBERNETES_API_SERVER_PORT/foo
```

```
kubectl apply -f - <<EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:unauthenticated
EOF
```

```
kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
kubectl auth can-i create pods -n kube-system --as=system:anonymous
curl -k https://KUBERNETES_API_SERVER_HOST:KUBERNETES_API_SERVER_PORT/foo
```

**Example Results for Test 1:**

```
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/foo\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}vagrant@verify-cluster:~$ kubectl apply -f - <<EOF
> kind: ClusterRoleBinding
> apiVersion: rbac.authorization.k8s.io/v1
> metadata:
>   name: test
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: test
> subjects:
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:unauthenticated
> EOF
clusterrolebinding.rbac.authorization.k8s.io/test created
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/foo\": RBAC: clusterrole.rbac.authorization.k8s.io \"test\" not found",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}vagrant@verify-cluster:~$ 
```

**Test 2 with Results:** Kubernetes API server updated with `--anonymous-auth=false`.

```
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl apply -f - <<EOF
> kind: ClusterRoleBinding
> apiVersion: rbac.authorization.k8s.io/v1
> metadata:
>   name: test
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: test
> subjects:
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:serviceaccounts
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:unauthenticated
> EOF
clusterrolebinding.rbac.authorization.k8s.io/test created
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no - RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}vagrant@verify-cluster:~$
```

#### Anything else we need to know?

This issue was originally filed at https://hackerone.com but was closed. I was told to open an issue here.

#### Kubernetes version

<details>
Kubernetes versions 1.26.15 and 1.30.0-rc.1 were tested, but other versions likely impacted. Test was conducted against Kubernetes clusters deployed using IBM Cloud Kubernetes Service.
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
IBM Cloud Kubernetes Service
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

不涉及

---

## Issue #124397 After restart Kubelet, node will become notReady in first kubelet update period.

- Issue 链接：[#124397](https://github.com/kubernetes/kubernetes/issues/124397)

### Issue 内容

#### What happened?

After restart Kubelet, node will become notReady in first kubelet update period.

node condition is :
```
  - lastHeartbeatTime: "2024-04-19T06:40:55Z"
    lastTransitionTime: "2024-04-19T06:40:55Z"
    message: container runtime status check may not have completed yet
    reason: KubeletNotReady
    status: "False"
    type: Ready
```

in kubelet's logs:
```
I0419 14:40:55.510166 1902465 setters.go:577] "Node became not ready" node="mjq-k8stestarsenalwork-1" condition={Type:Ready Status:False LastHeartbeatTime:2024-04-19 14:40:55.510127183 +0800 CST m=+5.226928586 LastTransitionTime:2024-04-19 14:40:55.510127183 +0800 CST m=+5.226928586 Reason:KubeletNotReady Message:container runtime status check may not have completed yet}
```

when go in to next period (default 10s), node status resume to Ready.

#### What did you expect to happen?

Restart Kubelet should not make node status change to notReady.

#### How can we reproduce it (as minimally and precisely as possible)?

Restart Kubelet, and watch node status for second
```
watch -n 1 'kubectl get no <node-name>'
```


#### Anything else we need to know?

The root case maybe here:
When kubelet start, it will start two go routine, one detect runtime status, one check node status and update to APIServer;
detect runtime status:

in `updateRuntimeUp` it will update `lastBaseRuntimeSync`:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet.go#L2909

in `kl.runtimeState.runtimeErrors` it will check runtimeStatus include `lastBaseRuntimeSync`:

https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/runtime.go#L108

`kl.runtimeState.runtimeErrors` is one of the notReady setter func:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet_node_status.go#L748

So if nodestatus execute before `updateRuntimeUp`, kubelet will update a node status with notReady condition:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet.go#L1635-L1641
In kubelet starting,  both `syncNodeStatus` go routine and `fastStatusUpdateOnce` will be called before `updateRuntimeUp`,  so the first node status is notReady.

#### Kubernetes version

In our cluster is v1.22.17, but I belive it can reproduce in latest version.

<details>

```console
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console

# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124392 scheduler: ```ImageLocality``` gives nodes in different situation the same score

- Issue 链接：[#124392](https://github.com/kubernetes/kubernetes/issues/124392)

### Issue 内容

#### What happened?

I managed to reproduce [a bug that went stale](https://github.com/kubernetes/kubernetes/issues/112573) on the latest main branch. And also noticed a new situation that may trigger the bug.

The ```ImageLocality``` plugin may give a same score to nodes in different situations.

The related code:
```go
imageScores := sumImageScores(nodeInfo, pod, totalNumNodes, logger, nodeName)
score := calculatePriority(imageScores, len(pod.Spec.InitContainers)+len(pod.Spec.Containers), logger)

func calculatePriority(sumScores int64, numContainers int, logger klog.Logger) int64 {
	maxThreshold := maxContainerThreshold * int64(numContainers)
	if sumScores < minThreshold {
		sumScores = minThreshold
	} else if sumScores > maxThreshold {
		sumScores = maxThreshold
	}
	return framework.MaxNodeScore * (sumScores - minThreshold) / (maxThreshold - minThreshold)
}
```

**This will happen when the a node has a fully pulled image that is relatively small, another doesn't** (But still need time to pull, in this case image size is about 70MB). 

The node with fully pulled image has an original ```sumScores``` too close to ```minThreshold```, and the other node without that image has an original ```sumScores``` = 0. So after clip the difference of two nodes' ```sumScores``` is relatively small compared to ```maxThreshold - minThreshold```. 

Then the plugin calculate the score, and this will happen: For the first node's score ```= 100 * 0.0001 = 0```, for the second node' score ```= 100 * 0 = 0```. Resulting the same score even if the first node has fully pulled this image (about 70MB) and the second node hasn't.

---

**And I think this will also happen when a node has many relatively large image, another doesn't (the second also has some image but not as large as the first one).** Then the first node's ```sumScores``` is far more beyond ```maxThreshold```, and the second node's ```sumScores``` is just a little bit smaller than ```maxThreshold```. After clipping, the first node's ```sumScores```  becomes ```maxThreshold```.

Then the plugin calculate the score, and this will happen: For the first node's score ```= 100 * 1 = 100```, for the second node' score ```= 100 * 0.999 = 100```. Resulting the same score even if the first node has far more image than the second node.

#### What did you expect to happen?

ImageLocality gives two nodes a different score.

#### How can we reproduce it (as minimally and precisely as possible)?

I first create two nodes (worker1 with fully pulled image, and worker2 just start pulling), then apply a pod using the following specification. 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  containers:
    - name: test-container
      image: gke.gcr.io/calico/cni:v3.23.1-gke.0
```

and here is the log (added some log to see what happened in image_locality):
```bash
[image_locality.go:68] "ImageScores" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker" imageScores=25531886
[image_locality.go:93] "calculatePriority" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker" minThreshold=24117248 maxThreshold=1048576000 framework.MaxNodeScore=100
[image_locality.go:68] "ImageScores" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker2" imageScores=0
[image_locality.go:93] "calculatePriority" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker2" minThreshold=24117248 maxThreshold=1048576000 framework.MaxNodeScore=100
[image_locality.go:112] ImageName="gke.gcr.io/calico/cni:v3.23.1-gke.0" Size="76595659" node="kind-worker"
...
[schedule_one.go:784] "Plugin scored node for pod" pod="default/nginx3" plugin="ImageLocality" node="kind-worker" score=0
...
[schedule_one.go:784] "Plugin scored node for pod" pod="default/nginx3" plugin="ImageLocality" node="kind-worker2" score=0
[schedule_one.go:851] "Calculated node's final score for pod" pod="default/nginx3" node="kind-worker" score=492
[schedule_one.go:851] "Calculated node's final score for pod" pod="default/nginx3" node="kind-worker2" score=492
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.29.1
Server Version: v1.30.0-

Actually this is reproduced on the latest main branch.
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124391 errors running update-codegen.sh

- Issue 链接：[#124391](https://github.com/kubernetes/kubernetes/issues/124391)

### Issue 内容

#### What happened?
Initially, running `hack/update-codegen.sh` returned:
```
hack/verify-codegen.sh
+++ [0418 21:16:48] Generating protobufs for 69 targets
go: -mod may only be set to readonly or vendor when in workspace mode, but it is set to "mod"
	Remove the -mod flag to use the default readonly value,
	or set GOWORK=off to disable workspace mode.
```
so I set `export GOWORK=off` in [`kube::golang::setup_env()`](https://github.com/kubernetes/kubernetes/blob/master/hack/lib/golang.sh#L590), which effectively disabled go workspace.

This fixed the error above, but led to a list of errors from `deepcopy-gen` even though my `kubernetes` repo is already under `$GOPATH/src/k8s.io`:
```
hack/verify-codegen.sh
+++ [0418 17:39:21] Generating protobufs for 69 targets
+++ [0418 17:40:04] Generating deepcopy code for 252 targets
F0418 17:40:09.204730   86904 main.go:107] Error: failed making a parser: error(s) in "./staging/src/k8s.io/api/admission/v1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admission/v1
error(s) in "./staging/src/k8s.io/api/admission/v1beta1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admission/v1beta1
error(s) in "./staging/src/k8s.io/api/admissionregistration/v1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admissionregistration/v1
error(s) in "./staging/src/k8s.io/api/admissionregistration/v1alpha1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admissionregistration/v1alpha1
...
```

#### What did you expect to happen?

`hack/update-codegen.sh` should work.

#### How can we reproduce it (as minimally and precisely as possible)?

* Change [this line](https://github.com/kubernetes/kubernetes/blob/master/hack/lib/golang.sh#L590) to `export GOWORK=off` and temporarily commit it (or else script will complain dirty working dir).
* Run `hack/update-codegen.sh`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.13
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124378 Garbage collector can create invalid virtual nodes

- Issue 链接：[#124378](https://github.com/kubernetes/kubernetes/issues/124378)

### Issue 内容

#### What happened?

Disclaimer: I came across this when studying the GC code, but haven't observed the issue directly (or tried to).

In `GraphBuilder#addDependentToOwners`, a virtual node is created if the owner hasn't been observed in the graph yet. However, it blindly uses the dependent's namespace for this owner node. This will be incorrect if the owner is a non-namespaced resource.

See https://github.com/kubernetes/kubernetes/blob/183aca3cad427f28b746653075a6959cf4edf316/pkg/controller/garbagecollector/graph_builder.go#L411.

I'm reporting this as a kind of low-effort thing as I do not have capacity of interest in resolving this myself. Feel free to close if it doesn't meet community standards.

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

-

#### Anything else we need to know?

-

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

根据您提供的 Issue 内容，问题描述是在垃圾收集器的 `GraphBuilder#addDependentToOwners` 方法中，当所有者节点尚未在图中观察到时，会创建一个虚拟节点。然而，它使用了从属节点的命名空间来设置所有者节点的命名空间，如果所有者是一个非命名空间（cluster-scoped）的资源，这将是不正确的。

经过分析，此问题可能导致垃圾收集器的内部图结构中出现不正确的虚拟节点，可能影响垃圾收集器对资源依赖关系的追踪和管理，导致一些资源未能被正确地垃圾回收或产生其他非预期的行为。

然而，从安全性的角度来看，此问题不涉及潜在的安全风险，原因如下：

1. **攻击者无法利用该问题进行攻击**：Kubernetes API 服务器在处理资源的 `ownerReferences` 时，会进行严格的权限校验。只有具备删除目标所有者资源权限的用户，才能创建指向该资源的 `ownerReference`。这意味着攻击者无法通过伪造或操纵 `ownerReference` 来影响垃圾收集器的行为，以删除他们无权删除的资源。即使垃圾收集器在内部维护的图中存在不正确的虚拟节点，攻击者也无法利用这一点来突破权限边界或提升权限。

2. **该问题不太可能成为一个高危漏洞**：根据 CVSS 3.1 评分标准，此问题对机密性、完整性和可用性没有直接的高影响。它不会导致未经授权的访问、数据泄露或服务不可用等高风险后果。因此，即使被认定为漏洞，其严重性评级也不会达到 High 或以上的级别，无法满足分配 CVE 编号的条件。

因此，综合以上分析，该 Issue 不涉及潜在的安全风险。

---

## Issue #124376 Kubelet CredentialProvider Fails to Connect to Local Docker Registry

- Issue 链接：[#124376](https://github.com/kubernetes/kubernetes/issues/124376)

### Issue 内容

#### What happened?

I have configured the Kubelet CredentialProvider according to the documentation to connect to my local Docker registry. However, I am encountering issues when attempting to pull images from the registry.

I followed the documentation to set up the Kubelet CredentialProvider to connect to my local Docker registry. Despite following the steps outlined, I am facing difficulties when trying to pull images from the registry. Specifically, I receive a "403 Forbidden" status code when the Kubelet attempts to access the image repository on the local registry.

kubectl get pods -o wide 
`NAME                                  READY   STATUS             RESTARTS   AGE   IP               NODE                          NOMINATED NODE   READINESS GATES
example-deployment-7bc6984597-4jtbd   0/1     ImagePullBackOff   0          18m   10.250.172.164   node-worker   <none>           <none>`

when I go to the node and check the logs of the kubelet I saw this error
`Apr 18 16:05:46 node-worker kubelet[20981]: E0418 16:05:46.179931   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ErrImagePull: \"failed to pull and unpack image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\": failed to resolve reference \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\": unexpected status from HEAD request to https://URL/v2/angie-docker-dev/jarvis-monitor/manifests/latest: 403 Forbidden\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"
Apr 18 16:05:46 node-worker kubelet[20981]: E0418 16:05:46.450655   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ImagePullBackOff: \"Back-off pulling image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\"\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"
Apr 18 16:05:47 node-worker kubelet[20981]: E0418 16:05:47.453020   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ImagePullBackOff: \"Back-off pulling image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\"\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"`

 kubectl -n jarvis describe po example-deployment-7bc6984597-4nc6k
```
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  6m                     default-scheduler  Successfully assigned jarvis/example-deployment-7bc6984597-4nc6k to algoobjd-test-1-k8sworker-1
  Warning  Failed     4m40s (x6 over 5m59s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling    4m25s (x4 over 5m59s)  kubelet            Pulling image "URL/angie-docker-dev/jarvis-monitor:latest"
  Warning  Failed     4m25s (x4 over 5m59s)  kubelet            Failed to pull image "URL/angie-docker-dev/jarvis-monitor:latest": failed to pull and unpack image "URL/angie-docker-dev/jarvis-monitor:latest": failed to resolve reference "URL/angie-docker-dev/jarvis-monitor:latest": unexpected status from HEAD request to https://URL/v2/angie-docker-dev/jarvis-monitor/manifests/latest: 403 Forbidden
  Warning  Failed     4m25s (x4 over 5m59s)  kubelet            Error: ErrImagePull
  Normal   BackOff    48s (x21 over 5m59s)   kubelet            Back-off pulling image "URL/angie-docker-dev/jarvis-monitor:latest"
```

I create a few files in every nodes
URL.json

exist in every node in path:
/etc/kubernetes/registries/URL.json

the context of file is:
{"username":"YYY","password":"XXX"}

ecr-credential-provider : [https://raw.githubusercontent.com/JonTheNiceGuy/generic-credential-provider/main/generic-credential-provider](url)

file image-credential-provider-config.json:
`{
    "apiVersion": "kubelet.config.k8s.io/v1",
    "kind": "CredentialProviderConfig",
    "providers": [
      {
        "name": "generic-credential-provider",
        "matchImages": [
          "URL"
        ],
        "apiVersion": "credentialprovider.kubelet.k8s.io/v1"
      }
    ]
  }`

and kubelet extra args
`KUBELET_EXTRA_ARGS="--image-credential-provider-bin-dir /usr/local/bin/image-credential-provider --image-credential-provider-config /etc/kubernetes/image-credential-provider-config.json"`

I put it in file --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf.

I tested this code with the command
`echo '{
  "apiVersion": "credentialprovider.kubelet.k8s.io/v1",
  "kind": "CredentialProviderRequest",
  "image": "URL/docker-dev/jarvis-monitor:latest"
}' | /usr/local/bin/image-credential-provider/generic-credential-provider`

the output (like [https://kubernetes.io/docs/reference/config-api/kubelet-credentialprovider.v1/#credentialprovider-kubelet-k8s-io-v1-CredentialProviderResponse](url)
`{
    "kind": "CredentialProviderResponse",
    "apiVersion": "credentialprovider.kubelet.k8s.io/v1",
    "cacheKeyType": "Registry",
    "cacheDuration": "0",
    "auth": {
        "URL": {
            "username": "XXX",
            "password": "YYY"
        }
    }
}`

#### What did you expect to happen?

I expected that every node in the Kubernetes cluster would successfully pull Docker images from the local Docker registry without encountering any authentication or access issues. Specifically, I anticipated that the Kubelet CredentialProvider configuration would enable seamless access to the registry, allowing containers to start properly on each node.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the issue on an on-premises Kubernetes cluster, follow these steps:

1. Set up a Kubernetes cluster using your on-premises infrastructure.
2. Configure the Kubelet CredentialProvider to connect to your local Docker registry as per the documentation.
3. Deploy a pod that uses an image from the local Docker registry.
4. Monitor the Kubelet logs or events for any errors during the image pull process.
5. Verify if the pod starts successfully or if it encounters any issues related to image pulling.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8
```

</details>


#### Cloud provider

<details>
on-prem
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124370 error while initializing kubeadm on ubuntu 22.04

- Issue 链接：[#124370](https://github.com/kubernetes/kubernetes/issues/124370)

### Issue 内容

#### What happened?

sudo kubeadm init --control-plane-endpoint=master-node --upload-certs
I0418 16:55:58.530076   12569 version.go:256] remote version is much newer: v1.30.0; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0418 16:55:59.851437   12569 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master-node] and IPs [10.96.0.1 192.168.1.36]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master-node] and IPs [192.168.1.36 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master-node] and IPs [192.168.1.36 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster

kubelet and docker status are active(running)
and i don't habe kubelet.log file inside /var/log was that the problem?
i am facing this error from past two weeks ,any solutions?

#### What did you expect to happen?

kubeadm initiated successfully

#### How can we reproduce it (as minimally and precisely as possible)?

no idea

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version 
# paste output here
```Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>


sudo kubectl cluster-info
E0418 17:16:31.257788   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.258103   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.259393   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.259588   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.261055   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</details>

#### OS version

<details>

```console
# On Linux: yes
$ cat /etc/os-release
# paste output here[sudo] password for rivertech: 
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy



$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>
kubeadm kublet kubectl

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124368 exec /usr/local/bin/docker-entrypoint.sh argument list too long

- Issue 链接：[#124368](https://github.com/kubernetes/kubernetes/issues/124368)

### Issue 内容

#### What happened?

My containers are in the state CrashLoopBackOff while deploying. when I check the logs I found that _**exec /usr/local/bin/docker-entrypoint.sh argument list too long**_

When I try to deploy new containers , same issue.

#### What did you expect to happen?

Every containers are running successfully for few months then suddenly all containers are crashed.

#### How can we reproduce it (as minimally and precisely as possible)?

I do kubectl apply -f deployment.yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.0

#### Cloud provider

Running on-premises

#### OS version


# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy


uname -a
Linux  5.15.0-102-generic

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

contained 
version : containerd://1.6.31

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124359 kubeadm uses and configures kube-log-runner

- Issue 链接：[#124359](https://github.com/kubernetes/kubernetes/issues/124359)

### Issue 内容

#### What happened?

klog command line flags [are deprecated](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components) starting with Kubernetes v1.23 and removed in Kubernetes v1.26，I hope to use kube-log-runner to configure log output to a specified file，I deploy using kubeadm

#### What did you expect to happen?

I deploy using kubeadm，I hope to use kube-log-runner to configure log output to a specified file。How should I edit the kube-apiserver.yaml file

/apps/k8s/bin/kube-log-runner -log-file=/apps/k8s/log/kube-apiserver.log --also-stdout=false /apps/k8s/bin/kube-apiserver $KUBE_APISERVER_OPTS



#### How can we reproduce it (as minimally and precisely as possible)?

Deploy kubernetes using kubeadm

#### Anything else we need to know?

_No response_

#### Kubernetes version

```
kubeadm version: &version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T10:39:04Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
```

#### Cloud provider

<details>
nothing
</details>


#### OS version

```
NAME="Rocky Linux"
VERSION="8.7 (Green Obsidian)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="8.7"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Rocky Linux 8.7 (Green Obsidian)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:8:GA"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-8"
ROCKY_SUPPORT_PRODUCT_VERSION="8.7"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.7"
```



```
[root@k8s-master-01 manifests]# uname -rn
k8s-master-01 6.7.6-1.el8.elrepo.x86_64
```

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124347 [bug] StorageError: invalid object

- Issue 链接：[#124347](https://github.com/kubernetes/kubernetes/issues/124347)

### Issue 内容

Copied from https://github.com/kubernetes-sigs/controller-runtime/issues/1881 where this was originally reported, as it is unlikely this has anything to do with controller-runtime. Unfortunately, the prow transfer plugin doesn't support cross-org transfer :(

/kind bug
/sig api-machinery
/cc @stijndehaes @pier-oliviert
Hi All,

We are seeing a lot of error logs when running our operator into production. And we can't figure out what is happening. The basic flow of what we do goes as follows:

```
func (r *SparkReconciler) createSparkSubmitterPod(ctx context.Context, spark *runtimev1.Spark) error {
	var pod corev1.Pod
	err := r.Get(ctx, client.ObjectKey{
		Name:      submitterPodName(spark),
		Namespace: spark.Namespace,
	}, &pod)
	if apierrors.IsNotFound(err) {
		r.Log.V(1).Info("Creating the spark submitter pod")
		return r.doCreateSparkSubmitterPod(ctx, spark)
	}
	return errors.Wrap(err, "Failed getting the pod")
}
```
As you can see in the code snippet. We create a pod only when it does not exist. Because we read from cache we know that the get call can return not found when the pod exists. We do the following in the doCreateSparkSubmitterPod:

```
if err := r.Client.Create(ctx, pod); err != nil {
    if apierrors.IsAlreadyExists(err) {
	return nil
    }
    return errors.Wrap(err, "Failed creating the pod")
}
```
So we capture an already exists error. However we tend to see a lot of errors like this when running into production:
```
"error":"Operation cannot be fulfilled on pods \"255f70af-5699-46c6-8002-a4df45af5209\": StorageError: invalid object, Code: 4, Key: /registry/pods/addatatest2/255f70af-5699-46c6-8002-a4df45af5209, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: f8104e60-1a9d-40ca-847e-52bc0f556844, UID in object meta: "
```

We have no clue on how to proceed on this. I think it might happen when the pod already exists. But not entirely sure. Perhaps someone can shed a light on this?

### 分析结果

不涉及。

---

## Issue #124345 Kubelet restart cause running pod restart with UnexpectedAdmissionError when pods have initContainers and external devices like GPU 

- Issue 链接：[#124345](https://github.com/kubernetes/kubernetes/issues/124345)

### Issue 内容

#### What happened?

When restarting kubelet, it will restart the running pods with UnexpectedAdmissionError when pods' initContainers and containers both use external devices like GPU 

#### What did you expect to happen?

Restart kubelet should not cause running pods to restart

#### How can we reproduce it (as minimally and precisely as possible)?

1. create some pods with nvidia.com/gpu and initContainers which also use GPUs
2. wait until initContainers exit
3. restart kubelet

#### Anything else we need to know?

there was an issue and a fix for running pods with devices, but it looks like the initContainers is not counted as should skip containers.

the fix has cherry-picked to v1.25.16

- issue: https://github.com/kubernetes/kubernetes/issues/118559
- fix: https://github.com/kubernetes/kubernetes/pull/118635
- cherry-pick v1.25 since v1.25.14: https://github.com/kubernetes/kubernetes/pull/119707

Related Issues:
- Completed Pod will also be affected: https://github.com/kubernetes/kubernetes/issues/117955
- memory manager related kubelet restart cause UnexpectedAdmissionError: https://github.com/kubernetes/kubernetes/issues/123971

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.25.16
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124336 CVE-2024-3177: Bypassing mountable secrets policy imposed by the ServiceAccount admission plugin

- Issue 链接：[#124336](https://github.com/kubernetes/kubernetes/issues/124336)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N) - **Low** (2.7)

A security issue was discovered in Kubernetes where users may be able to launch containers that bypass the mountable secrets policy enforced by the ServiceAccount admission plugin when using containers, init containers, and ephemeral containers with the envFrom field populated. The policy ensures pods running with a service account may only reference secrets specified in the service account’s secrets field. Kubernetes clusters are only affected if the ServiceAccount admission plugin and the `kubernetes.io/enforce-mountable-secrets` annotation are used together with containers, init containers, and ephemeral containers with the envFrom field populated. 

#### Am I vulnerable?

The ServiceAccount admission plugin is used. Most cluster should have this on by default as recommended in  https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#serviceaccount
The kubernetes.io/enforce-mountable-secrets annotation is used by a service account. This annotation is not added by default. Pods using containers, init containers, and ephemeral containers with the envFrom field populated.

##### Affected Versions

kube-apiserver v1.29.0 - v1.29.3
kube-apiserver v1.28.0 - v1.28.8
kube-apiserver <= v1.27.12

#### How do I mitigate this vulnerability?

This issue can be mitigated by applying the patch provided for the kube-apiserver component. The patch prevents containers, init containers, and ephemeral containers with the envFrom field populated from bypassing the mountable secrets policy enforced by the ServiceAccount admission plugin.

Outside of applying the provided patch, there are no known mitigations to this vulnerability.

##### Fixed Versions

- kube-apiserver master - fixed by #124322
- kube-apiserver v1.29.4 - fixed by #124325
- kube-apiserver v1.28.9 - fixed by #124326
- kube-apiserver v1.27.13 - fixed by #124327

To upgrade, refer to the documentation:
https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/ 

#### Detection

Pod update requests using a container, init container, or ephemeral container with the envFrom field populated that exploits this vulnerability with unintended secret will be captured in API audit logs. You can also use the following kubectl command to find active pods using the `kubernetes.io/enforce-mountable-secrets` annotation. 

`kubectl get serviceaccounts --all-namespaces -o jsonpath="{range .items[?(@.metadata.annotations['kubernetes\.io/enforce-mountable-secrets']=='true')]}{.metadata.namespace}{'\t'}{.metadata.name}{'\n'}{end}"` 

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by tha3e1vl. 

The issue was fixed and coordinated by the fix team: 

Rita Zhang @ritazh
Joel Smith @joelsmith
Mo Khan @enj

and release managers:
Sascha Grunert @saschagrunert
Jeremy Rickard @jeremyrickard

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### 分析结果

不涉及。

---

## Issue #124332 failed to assign quota after kubelet was restarted

- Issue 链接：[#124332](https://github.com/kubernetes/kubernetes/issues/124332)

### Issue 内容

#### What happened?

After kubelet was restarted, kubelet starts to output quota related errors and the total count of entries in `/etc/projects` and `/etc/projid` decreased.

```console
I0416 17:38:13.086019  157568 empty_dir.go:306] Set quota on /var/lib/kubelet/pods/e5149231-3a81-41e0-87be-579846f6caea/volumes/kubernetes.io~configmap/config failed assign quota FAILED exit status 1
I0416 17:38:13.086051  157568 operation_generator.go:838] "MountVolume.markVolumeErrorState leaving volume uncertain" volumeName=kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config
E0416 17:38:13.086130  157568 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config podName:e5149231-3a81-41e0-87be-579846f6caea nodeName:}" failed. No retries permitted until 2024-04-16 17:40:15.086106798 +0800 CST m=+376.261198810 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "config" (UniqueName: "kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config") pod "xxx-yyy" (UID: "e5149231-3a81-41e0-87be-579846f6caea") : assign quota FAILED exit status 1
I0416 17:38:13.086156  157568 event.go:294] "Event occurred" object="xxx/yyy" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"config\" : assign quota FAILED exit status 1"
```

Similar issues: #115309

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

1. ext4 fs with quota enabled and enforced `tune2fs -O project -Q prjquota /dev/xxx; mount -o prjquota ...`
2. enable featuregate `LocalStorageCapacityIsolationFSQuotaMonitoring` in kubelet config
3. register this node to k8s (start kubelet): works well
4. restart kubelet: reproduced

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:23:27Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:16:45Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
IDC
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"

$ uname -a
Linux xxx 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.6.31
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124310 Pod soft evicted is still ready in kubernetes 1.24+ .

- Issue 链接：[#124310](https://github.com/kubernetes/kubernetes/issues/124310)

### Issue 内容

#### What happened?

If pod was soft evicted, it will keep ready until all container dead. Endpoints of the pod will not be removed.

When kubelet triggers soft eviction, the status manager will try to report the phase as failed.
And there are still running containers, the phase of newStatus will be overwritten by oldStatus.

https://github.com/kubernetes/kubernetes/blob/9791f0d1f39f3f1e0796add7833c1059325d5098/pkg/kubelet/status/status_manager.go#L1097-L1103

And then status manager will calculate pod condition by phase of oldStatus.
https://github.com/kubernetes/kubernetes/blob/9791f0d1f39f3f1e0796add7833c1059325d5098/pkg/kubelet/status/status_manager.go#L1109-L1117

This will lead to pod still ready, and the endpoint will not be removed until the container is killed, even if the Prestop Hook has been executed.

#### What did you expect to happen?

Kubelet will mark pod as not ready, if eviction triggered.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start kubelet with args `--eviction-soft-grace-period=allocatableMemory.available=10s --eviction-soft=allocatableMemory.available<2Gi`
2. Create a pod and handle SIGTERM.
3. Run stress in pod cgroup.
4. Run `for c in `seq 1 1000`; do kubectl get pod -o yaml <pod-name> > `date +%T`.yaml; sleep 1; done

We'll see pod still ready until failed.

#### Anything else we need to know?

From https://github.com/kubernetes/kubernetes/pull/117822.

#### Kubernetes version

v1.26.1 or v1.28.3

#### Cloud provider

No matter.

#### OS version

No matter.

#### Install tools

No matter.

#### Container runtime (CRI) and version (if applicable)

No matter.

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

No matter.

### 分析结果

不涉及

---

## Issue #124308 StartError (exit code 128) when pod tries to mount configmap that is being changed at same time

- Issue 链接：[#124308](https://github.com/kubernetes/kubernetes/issues/124308)

### Issue 内容

#### What happened?

This is happening when deploying a pod with argo workflow and there is another job running which is editing the configmap
The workflow failed with following error 
`
StartError (exit code 128): failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting "/var/lib/kubelet/pods/f22aa0b7-afdc-497c-beb3-ccb394e8428d/volume-subpaths/config/wait/9" to rootfs at "/mainctrfs/my.py": mount /var/lib/kubelet/pods/f22aa0b7-afdc-497c-beb3-ccb394e8428d/volume-subpaths/my-py/wait/9:/mainctrfs/my.py (via /proc/self/fd/6), flags: 0x5001, data: context="system_u:object_r:data_t:s0:c923,c1020": no such file or directory: unknown
`

#### What did you expect to happen?

I expected the pod to come up successfully with whatever version of configmap is available 

#### How can we reproduce it (as minimally and precisely as possible)?

This is little hard to reproduce, but to reproduce:
1. Run a script or job to replace/overwrite a configmap
2. at the same time try to create a pod which is mounting data from configmap with subpath

#### Anything else we need to know?

_No response_

#### Kubernetes version

client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.8-gke.1067004


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

Related Issues:
https://github.com/argoproj/argo-workflows/issues/11983

### 分析结果

不涉及

---

## Issue #124307 Overlapping labels can lead to HPA matching incorrect pods

- Issue 链接：[#124307](https://github.com/kubernetes/kubernetes/issues/124307)

### Issue 内容

#### What happened?

When an HPA targets a Deployment which has a label selector matching Pods that don't belong to it (overlapping labels, for example), those "other" Pods are considered by the HPA to be part of the targeted HPA.

#### What did you expect to happen?

I have always been lead to believe that this behaviour is correct and the user should have labels that are specific enough to not overlap other Deployments.
This is stated [here](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment):
> Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.

However, recently the VPA fixed the same behaviour in https://github.com/kubernetes/autoscaler/pull/6460

Which makes me wonder if I should expect the HPA to test the owner reference too, and only target that Deployment's pods.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Create 2 deployments, both with the same labels/selectors
2. In 1 Deployment set resources
3. Deploy an HPA (and metrics-server)
4. Watch the events of the HPA throw error events related to Pods not under control of the HPA's deployment
   1. ie: `  Warning  FailedGetResourceMetric       87s (x3 over 117s)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container ubuntu of Pod other-64886557cb-ldtnt`

<details>
  <summary>YAML files</summary>
  
  ```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: workload
  name: workload
spec:
  replicas: 1
  selector:
    matchLabels:
      app: other
  template:
    metadata:
      labels:
        app: other
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - sha256sum /dev/zero
        image: ubuntu
        imagePullPolicy: Always
        name: nginx
        resources:
          requests:
            cpu: 10m
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: other
  name: other
spec:
  replicas: 1
  selector:
    matchLabels:
      app: other
  template:
    metadata:
      labels:
        app: other
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - sleep infinity
        image: ubuntu
        imagePullPolicy: Always
        name: ubuntu
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: workload
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: workload
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  ```
</details>


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>

- kind
- GKE
- kOps (AWS)

</details>


#### OS version

_No response_

#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124286 Kubernetes not seeing existing containerd images

- Issue 链接：[#124286](https://github.com/kubernetes/kubernetes/issues/124286)

### Issue 内容

#### What happened?

Hi, I'm using  ctr to export and import images because there's network issue which does not allow gcr.io images to be downloaded in my cluster. It worked well for a few images, I installed Knative and some other componenes in this way. However, when I install Tekton, I fond that somehow kubernetes is not using existing images.

The pods keeps pulling images:

```
$ kubectl get pods -n tekton-pipelines
NAME                                                READY   STATUS             RESTARTS   AGE
tekton-dashboard-84c6c97d8f-6vhbz                   0/1     ImagePullBackOff   0          13m

$ kubectl describe pod -n tekton-pipelines tekton-dashboard-84c6c97d8f-6vhbz
...
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  14m                   default-scheduler  Successfully assigned tekton-pipelines/tekton-dashboard-84c6c97d8f-6vhbz to node02
  Warning  Failed     11m (x2 over 13m)     kubelet            Failed to pull image "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": failed to resolve reference "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": failed to do request: Head "https://gcr.io/v2/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard/manifests/sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": dial tcp 108.177.125.82:443: i/o timeout
```

However , the images is already present in my cluter nodes:

```
$ ctr -n k8s.io images list | grep gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38
gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38                   application/vnd.oci.image.index.v1+json                   sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 24.0 MiB  linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x                         io.cri-containerd.image=managed
```

And to verify that it's not a problem of ImagePullPolicy, I tried :
```
$ kubectl run tekton-dashboard-test --image=gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --image-pull-policy=Never

$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
tekton-dashboard-test   0/1     ErrImageNeverPull   0          10m
```

Is this a bug?



#### What did you expect to happen?

Images imported via `ctr import` should be present in kubernetes

#### How can we reproduce it (as minimally and precisely as possible)?

1. A kubernetes cluster with containerd runtime
2. pull image in another machine and export it to a tar file:
```
sudo ctr images pull gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --all-platforms

sudo ctr images export dashboard-all-arch.tar gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38
```
3.  Import the tar file in all kubernetes nodes:
```
ctr -n k8s.io images import dashboard-all-arch.tar
```
4. Try to start a pod with the imported image:
```
kubectl run tekton-dashboard-test --image=gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --image-pull-policy=Never
```

#### Anything else we need to know?

This only happens to the tekton images somehow, I've installed other components in this way and worked before:

```
kubectl get pods --all-namespaces
NAMESPACE                    NAME                                                   READY   STATUS              RESTARTS   AGE
calico-apiserver             calico-apiserver-549fdf488f-6m84p                      1/1     Running             0          44h
calico-apiserver             calico-apiserver-549fdf488f-dwz7v                      1/1     Running             0          44h
calico-system                calico-kube-controllers-7bc6b5bb8-5dnsf                1/1     Running             0          44h
calico-system                calico-node-c4zxq                                      1/1     Running             0          27h
calico-system                calico-node-jdw2v                                      1/1     Running             0          44h
calico-system                calico-node-jw4kx                                      1/1     Running             0          27h
calico-system                calico-typha-5c754949c6-7zzdx                          1/1     Running             0          27h
calico-system                calico-typha-5c754949c6-qhfwz                          1/1     Running             0          44h
calico-system                csi-node-driver-9wmmd                                  2/2     Running             0          44h
calico-system                csi-node-driver-gpjqq                                  2/2     Running             0          27h
calico-system                csi-node-driver-sfqm7                                  2/2     Running             0          27h
default                      tekton-dashboard-test                                  0/1     ErrImageNeverPull   0          16m
knative-eventing             eventing-controller-68fb776c69-lvqjn                   1/1     Running             0          20h
knative-eventing             eventing-webhook-664866f89f-pvz6v                      1/1     Running             0          20h
knative-eventing             imc-controller-848cf74bc8-5bkxt                        1/1     Running             0          20h
knative-eventing             imc-dispatcher-57789765c6-7cmvz                        1/1     Running             0          20h
knative-eventing             kafka-broker-dispatcher-7c8b4cfc67-qxjqv               1/1     Running             0          20h
knative-eventing             kafka-broker-receiver-f8f488bd7-wsxnd                  1/1     Running             0          20h
knative-eventing             kafka-controller-b595dc6d-cg5dp                        1/1     Running             0          20h
knative-eventing             kafka-webhook-eventing-5d985c95d4-thg56                1/1     Running             0          20h
knative-eventing             mt-broker-controller-d8c869469-99g2q                   1/1     Running             0          20h
knative-eventing             mt-broker-filter-56d967db6-9q9hk                       1/1     Running             0          20h
knative-eventing             mt-broker-ingress-6c59758448-p2trg                     1/1     Running             0          20h
knative-serving              activator-58db57894b-ntvhb                             1/1     Running             0          21h
knative-serving              autoscaler-76f95fff78-f6r9q                            1/1     Running             0          21h
knative-serving              controller-7dd875844b-xw96w                            1/1     Running             0          21h
knative-serving              net-kourier-controller-6559c556d7-t54fk                1/1     Running             0          20h
knative-serving              webhook-d8674645d-ggfg9                                1/1     Running             0          21h
kourier-system               3scale-kourier-gateway-9bd7579-tt8sx                   1/1     Running             0          20h
kube-system                  coredns-76f75df574-f829g                               1/1     Running             0          45h
kube-system                  coredns-76f75df574-hg2sj                               1/1     Running             0          45h
kube-system                  csi-nfs-controller-646d4f97cb-s4k9k                    4/4     Running             0          24h
kube-system                  csi-nfs-node-dhklg                                     3/3     Running             0          24h
kube-system                  csi-nfs-node-h5w94                                     3/3     Running             0          24h
kube-system                  csi-nfs-node-prvp7                                     3/3     Running             0          24h
kube-system                  etcd-master                                            1/1     Running             0          45h
kube-system                  kube-apiserver-master                                  1/1     Running             0          45h
kube-system                  kube-controller-manager-master                         1/1     Running             0          45h
kube-system                  kube-proxy-g6llx                                       1/1     Running             0          27h
kube-system                  kube-proxy-tsp6d                                       1/1     Running             0          45h
kube-system                  kube-proxy-vzlbf                                       1/1     Running             0          27h
kube-system                  kube-scheduler-master                                  1/1     Running             0          45h
kube-system                  metrics-server-84989b68d9-fw5wt                        1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-api-6c6b9896d4-gn82c              1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-auth-8546cbf77b-qnph8             1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-kong-75bb76dd5f-d5cr6             1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-metrics-scraper-6485f66c7-5dxvj   1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-web-6fc6cd548-bhf9j               1/1     Running             0          18h
metallb-system               controller-756c6b677-ggl74                             1/1     Running             0          134m
metallb-system               speaker-5m9p7                                          1/1     Running             0          134m
metallb-system               speaker-pmw7p                                          1/1     Running             0          134m
metallb-system               speaker-tmst5                                          1/1     Running             0          134m
tekton-pipelines-resolvers   tekton-pipelines-remote-resolvers-7ccf88788-6wxdk      0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-dashboard-84c6c97d8f-6vhbz                      0/1     ImagePullBackOff    0          23m
tekton-pipelines             tekton-events-controller-7856d7f897-jg8p7              0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-pipelines-controller-7cd6dd989f-hg2z6           0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-pipelines-webhook-6779f85664-7qt7h              0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-triggers-controller-5b6d5f54b7-psvfh            0/1     ImagePullBackOff    0          82m
tekton-pipelines             tekton-triggers-core-interceptors-f58696689-qk5bt      0/1     ImagePullBackOff    0          82m
tekton-pipelines             tekton-triggers-webhook-689688fc54-cfblg               0/1     ImagePullBackOff    0          82m
tigera-operator              tigera-operator-6bfc79cb9c-r585l                       1/1     Running             0          44h
```

#### Kubernetes version

<details>

```console
$ kubectl version
kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

The cluster is created manually with 3  ubuntu vms. Here's my kubernetes information:
<details>

```
$ kubectl get nodes -o wide
NAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master   Ready    control-plane   45h   v1.29.3   10.19.30.61   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
node01   Ready    <none>          27h   v1.29.3   10.19.30.64   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
node02   Ready    <none>          27h   v1.29.3   10.19.30.13   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
```
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
uname -a
Linux master.agent-less.ksord.com 5.15.0-102-generic #112-Ubuntu SMP Tue Mar 5 16:50:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm init ...
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
ctr version
Client:
  Version:  1.6.28
  Revision: ae07eda36dd25f8a1b98dfbf587313b99c0190bb
  Go version: go1.21.8

Server:
  Version:  1.6.28
  Revision: ae07eda36dd25f8a1b98dfbf587313b99c0190bb
  UUID: 394309a2-cee9-40bd-a675-1f46af6cb45
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124282 Scheduler still counts InitContainer resource requests after a pod finishes initialization

- Issue 链接：[#124282](https://github.com/kubernetes/kubernetes/issues/124282)

### Issue 内容

#### What happened?

I created a 1-node EKS cluster with that node having 4 CPU cores. I made sure at least 3.5 cores were allocatable after running all the daemonsets.

I created a deployment that has InitContainers, here is a summary of what that looked like (I replaced container names with dummy values and removed all the unnecessary container configuration).
```
initContainers:
    - image: 'test/initialization:v3.1.3'
      name: initialize
      resources:
        requests:
          cpu: 3
containers:
    - image: test/do-stuff
      resources:
        requests:
          cpu: 30m
```

I set the number of replicas to 1 and a pod successfully got scheduled, initialized and transitioned to the running phase.

I then bumped the number of replicas to 2, expecting the second pod to become scheduled since the first pod already finished initialization. However, it failed to schedule
```
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  57s (x2 over 6m3s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
```

#### What did you expect to happen?

I expected the second pod to go through since the first pod had already finished initialization and the CPU it requests for running would definitely leave more than 3 cores allocatable for the next pod.


From the documentation, I know the scheduler computes the "effective resource requests" on pods with InitContainers by getting the maximum requests across InitContainers and containers. I understand how this is essential when deciding where to place a pod.

However, if a pod finishes initialization and transitions to the running phase, I'd expect the scheduler to discard the requests specified on InitContainers when it's computing how many resources are already scheduled on a particular node.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start with a minimal cluster with one node and keep note of how many CPU is allocatable.
2. Run 1 pod on the cluster that has an InitContainer comprising 75% of the allocatable CPU and a container with negligible CPU requests
3. Wait till that pod finishes initialization and then run another one. The other one will not be schedulable.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"29+", GitVersion:"v1.29.1-eks-b9c9ed7", GitCommit:"07600c74de018baffb16c82771a48adcb843a932", GitTreeState:"clean", BuildDate:"2024-03-02T03:46:35Z", GoVersion:"go1.21.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
EKS (Amazon AWS)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux <hostname trimmed> 5.10.205-195.807.amzn2.x86_64 #1 SMP Tue Jan 16 18:28:59 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

- EKS Cluster was installed using terraform
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.11 on EKS
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>


- coredns from Amazon EKS version: `v1.11.1-eksbuild.6`
- VPC CNI from Amazon EKS version: `v1.16.2-eksbuild.1`
- Kube Proxy from Amazon EKS version: `v1.29.0-eksbuild.2`
- Amazon EBS CSI Driver from Amazon EKS version: `v1.27.0-eksbuild.1`
</details>


### 分析结果

不涉及。

---

## Issue #124275 kube-scheduler: pod stuck in `PENDING` but added to active queue

- Issue 链接：[#124275](https://github.com/kubernetes/kubernetes/issues/124275)

### Issue 内容

#### What happened?

We are writing a scheduler plugin using the scheduling framework. However, when creating a pod that uses out secondary scheduler e.g. by using

```
schedulerName: my-scheduler
```

The scheduler logs:
```
I0411 14:59:41.249623       1 eventhandlers.go:126] "Add event for unscheduled pod" pod="ns/pod1"
I0411 14:59:41.249678       1 scheduling_queue.go:575] "Pod moved to an internal scheduling queue" pod="ns/pod1" event="PodAdd" queue="Active"
```

but then nothing happens. The pod is stuck in `PENDING` state. Also, no events are created. 
We have the same scheduler running against a k8s v1.29 EKS cluster fine. It appears the pod is never retrieved from the active queue, e.g. [this line](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/schedule_one.go#L68) is never hit.

#### What did you expect to happen?

The pod should get scheduled or fail unscheduled.

#### How can we reproduce it (as minimally and precisely as possible)?

Build a scheduler plugin using versions below and run on k8s v1.21.14.

#### Anything else we need to know?

Versions from `go.mod`:
<details>

```console
k8s.io/api => k8s.io/api v0.29.0
k8s.io/apiextensions-apiserver => k8s.io/apiextensions-apiserver v0.29.0
k8s.io/apimachinery => k8s.io/apimachinery v0.29.0
k8s.io/apiserver => k8s.io/apiserver v0.29.0
k8s.io/cli-runtime => k8s.io/cli-runtime v0.29.0
k8s.io/client-go => k8s.io/client-go v0.29.0
k8s.io/cloud-provider => k8s.io/cloud-provider v0.29.0
k8s.io/cluster-bootstrap => k8s.io/cluster-bootstrap v0.29.0
k8s.io/code-generator => k8s.io/code-generator v0.29.0
k8s.io/component-base => k8s.io/component-base v0.29.0
k8s.io/component-helpers => k8s.io/component-helpers v0.29.0
k8s.io/controller-manager => k8s.io/controller-manager v0.29.0
k8s.io/cri-api => k8s.io/cri-api v0.29.0
k8s.io/csi-translation-lib => k8s.io/csi-translation-lib v0.29.0
k8s.io/dynamic-resource-allocation => k8s.io/dynamic-resource-allocation v0.29.0
k8s.io/endpointslice => k8s.io/endpointslice v0.29.0
k8s.io/kms => k8s.io/kms v0.29.0
k8s.io/kube-aggregator => k8s.io/kube-aggregator v0.29.0
k8s.io/kube-controller-manager => k8s.io/kube-controller-manager v0.29.0
k8s.io/kube-proxy => k8s.io/kube-proxy v0.29.0
k8s.io/kube-scheduler => k8s.io/kube-scheduler v0.29.0
k8s.io/kubectl => k8s.io/kubectl v0.29.0
k8s.io/kubelet => k8s.io/kubelet v0.29.0
k8s.io/kubernetes => k8s.io/kubernetes v1.29.0
k8s.io/legacy-cloud-providers => k8s.io/legacy-cloud-providers v0.29.0
k8s.io/metrics => k8s.io/metrics v0.29.0
k8s.io/mount-utils => k8s.io/mount-utils v0.29.0
k8s.io/pod-security-admission => k8s.io/pod-security-admission v0.29.0
k8s.io/sample-apiserver => k8s.io/sample-apiserver v0.29.0
k8s.io/sample-cli-plugin => k8s.io/sample-cli-plugin v0.29.0
k8s.io/sample-controller => k8s.io/sample-controller v0.29.0
```
</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.13", GitCommit:"80ec6572b15ee0ed2e6efa97a4dcd30f57e68224", GitTreeState:"clean", BuildDate:"2022-05-24T12:40:44Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.14", GitCommit:"0f77da5bd4809927e15d1658fb4aa8f13ad890a5", GitTreeState:"clean", BuildDate:"2022-06-15T14:11:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124264 Deployment rollout status not reliable when using `status.conditions`

- Issue 链接：[#124264](https://github.com/kubernetes/kubernetes/issues/124264)

### Issue 内容

#### What happened?

Currently I am working on a project where we implement a Kubernetes operator and we decided that for some flows we will need to wait for some deployments to complete the rollout before advancing to some other steps from our processes.
So, for an old deployment we only update an image in the pod template and then we wait for the updated deployment to finish the rollout.

The way the status of the Deployment is built/implemented is error prone and not reliable if we only take into account the `status.conditions` (as suggested in the documentation)

#### What did you expect to happen?

After updating the spec of a deployment, when the `status.ObservedGeneration` is changed to the latest `metadata.Generation`, the `status.conditions` should also be updated.

#### How can we reproduce it (as minimally and precisely as possible)?

We found out that if:
- we start with a Deployment having a `status.condition` of type `Progressing` with the reason `ProgressDeadlineExceeded`
- update the deployment spec (the only change would be an image from a container from the pod template)
- on a timer (every second), start retrieving the deployment using the API and check the status
Then, under some circumstances (so, not always; it's a concurrency/timing issue), the first change of the `status.observedGeneration` toward the new `metadata.generation` value, comes with *no change*(!) in the `status.conditions` (the old conditions are still there *without any single change*(!)). 
According to the Kubernetes documentation (and also the way the Operator SDK checks this rollout status), the interpretation of these would lead to say that the rollout failed, which is not accurate, because the actual rollout will only start a few seconds later...

This is an example from a log describing the issue (please observe that the two blocks are logged at a few milliseconds distance and the `status.conditions` are the same even the `status.ObservedGeneration` changed):
```
# 10:53:18.814 > Metadata.Generation: 2 | Status >> ObservedGeneration: 1, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: 1, unAvailableReplicas: 1
  Conditions:
    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.
    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet "my-deployment-5b8dc498c" has timed out progressing.
 => completed: False, failed: False

# 10:53:18.818 > Metadata.Generation: 2 | Status >> ObservedGeneration: 2, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: , unAvailableReplicas: 1
  Conditions:
    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.
    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet "my-deployment-5b8dc498c" has timed out progressing.
 => completed: False, failed: True
```


#### Anything else we need to know?

Please advise on how to reliably interpret the rollout status of a Deployment.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8+k3s1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8+k3s1
```

</details>


#### Cloud provider

<details>
no cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="K3s v1.28.8+k3s1"


$ uname -a
 Linux 3432b46b9978 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 GNU/Linux




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124260 Broken connectivity while using IPVS, ExternalIP and externalTrafficPolicy: Local

- Issue 链接：[#124260](https://github.com/kubernetes/kubernetes/issues/124260)

### Issue 内容

#### What happened?

First of all, I read the related issues (https://github.com/kubernetes-sigs/kubespray/issues/10572, https://github.com/k3s-io/k3s/issues/7183, https://github.com/kubernetes/kubernetes/issues/121272, https://github.com/kubernetes/kubernetes/issues/117613) and realized that the problem is still exists in mainstream.

I'm dealing with a Kubernetes cluster managed by Kubespray. It's set up in IPVS mode and uses Calico as a CNI. The cluster is tightly coupled with our network infrastructure: it announces all the types of Kubernetes IPs into ToR switches via BGP (so called IP Fabric Network Design).

In this scheme, the simplest way to assign a globally routed IP into a Service is to declare an ExternalIP. This doesn't require to interact with any kind of Kubernetes API using third-party software (like LoadBalancerIP do), Calico simply catches up the IPs like this and informs the switches about. That's why I can't consider ExternalIPs as an ancient concept (https://github.com/kubernetes/kubernetes/issues/121272#issuecomment-1770986681), they simplify the things.

All the ExternalIPs are automatically assigned to kube-ipvs0 which is great and all work fine with "externalTrafficPolicy: Cluster". But if I change the policy to Local, connections from outside a Kubernetes node to the corresponding ExternalIP become broken with "icmp-port-unreachable" error caused by iptables:

```
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-HEALTH-CHECK-NODE-PORT dst -j RETURN
-A KUBE-IPVS-FILTER -m conntrack --ctstate NEW -m set --match-set KUBE-IPVS-IPS dst -j REJECT --reject-with icmp-port-unreachable
```

#### What did you expect to happen?

It seems the only thing that the externalTrafficPolicy does in the network stack is moving ExternalIPs between KUBE-EXTERNAL-IP and KUBE-EXTERNAL-IP-LOCAL ipsets, but there's no equivalent RETURN rule for the -LOCAL one in the iptables. If I insert the rule manually:

```
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP-LOCAL dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-HEALTH-CHECK-NODE-PORT dst -j RETURN
-A KUBE-IPVS-FILTER -m conntrack --ctstate NEW -m set --match-set KUBE-IPVS-IPS dst -j REJECT --reject-with icmp-port-unreachable
```
"externalTrafficPolicy: Local" works as expected. BTW, I have no idea why we should reject anything here at all, do you?

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a single node with IPVS mode. Declare a Service with ExternalIP and externalTrafficPolicy: Local. See the ExternalIP works locally, but filtered from outside the node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```
# kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.5", GitCommit:"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c", GitTreeState:"clean", BuildDate:"2023-08-24T00:48:26Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.5", GitCommit:"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c", GitTreeState:"clean", BuildDate:"2023-08-24T00:42:11Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
```

</details>

#### Cloud provider

Self-managed cluster via Kubespray v2.23.0

#### OS version

<details>

```
# cat /etc/os-release 
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

```
# calicoctl version
Client Version:    v3.25.2
Git commit:        978a0e4bc
Cluster Version:   v3.25.2
Cluster Type:      kubespray,bgp,kubeadm,kdd,k8s
```

</details>

### 分析结果

不涉及

---

## Issue #124258 grandson ophan process started via kubectl exec not adopted by pid 1

- Issue 链接：[#124258](https://github.com/kubernetes/kubernetes/issues/124258)

### Issue 内容

#### What happened?

```console
kubectl exec -- sh

sh-4.2# nohup  sh -c 'sleep 10000' &
sh-4.2# exit

kubectl exec -- sh
sh-4.2# ps -efH
UID         PID   PPID  C STIME TTY          TIME CMD
root        623      0  0 09:51 pts/0    00:00:00 sh -l
root        641    623  0 09:51 pts/0    00:00:00   ps -efH
root        621      0  0 09:50 ?        00:00:00 sleep 1000
root          1      0  0 09:07 ?        00:00:00 /usr/local/bin/dumb-init
```

sleep process ppid alway 0



#### What did you expect to happen?

 ppid should be 1 (sth like dumb-init)


#### How can we reproduce it (as minimally and precisely as possible)?

just run the command above

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647", GitTreeState:"clean", BuildDate:"2023-05-17T14:20:07Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.0", GitCommit:"ab69524f795c42094a6630298ff53f3c3ebab7f4", GitTreeState:"clean", BuildDate:"2021-12-07T18:09:57Z", GoVersion:"go1.17.3", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.23) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

no


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124255 removeMissingExtendedResources() did not remove unknown extension resources from InitContainer

- Issue 链接：[#124255](https://github.com/kubernetes/kubernetes/issues/124255)

### Issue 内容

#### What happened?

The function removeMissingExtendedResources() located at pkg/kubelet/lifecycle/predicate.go:217, is designed to remove any extended resources from a container’s requests that are not found in nodeInfo.Allocatable before the pod is admitted. This is necessary to support cluster-level resources, which are extended resources that are unknown to nodes.
However, this function only removes missingExtendedResources from the container’s requests, and does not process the requests of initContainers. This could lead to issues, as initContainers might request resources that do not exist on the node, which could result in the pod failing to be admitted or other unforeseen issues. As such, it may be necessary to refine the removeMissingExtendedResources() function to also handle initContainer’s requests.

#### What did you expect to happen?

Refine the removeMissingExtendedResources() function to also handle initContainer’s requests.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod that requests cluster-level extended resources in both the container and InitContainer.

#### Anything else we need to know?

_No response_

#### Kubernetes version

The problem persists in the latest version.


#### Cloud provider

-

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124244 One pv with NFS can be mounted by two pods even with ReadWriteOncePod enabled

- Issue 链接：[#124244](https://github.com/kubernetes/kubernetes/issues/124244)

### Issue 内容

#### What happened?

Cloud provider: openstack
Csi driver: NFS(Manila)

- First, I got one replicaSet with 1 replica and RWOP enabled so pod A comes up in node A. 
- Then, edit the nodeSelector to node B and forcibly delete the pod A which results in the pod A in terminate state and pod B is running in node B. 
- During this short period, pod A and pod B can both write data to the same pv even with RWOP enabled.
- After pod A is eliminated, pod B fully controls the volume.

#### What did you expect to happen?

The pv should always read-write by a single pod but not access by two pods when the old one is in grace period.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply a replicaSet with 1 replica and RWOP enabled like:
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      terminationGracePeriodSeconds: 120
      containers:
      - name: writer
        image: busybox
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: my-volume
          mountPath: "/mnt"
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            echo $MY_POD_NAME $(date) >> /mnt/myfile.txt;
            sleep 1;
          done
      volumes:
      - name: my-volume
        persistentVolumeClaim:
          claimName: pvc-rwop
     nodeSelector:
       xxx
```
3. After pod is running, change the nodeSelector in replicaSet to another node
4. Forcibly delete the pod with `kubectl delete pod test-5nffh --force `
5. A new node `test-g6vnt `should come up in a different node and the old pod should be terminated
6. At this time, pv is writable for these two pods and the file /mnt/myfile.txt is overlapped like this:
```
# cat /mnt/myfile.txt
test-5nffh Tue Apr 9 07:29:03 UTC 2024
test-5nffh Tue Apr 9 07:29:04 UTC 2024
test-5nffh Tue Apr 9 07:29:05 UTC 2024
test-g6vnt Tue Apr 9 07:29:06 UTC 2024
test-5nffh Tue Apr 9 07:29:06 UTC 2024
test-g6vnt Tue Apr 9 07:29:07 UTC 2024
test-5nffh Tue Apr 9 07:29:07 UTC 2024
test-g6vnt Tue Apr 9 07:29:08 UTC 2024
test-5nffh Tue Apr 9 07:29:08 UTC 2024
```
7. That means during the shutdown period, pv can be accessible for two pods simultaneously 



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 
Server Version: v1.28.6
```

</details>


#### Cloud provider

<details>
openstack
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
VERSION="15-SP3"
VERSION_ID="15.3"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP3"
$ uname -a
Debian 6.1.77-0gardenlinux1 (2024-02-12) x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
csidriver is: nfs.manila.csi.openstack.org
</details>


### 分析结果

不涉及

---

## Issue #124237 ValidatingAdmissionPolicy using CRDs as paramKind can fail due to 30s discovery mechanism

- Issue 链接：[#124237](https://github.com/kubernetes/kubernetes/issues/124237)

### Issue 内容

#### What happened?

When a ValidatingAdmissionPolicy is using CRDs as paramKind, it can result in `failed to find resource referenced by paramKind` error if the custom resource is created around the same time as the vap resource. This could result in new resources getting blocked by this vap because it thinks the custom type does not exist assuming fail policy is set to `fail`. 

As discussed with @alexzielenski on slack:
> It can take up to 30s for the discovery mechanism to pickup the change to the CRD list

#### What did you expect to happen?

Allow retries on CRD list to resync without breaking the vap. 

#### How can we reproduce it (as minimally and precisely as possible)?

A example of failed unit test https://github.com/open-policy-agent/gatekeeper/actions/runs/8070764571/job/22048871826?pr=3289 the test tries to create a custom resource after the vap resource is created and this new resource is blocked because it thinks the custom type referenced by paramKind does not exist

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.29
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124234 CEL cost budget exceeded when using messageExpression

- Issue 链接：[#124234](https://github.com/kubernetes/kubernetes/issues/124234)

### Issue 内容

#### What happened?

I have a CRD with the following property within the top level spec schema:
```
config:
  type: object
  x-kubernetes-validations:
    - messageExpression: '''invalid attempts: '' + string(self.attempts)'
      rule: self.attempts >= 0
  properties:
    attempts:
      type: integer
      maximum: 2147483647
      minimum: -2147483648
```

The API server rejects the CRD with the following error:
```
* spec.validation.openAPIV3Schema.properties[spec].properties[config].x-kubernetes-validations[0].messageExpression: Forbidden: estimated messageExpression cost exceeds budget by factor of more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and maxLength where arrays, maps, and strings are declared)
* spec.validation.openAPIV3Schema.properties[spec].properties[config].x-kubernetes-validations[0].messageExpression: Forbidden: contributed to estimated rule cost total exceeding cost limit for entire OpenAPIv3 schema
* spec.validation.openAPIV3Schema: Forbidden: x-kubernetes-validations estimated rule cost total for entire OpenAPIv3 schema exceeds budget by factor of more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and maxLength where arrays, maps, and strings are declared)
```

This entire CRD contains a single CEL rule, and this is only a problem when using `messageExpression` in the validation rule. 

#### What did you expect to happen?

I would expect `string(intValue)` to not cause the cost budget of the validation rule to be exceeded.

#### How can we reproduce it (as minimally and precisely as possible)?

Use a similar schema as shown in the example.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:21:19Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T22:24:00Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

Kind cluster


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124233 Deployment does not apply multiplexed ports correctly to my Pods

- Issue 链接：[#124233](https://github.com/kubernetes/kubernetes/issues/124233)

### Issue 内容

#### What happened?

I'm running a DNS server (dnsmasq) Deployment, which is exposed to the internet via a Traefik Reverse Proxy. The DNS protocol requires me to multiplex different protocols on the same port, as DNS queries can come in with either TCP or UDP protocol.

On both the dnsmasq and the Traefik deployments, I've run into the issue that the K8s API "swallows" one of the muliplexed ports, effectively rendering the DNS server useless (as either TCP or UDP traffic can't reach the Pod). 

To make it short: 
When I apply this port configuration to a container:
```
ports:
  - containerPort: 1053
    name: udp
    protocol: UDP
  - containerPort: 1053
    name: tcp
    protocol: TCP
```

only one of both ports is actually applied (and it seems to be random which of those ports is applied).

**The issue is not reproducible 100% of the time! In my short study, it occured about every 5th time I tried. The exact conditions that must be met to reproduce this issue are unknown to me.**

My current workaround is to manually add the missing port by editing the Deployment with kubectl/ArgoCD after it was initially applied. This has consistently resolved the issue, but is not a viable long-term solution for me.

#### What did you expect to happen?

I expected that both protocols are usable on the same port after the first `kubectl apply`.

#### How can we reproduce it (as minimally and precisely as possible)?

Keep in mind that this issue is occuring randomly, and you might need to try a few times until you are able to reproduce it.
You can reproduce the issue by `kubectl apply`ing this Deployment manifest:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multiplexed-port
  namespace: multiplexed-port
spec:
  replicas: 1
  selector:
    matchLabels:
      app: multiplexed-port
  template:
    metadata:
      labels:
        app: multiplexed-port
    spec:
      containers:
        - name: multiplexed-port
          image: busybox:latest
          command: ["sh", "-c", "echo 'Hello, World!' && sleep infinity"]
          ports:
            - containerPort: 1053
              name: udp
              protocol: UDP
            - containerPort: 1053
              name: tcp
              protocol: TCP
```
After the manifest was applied to the cluster, run `kubectl describe` to see that only one of both ports was applied.

I did not test the issue with a naked Pod or other pod controllers like `DaemonSet` or `StatefulSet`.


#### Anything else we need to know?

I'm using ArgoCD (currently v2.9.6) to manage my Deployments. When one of the multiplexed ports is not correctly applied, ArgoCD does *not* show an OutOfSync status for my Deployment, indicating that the K8s API does not work correctly here.
The `kubectl.kubernetes.io/last-applied-configuration` annotation also shows both ports have been applied, while only one is displayed when I run `kubectl describe`.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.9
WARNING: version difference between client (1.29) and server (1.27) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
We're using an AKS cluster with AzureLinux.
</details>


#### OS version

<details>

I'm using MacOS Sonoma (14.4).
The cluster is always running on the latest version of AzureLinux.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124230 Mounted volume size in pod is smaller than size declared in PVC.

- Issue 链接：[#124230](https://github.com/kubernetes/kubernetes/issues/124230)

### Issue 内容

#### What happened?

Hello,

Mounted volume size in pod is smaller than size declared in PVC.
I don't know if this is the usual behavior or it is a bug.

Thanks for your help

#### What did you expect to happen?

I expect when I exec it the container and I hit the command df -h to see 20Gi for the path /mypath/ but I only see 4Gi.

#### How can we reproduce it (as minimally and precisely as possible)?


```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ex1
  namespace: ns1
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 20Gi
  storageClassName: ibmc-s3fs-standard-perf-regional
  volumeMode: Filesystem
```
---
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  namespace: ns01
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      volumeMode:
      - name: vol
        PersistentVolumeClaim:
          claimName: ex1
      containers:
      #
      #
        volumeMounts:
          - mountPath: /mypath/
            name: vol
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version : 1.11
Server Version : 1.26
</details>


#### Cloud provider

<details>
IBM Cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Ubuntu 20
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124224 Race condition in PV controller storeObjectUpdate()

- Issue 链接：[#124224](https://github.com/kubernetes/kubernetes/issues/124224)

### Issue 内容

#### What happened?

PV controller (in KCM) has its own cache in addition to the informer cache, to store updates from itself. However, we've identified 2 possible race condition when invoking `storeObjectUpdate()` from multiple goroutines.

`storeObjectUpdate()` is currently invoked from `setClaimProvisioner()` and `syncUnboundClaim()`, which runs in 2 different goroutine. And I would expect more concurrent access for enhanced throughput in the future.

Race conditions:
1. internal update overwrites external delete. When a object is deleted from APIServer, an update request from controller itself may still in-flight. After the update returns, it tries to insert the object back to the cache, overwriting deletion propagated from informer. Resulting in the object staying in cache forever. Subsequently causing, for example, PV stay in Bound state forever and cannot be released.
2. read-compare-write in `storeObjectUpdate()`. When invoked concurrently, an old version (defined by ResourceVersion) may overwrite new version. Resulting in 15s cache stall (should be fixed by next sync).

#### What did you expect to happen?

No race conditions. Internal cache stays consistent on concurrent access.

#### How can we reproduce it (as minimally and precisely as possible)?

Hard to reproduce, this only happens with a small chance.

We observed condition 1 when we try to optimize the throughput of PV controller.

#### Anything else we need to know?

To fix race condition 2, I propose: add locks around `storeObjectUpdate()`

To fix race condition 1, I propose: explicitly differentiate update and add event, do not allow add for internal update response.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124209 SCTP Server on agnhost image crashes when a client connects from the same pod

- Issue 链接：[#124209](https://github.com/kubernetes/kubernetes/issues/124209)

### Issue 内容


How to repro:
1. Run an agnhost pod listening on sctp
```
spec:
  containers:
  - command:
    - /agnhost
    - netexec
    - --sctp-port
    - "8080"
    image: registry.k8s.io/e2e-test-images/agnhost:2.39
```

2. Exec into the pod and use the connect command to probe against the same pod
```
/agnhost connect --timeout=2s --protocol=sctp localhost:8080
bash-5.0# FATA[0014] execing command in container: command terminated with exit code 137
```

The stack trace 


```
I0406 18:21:17.102027       1 log.go:195] Started HTTP server on port 8080
I0406 18:21:17.102399       1 log.go:195] Started UDP server on port  8081
I0406 18:21:17.102465       1 log.go:195] Started SCTP server
I0406 18:22:07.943777       1 log.go:195] SCTP server exited
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0x15097bb]

goroutine 77 [running]:
k8s.io/kubernetes/test/images/agnhost/netexec.startSCTPServer(0x0?)
        /go/src/k8s.io/kubernetes/test/images/agnhost/netexec/netexec.go:696 +0x3fb
created by k8s.io/kubernetes/test/images/agnhost/netexec.main
        /go/src/k8s.io/kubernetes/test/images/agnhost/netexec/netexec.go:217 +0x22f
root@kind-worker:/#

```

that points to 

https://github.com/kubernetes/kubernetes/blob/f8930f980d2986f9e486b04c14c3e93e57bdbe12/test/images/agnhost/netexec/netexec.go#L696

and this is from the vendored library vendor/github.com/ishidawataru/sctp/sctp.go , 

/kind bug
/sig network 
/help


### 分析结果

不涉及。

---

## Issue #124208 2 sets of containers show after updating the deployment yaml.

- Issue 链接：[#124208](https://github.com/kubernetes/kubernetes/issues/124208)

### Issue 内容

#### What happened?

There are 2 sets containers for a deployment. After remove a deployment, containers related to that deployment is not being controlled. And I cannot remove them.

#### What did you expect to happen?

The old version containers are deleted when I apply new configurations yaml. And containers with new version image start to work.

#### How can we reproduce it (as minimally and precisely as possible)?

I deployed deployment services. And I make another docker image and put them on my workers. I change the  image name in the deployment yaml file. And use kubectl apply to update the resources. The old version containers do not disappear but new version containers were created. When I delete the deployment, the containers created by  new version image are still running. Delete deployment again it showed me "Error from server (NotFound): deployments.apps "picdetecter-deployment" not found."

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3

```

</details>


#### Cloud provider

<details>
oracle
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$  cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux master 5.15.0-1051-oracle #57-Ubuntu SMP Wed Jan 24 18:29:02 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
ordinary installation.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker version 26.0.0, build 2ae903e
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
weavenet.
</details>


### 分析结果

不涉及

---

## Issue #124207 Endpoint selected by multiple headless services only has 1 dns hostname

- Issue 链接：[#124207](https://github.com/kubernetes/kubernetes/issues/124207)

### Issue 内容

#### What happened?

A pod in multiple endpoint slices selected by a headless service only resolves the hostname for one of the services.

Per the [spec](https://github.com/kubernetes/dns/blob/master/docs/specification.md#241---aaaaa-records):
> There must be an A record for each ready endpoint of the headless Service with IPv4 address <endpoint-ip> as shown below.

So I feel like I'm missing something?

#### What did you expect to happen?

Both names resolve (maybe?)

#### How can we reproduce it (as minimally and precisely as possible)?

- create 2x statefusets.
- add a shared selector to both
- add a unique selector to each
- create 3x headless services: one for each selector

now things get interesting:
- the shared service will show all endpoints from both pods, and the endpoint slices will show they as ready and serving
- DNS will only resolve for whatever you said SS serviceName to (perhaps deliberately?)
- SS subdomain is overwritten to match serviceName if specified?

end result is only `second-pod.second-service-name.ns.svc.cluster.local` resolves to the pod IP, even though `second-pod.shared-service.ns.svc.cluster.local` should too?

my read is that is due to this logic:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/endpointslice/util/controller_utils.go#L113-L117
https://github.com/coredns/coredns/blob/e3f83cb1fabb9b1cbaffb9df3c4b65476e92c39b/plugin/kubernetes/kubernetes.go#L476-L480

so who is wrong?

#### Anything else we need to know?

```bash
$ kubectl apply -f repro.yaml

# when using serviceName: shared
$ k exec -it first-0 -- dig second-0.shared.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> second-0.shared.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 10827
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: 5b180353a44f56c4 (echoed)
;; QUESTION SECTION:
;second-0.shared.default.svc.cluster.local. IN A

;; ANSWER SECTION:
second-0.shared.default.svc.cluster.local. 5 IN	A 10.7.225.77

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:31 UTC 2024
;; MSG SIZE  rcvd: 139
```

```bash
# same. just highlighting the 2nd subdomain doesn't resolve. perhaps expected
$ k exec -it first-0 -- dig second-0.second.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> second-0.second.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 39439
;; flags: qr aa rd; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: d8f070b19ee2c55e (echoed)
;; QUESTION SECTION:
;second-0.second.default.svc.cluster.local. IN A

;; AUTHORITY SECTION:
cluster.local.		5	IN	SOA	ns.dns.cluster.local. hostmaster.cluster.local. 1712413987 7200 1800 86400 5

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:27 UTC 2024
;; MSG SIZE  rcvd: 175

```

```bash
# fine
$ k exec -it first-0 -- dig first-0.first.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> first-0.first.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 28555
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: 474b9b921327388d (echoed)
;; QUESTION SECTION:
;first-0.first.default.svc.cluster.local. IN A

;; ANSWER SECTION:
first-0.first.default.svc.cluster.local. 5 IN A	10.7.225.73

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:39 UTC 2024
;; MSG SIZE  rcvd: 135
```

interestingly, on the endpoints with issue, I see in endpointslice yaml they don't have the hostname due to the k8s code above

<details>

<summary>long bash outputs of dig and endpoint slices full yaml output </summary>

```bash
$ k get ep -o yaml first second shared
apiVersion: v1
items:
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:31:49Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    labels:
      service.kubernetes.io/headless: ""
    name: first
    namespace: default
    resourceVersion: "4546341"
    uid: 884b42c2-1a3c-4f90-95e4-f82a8383c8f9
  subsets:
  - addresses:
    - hostname: first-0
      ip: 10.7.225.73
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: first-0
        namespace: default
        uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    ports:
    - name: rest
      port: 8501
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:30:43Z"
    labels:
      service.kubernetes.io/headless: ""
    name: second
    namespace: default
    resourceVersion: "4546684"
    uid: 314a8f21-9133-4b6d-8f5e-5b25907cbf1a
  subsets:
  - addresses:
    - ip: 10.7.225.77
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: second-0
        namespace: default
        uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    ports:
    - name: rest
      port: 8501
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    labels:
      service.kubernetes.io/headless: ""
    name: shared
    namespace: default
    resourceVersion: "4546685"
    uid: 099d86cb-4418-44e7-b7d9-8fdb1ef0b3fb
  subsets:
  - addresses:
    - ip: 10.7.225.73
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: first-0
        namespace: default
        uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    - hostname: second-0
      ip: 10.7.225.77
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: second-0
        namespace: default
        uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    ports:
    - name: rest
      port: 8080
      protocol: TCP
kind: List
metadata:
  resourceVersion: ""

$ k get endpointslice | rg '(first|second|shared)'
first-lr9rb                       IPv4          8501      10.7.225.73               21m
second-fhvbk                      IPv4          8501      10.7.225.77               17m
shared-4ph6n                      IPv4          8080      10.7.225.73,10.7.225.77   21m

$ k get endpointslice -o yaml first-lr9rb second-fhvbk shared-4ph6n
apiVersion: v1
items:
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.73
    conditions:
      ready: true
      serving: true
      terminating: false
    hostname: first-0
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: first-0
      namespace: default
      uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:31:49Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    generateName: first-
    generation: 6
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: first
      service.kubernetes.io/headless: ""
    name: first-lr9rb
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: first
      uid: 7d50bcea-2070-4484-a049-b63f8b67ff7f
    resourceVersion: "4546340"
    uid: b2da77c4-fa28-4be2-a562-e9d19cc0d849
  ports:
  - name: rest
    port: 8501
    protocol: TCP
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.77
    conditions:
      ready: true
      serving: true
      terminating: false
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: second-0
      namespace: default
      uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:30:43Z"
    generateName: second-
    generation: 16
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: second
      service.kubernetes.io/headless: ""
    name: second-fhvbk
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: second
      uid: 28d521bc-1468-4e6a-a156-6f927494aead
    resourceVersion: "4546686"
    uid: 483b27b1-ba59-4ac2-9940-a465f43b8fc2
  ports:
  - name: rest
    port: 8501
    protocol: TCP
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.73
    conditions:
      ready: true
      serving: true
      terminating: false
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: first-0
      namespace: default
      uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    zone: "208"
  - addresses:
    - 10.7.225.77
    conditions:
      ready: true
      serving: true
      terminating: false
    hostname: second-0
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: second-0
      namespace: default
      uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    generateName: shared-
    generation: 23
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: shared
      service.kubernetes.io/headless: ""
    name: shared-4ph6n
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: shared
      uid: a3c5d040-1905-447c-a457-6b72ba26f0b7
    resourceVersion: "4546683"
    uid: 0110bd21-b87c-4380-83bb-0c89f13098c2
  ports:
  - name: rest
    port: 8080
    protocol: TCP
kind: List
metadata:
  resourceVersion: ""
```

</details>


<details>
<summary>yamls</summary>

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    unique: &name first
    shared: shared
  name: *name
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      unique: *name
  serviceName: *name
  template:
    metadata:
      labels:
        unique: *name
        shared: shared
    spec:
      containers:
      - command: ["sleep", "infinity"]
        image: docker.io/nicolaka/netshoot@sha256:b569665f0c32391b93f4de344f07bf6353ddff9d8c801ac3318d996db848a64c
        imagePullPolicy: IfNotPresent
        name: server
        ports:
        - containerPort: 8080
          name: rest
          protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      subdomain: *name
      terminationGracePeriodSeconds: 1
      tolerations:
      - effect: NoSchedule
        key: is_cpu_compute
        operator: Exists
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
---
apiVersion: v1
kind: Service
metadata:
  name: &name first
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8501
    protocol: TCP
    targetPort: 8501
  selector:
    unique: *name
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    unique: &name second
    shared: shared
  name: *name
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      unique: *name
  serviceName: shared
  template:
    metadata:
      labels:
        unique: *name
        shared: shared
    spec:
      containers:
      - command: ["sleep", "infinity"]
        image: docker.io/nicolaka/netshoot@sha256:b569665f0c32391b93f4de344f07bf6353ddff9d8c801ac3318d996db848a64c
        imagePullPolicy: IfNotPresent
        name: server
        ports:
        - containerPort: 8080
          name: rest
          protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      subdomain: shared # <--- important to repro
      terminationGracePeriodSeconds: 1
      tolerations:
      - effect: NoSchedule
        key: is_cpu_compute
        operator: Exists
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
---
apiVersion: v1
kind: Service
metadata:
  name: &name second
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8501
    protocol: TCP
    targetPort: 8501
  selector:
    unique: *name
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: &name shared # <-- this one will have issues
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    shared: *name
  sessionAffinity: None
  type: ClusterIP
---
```

</details>


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.13
WARNING: version difference between client (1.29) and server (1.26) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
tested across 3x providers so far with same behavior, multiple k8s versions but not latest master
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
...coredns?
</details>


### 分析结果

不涉及

---

## Issue #124204 invalid bearer token, service account token has been invalidated

- Issue 链接：[#124204](https://github.com/kubernetes/kubernetes/issues/124204)

### Issue 内容

#### What happened?

My K8s cluster kube apiserver has a large number of invalid bearer tokens, and the service account token has been invalidated
Phenomenon:
1. All are concentrated on one kube apiserver node
2. I did not find any functional damage in the cluster, including related business Pods
![image](https://github.com/kubernetes/kubernetes/assets/20591674/791b6289-8fec-4c19-a9d8-55794ff19dc2)

#### What did you expect to happen?

How do I know where this part of the request comes from and how do I handle it

#### How can we reproduce it (as minimally and precisely as possible)?

no

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.6", GitCommit:"ff2c119726cc1f8926fb0585c74b25921e866a28", GitTreeState:"clean", BuildDate:"2023-01-18T19:15:26Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Linux 5.4.86-1.el7.elrepo.x86_64 #1 SMP Tue Dec 29 10:39:46 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
</details>


#### Install tools

<details>
kubeasz
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd v1.6.19
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124201 api: v1.KubeSchedulerConfiguration Go type is not compatible with controller-gen

- Issue 链接：[#124201](https://github.com/kubernetes/kubernetes/issues/124201)

### Issue 内容

## What would you like to be added?

I am using the `kubescheduler.config.k8s.io/v1.KubeSchedulerConfiguration` type as a field in my CustomResourceDefinition. However, when I try to generate CRD manifests from the Go type for the CRD using the command I get various errors.

## Why is this needed

I'm curious if this use case is supported. It can help projects like kubeadm, [Cluster API](https://cluster-api.sigs.k8s.io/) or [Kamaji](https://kamaji.clastix.io/) accept KubeSchedulerConfiguration objects as a field to customize the provisioned cluster's kube-scheduler config differently between clusters. The only other option to do so is to either deal with `runtime.RawExtensions` or use a YAML/JSON stored as `string`.

## Errors from `controller-gen crd`

When I use the `controller-gen crd` command to generate CustomResourceDefinition manifests for a Go type that has a struct member of type KubeSchedulerConfiguration, I get errors about float types (presumably QPS/Burst) in the `componentbaseconfigv1alpha1.ClientConnectionConfiguration`:

> `
/Users/abalkan/go-athens/pkg/mod/k8s.io/component-base@v0.29.1/config/v1alpha1/types.go:79:6: found float, the usage of which is highly discouraged, as support for them varies across languages. Please consider serializing your float as string instead. If you are really sure you want to use them, re-run with crd:allowDangerousTypes=true`

And a few errors about various [[]byte fields](https://github.com/kubernetes/kube-scheduler/blob/94ff27f8b9eb26561417aa148314397395e6faef/config/v1/types.go#L385-L396) that have `+listType=atomic`:

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:388:2: must apply listType to an array, found string`

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:392:2: must apply listType to an array, found string`

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:396:2: must apply listType to an array, found string`

/sig scheduling
/sig api-machinery
/kind bug

### 分析结果

不涉及。

---

## Issue #124192 Import cycle while generating configuration with applyconfig-gen

- Issue 链接：[#124192](https://github.com/kubernetes/kubernetes/issues/124192)

### Issue 内容

#### What happened?

`applyconfig-gen` code generator creates import cycle in case of multi-token group names (e.g. resource.io).

#### What did you expect to happen?

It should generate configuration files with valid go syntax.

#### How can we reproduce it (as minimally and precisely as possible)?

Checkout https://github.com/mtrqq/applygen-cycle-repro/tree/main for detailed reproduction. 

Generally the scenario should look like this - create a custom resource API with 1 resource referencing another one and name which includes as least 1 dot (like resource.io). Run `applyconfig-gen` on this configuration, as the result - we are getting import cycle in one of the configuration files

#### Anything else we need to know?

_No response_

#### Kubernetes version

https://github.com/kubernetes/kubernetes/commit/d9c54f69d4bb7ae1bb655e1a2a50297d615025b5

OR

https://github.com/kubernetes/code-generator/commit/670586590c4574bdb99793d8373d8e4431ee30db

#### Cloud provider

NONE

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124188 kube-aggregator proxyHandler does not set traceparent

- Issue 链接：[#124188](https://github.com/kubernetes/kubernetes/issues/124188)

### Issue 内容

#### What happened?

Extension API servers do not currently receive the `traceparent` header when requests are proxied through kube-aggregator, so spans are not linked properly.

#### What did you expect to happen?

Traces from the extension API sever should be linked to the parent in kube-aggregator.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure tracing for kube-apiserver
2. Configure tracing for sample-apiserver and aggregate it to kube-apiserver
3. Make a request to kube-apiserver for one of the resources from sample-apiserver
4. Relevant sample-apiserver traces will not be linked to the kube-apiserver traces

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

This problem currently exists on `master`.

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
Linux work 6.5.0-26-generic #26-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar  5 21:19:28 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124176 ephemeral containers were repeatedly created and had inconsistent status after being manually deleted

- Issue 链接：[#124176](https://github.com/kubernetes/kubernetes/issues/124176)

### Issue 内容

#### What happened?

ephemeral containers were repeatedly created and had inconsistent status after being manually deleted

#### What did you expect to happen?

https://github.com/kubernetes/kubernetes/blob/d9c54f69d4bb7ae1bb655e1a2a50297d615025b5/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L893-L897

ebpemeral containers should never be restarted

#### How can we reproduce it (as minimally and precisely as possible)?

created a demo pod like this:
```
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-pod
spec:
  containers:
  - name: process
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests:
        cpu: 0.1
        memory: 20Mi
      limits:
        cpu: 0.1
        memory: 20Mi
  # ephemeralContainers:
  # - name: debugger
  #   image: busybox
  #   command:
  #   - sleep
  #   - "1000"
```
kubectl apply -f ephemeral-pod.yaml
and then debug ephemeral-pod like this:
```
  # ephemeralContainers:
  # - name: debug
  #   image: busybox
  #   command:
  #   - sleep
  #   - "1000"
```

everything is normal now, the epemeral container is created and runs normally.
now go to the machine and stop the epemeral container:
```
crictl stop 9c684c6c31e2b
crictl rm 9c684c6c31e2b
```

After deletion, the debugger ephemeral container is created again, but the status is inconsistent with the ephemeral container in the pod.
![image](https://github.com/kubernetes/kubernetes/assets/30427474/a192142a-86fb-4299-be94-5af2247f4edc)

Delete again:
![image](https://github.com/kubernetes/kubernetes/assets/30427474/25fb5c94-d0d0-4a3f-8731-921bd0c7b879)

kubelet log:
![image](https://github.com/kubernetes/kubernetes/assets/30427474/8cd63376-a17c-4697-86a8-7121307c1034)
```
Apr 04 07:54:11 minikube kubelet[25225]: I0404 07:54:11.953549   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
Apr 04 07:54:21 minikube kubelet[25225]: I0404 07:54:21.951540   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
Apr 04 07:54:31 minikube kubelet[25225]: I0404 07:54:31.955727   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
macos minikube
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124166 The setting "internalTrafficPolicy: Local" leads to an additional 1 second delay

- Issue 链接：[#124166](https://github.com/kubernetes/kubernetes/issues/124166)

### Issue 内容

#### What happened?

In Kubernetes version 1.26.4, modifying the service parameter "internalTrafficPolicy: Local" results in a 1-second delay when accessing pod applications through the service.However, in Kubernetes version 1.25.5, it functions normally.

Kubernetes 1.26.4
![1712136070710](https://github.com/kubernetes/kubernetes/assets/74498996/fd7f0d69-cf00-4728-9e98-83aeea1f7009)

Kubernetes 1.25.5
![1712136118708](https://github.com/kubernetes/kubernetes/assets/74498996/a9219bae-c289-4d9f-a7a7-cf2f29d4b3af)


#### What did you expect to happen?

。

#### How can we reproduce it (as minimally and precisely as possible)?

。

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Kubernetes 1.26.4
Kubernetes 1.25.5
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124164 failed to run `update-codegen.sh`

- Issue 链接：[#124164](https://github.com/kubernetes/kubernetes/issues/124164)

### Issue 内容

#### What happened?

```
~# make update SILENT=false
Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
Running update-codegen
+++ [0403 16:52:58] Generating protobufs for 68 targets
unknown flag: --output-dir
Usage of go-to-protobuf:
      --add_dir_header                   If true, adds the file directory to the header of the log messages
      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)
      --apimachinery-packages string     comma-separated list of directories to get apimachinery input types from which are needed by any API. Directories prefixed with '-' are not generated, directories prefixed with '+' only create types with explicit IDL instructions. (default "+k8s.io/apimachinery/pkg/util/intstr,+k8s.io/apimachinery/pkg/api/resource,+k8s.io/apimachinery/pkg/runtime/schema,+k8s.io/apimachinery/pkg/runtime,k8s.io/apimachinery/pkg/apis/meta/v1,k8s.io/apimachinery/pkg/apis/meta/v1beta1,k8s.io/apimachinery/pkg/apis/testapigroup/v1")
      --clean                            If true, remove all generated files for the specified Packages.
      --conditional string               An optional Golang build tag condition to add to the generated Go code
      --drop-embedded-fields string      Comma-delimited list of embedded Go types to omit from generated protobufs (default "k8s.io/apimachinery/pkg/apis/meta/v1.TypeMeta")
  -h, --go-header-file string            File containing boilerplate header text. The string YEAR will be replaced with the current 4-digit year.
      --keep-gogoproto                   If true, the generated IDL will contain gogoprotobuf extensions which are normally removed
      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)
      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)
      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
      --logtostderr                      log to standard error instead of files (default true)
      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)
      --only-idl                         If true, only generate the IDL for each package.
  -o, --output-base string               Output base; defaults to $GOPATH/src/ (default "/Users/cyclinder/Desktop/code/kubernetes/_output/local/go/src")
  -p, --packages string                  comma-separated list of directories to get input types from. Directories prefixed with '-' are not generated, directories prefixed with '+' only create types with explicit IDL instructions.
      --proto-import strings             The search path for the core protobuf .protos, required; defaults $GOPATH/src/k8s.io/kubernetes/vendor/github.com/gogo/protobuf/protobuf. (default [/Users/cyclinder/Desktop/code/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/github.com/gogo/protobuf/protobuf])
      --skip-generated-rewrite           If true, skip fixing up the generated.pb.go file (debugging only).
      --skip_headers                     If true, avoid header prefixes in the log messages
      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)
      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2)
      --trim-path-prefix string          If set, trim the specified prefix from --output-package when generating files.
  -v, --v Level                          number for the log level verbosity (default 0)
      --vendor-output-base string        The vendor/ directory to look for packages in; defaults to $PWD/vendor/. (default "/Users/cyclinder/Desktop/code/kubernetes/vendor")
      --verify-only                      If true, only verify existing output, do not write anything.
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging
unknown flag: --output-dir
!!! [0403 16:53:00] Call tree:
!!! [0403 16:53:00]  1: /Users/cyclinder/Desktop/code/kubernetes/hack/update-codegen.sh:885 codegen::protobuf(...)
Running update-codegen FAILED
make: *** [update] Error 1
~# protoc --version
libprotoc 23.4
```

#### What did you expect to happen?

`make update` works

#### How can we reproduce it (as minimally and precisely as possible)?

```
~# ./hack/install-protoc.sh
~# export PATH="/Users/cyclinder/Desktop/code/kubernetes/third_party/protoc:${PATH}"
~# make update SILENT=false
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124162 update-go-workspace.sh doesn't support for MacOS

- Issue 链接：[#124162](https://github.com/kubernetes/kubernetes/issues/124162)

### Issue 内容

#### What happened?

```
make update
Running in silent mode, run with SILENT=false if you want to see script logs.
Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
dirname: illegal option -- z
usage: dirname path
```

#### What did you expect to happen?

`make update` can run on MacOS

#### How can we reproduce it (as minimally and precisely as possible)?

Run `make-update` on the MacOS

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
master

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124155 `hack/verify-file-sizes.sh` does not support Mac OS X

- Issue 链接：[#124155](https://github.com/kubernetes/kubernetes/issues/124155)

### Issue 内容

#### What happened?

The `hack/verify-file-sizes.sh` script encounters an error when executed on Mac OS X because the `stat --printf=%s` option is not supported. The error message appears as follows:

```
ERROR: usage: stat [-FLnq] [-f format | -l | -r | -s | -x] [-t timefmt] [file ...]
```

#### What did you expect to happen?

I expected the `hack/verify-file-sizes.sh` script to execute successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Execute the `hack/verify-file-sizes.sh` command on macOS.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
$ uname -a
Darwin <name> 22.6.0 Darwin Kernel Version 22.6.0: Mon Feb 19 19:43:41 PST 2024; root:xnu-8796.141.3.704.6~1/RELEASE_ARM64_T8103 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124150 make test on darwin fails with error "build constraints exclude all Go files"

- Issue 链接：[#124150](https://github.com/kubernetes/kubernetes/issues/124150)

### Issue 内容

#### What happened?

Tried running make test against local latest fork of kubernetes master

```
%  make test
+++ [0402 17:43:52] Set GOMAXPROCS automatically to 8
WARNING: ulimit -n (files) should be at least 1000, is 256, may cause test failure
+++ [0402 17:47:36] Running tests without code coverage and with -race
package k8s.io/kubernetes/pkg/proxy/iptables: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/iptables
package k8s.io/kubernetes/pkg/proxy/ipvs/ipset: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/ipset
package k8s.io/kubernetes/pkg/proxy/ipvs/ipset/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/ipset/testing
package k8s.io/kubernetes/pkg/proxy/ipvs/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/testing
package k8s.io/kubernetes/pkg/proxy/ipvs/util: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/util
package k8s.io/kubernetes/pkg/proxy/ipvs/util/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/util/testing
package k8s.io/kubernetes/pkg/proxy/nftables: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/nftables
package k8s.io/kubernetes/pkg/util/iptables/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/util/iptables/testing
make: *** [test] Error 1

```

Go version
```
% go version
go version go1.22.0 darwin/amd64
```

Go env

```
% go env
GO111MODULE='on'
GOARCH='amd64'
GOBIN=''
GOCACHE='/Users/karthikkn/Library/Caches/go-build'
GOENV='/Users/karthikkn/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFLAGS=''
GOHOSTARCH='amd64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMODCACHE='/Users/karthikkn/go/pkg/mod'
GONOPROXY='github.com/PDeXchange'
GONOSUMDB='github.com/PDeXchange'
GOOS='darwin'
GOPATH='/Users/karthikkn/go'
GOPRIVATE='github.com/PDeXchange'
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/Users/karthikkn/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.22.0.darwin-amd64'
GOSUMDB='sum.golang.org'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/Users/karthikkn/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.22.0.darwin-amd64/pkg/tool/darwin_amd64'
GOVCS=''
GOVERSION='go1.22.0'
GCCGO='gccgo'
GOAMD64='v1'
AR='ar'
CC='clang'
CXX='clang++'
CGO_ENABLED='1'
GOMOD='/Users/karthikkn/karthik-k8-workspace/kubernetes/go.mod'
GOWORK='/Users/karthikkn/karthik-k8-workspace/kubernetes/go.work'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
PKG_CONFIG='pkg-config'
GOGCCFLAGS='-fPIC -arch x86_64 -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/j0/f79y3_5d5svcgbm4_yfjg91w0000gn/T/go-build1055166348=/tmp/go-build -gno-record-gcc-switches -fno-common'
```

#### What did you expect to happen?

Successfully able to run the test

#### How can we reproduce it (as minimally and precisely as possible)?

Try running make test on darwin os.
```
make test
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
Latest master

```
% git log --oneline
e3bb757129c (HEAD -> master, origin/master, origin/HEAD) Merge pull request #124143 from enj/enj/i/svm_update_crd_flake
f338ab5f959 svm test: wait after updating CRD to avoid flakes
79c61d5f030 Merge pull request #124124 from carlory/fix-124120
363fee59e4b fix panic with SIGSEGV in kubeadm certs check-expiration
3dedb8eb8c1 Merge pull request #124111 from liggitt/deflake-authz-test
```

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Darwin Karthiks-MacBook-Pro.local 23.4.0 Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:31 PST 2024; root:xnu-10063.101.15~2/RELEASE_X86_64 x86_64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

