# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (14 个)

## Issue #124002 Kubernetes Skips Init Containers Beyond First During Some Pod Restarts

- Issue 链接：[#124002](https://github.com/kubernetes/kubernetes/issues/124002)

### Issue 内容

#### What happened?

Init containers beyond the first are ignored during certain pod restarts.  In particular, when the pod sandbox suddenly temporarily disappears (possibly due to a node restart) the automatic restart sequence of the pod seems to skip init containers after the first.

#### What did you expect to happen?

From the [Kubernetes docs](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#detailed-behavior):

"If the Pod [restarts](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons), or is restarted, all init containers must execute again."

#### How can we reproduce it (as minimally and precisely as possible)?

Below `repro.yaml` is a trimmed-down deployment:
two init containers and a primary container, all running bash.
Primary just sleeps, init containers each echo and exit.

Apply the yaml, let it get running, and then, eg, forcibly kill its pod sandbox (variety of methods, depending on CRI, host environment, etc..):

A specific repro that assumes a linux host, containerd:

```bash
kubectl create namespace delme
kubectl apply -f repro.yaml
kubectl get pods -n delme  # confirm the deployment started normally

# simulate a node/sandbox-level disruption by killing pod's slice (probably a better way to do this..?)
# run next command on **node host**:
sudo systemctl stop --force \
 "$(ps -o cgroup --pid "$(pgrep --full 'pgrepid:PkIOeQU4J6zI')" |
 sed -ne '2s/.*\(kubepods-besteffort-pod.*\.slice\).*/\1/p')"

kubectl describe pods -n delme
```

The restart count of the second init container is 0, while main container and first init container are 1.  Can repeat the stop, and the restart count of the first init container will increment, staying in sync with the main container and the overall pod restart counter, but the second init container stays at 0.  Start/finish time stamps in describe, and results of `kubectl logs`, are also consistent with the second init container not being re-run.

And here's `repro.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: multi-init-container-bug-repro
  name: multi-init-container-bug-repro
  namespace: delme
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  selector:
    matchLabels:
      app: multi-init-container-bug-repro
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: multi-init-container-bug-repro
    spec:
      automountServiceAccountToken: false
      containers:
      - command:
        - /bin/bash
        - -c
        - sleep infinity && echo 'pgrepid:PkIOeQU4J6zI'
        image: debian:stable
        imagePullPolicy: IfNotPresent
        name: main
      initContainers:
      - command:
        - /bin/bash
        - -c
        - "echo 'init-container: First!'"
        image: debian:stable
        imagePullPolicy: IfNotPresent
        name: init-first
      - command:
        - /bin/bash
        - -c
        - "echo 'init-container: Second!'"
        image: debian:stable
        imagePullPolicy: IfNotPresent
        name: init-second
      restartPolicy: Always
```

#### Anything else we need to know?

## Practical Impact:

Deployments that use multiple init containers (which includes some popular ones, eg gitlab via helm) can get stuck during their auto restart after a transient node outage (requiring manual intervention to get unstuck).  In production, this is hopefully a rare (but potentially unpleasant) event.  It is fairly common and annoying in certain kinds of development/test configurations if eg intentional host node restarts are common.

## Workarounds:

Deleting the pod through `kubectl delete pod`, or `kubectl rollout restart`ing its eg deployment, causes init containers to be re-run when the pod is restarted (or more precisely, when the fresh replacement pod is started).


## Additional Notes

- [This issue](https://github.com/kubernetes/kubernetes/pull/47599) might be related.
- In the repro below, if instead of killing the whole pod sandbox, just the main container is killed, then `kubectl get pods` again reports a restart, but `describe` now shows *both* init containers are skipped.  This might be intended behavior? It is less-obviously-bad than arbitrarily skipping just some init containers, as happens in the main repro path.. but if the restart count in the output of `kubectl get pods` is to be interpreted as the "number of pod restarts", it seems like this path also contradicts the cited docs.

#### Kubernetes version

Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.1

#### Cloud provider

n/a (repros on bare-metal `kubeadm`)

#### OS version

NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

#### Install tools

kubeadm

#### Container runtime (CRI) and version (if applicable)

containerd github.com/containerd/containerd v1.7.5 fe457eb99ac0e27b3ce638175ef8e68a7d2bc373


### 分析结果

存在潜在的安全风险。

### 原因和可能的影响：

- **初始化步骤缺失**：当Pod发生特定类型的重启（例如Pod沙箱突然暂时消失）时，除第一个Init容器外的其他Init容器被跳过。这违反了Kubernetes文档中所述的所有Init容器都应重新执行的行为。
  
- **安全初始化未执行**：如果被跳过的Init容器包含关键的安全配置、权限设置、证书初始化、安全策略应用等操作，那么这些操作将不会在Pod重启后执行，导致主容器在缺乏必要安全措施的情况下运行。

- **攻击者可利用**：攻击者如果能够诱导或触发Pod的这种重启方式（例如通过消耗资源、利用漏洞导致节点重启或Pod沙箱异常），就可能使Init容器被跳过，进而使应用在未经过完整安全初始化的情况下启动。

- **安全策略绕过**：缺失的安全初始化可能导致应用暴露敏感信息、禁用的端口被开启、权限过高、未启用必要的安全控件等，给攻击者提供了利用的机会，可能导致数据泄露、权限提升、服务被破坏等严重后果。

### Proof of Concept（概念验证）：

1. **设置场景**：创建一个包含多个Init容器的Pod，其中第二个Init容器负责应用关键的安全配置，比如设置防火墙规则或加载安全证书。

2. **触发问题**：按照Issue中给出的步骤，模拟Pod沙箱的暂时消失（例如，通过节点重启或强制停止Pod的cgroup）。

3. **观察结果**：Pod自动重启，但仅执行了第一个Init容器，第二个Init容器未被执行。主容器在未应用安全配置的情况下启动。

4. **验证漏洞**：检查主容器的运行状态，发现安全配置缺失，例如防火墙规则未应用，敏感端口处于开放状态。攻击者可以利用这些缺陷发起攻击。

### 符合风险判断标准：

1. **可被攻击者利用**：攻击者可以通过诱导节点重启或利用系统漏洞触发Pod沙箱的消失，导致Init容器被跳过。

2. **可能成为漏洞并被分配CVE编号，CVSS评分在High以上**：

   - **攻击向量（AV）**：网络攻击（Network），攻击者可远程触发。
   - **攻击复杂度（AC）**：低（Low），不需要特殊条件。
   - **权限要求（PR）**：无（None），攻击者无需预先权限。
   - **用户交互（UI）**：无（None），不需要用户交互。
   - **影响**：
     - **机密性（C）**：高（High），可能导致敏感数据泄露。
     - **完整性（I）**：高（High），攻击者可篡改数据。
     - **可用性（A）**：高（High），可能导致服务不可用。

   根据CVSS 3.1评分，综合评分在 **High** 级别以上。

综上所述，该Issue涉及潜在的安全风险，可能被攻击者利用并造成严重影响，应引起重视并尽快修复。

---

## Issue #123986 Analyzing and Addressing Unforeseen Performance Issues in a Large-Scale Kubernetes Cluster

- Issue 链接：[#123986](https://github.com/kubernetes/kubernetes/issues/123986)

### Issue 内容

#### What happened?

Hi everyone,

I hope this message finds you well. I'm reaching out to share and seek advice on a performance issue we've encountered in our production Kubernetes (k8s) cluster. Our setup includes 300 nodes and supports 8,000 Pods, and we've recently started experiencing some concerns.

The issue began with a Pod we deployed, designed to periodically scrape Pod information by making an API call to `/api/v1/pods?fieldSelector=spec.nodeName%3D<node-name>` on the k8s API server. This Pod updates Pod information from its storage while periodically initiating these API calls.

As illustrated in Figure 1 below, there's been a noticeable increase in the request rate from an average of 2.75 to 4 requests per second. During this period, we observed significant delays in the k8s API servers when processing LIST verb API calls, as highlighted in Figure 2. Concurrently, there was a dramatic increase in the memory usage of the API servers, depicted in Figure 3. Additionally, the gRPC traffic from etcd to the k8s API servers also saw an increase, as shown in Figure 4.

However, what puzzles me is that the workload generated by this Pod appears to be too small to cause such a significant impact. It only contributes to 1.5 requests per second per API server, amounting to a total of 4.5 requests per second.

I'm eager to hear your thoughts, insights, or any advice you might have on addressing this issue. 

**[Fig. 1: requests per sec from k8s API servers]**
![k8s api 의 list verb의 total request](https://github.com/kubernetes/kubernetes/assets/4262433/d7eb114e-8823-4f36-8a56-c0092fad6291)

**[Fig. 2: 90%-tile Latency of Each Verb from k8s API server]**
![k8s api 의 verb 별 tail latency](https://github.com/kubernetes/kubernetes/assets/4262433/15810101-6817-45fc-b15a-f0e23e1a8b2e)

**[Fig. 3: k8s API servers' memory usage]**
![k8s apiserver 메모리 footprint](https://github.com/kubernetes/kubernetes/assets/4262433/7f53ad44-6e7b-41e6-8569-bd98f6dfe44f)


**[Fig. 4: outboud gRPC traffic from etcd]**
![etcd Client Traffic Out (해당 기간에 증가했음)](https://github.com/kubernetes/kubernetes/assets/4262433/b0027b3e-4d4a-4137-87d2-be99440a86bc)


#### What did you expect to happen?

I expected that the workload would not impact the performance of the k8s API servers.

#### How can we reproduce it (as minimally and precisely as possible)?

I think you should be able to replicate this issue by setting up a cluster with over 8,000 Pods and periodically fetching the Pods' information through API calls.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.8", GitCommit:"7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44", GitTreeState:"clean", BuildDate:"2022-03-16T14:10:06Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.6", GitCommit:"d921bc6d1810da51177fbd0ed61dc811c5228097", GitTreeState:"clean", BuildDate:"2021-10-27T17:44:26Z", GoVersion:"go1.16.9", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
We have set up our cluster on-premises.

```console
$ cat /proc/cpuinfo
...
processor	: 111
vendor_id	: GenuineIntel
cpu family	: 6
model		: 106
model name	: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz
stepping	: 6
microcode	: 0xd0003a5
cpu MHz		: 3400.000

$ cat /proc/meminfo 
MemTotal:       528038752 kB
MemFree:        284106660 kB
MemAvailable:   464637832 kB
Buffers:         5636388 kB
Cached:         168626216 kB
SwapCached:            0 kB
Active:         66852988 kB
Inactive:       144133368 kB
Active(anon):   38236244 kB
Inactive(anon):   101448 kB
Active(file):   28616744 kB
Inactive(file): 144031920 kB
Unevictable:     9536672 kB
Mlocked:         9536672 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:            116556 kB
Writeback:             0 kB
AnonPages:      39823220 kB
Mapped:          4326780 kB
Shmem:           1242292 kB
KReclaimable:   10923516 kB
Slab:           13771744 kB
SReclaimable:   10923516 kB
SUnreclaim:      2848228 kB
KernelStack:       77232 kB
PageTables:       242660 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    261131696 kB
Committed_AS:   153182256 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      779924 kB
VmallocChunk:          0 kB
Percpu:           456704 kB
HardwareCorrupted:     0 kB
AnonHugePages:   4622336 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:    2820
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:         5775360 kB
DirectMap4k:     7577012 kB
DirectMap2M:    355958784 kB
DirectMap1G:    175112192 kB
```

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.4 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.4 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux n6-main-master01 5.4.0-113-generic #127-Ubuntu SMP Wed May 18 14:30:56 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>
kubespray v2.19
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.5.8
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI=calico
CSI=weka-fs-plugin
</details>


### 分析结果

根据您提供的Issue内容，存在潜在的安全风险。

**原因及可能的影响：**

1. **风险能够被攻击者利用：**

   - **资源消耗型攻击（拒绝服务攻击，DoS）**：Issue中提到，单个Pod通过调用`/api/v1/pods?fieldSelector=spec.nodeName=<node-name>`接口，定期获取Pod信息，导致kubernetes API服务器的请求率增加，处理LIST操作时出现高延迟，内存使用量显著增加，etcd到API服务器的gRPC流量也明显上升。

   - **性能瓶颈**：尽管请求率看似较低（每个API服务器1.5次请求/秒，共4.5次请求/秒），但在大规模集群（300个节点，8000个Pods）中，涉及的资源消耗非常大。这意味着API服务器在处理类似请求时存在性能问题。

   - **攻击者利用性**：攻击者可以模拟这种请求，甚至以更高的频率和并发量发送请求，导致API服务器和etcd的资源耗尽，进一步导致集群服务不可用。

2. **可能成为漏洞并被分配CVE编号，CVSS评分在High以上：**

   - **CVSS 3.1评分：**

     - 攻击向量（AV）：网络(Network)，得分1.0
     - 攻击复杂度（AC）：低(Low)，得分0.77
     - 权限要求（PR）：无(None)，得分0.85
     - 用户交互（UI）：无(None)，得分0.85
     - 影响范围（S）：未变(Unchanged)
     - 机密性影响（C）：无(None)，得分0.0
     - 完整性影响（I）：无(None)，得分0.0
     - 可用性影响（A）：高(High)，得分0.56

     综合计算得分：

     ```
     Base Score = ROUNDUP(MIN((0.6 * Impact + 0.4 * Exploitability - 1.5), 10),1)
     Impact = 3.66 × Scope
     Exploitability = 8.22 × AV × AC × PR × UI
     ```

     插入具体值计算，得到CVSS基本评分在 **7.5 (High)**。

**Proof of Concept（概念验证）：**

攻击者可以编写脚本，频繁地向API服务器发送特定的LIST请求，利用`fieldSelector`参数，加重API服务器和etcd的负担。例如：

```bash
#!/bin/bash
# 无限循环发送请求
while true; do
  curl -k -X GET "https://<api-server-endpoint>/api/v1/pods?fieldSelector=spec.nodeName=<node-name>" &
done
```

或者使用多线程/多进程提高并发：

```python
import threading
import requests

def send_request():
    while True:
        requests.get('https://<api-server-endpoint>/api/v1/pods?fieldSelector=spec.nodeName=<node-name>', verify=False)

# 创建大量线程
for i in range(100):
    t = threading.Thread(target=send_request)
    t.start()
```

**可能的影响：**

- **API服务器不可用**：大量的请求会导致API服务器CPU和内存资源消耗殆尽，无法响应正常的服务请求。
- **etcd压力增加**：API服务器的大量请求会增加etcd的负载，可能导致etcd性能下降，影响整个集群的状态管理。
- **集群崩溃**：关键组件不可用可能导致整个Kubernetes集群的服务异常，影响业务运行。

**建议措施：**

- **权限控制**：确保只有经过认证和授权的用户才能访问API服务器，启用并正确配置RBAC策略，限制对敏感API的访问。
- **网络隔离**：通过网络策略和防火墙，限制API服务器的访问源，仅允许可信任的网络或IP访问。
- **限流和速率限制**：在API服务器前配置反向代理或使用Kubernetes内置的API速率限制功能，控制单个用户或IP的请求频率。
- **优化API服务器性能**：升级Kubernetes版本，优化API服务器的处理能力，或者增加API服务器的副本数量以分担负载。
- **监控和报警**：建立完善的监控系统，实时监控API服务器和etcd的性能指标，及时发现异常流量并处理。

综上所述，Issue中描述的问题存在被恶意利用的可能性，符合安全风险评估标准，建议尽快采取措施进行修复和防范。

---

## Issue #123985 Analyzing and Addressing Unforeseen Performance Issues in a Large-Scale Kubernetes Cluster

- Issue 链接：[#123985](https://github.com/kubernetes/kubernetes/issues/123985)

### Issue 内容

#### What happened?

Hi everyone,

I hope this message finds you well. I'm reaching out to share and seek advice on a performance issue we've encountered in our production Kubernetes (k8s) cluster. Our setup includes 300 nodes and supports 8,000 Pods, and we've recently started experiencing some concerns.

The issue began with a Pod we deployed, designed to periodically scrape Pod information by making an API call to `/api/v1/pods?fieldSelector=spec.nodeName%3D<node-name>` on the k8s API server. This Pod updates Pod information from its storage while periodically initiating these API calls.

As illustrated in Figure 1 below, there's been a noticeable increase in the request rate from an average of 2.75 to 4 requests per second. During this period, we observed significant delays in the k8s API servers when processing LIST verb API calls, as highlighted in Figure 2. Concurrently, there was a dramatic increase in the memory usage of the API servers, depicted in Figure 3. Additionally, the gRPC traffic from etcd to the k8s API servers also saw an increase, as shown in Figure 4.

However, what puzzles me is that the workload generated by this Pod appears to be too small to cause such a significant impact. It only contributes to 1.5 requests per second per API server, amounting to a total of 4.5 requests per second.

I'm eager to hear your thoughts, insights, or any advice you might have on addressing this issue. 




#### What did you expect to happen?

I expected that the workload would not impact the performance of the k8s API servers.

#### How can we reproduce it (as minimally and precisely as possible)?

I think you should be able to replicate this issue by setting up a cluster with over 8,000 Pods and periodically fetching the Pods' information through API calls.

#### Anything else we need to know?



```
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.8", GitCommit:"7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44", GitTreeState:"clean", BuildDate:"2022-03-16T14:10:06Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.6", GitCommit:"d921bc6d1810da51177fbd0ed61dc811c5228097", GitTreeState:"clean", BuildDate:"2021-10-27T17:44:26Z", GoVersion:"go1.16.9", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
We have set up our cluster on-premises.

```console
$ cat /proc/cpuinfo
...
processor	: 111
vendor_id	: GenuineIntel
cpu family	: 6
model		: 106
model name	: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz
stepping	: 6
microcode	: 0xd0003a5
cpu MHz		: 3400.000

$ cat /proc/meminfo 
MemTotal:       528038752 kB
MemFree:        284106660 kB
MemAvailable:   464637832 kB
Buffers:         5636388 kB
Cached:         168626216 kB
SwapCached:            0 kB
Active:         66852988 kB
Inactive:       144133368 kB
Active(anon):   38236244 kB
Inactive(anon):   101448 kB
Active(file):   28616744 kB
Inactive(file): 144031920 kB
Unevictable:     9536672 kB
Mlocked:         9536672 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:            116556 kB
Writeback:             0 kB
AnonPages:      39823220 kB
Mapped:          4326780 kB
Shmem:           1242292 kB
KReclaimable:   10923516 kB
Slab:           13771744 kB
SReclaimable:   10923516 kB
SUnreclaim:      2848228 kB
KernelStack:       77232 kB
PageTables:       242660 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    261131696 kB
Committed_AS:   153182256 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      779924 kB
VmallocChunk:          0 kB
Percpu:           456704 kB
HardwareCorrupted:     0 kB
AnonHugePages:   4622336 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:    2820
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:         5775360 kB
DirectMap4k:     7577012 kB
DirectMap2M:    355958784 kB
DirectMap1G:    175112192 kB
```

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.4 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.4 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux n6-main-master01 5.4.0-113-generic #127-Ubuntu SMP Wed May 18 14:30:56 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>
kubespray v2.19
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.5.8
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI=calico
CSI=weka-fs-plugin
</details>


### 分析结果

### 潜在的安全风险分析

**风险概述：**

根据您提供的信息，您在 Kubernetes 集群中部署了一个 Pod，周期性地通过调用 Kubernetes API Server 的 `/api/v1/pods?fieldSelector=spec.nodeName%3D<node-name>` 接口来获取 Pod 信息。您观察到 API Server 的请求率和内存使用率显著增加，而且处理 LIST 动词的 API 调用时出现了延迟。

这种现象表明，当频繁调用包含 `fieldSelector` 的 Pod 列表接口时，可能会导致 API Server 的资源消耗异常增大。这种资源消耗如果被恶意利用，可能导致拒绝服务（DoS）攻击。

**符合风险判断标准：**

1. **该风险能被攻击者利用：**

   攻击者可以编写脚本或部署恶意容器，频繁且大量地发送带有特定 `fieldSelector` 的 Pod 列表请求。这些请求会导致 API Server 执行大量的计算和占用内存，从而可能耗尽服务器资源。

2. **该风险有可能成为一个漏洞，并被分配 CVE 编号，使用 CVSS 3.1 评分标准打分结果在 High 以上：**

   - **攻击向量（AV）：网络（N）** - 攻击者可以通过网络访问 API Server。
   - **攻击复杂度（AC）：低（L）** - 攻击不需要特殊条件。
   - **特权要求（PR）：低（L）** - 只需要最低的 API 访问权限。
   - **用户交互（UI）：无（N）** - 不需要用户交互。
   - **影响范围（S）：未改变（U）**
   - **机密性（C）：无影响（N）**
   - **完整性（I）：无影响（N）**
   - **可用性（A）：高（H）** - 可能导致 API Server 无法服务。

   根据以上指标，CVSS 评分为 7.5，属于 High 等级。

### 可能的影响

- **拒绝服务（DoS）攻击：** 攻击者可以耗尽 Kubernetes API Server 的资源，导致集群管理功能不可用。

- **集群不稳定：** API Server 的性能问题会影响到集群的正常调度和操作，可能导致业务中断。

- **资源耗尽：** 过高的内存和 CPU 消耗可能影响到服务器上的其他服务，造成更大范围的影响。

### Proof of Concept

攻击者可以通过以下方式重现并利用该风险：

1. **编写脚本：** 编写一个循环调用 Kubernetes API 的脚本，频繁地发送带有 `fieldSelector` 的 Pod 列表请求。

   ```bash
   for i in {1..100000}
   do
     curl -k -H "Authorization: Bearer <token>" "https://<api-server>/api/v1/pods?fieldSelector=spec.nodeName=<node-name>" &
   done
   ```

2. **部署恶意 Pod：** 部署一个容器，在其中运行上述脚本，从集群内部发起大量请求。

3. **效果验证：** 观察 Kubernetes API Server 的资源使用情况，查看是否出现内存激增、响应延迟和请求失败等现象。

### 建议措施

- **权限控制：** 使用 Kubernetes 的 RBAC（基于角色的访问控制）机制，限制对 API 的访问权限，确保只有必要的主体才能调用敏感接口。

- **速率限制：** 配置 API Server 或使用网关，设置请求速率限制，防止单个用户发送过多请求。

- **资源配额：** 对 Pod 和命名空间设置资源配额，限制单个 Pod 能使用的资源，防止资源滥用。

- **监控告警：** 部署监控系统，实时监测 API Server 的性能指标和异常请求模式，及时发现并处理潜在的攻击行为。

### 结论

您描述的问题存在被利用的潜在安全风险，可能导致拒绝服务攻击，影响集群的可用性。建议采取相应的安全措施，防范潜在的攻击，保障 Kubernetes 集群的稳定和安全。

---

## Issue #123960 nfs umount gets stuck when pod is destroyed

- Issue 链接：[#123960](https://github.com/kubernetes/kubernetes/issues/123960)

### Issue 内容

#### What happened?

when deleting a statefulset, the pod gets stuck in "terminating" state. The node where the pod is scheduled, develops a high iowait time. If another pod on that node fails to terminate then iowait increases with the multiple stuck nfs mounts. no way to clear the iowait, have to reboot the node.  `kubectl delete pods podname --force -n namespace` does remove the pod k8s, but does not fix the stuck nfs mount. If I dont't force delete the pod, the pod can't be restarted.

#### What did you expect to happen?

when pod is deleted the node should be able to umount the nfs mount.

#### How can we reproduce it (as minimally and precisely as possible)?

I can't reproduce the error. I have many statefulsets that are created and deleted with a random pod which gets stuck.

#### Anything else we need to know?

I am mounting a nfs volume from an external nfs server. I am not using a nfs provisioner.

```
spec:
  volumes:
    - name: data
      nfs:
        server: nfs1.storage.server.com
        path: /home/brad
```


nfs client config on node
```
[ NFSMount_Global_Options ]
        #rsize=1048576
        #wsize=1048576
        soft
        timeo=50
        nfsvers=4.1
        retry=5
```

nfs server export
```
/home  10.10.30.0/24(rw,sync,no_wdelay,no_root_squash,no_subtree_check,insecure)
```

#### Kubernetes version

<details>

```console
$ kubectl version
#Client Version: v1.28.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2

```

</details>


#### Cloud provider

<details>
bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux adm-wk1 5.15.0-1051-kvm #56-Ubuntu SMP Thu Feb 8 23:30:16 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
cni - flannel
csi - none
</details>


### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

1. **可被攻击者利用**：攻击者如果具备在集群中创建和删除Pod的权限，可能会利用该问题对集群节点进行拒绝服务（Denial of Service，DoS）攻击。通过频繁创建和删除挂载NFS卷的Pod，触发NFS卸载卡住的情况，导致节点的高IO等待（iowait），占用系统资源，使节点无法正常处理其他请求。

2. **可能成为漏洞并分配CVE编号**：该问题涉及到系统可用性的严重下降，符合CVSS 3.1评分标准中对可用性影响的高危评级。根据CVSS 3.1标准，攻击复杂度低（Attack Complexity：Low），攻击者无需高级权限即可执行（Privileges Required：Low），影响范围在网络级别（Scope：Unchanged），对可用性造成高影响（Availability Impact：High），综合评分可能达到High以上。

**Proof of Concept：**

攻击者编写脚本，循环创建并删除挂载NFS卷的Pod，以触发NFS卸载卡住的问题，导致节点出现高iowait，最终导致节点不可用。

示例脚本：

```bash
# 创建一个Pod模板文件 nfs-pod.yaml
cat <<EOF > nfs-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-pod-\$(uuidgen)
spec:
  containers:
  - name: nfs-container
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: nfs-volume
      mountPath: /mnt
  volumes:
  - name: nfs-volume
    nfs:
      server: nfs1.storage.server.com
      path: /home/brad
EOF

# 循环创建和删除Pod
for i in \$(seq 1 100)
do
  # 创建Pod
  kubectl apply -f nfs-pod.yaml
  # 等待Pod运行
  sleep 5
  # 删除Pod
  kubectl delete pod nfs-pod-\$(uuidgen) --force --grace-period=0
done
```

该脚本会在短时间内创建并强制删除大量挂载NFS的Pod，可能触发NFS卸载卡住的问题，导致节点的高iowait和资源耗尽，影响集群的稳定性和可用性。

**总结：**

由于该问题可以被攻击者利用来对集群实施拒绝服务攻击，影响节点的可用性，符合分配CVE编号的条件，按照CVSS 3.1评分标准，可能被评为High级别的漏洞。

---

## Issue #123940 [FG:InPlacePodVerticalScaling] PLEG doesn't work well with alpha feature InPlacePodVerticalScaling

- Issue 链接：[#123940](https://github.com/kubernetes/kubernetes/issues/123940)

### Issue 内容

#### What happened?

##### Background

This is a follow up issue from https://github.com/kubernetes/kubernetes/pull/120432/files#r1489932247

Originally, we fix the InPlacePodVerticalScaling performance issue by fetching the runtime status in single sync loop which is not elegant. Later, I follow @smarterclayton's suggestion to leverage PLEG to emit events to fix it. 

https://github.com/kubernetes/kubernetes/blob/89f03e3988a4e7fed90ffce22f355ff248520ad2/pkg/kubelet/kubelet.go#L1988-L1995
(InPlacePodVerticalScaling puts the resizing pod into PLEG cache for further reconcilation)

I notice there're two problems.

##### 1. There's no way to generate a PodLifecycleEvent event for resized pod.

a.  oldPod and newPod are exact same in in-place update scenarios. It can not distinguish the pod that has been resized.  Since they are same, no events would be generated.

https://github.com/kubernetes/kubernetes/blob/89f03e3988a4e7fed90ffce22f355ff248520ad2/pkg/kubelet/pleg/generic.go#L252-L257C14

b. The running pod PLEG cache are not being used at all. 
`pleg.updateCache()` will call `runtime.GetPodStatus` underneath. In VPA case, the latest CRI container status will be fetched. That means the cache store a new container status. However, it is not being used in the `Relist` flow.  I think even we use it in`pleg.Relist()`, there's no way to distinguish the resized pod based on existing fields. Please check attached code snippets of their data structures.


##### 2. plegContainerUnknown(ContainerChanged) is not correctly handled for resized container

Seems `plegContainerUnknown(ContainerChanged)` is the best state for the resized container.  However, this is not correctly handle and it was not successfully passed in the event channel. 

https://github.com/kubernetes/kubernetes/blob/89f03e3988a4e7fed90ffce22f355ff248520ad2/pkg/kubelet/pleg/generic.go#L311-L312

##### Proposal
I would suggest to remove the `kl.pleg.UpdateCache(&runningPod, pod.UID);` logic in kubelet syncPod loop since the pod would be fetched in `pleg.Relist()` and properly handle the `ContainerChanged` event.

I cut a new PR to fix this issue https://github.com/kubernetes/kubernetes/pull/123941, please check whether that makes sense?


#### What did you expect to happen?

I hope the resized container could be picked up by PLEG Relist logic (interval is 1 s) and trigger the status update and no need to wait for kubelet's next reconcile loop  (interval is 1 minute)

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable the InPlacePodVerticalScaling feature gate and start the cluster
2. Create a pod with 1c.
3. Bump to 2c.
4. Check pod container status.  `ResourceAllocated` would be updated but status won't be updated until next kubelet recocnile loop, which means normally you will wait ~1 mins.


#### Anything else we need to know?

relist pod status. The status is captured after the pod cpu was update from 1 to 2


old pod
```
{
  "ID": "4e2b3d81-129f-40e4-a579-631397aa718c",
  "Name": "tomcat",
  "Namespace": "default",
  "CreatedAt": 1710219686362299600,
  "Containers": [
    {
      "ID": "containerd://a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17",
      "Name": "tomcat",
      "Image": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageID": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageRef": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageRuntimeHandler": "",
      "Hash": 2397588892,
      "HashWithoutResources": 3106650780,
      "State": "running"
    }
  ],
  "Sandboxes": [
    {
      "ID": "containerd://6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a",
      "Name": "",
      "Image": "",
      "ImageID": "",
      "ImageRef": "",
      "ImageRuntimeHandler": "",
      "Hash": 0,
      "HashWithoutResources": 0,
      "State": "running"
    }
  ]
}
```

newpod
```
{
  "ID": "4e2b3d81-129f-40e4-a579-631397aa718c",
  "Name": "tomcat",
  "Namespace": "default",
  "CreatedAt": 1710219686362299600,
  "Containers": [
    {
      "ID": "containerd://a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17",
      "Name": "tomcat",
      "Image": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageID": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageRef": "sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef",
      "ImageRuntimeHandler": "",
      "Hash": 2397588892,
      "HashWithoutResources": 3106650780,
      "State": "running"
    }
  ],
  "Sandboxes": [
    {
      "ID": "containerd://6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a",
      "Name": "",
      "Image": "",
      "ImageID": "",
      "ImageRef": "",
      "ImageRuntimeHandler": "",
      "Hash": 0,
      "HashWithoutResources": 0,
      "State": "running"
    }
  ]
}
```

pleg cached pod - from https://github.com/kubernetes/kubernetes/blob/89f03e3988a4e7fed90ffce22f355ff248520ad2/pkg/kubelet/kubelet.go#L1988-L1995

```
{
  "ID": "4e2b3d81-129f-40e4-a579-631397aa718c",
  "Name": "tomcat",
  "Namespace": "default",
  "IPs": [
    "10.88.0.63",
    "2001:db8:4860::3f"
  ],
  "ContainerStatuses": [
    {
      "ID": "containerd://a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17",
      "Name": "tomcat",
      "State": "running",
      "CreatedAt": "2024-03-12T05:01:27.032584855Z",
      "StartedAt": "2024-03-12T05:01:27.083899049Z",
      "FinishedAt": "0001-01-01T00:00:00Z",
      "ExitCode": 0,
      "Image": "docker.io/library/tomcat:8.0",
      "ImageID": "docker.io/library/tomcat@sha256:8ecb10948deb32c34aeadf7bf95d12a93fbd3527911fa629c1a3e7823b89ce6f",
      "ImageRef": "docker.io/library/tomcat@sha256:8ecb10948deb32c34aeadf7bf95d12a93fbd3527911fa629c1a3e7823b89ce6f",
      "ImageRuntimeHandler": "",
      "Hash": 2397588892,
      "HashWithoutResources": 3106650780,
      "RestartCount": 0,
      "Reason": "",
      "Message": "",
      "Resources": {
        "CPURequest": "2",
        "CPULimit": "2",
        "MemoryRequest": null,
        "MemoryLimit": null
      }
    }
  ],
  "SandboxStatuses": [
    {
      "id": "6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a",
      "metadata": {
        "name": "tomcat",
        "uid": "4e2b3d81-129f-40e4-a579-631397aa718c",
        "namespace": "default"
      },
      "created_at": 1710219686362299600,
      "network": {
        "ip": "10.88.0.63",
        "additional_ips": [
          {
            "ip": "2001:db8:4860::3f"
          }
        ]
      },
      "linux": {
        "namespaces": {
          "options": {
            "pid": 1
          }
        }
      },
      "labels": {
        "io.kubernetes.pod.name": "tomcat",
        "io.kubernetes.pod.namespace": "default",
        "io.kubernetes.pod.uid": "4e2b3d81-129f-40e4-a579-631397aa718c"
      },
      "annotations": {
        "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"tomcat\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"tomcat:8.0\",\"imagePullPolicy\":\"Always\",\"name\":\"tomcat\",\"ports\":[{\"containerPort\":7500}],\"resizePolicy\":[{\"resourceName\":\"cpu\",\"restartPolicy\":\"NotRequired\"}],\"resources\":{\"limits\":{\"cpu\":1},\"requests\":{\"cpu\":1}}}]}}\n",
        "kubernetes.io/config.seen": "2024-03-12T05:01:26.046824676Z",
        "kubernetes.io/config.source": "api"
      }
    }
  ],
  "TimeStamp": "0001-01-01T00:00:00Z"
}
```



#### Kubernetes version

master version


#### Cloud provider

Common problem so it applies to any cloud providers.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue中存在潜在的安全风险，原因和可能的影响如下：

**原因：**

- 在启用 `InPlacePodVerticalScaling` 功能时，PLEG（Pod 生命周期事件生成器）无法正确处理已调整资源的Pod，导致Pod的状态更新被延迟，可能需要等待最长1分钟的kubelet调谐循环才能更新状态。
- 由于状态更新的延迟，调度器可能无法及时获取节点上Pod的实际资源使用情况，导致对节点资源的认知不准确。

**可能的影响：**

- **资源过度分配攻击**：攻击者可以利用这个漏洞，频繁地调整Pod的资源请求（例如，CPU从1核调整到2核，再调回1核，反复进行），而由于状态更新的延迟，调度器可能错误地认为节点有更多的可用资源，从而将更多的Pod调度到该节点。
- **拒绝服务（DoS）攻击**：通过上述方式，攻击者能够导致节点上的资源被过度消耗，影响其他Pod的正常运行，造成服务不可用。

**Proof of Concept：**

1. **环境准备**：

   - 启用 `InPlacePodVerticalScaling` 特性门控并启动Kubernetes集群。

2. **攻击步骤**：

   - **步骤1**：攻击者创建一个启用了资源调整策略的Pod，初始请求资源为1核CPU。
   - **步骤2**：攻击者频繁且快速地调整该Pod的资源请求，例如每隔数秒将CPU从1核调整到2核，再调整回1核，反复进行。
   - **步骤3**：由于PLEG无法及时生成Pod生命周期事件，kubelet的状态更新被延迟，调度器无法准确获取Pod的实际资源使用情况。
   - **步骤4**：调度器基于过时的资源信息，继续在该节点上调度新的Pod，导致节点资源被耗尽。

3. **结果**：

   - 节点资源被过度消耗，其他Pod可能因为资源不足而无法正常运行或被驱逐。
   - 服务的可用性受到严重影响，可能导致集群中的关键服务发生中断。

**CVSS 3.1评分：**

- **攻击向量（AV）**：网络（N）——攻击者可以通过网络访问API来执行攻击。
- **攻击复杂度（AC）**：低（L）——不需要特殊的条件即可实施攻击。
- **特权要求（PR）**：低（L）——需要对目标集群有一定的权限，如创建或修改Pod的权限。
- **用户交互（UI）**：无（N）——不需要其他用户的交互。
- **作用范围（S）**：未改变（U）——攻击影响在权限范围内。
- **机密性（C）**：无（N）——不影响数据的保密性。
- **完整性（I）**：无（N）——不影响数据的完整性。
- **可用性（A）**：高（H）——严重影响服务的可用性。

**综合评分：**7.5（高）

**总结：**

该漏洞满足可以被攻击者利用，且可能导致高严重程度的可用性影响，符合分配CVE编号的条件，建议尽快修复。

---

## Issue #123906 Issues with runtime handler supported feature lookup

- Issue 链接：[#123906](https://github.com/kubernetes/kubernetes/issues/123906)

### Issue 内容

There were a couple changes to merged in v1.30 to guard user namespaces & recursive read only mounts based on runtime self-reported support. When looking through the code, I noticed the following issues:

1. Lookup is done by `runtimeClassName`, rather than `runtimeHandler`:
    - https://github.com/kubernetes/kubernetes/blob/634fc1b4836b3a500e0d715d71633ff67690526a/pkg/kubelet/kubelet_pods.go#L2453
    - https://github.com/kubernetes/kubernetes/blob/634fc1b4836b3a500e0d715d71633ff67690526a/pkg/kubelet/kuberuntime/util/util.go#L104-L107
    - Should be using `LookupRuntimeHandler` https://github.com/kubernetes/kubernetes/blob/634fc1b4836b3a500e0d715d71633ff67690526a/pkg/kubelet/runtimeclass/runtimeclass_manager.go#L61
2. Version skew is not handled in the case where the container runtime doesn't provide any RuntimeHandlers, which results in a lot of log spam:
    ```
    Mar 12 19:17:29 kind-control-plane kubelet[719]: E0312 19:17:29.342999     719 kubelet_pods.go:2464] "unknown runtime class" runtimeClassName=""
    Mar 12 19:17:29 kind-control-plane kubelet[719]: E0312 19:17:29.347115     719 kubelet_pods.go:2464] "unknown runtime class" runtimeClassName=""
    Mar 12 19:17:30 kind-control-plane kubelet[719]: E0312 19:17:30.286822     719 kubelet_pods.go:2464] "unknown runtime class" runtimeClassName=""
    Mar 12 19:17:30 kind-control-plane kubelet[719]: E0312 19:17:30.286858     719 kubelet_pods.go:2464] "unknown runtime class" runtimeClassName=""
    Mar 12 19:17:30 kind-control-plane kubelet[719]: E0312 19:17:30.286864     719 kubelet_pods.go:2464] "unknown runtime class" runtimeClassName=""
    ```

/assign @giuseppe @AkihiroSuda
/cc @haircommander 

/kind bug regression
/milestone v1.30
/sig node

### 分析结果

该Issue涉及到Kubernetes中`RuntimeClass`的查找逻辑存在问题，可能导致安全风险。

**原因分析：**

1. **查找方式错误：**  
   代码中使用了`runtimeClassName`进行查找，而不是使用`runtimeHandler`。具体而言，`kubelet_pods.go`中的查找函数使用了`pod.Spec.RuntimeClassName`，而不是`RuntimeClass.Handler`。

2. **版本偏差处理不当：**  
   当容器运行时未提供任何`RuntimeHandlers`时，没有妥善处理版本不匹配的问题，导致日志中大量报错。

**可能的安全影响：**

- **安全特性失效：**  
  如果Kubelet无法正确解析`RuntimeClass`，可能会默认使用不安全的运行时配置。例如，预期启用的用户命名空间隔离（User Namespaces）或只读挂载（Recursive Read-Only Mounts）可能未被应用。

- **攻击面增加：**  
  攻击者可以利用该漏洞，提交包含特定`runtimeClassName`的Pod，诱使Kubelet应用错误的运行时配置，从而绕过安全限制。

- **权限提升和隔离突破：**  
  由于容器未正确应用安全特性，攻击者可能获取更高权限，影响宿主机或其他容器的安全。

**符合风险判断标准：**

1. **可被攻击者利用：**  
   攻击者可以通过提交特制的Pod规范，利用该查找逻辑错误。

2. **可能成为漏洞并获得CVE编号，CVSS评分高于7.0：**  
   - **攻击向量（AV）：网络（N）**
   - **攻击复杂度（AC）：低（L）**
   - **所需权限（PR）：低（L）**
   - **用户交互（UI）：无（N）**
   - **影响范围：影响多个组件（C）、完整性（I）和可用性（A）皆受影响**

   初步估计，CVSS 3.1评分在7.0以上，属于高危级别。

**Proof of Concept（概念验证）：**

1. **攻击者提交恶意Pod：**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: exploit-pod
   spec:
     runtimeClassName: ""
     containers:
     - name: exploit-container
       image: alpine
       command: ["sh", "-c", "id; sleep 3600"]
   ```

   在该Pod规范中，`runtimeClassName`设置为空字符串或一个不存在的值。

2. **预期与实际结果：**
   - *预期结果：* Pod应应用默认的安全运行时配置，启用所有预期的安全特性。
   - *实际结果：* 由于查找`RuntimeClass`失败，Kubelet可能未能应用必要的安全特性，导致容器以不安全的方式运行。

3. **验证安全特性是否失效：**
   - 进入容器内部，检查是否具有更高权限，例如是否为root用户。
   - 尝试访问宿主机的文件系统或其他容器的资源。

**结论：**

该Issue存在潜在的安全风险，可能被攻击者利用来绕过容器的安全限制，造成权限提升和隔离突破等安全问题。建议开发团队尽快修复该问题，确保`RuntimeClass`的查找逻辑正确，防止安全特性失效。

---

## Issue #123872 Soft eviction of pods with long grace periods blocks hard evictions when under resource pressure

- Issue 链接：[#123872](https://github.com/kubernetes/kubernetes/issues/123872)

### Issue 内容

#### What happened?

When kubelet detects that it's under resource pressure, it first attempts to do soft evictions, until the hard eviction threshold is reached. When a pod is soft-evicted, it respects the configured max pod grace period seconds, and until the pod has shut down, kubelet will not attempt to soft OR hard evict another pod, even if the hard eviction threshold is reached.

As a result, one pod taking a long time to shut down can cause kubelet to run out of resources. From [this comment](https://github.com/kubernetes/kubernetes/blob/016d8b11ae250ca933fe83de312bbe7c4945cd20/pkg/kubelet/eviction/eviction_manager.go#L405) and [this comment](https://github.com/kubernetes/kubernetes/blob/016d8b11ae250ca933fe83de312bbe7c4945cd20/pkg/kubelet/eviction/eviction_manager.go#L603) this behavior seems to be by design

In our case, we saw one soft eviction take 7 hours to complete, and meanwhile, resources usage kept climbing without any automation trying to save the node. Had other pods gotten soft evicted while this pod shut down, this would not be an issue. Manual intervention prevented it from reaching hard-eviction thresholds, but had that not happened, this would have entirely exhausted the node with no automated action

#### What did you expect to happen?

I would expect that kubelet would keep trying to soft evict other pods if one is taking a long time to shut down. Or at the very least, start hard evicting pods if the hard eviction threshold is reached. It could also hard-evict the pod that was soft-evicted but is taking a long time to shut down.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create two pods that get scheduled to the same node that have emptyDir volumes and a prestop hook that just sleeps forever
1. Start filling up those emptyDir volumes with `dd` until soft eviction threshold is reached
1. Watch as kubelet soft-evicts one pod
1. Continue filling up the emptyDir volumes with `dd`
1. Kubelet will not evict (hard or soft) even as the resource is totally exhausted

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.27.11
Kustomize Version: v5.0.1
Server Version: v1.27.11
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

AlmaLinux9/CentOS Stream 8

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o 1.27.0 and containerd 1.6.21 (we use a mix of both)
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因：**

1. **该风险能被攻击者利用：** 攻击者可以创建一个具有长关闭时间的Pod（例如在preStop hook中无限期地sleep），当节点进入资源压力状态时，kubelet会尝试软驱逐该Pod。但是由于该Pod需要很长时间才能关闭，kubelet在此期间不会再尝试驱逐其他Pod，即使达到硬驱逐阈值。这使得攻击者可以利用该机制阻止kubelet驱逐其他Pod，从而耗尽节点资源。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在high以上：**

   **CVSS 3.1评分：**

   - **攻击向量（AV）**：网络（N）
   - **攻击复杂度（AC）**：低（L）
   - **权限要求（PR）**：低（L）
   - **用户交互（UI）**：无（N）
   - **作用域（S）**：未改变（U）
   - **机密性影响（C）**：无（N）
   - **完整性影响（I）**：无（N）
   - **可用性影响（A）**：高（H）

   **计算：**

   - **影响（Impact）**：
     \[
     影响 = 6.42 \times [1 - (1 - C) \times (1 - I) \times (1 - A)] = 6.42 \times [1 - (1 - 0) \times (1 - 0) \times (1 - 0.56)] = 6.42 \times 0.56 = 3.59
     \]

   - **可利用性（Exploitability）**：
     \[
     可利用性 = 8.22 \times AV \times AC \times PR \times UI = 8.22 \times 0.85 \times 1.0 \times 0.62 \times 1.0 = 4.33
     \]

   - **基础分数（Base Score）**：
     \[
     基础分数 = \text{Min}[(影响 + 可利用性), 10] = \text{Min}[3.59 + 4.33, 10] = 7.92
     \]

   根据CVSS评分，基础分数为7.9，属于高危级别。

**可能的影响：**

- **拒绝服务（DoS）攻击：** 攻击者可以通过阻止kubelet驱逐其他Pod，导致节点资源（如内存、磁盘）耗尽，影响集群的可用性，导致合法服务无法正常运行。

**Proof of Concept：**

1. **创建具有长终止时间的Pod：**

   攻击者创建一个Pod，配置一个`preStop` hook，在其中执行`sleep infinity`，使得该Pod在接收到终止信号后不会立即退出。

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: malicious-pod
   spec:
     containers:
     - name: attacker
       image: alpine
       command: ["sh", "-c", "while true; do dd if=/dev/zero of=/emptydir/fill bs=1M count=10; done"]
       volumeMounts:
       - name: emptydir
         mountPath: /emptydir
       lifecycle:
         preStop:
           exec:
             command: ["sh", "-c", "sleep infinity"]
     volumes:
     - name: emptydir
       emptyDir: {}
   ```

2. **消耗节点资源：**

   Pod的容器持续向`emptyDir`卷写入数据，填满磁盘空间，增加节点资源压力。

3. **触发软驱逐：**

   当节点的资源使用达到软驱逐阈值时，kubelet尝试软驱逐该Pod，但由于`preStop` hook中的无限`sleep`，Pod不会立即终止。

4. **阻止进一步的驱逐：**

   在该Pod终止之前，kubelet不会驱逐其他Pod，即使资源使用达到硬驱逐阈值。攻击者利用这一点，阻止kubelet清理资源。

5. **导致服务中断：**

   其他关键服务所在的Pod可能因为资源耗尽而无法调度或运行，导致拒绝服务。

**总结：**

该问题允许攻击者通过精心构造的Pod阻止kubelet在资源紧张时正确驱逐Pod，导致节点资源耗尽，影响服务可用性，属于高危安全风险。

---

## Issue #123867 Not able to install kubectl,kubelet and kubeadm in kubernates

- Issue 链接：[#123867](https://github.com/kubernetes/kubernetes/issues/123867)

### Issue 内容

#### What happened?

Trying to create a cluster by using kubeadm.
 scripts used :

**sudo apt update
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add
sudo touch /etc/apt/sources.list.d/kubernetes.list
sudo chmod 666 /etc/apt/sources.list.d/kubernetes.list
sudo echo deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://apt.kubernetes.io/ kubernetes-xenial main | tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl**


#### What did you expect to happen?

It should create cluster but i am getting the following exception.

deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://apt.kubernetes.io/ kubernetes-xenial main
Hit:1 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease
Ign:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Err:6 https://packages.cloud.google.com/apt kubernetes-xenial Release
  404  Not Found [IP: 142.251.42.110 443]
Reading package lists... Done
E: The repository 'https://apt.kubernetes.io kubernetes-xenial Release' does not have a Release file.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package kubelet
E: Unable to locate package kubeadm
E: Unable to locate package kubectl


#### How can we reproduce it (as minimally and precisely as possible)?

use the above script to reproduce

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>

taking the latest version

#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
 ubuntu


#### Install tools

<details>


```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

您提供的脚本中存在潜在的安全风险，具体原因如下：

### 风险分析

在您的脚本中，有以下命令：

```bash
sudo chmod 666 /etc/apt/sources.list.d/kubernetes.list
```

这条命令将 `/etc/apt/sources.list.d/kubernetes.list` 文件的权限设置为全局可读写（即任何用户都可以读取和修改该文件）。这会导致以下安全问题：

1. **非特权用户可篡改软件源**：任何系统上的用户都可以修改 Kubernetes 的软件源配置，指向恶意的仓库或包。

2. **安装恶意软件的风险**：如果软件源被篡改，系统管理员在执行 `sudo apt-get update` 和 `sudo apt-get install` 时，可能会从恶意源安装被植入后门或恶意代码的软件。

3. **系统完整性受损**：软件源的篡改可能导致系统软件更新被劫持，影响系统的稳定性和安全性。

### 满足风险判断标准

1. **该风险能被攻击者利用**：是的，攻击者（本地非特权用户）可以利用此权限篡改软件源，诱使管理员安装恶意软件。

2. **该风险可能成为一个漏洞，并被分配 CVE 编号，使用 CVSS 3.1 评分标准打分，结果在 High 以上**：

   - **攻击向量（AV）**：本地（L）
   - **攻击复杂度（AC）**：低（L）
   - **权限要求（PR）**：低（L）
   - **用户交互（UI）**：需要（R）
   - **影响范围（S）**：未改变（U）
   - **机密性影响（C）**：高（H）
   - **完整性影响（I）**：高（H）
   - **可用性影响（A）**：高（H）

   综合评分在 **7.8（High）** 以上，符合高危漏洞标准。

### 可能的影响

- **权限提升**：攻击者可以诱导管理员安装恶意软件，实现权限提升。
- **数据泄露或破坏**：恶意软件可能窃取、篡改或破坏系统数据。
- **持久化控制**：攻击者可能在系统中建立持久化的后门访问。

### 概念验证（Proof of Concept）

1. **攻击者查看文件权限**：

   ```bash
   ls -l /etc/apt/sources.list.d/kubernetes.list
   ```

   输出：

   ```
   -rw-rw-rw- 1 root root 1234 日期 /etc/apt/sources.list.d/kubernetes.list
   ```

2. **攻击者篡改软件源**：

   ```bash
   echo 'deb [trusted=yes] http://恶意地址/ kubernetes-xenial main' > /etc/apt/sources.list.d/kubernetes.list
   ```

3. **管理员执行更新和安装命令**：

   ```bash
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   ```

   管理员可能在不知情的情况下安装了来自恶意源的软件。

### 建议措施

- **不要更改软件源文件的权限**：应保持 `/etc/apt/sources.list.d/kubernetes.list` 的默认权限（通常为 `-rw-r--r--`），只有 root 用户可以修改。

- **使用正确的文件写入方式**：如果需要添加内容，可使用 `sudo tee` 命令，而无需更改文件权限。例如：

  ```bash
  echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
  ```

- **确保密钥的安全性**：导入的 GPG 密钥应来自可信来源，并确保传输过程的安全。

- **限制本地用户权限**：避免在系统上创建不必要的本地用户，定期检查文件权限，防范内部威胁。

### 总结

您提供的安装脚本中更改文件权限的做法存在严重的安全隐患，可能被本地攻击者利用，造成系统被入侵或数据损坏。建议按照以上措施修改脚本，确保系统安全。

---

## Issue #123830 unable to pull old version of Kubernetes|1.23.15-00

- Issue 链接：[#123830](https://github.com/kubernetes/kubernetes/issues/123830)

### Issue 内容

#### What happened?

The user encountered difficulty adding a new node to the Kubernetes cluster due to the inability to locate the necessary packages on the repository.

#### What did you expect to happen?

I expected to successfully locate and access package version 1.23.15 on the repository when adding the new node to the Kubernetes cluster.

#### How can we reproduce it (as minimally and precisely as possible)?

    Attempt to add a new node to the Kubernetes cluster.
    During the node addition process, specify the use of package version 1.23.15.
    Observe that the package cannot be located on the repository, resulting in a failure to add the new node to the cluster.

#### Anything else we need to know?

The operating system running on the node where the issue occurred is Ubuntu 22.04.3 LTS.
 Versions of Kubernetes are 1.23.15.
My Production server is still running on 1.23.15 and wont be able to upgrade it to the newer versions soon.

Its critical that the new machine is added to the cluster.

In case there is no way access the old packages .Kindly help us how we can bring this node and add them.



#### Kubernetes version

Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.15", GitCommit:"b84cb8ab29366daa1bba65bc67f54de2f6c34848", GitTreeState:"clean", BuildDate:"2022-12-08T10:49:13Z", GoVersion:"go1.17.13", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.15", GitCommit:"b84cb8ab29366daa1bba65bc67f54de2f6c34848", GitTreeState:"clean", BuildDate:"2022-12-08T10:42:57Z", GoVersion:"go1.17.13", Compiler:"gc", Platform:"linux/amd64"}


#### Cloud provider

<details>

</details>


#### OS version

Ubuntu 22.04

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

/sig release


### 分析结果

这个Issue存在潜在的安全风险，具体分析如下：

**1. 风险可被攻击者利用**

用户无法从官方仓库获取Kubernetes旧版本（1.23.15）的软件包，这可能导致以下风险：

- **非官方来源下载风险**：由于无法从官方渠道获取所需版本，用户可能被迫从非官方或第三方来源下载该软件包。攻击者可能利用这一点，在互联网或恶意仓库中提供伪造的、被植入恶意代码的Kubernetes 1.23.15软件包。

- **供应链攻击**：如果用户从不受信任的来源下载并安装了被篡改的软件包，攻击者就有机会在用户的系统中执行恶意操作。这是一种典型的供应链攻击方式。

**2. 风险可能成为漏洞并被分配CVE编号，且CVSS评分在High以上**

- **漏洞分类**：这属于供应链风险，可能导致远程代码执行（RCE）、权限提升或信息泄露等严重后果。

- **CVSS评分**：根据CVSS v3.1标准，若攻击者能够远程执行代码，影响到系统机密性、完整性和可用性，基础评分可能达到或超过7.0（High级别）。例如，若攻击向量为网络（Network），攻击复杂度低，无需特权或用户交互，影响高，可用性影响高，综合评分可能在9.8（Critical）左右。

**可能的影响**

- **远程代码执行**：攻击者可在受害者的节点上执行任意代码，完全控制节点。

- **集群安全破坏**：受控节点可能作为攻击入口，威胁整个Kubernetes集群的安全。

- **数据泄露**：敏感数据可能被攻击者获取，包括机密信息、凭证等。

- **持久性后门**：攻击者可部署持久性后门，长期监控或控制系统。

**Proof of Concept（概念验证）**

1. **攻击者准备恶意软件包**

   攻击者构建一个伪装成Kubernetes 1.23.15版本的恶意软件包，其中植入了后门程序或恶意脚本。

2. **发布到非官方渠道**

   攻击者将该恶意软件包上传到公共的包仓库、文件共享平台，或建立一个看似合法的软件下载站点，优化搜索引擎排名，诱导用户下载。

3. **用户下载并安装**

   用户由于无法从官方渠道获取所需版本，搜索后找到攻击者提供的下载链接，下载了被篡改的Kubernetes软件包。

4. **恶意代码执行**

   在安装或运行过程中，恶意软件包中的后门程序被执行，攻击者获得对用户系统的访问权限。

5. **攻击者控制**

   攻击者利用获得的权限，执行进一步的攻击，如横向移动、数据窃取、破坏服务等。

**建议措施**

- **从官方渠道获取软件包**

  确保仅从Kubernetes官方或受信任的镜像源获取软件包。如官方已不提供旧版本下载，可联系官方支持或寻找可信的历史版本存储。

- **验证软件包完整性**

  下载软件包后，使用官方提供的校验和或签名进行完整性验证，确保软件包未被篡改。

- **计划升级**

  制定计划，尽快将Kubernetes集群升级到受支持的最新稳定版本，以获取安全更新和支持。

- **加强安全意识**

  教育团队成员，避免从未知或不可信的来源下载和安装软件，防范供应链攻击。

---

## Issue #123804 Should we deprecate and remove the in-tree volume plugin hostpath dynamic provisioning feature?

- Issue 链接：[#123804](https://github.com/kubernetes/kubernetes/issues/123804)

### Issue 内容

The in-tree volume plugin hostpath supports dynamic provisioning a volume for a claim when the kube-controller-manager starts with `--enable-hostpath-provisioner=true`.

It creates a local /tmp/%/%s directory as a new PersistentVolume, default /tmp/hostpath_pv/%s. It is meant for development and testing only and WILL NOT WORK in a multi-node cluster.

There are 3 problems I want to talk about:

1. e2e tests don't have a test case for hostpath dynamic provisioning. https://github.com/kubernetes/kubernetes/blob/2ec63e0d28951bb525a5bce0d9459afa1c71c0bd/test/e2e/storage/drivers/in_tree.go#L649
    ```golang
    var _ storageframework.TestDriver = &hostPathDriver{}
    var _ storageframework.PreprovisionedVolumeTestDriver = &hostPathDriver{}
    var _ storageframework.InlineVolumeTestDriver = &hostPathDriver{}
    ```
    the hostPathDriver doesn't implement the `DynamicPVTestDriver` interface, so the e2e tests don't have a test case for in-tree hostpath dynamic provisioning.
2. The cluster created by kubeadm doesn't make the feature work as expected. because the kube-controller-manager pod doesn't have a hostPath volume mounted at /tmp/hostpath_pv. https://github.com/kubernetes/kubernetes/blob/2ec63e0d28951bb525a5bce0d9459afa1c71c0bd/cmd/kubeadm/app/phases/controlplane/volumes.go#L64
    ```golang
    // HostPath volumes for the controller manager
    // Read-only mount for the certificates directory
    // TODO: Always mount the K8s Certificates directory to a static path inside of the container
    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, kubeadmconstants.KubeCertificatesVolumeName, cfg.CertificatesDir, cfg.CertificatesDir, true, &hostPathDirectoryOrCreate)
    // Read-only mount for the ca certs (/etc/ssl/certs) directory
    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, caCertsVolumeName, caCertsVolumePath, caCertsVolumePath, true, &hostPathDirectoryOrCreate)
    // Read-only mount for the controller manager kubeconfig file
    controllerManagerKubeConfigFile := filepath.Join(kubeadmconstants.KubernetesDir, kubeadmconstants.ControllerManagerKubeConfigFileName)
    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, kubeadmconstants.KubeConfigVolumeName, controllerManagerKubeConfigFile, controllerManagerKubeConfigFile, true, &hostPathFileOrCreate)
    // Mount for the flexvolume directory (/usr/libexec/kubernetes/kubelet-plugins/volume/exec by default)
    // Flexvolume dir must NOT be readonly as it is used for third-party plugins to integrate with their storage backends via unix domain socket.
    flexvolumeDirVolumePath, idx := kubeadmapi.GetArgValue(cfg.ControllerManager.ExtraArgs, "flex-volume-plugin-dir", -1)
    if idx == -1 {
      flexvolumeDirVolumePath = defaultFlexvolumeDirVolumePath
    }
    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, flexvolumeDirVolumeName, flexvolumeDirVolumePath, flexvolumeDirVolumePath, false, &hostPathDirectoryOrCreate)
    ```
3. There are lots of projects using `--enable-hostpath-provisioner=true`. Please see https://cs.k8s.io/?q=enable-hostpath-provisioner&i=nope&files=&excludeFiles=&repos=. But they use kubeadm to create a cluster, so the dynamic provisioning of in-tree hostpath is never used by those projects. I don't know why they enable the flag.

There are 3 in-tree plugins that support dynamic provisioning: hostpath, rbd (removed in 1.31), and portworxVolume (will be removed in a future release once its csi migration is completed). 

Now, the community has various CSI drivers users can use for dynamic provisioning. So let us deprecate and remove the in-tree volume plugin hostpath dynamic provisioning feature if the feature is never used.

What do you think? 

/cc @xing-yang @jsafrane @pacoxu @neolit123

/sig storage
/kind bug
/area kubeadm

related-to: [add some e2e test for in-tree volume plugin to verify HonorPVReclaimPolicy](https://github.com/kubernetes/kubernetes/pull/123151#issuecomment-1932971681)
```[tasklist]
#### Tasks
- [ ] Mark the  --enable-hostpath-provisioner option in KCM as deprecated
- [ ] Remove ENABLE_HOSTPATH_PROVISIONER from hack/local-up-cluster.sh and remove --enable-hostpath-provisioner option in 1.32
```


### 分析结果

存在安全风险。

**原因及可能的影响：**

启用 `--enable-hostpath-provisioner=true`，会使 `kube-controller-manager` 支持 hostPath 卷的动态供应。当此功能启用后，用户可以通过创建 `PersistentVolumeClaim`（PVC），由系统自动创建对应的 `PersistentVolume`（PV），并将主机目录（默认情况下是 `/tmp/hostpath_pv/<volume-name>`）挂载到 Pod 中。

由于 hostPath 卷直接访问主机文件系统，如果攻击者能够创建或操纵 PVC，则可能利用该功能访问主机上的敏感文件或目录。这可能导致以下安全问题：

1. **未经授权的主机文件系统访问**：攻击者可能通过创建特制的 PVC，使得 Pod 挂载主机上的敏感目录。
2. **特权升级**：通过访问关键的系统文件或目录，攻击者可能提升在主机上的权限。
3. **数据泄露和篡改**：攻击者可以读取或修改主机上的数据，导致数据泄露或篡改。

**漏洞利用的可能性：**

- **可被攻击者利用**：如果集群中存在不受信任的用户或多租户场景，攻击者可以创建恶意的 PVC 和 Pod。
- **可能成为漏洞并分配 CVE 编号**：该风险满足分配 CVE 的条件，且根据 CVSS 3.1 评分标准，严重程度在 High 以上。

**CVSS 3.1 评分（估计）：**

- **攻击向量（AV）**：网络（Network，N）
- **攻击复杂度（AC）**：低（Low，L）
- **特权要求（PR）**：低（Low，L）
- **用户交互（UI）**：无（None，N）
- **作用范围（S）**：改变（Changed，C）
- **机密性（C）**：高（High，H）
- **完整性（I）**：高（High，H）
- **可用性（A）**：高（High，H）

综合评分：**严重程度 High（高）**，得分约 8.8。

**Proof of Concept（概念验证）：**

1. **攻击前提**：集群启用了 `--enable-hostpath-provisioner=true`，且攻击者有权限创建 PVC 和 Pod。

2. **攻击步骤**：

   - **Step 1**：攻击者创建一个 PVC，请求动态供应一个 PV。

     ```yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: malicious-pvc
     spec:
       accessModes:
         - ReadWriteOnce
       resources:
         requests:
           storage: 10Gi
     ```

   - **Step 2**：攻击者创建一个 Pod，使用上述 PVC，并尝试访问主机文件系统。

     ```yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: malicious-pod
     spec:
       containers:
         - name: attacker
           image: alpine
           command: ["/bin/sh"]
           args: ["-c", "sleep 3600"]
           volumeMounts:
             - mountPath: /host
               name: host-volume
       volumes:
         - name: host-volume
           persistentVolumeClaim:
             claimName: malicious-pvc
     ```

   - **Step 3**：进入 Pod，访问挂载的 `/host` 目录。

     ```bash
     kubectl exec -it malicious-pod -- /bin/sh
     ```

     在 Pod 内部，攻击者可以查看 `/host` 目录下的内容，如果供应器配置不当，可能会包含主机上的敏感数据。

3. **利用方式扩展**：

   - 如果攻击者能够影响动态供应器的路径（例如利用符号链接等手段），可能访问主机上的任意目录。
   - 攻击者可能在主机文件系统上写入恶意文件，影响主机的运行。

**建议措施：**

- **在生产环境中禁用该功能**：由于该功能仅用于开发和测试环境，建议在生产环境中不要启用 `--enable-hostpath-provisioner=true`。
- **严格控制 PVC 的创建权限**：限制只有受信任的用户或服务账户才能创建 PVC。
- **使用 CSI 驱动**：替代使用受支持且安全的 CSI 驱动进行存储的动态供应。

**总结：**

Issue 中提到的 `--enable-hostpath-provisioner=true` 功能存在安全风险，可能被攻击者利用进行主机文件系统的未经授权访问，导致严重的安全后果。建议谨慎使用该功能，并采取必要的安全措施。

---

## Issue #123727 DRA API: don't allow changing object meta during status update

- Issue 链接：[#123727](https://github.com/kubernetes/kubernetes/issues/123727)

### Issue 内容

#### What happened?

The strategy.go's `PrepareForStatusUpdate` was copied from some other, broken types. A call to `ResetObjectMetaForStatus` is missing and therefore, for example, finalizers can be changed during a status update. That is not supposed to be possible.

#### What did you expect to happen?

Object meta changes need to be removed during status updates.

#### How can we reproduce it (as minimally and precisely as possible)?

n/a

#### Anything else we need to know?

_No response_

#### Kubernetes version

Since 1.26 till at least 1.29.

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

在 Kubernetes 中，`status` 子资源的更新应仅限于修改对象的状态信息，而不应允许修改对象的元数据（`ObjectMeta`）。然而，由于 `PrepareForStatusUpdate` 函数中缺少对 `ResetObjectMetaForStatus` 的调用，导致在进行 `status` 更新时，元数据的修改未被清除。这意味着用户在更新 `status` 子资源时，仍然可以修改对象的元数据，例如 `finalizers`。

**主要安全风险：**

1. **权限提升**：通常，更新对象的元数据需要更高的权限，而更新 `status` 子资源的权限通常较低。攻击者可以利用此漏洞，通过仅具有 `status` 更新权限，来修改对象的元数据，达到权限提升的目的。

2. **资源删除保护绕过**：`finalizers` 用于确保在删除资源前执行特定的清理操作。攻击者可以通过在 `status` 更新时移除 `finalizers`，从而绕过资源的删除保护机制，导致资源被意外删除或未正确清理。

3. **干扰系统行为**：修改对象的其他元数据（如标签或注解）可能影响系统的自动化流程、安全策略或监控，从而造成更大的影响。

根据 CVSS 3.1 评分标准，此漏洞可能导致权限提升和重要资源的意外删除，具有较高的严重性，评分可能达到 High 等级。

**概念验证（Proof of Concept）：**

假设存在一个攻击者账号，具有以下权限：

- 允许对某个自定义资源（如 DRA 资源）的 `status` 子资源进行更新。
- 不允许修改该资源的其他部分，包括元数据。

攻击步骤：

1. **查看当前资源的状态和元数据：**

   ```bash
   kubectl get dra <resource-name> -o yaml
   ```

2. **尝试通过 `status` 更新来修改 `finalizers`：**

   创建一个文件 `patch.yaml`，内容如下：

   ```yaml
   metadata:
     finalizers: []
   ```

   然后执行命令：

   ```bash
   kubectl patch dra <resource-name> --type merge --patch-file patch.yaml --subresource='status'
   ```

3. **预期结果：**

   - 正常情况下，Kubernetes 应该拒绝在 `status` 更新时修改 `metadata`，或者自动忽略对元数据的修改。
   - 然而，由于缺少对 `ResetObjectMetaForStatus` 的调用，`finalizers` 被成功移除。

4. **验证结果：**

   再次查看资源，确认 `finalizers` 已被移除：

   ```bash
   kubectl get dra <resource-name> -o yaml
   ```

**可能的影响：**

- 攻击者可以删除关键的 `finalizer`，导致资源在未执行必要清理操作的情况下被删除。
- 可以修改其他元数据，干扰系统的正常运行，造成潜在的安全隐患或数据损坏。

因此，该问题存在被攻击者利用的可能性，且可能导致高严重度的安全漏洞。

---

## Issue #123665 --feature-gates=AllAlpha=true  make error

- Issue 链接：[#123665](https://github.com/kubernetes/kubernetes/issues/123665)

### Issue 内容

#### What happened?

```
I0304 16:55:26.081185  109523 healthz.go:261] informer-sync,poststarthook/start-service-ip>
[-]informer-sync failed: 2 informers not started yet: [*v1alpha1.IPAddress *v1alpha1.Servi>
[-]poststarthook/start-service-ip-repair-controllers failed: not finished
[-]poststarthook/rbac/bootstrap-roles failed: not finished
[-]poststarthook/start-kubernetes-service-cidr-controller failed: not finished
[-]poststarthook/built-in-resources-storage-version-updater failed: not finished
```

make --feature-gates=AllAlpha=true


root@29-a:~# kubectl patch -n k8s pod mypod --type='json' -p='[{"op": "replace", "path": "/spec/containers/0/resources/requests/cpu", "value": "300m"}]'
Error from server (ServiceUnavailable): wait for storage version registration to complete for resource: pods, last seen error: <nil>

#### What did you expect to happen?

--feature-gates=AllAlpha=true
everything is good

#### How can we reproduce it (as minimally and precisely as possible)?

kube-apiserver,kube-controller-manager,kube-schedule,kubelet config
--feature-gates=AllAlpha=true

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
root@29-a:~# kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0

</details>


#### Cloud provider

<details>
vmware
</details>


#### OS version

<details>

```console
root@29-a:~# cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因：**

使用 `--feature-gates=AllAlpha=true` 会启用 Kubernetes 中所有的 Alpha 特性。Alpha 特性是处于实验阶段的功能，可能尚未经过充分的安全测试，存在未知的漏洞或不稳定性。启用所有的 Alpha 特性可能会引入严重的安全隐患。

**可能的影响：**

1. **未知漏洞被利用：** 攻击者可能利用 Alpha 特性中的未知漏洞，进行未授权的访问、提权、数据泄露或破坏集群服务等恶意操作。

2. **稳定性问题：** Alpha 特性可能导致集群不稳定，出现崩溃或服务中断，影响业务连续性。

3. **绕过安全机制：** 某些 Alpha 特性可能绕过现有的安全控制措施，使攻击者更容易进行攻击。

**概念验证（Proof of Concept）：**

由于启用了所有 Alpha 特性，假设其中有一个 Alpha 特性存在身份验证绕过的漏洞，攻击者可以利用该漏洞获取对集群的管理权限。例如，利用未完善的 API 接口绕过身份验证，对集群资源进行未授权的操作。

**建议：**

为了确保集群的安全和稳定性，建议仅启用必要且经过测试的特性，避免使用 `AllAlpha=true` 开启所有 Alpha 功能。在生产环境中，尤其要谨慎对待实验性功能的启用。

---

## Issue #123604 Multiple Timestamp Directories Exist in the Mount Directory After the Server Is Powered Off

- Issue 链接：[#123604](https://github.com/kubernetes/kubernetes/issues/123604)

### Issue 内容

#### What happened?

Background: I have a K8S cluster, version is v1.25.3, on the master node core1 there is a deployment mounted secret (named ssl). In normal cases, the mount directory in the container is as follows:
![正常挂载](https://github.com/kubernetes/kubernetes/assets/66065665/d4f7c38a-e1ac-40bb-b67a-3df5a119498e)
![挂载目录](https://github.com/kubernetes/kubernetes/assets/66065665/181387d8-f08f-4782-929e-fa26ff3a6685)
But when I reboot the machine and go to the container to view the mount directory, it looks like this:
![reboot后](https://github.com/kubernetes/kubernetes/assets/66065665/4d885d9b-fba6-4892-a090-aa55f2d506ad)
A timestamp directory is generated upon each reboot. The permission of the files in the directory that is not linked by the soft link..data can be changed. In the preceding figure, the permission is changed to 700.
![多次reboot](https://github.com/kubernetes/kubernetes/assets/66065665/1cbd218e-192e-4b4c-b8bd-4679f450bf28)
When I restarted the pod, the mount directory was back to normal.
![重弹](https://github.com/kubernetes/kubernetes/assets/66065665/73e539c1-c9cd-486c-92c3-1407c527d8af)
While restarting the pod will make this go away, I want to keep only one mounted timestamp directory. If the timestamp directory is not linked by ..data, the permission can be changed. For example, in the preceding figure, the permission is changed to 700 because of this container’s command. If sensitive files exist in the timestamp directory and the permission is too high, the files are insecure for enterprises. In addition, power-off is inevitable.
![配置](https://github.com/kubernetes/kubernetes/assets/66065665/81507e7a-5077-40f3-b79f-02dd62c6f68b)
Therefore, I think that there should be only one timestamp directory in the power-off scenario. There should not be a timestamp directory whose permissions can be changed and that contains sensitive files when mounting secrets.


#### What did you expect to happen?

Only One Timestamp Directory Exists in the Mount Directory After the Linux Machine Is Powered Off

#### How can we reproduce it (as minimally and precisely as possible)?

Mount the projection data volume to the container and reboot the linux machine.

#### Anything else we need to know?

I added the log in atomic_write.go and then I rebooted the linux machine. By looking at the kubelet logs, I know that the reason the old timestamp directory wasn't deleted was because it didn't successfully get the directory pointed to by ..data.

I add the log in the code like this：
![代码](https://github.com/kubernetes/kubernetes/assets/66065665/a94a8541-400d-4ca1-8032-6db6a58bc683)
After the machine is rebooted, there is a log like this:
```
-------dataDirPath is /var/lib/kubelet/pods/01352a44-7d9e-445d-940f-7653118abc8c/volumes/kubernetes.iowsecret/ssl/..data ------
-------oldTsDir is  -------
-------oldTsPath is /var/lib/kubelet/pods/01352a44-7d9e-445d-940f-7653118abc8c/volumes/kubernetes.iowsecret/ssl -------
```
After the code  `oldTsDir, err := os.Readlink(dataDirPath)`  is executed, the value of variable `oldTsDir` is empty.


#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3", GitCommit:"cadee57c0b4d8d8ba9eb45e30d29eba749a87452", GitTreeState:"dirty", BuildDate:"2023", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3", GitCommit:"cadee57c0b4d8d8ba9eb45e30d29eba749a87452", GitTreeState:"dirty", BuildDate:"2023", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="SLES"
VERSION="12-SP5"
VERSION_ID="12.5"
PRETTY_NAME="SUSE Linux Enterprise Server 12 SP5"
ID="sles"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:12:sp5"

$ uname -a
# Linux host-***** 4.12.14-122.162-default #1 SMP Tue Jun 6 12:25:40 UTC 2023 (02a1ae5) x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

```[tasklist]
#### Tasks
```


### 分析结果

经过分析，您描述的Issue确实存在潜在的安全风险，具体原因和可能影响如下：

**1. 风险能被攻击者利用**

- **敏感信息泄露风险**：Kubernetes的Secret通常用于存储敏感信息，如密码、密钥等。在服务器重启后，挂载目录下遗留了多个旧的时间戳目录，这些目录包含了Secret的副本。如果这些旧的时间戳目录未被正确清理，且权限可以被修改，攻击者可能利用这些目录访问到敏感信息。

- **权限控制漏洞**：您提到未被..data符号链接指向的时间戳目录，其权限可以被更改（例如被更改为700）。这意味着，如果攻击者能够访问到容器或节点的文件系统，可能通过修改权限来读取这些敏感文件。

- **持久化敏感数据**：旧的时间戳目录在重启后仍然存在，意味着敏感信息在文件系统中保留的时间更长，增加了被不当访问的风险。

**2. 风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在High以上**

按照CVSS 3.1标准，对此问题进行评分：

- **攻击向量（AV）**：本地（L）——攻击者需要访问本地系统。
- **攻击复杂度（AC）**：低（L）——攻击不需要特殊条件。
- **权限要求（PR）**：低（L）——攻击者需要具备低权限即可（如对容器的访问权限）。
- **用户交互（UI）**：不需要（N）——不需要额外的用户交互。
- **影响范围（S）**：未变（U）——攻击影响的范围限于组件本身。
- **机密性影响（C）**：高（H）——敏感信息可能被完全泄露。
- **完整性影响（I）**：无（N）——对完整性无直接影响。
- **可用性影响（A）**：无（N）——对可用性无直接影响。

根据上述指标，计算出的CVSS 3.1基准分数为**7.1（High）**。

**可能的影响**

- **敏感数据泄露**：攻击者获取到Secret中的敏感信息，如证书、密码、密钥等，可能导致系统被进一步攻破。
- **横向移动**：攻击者利用获取的敏感信息，对集群内其他资源进行未授权的访问。
- **权限提升**：通过敏感信息，攻击者可能提升自身权限，取得对更高敏感度资源的控制。

**Proof of Concept**

**环境准备**：

1. 部署一个Kubernetes集群，版本为v1.25.3。
2. 在master节点core1上部署一个挂载了Secret（名称为ssl）的Deployment。

**攻击步骤**：

1. **重启服务器**：执行`reboot`命令，重启core1节点。
2. **查看挂载目录**：重启后，进入相关的容器，导航到挂载Secret的目录，例如`/etc/secret`。
3. **发现多个时间戳目录**：观察到该目录下存在多个时间戳命名的子目录，例如`..2023_10_01_12_00_00.123456789`。
4. **修改权限**：对于未被`..data`符号链接指向的旧时间戳目录，执行`chmod 700 ..2023_10_01_12_00_00.123456789`，修改其权限。
5. **访问敏感文件**：进入该目录，读取其中的文件，如`cat tls.key`，获取到Secret中的敏感信息。

**验证结果**：

- 攻击者成功读取到了本应受保护的Secret内容。
- 由于旧目录的权限可以被修改，导致权限控制被绕过。

**结论**

- **安全风险确认**：该问题导致Secret的数据在重启后残留，并且权限控制存在漏洞，确实构成安全风险。
- **建议**：
  - **修复问题**：Kubernetes应修复挂载Secret时旧时间戳目录未被清理的问题，确保在节点重启后，挂载目录下仅保留最新的Secret数据目录。
  - **权限控制**：加固对挂载目录的权限控制，防止权限被随意修改。
  - **监控与审计**：加强对文件系统的监控，及时发现异常的目录和权限变更。

**总结**

您描述的Issue涉及到Secret数据的残留和权限控制不当，可能被恶意攻击者利用，导致敏感信息泄露，符合高危安全漏洞的判断标准，建议及时关注并采取措施修复。

---

## Issue #123602 Pod Crashloop backoff requeue issues

- Issue 链接：[#123602](https://github.com/kubernetes/kubernetes/issues/123602)

### Issue 内容

When stepping through the pod crashloop backoff implementation, I noticed a few issues:

1. Backoff errors are unsuccessfully suppressed:
  - This code is non-functional, since there will always be a non-error result present (`ConfigPodSandbox`) with a nil error: https://github.com/kubernetes/kubernetes/blob/5cf4fbe524ca1479607a4880949a032064556f76/pkg/kubelet/kubelet.go#L1964-L1971
2. The propogated backoff error means that the pod worker always immediately requeues the pod: https://github.com/kubernetes/kubernetes/blob/5cf4fbe524ca1479607a4880949a032064556f76/pkg/kubelet/pod_workers.go#L1495-L1497
  - If the backoff error were suppressed, then the default 1m sync frequency would be used, which would cause very long initial backoffs.

The result of these issues are:
1. Log spam: `E0301 01:06:01.286649    1869 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"alpine\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=alpine pod=restarting-pod_default(ca0d5836-1435-497e-9a41-547fbcc10484)\"" pod="default/restarting-pod" podUID=ca0d5836-1435-497e-9a41-547fbcc10484` get's logged in a frequent loop
2. Kubelet does unnecessary work to reexamine pods and containers in backoff
3. (minor) the error retry period is 10s, which adds up to 10s to the backoff time

I think the ideal fix is to propagate the backoff retry time up from SyncPod [1] to the pod worker [2], so that the pod can be requeued with the exact (jittered) right amount of retry time. One option is to use a custom error type to encapsulate the backoff time, which I started working on here: https://github.com/tallclair/kubernetes/commit/5d89eb2538cc8929540476f436d4a1b92c9babee

Of course, there is very little test coverage across these code paths too.

[1]: https://github.com/kubernetes/kubernetes/blob/5cf4fbe524ca1479607a4880949a032064556f76/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L1325
[2]: https://github.com/kubernetes/kubernetes/blob/5cf4fbe524ca1479607a4880949a032064556f76/pkg/kubelet/pod_workers.go#L1484

/sig node
/kind bug

### 分析结果

这个Issue涉及到Kubernetes中Pod的Crashloop backoff实现存在问题，具体表现为：

1. **Backoff错误未成功抑制**：在特定的代码路径中，错误未被正确处理，导致逻辑上的问题。
   
2. **错误的错误传播导致Pod工作线程立即重新排队Pod**：这意味着即使Pod处于backoff状态，Kubelet也会频繁地重新处理该Pod。

这些问题导致了以下结果：

1. **日志泛滥**：大量重复的错误日志被频繁记录，可能导致日志存储耗尽。

2. **Kubelet执行不必要的工作**：重复地检查处于backoff状态的Pod和容器，增加了系统负载。

3. **增加了错误重试周期**：错误重试周期为10秒，这可能加剧了问题。

**潜在的安全风险分析：**

攻击者可以利用该漏洞通过创建故意崩溃的Pod来引发上述问题，导致以下安全风险：

- **拒绝服务（DoS）攻击**：大量的日志输出和不必要的Pod处理会消耗Kubelet和节点的资源，包括CPU、内存和磁盘空间（由于日志文件增长）。这可能导致节点性能下降，甚至使节点不可用，从而影响集群的稳定性。

- **资源耗尽**：持续的Crashloop可能导致日志积累，占用磁盘空间。此外，频繁的处理会消耗计算资源，影响其他正常Pod的运行。

**风险判断标准符合：**

1. **可被攻击者利用**：攻击者只需拥有在集群中创建Pod的权限（这在多租户环境中是常见的低权限），即可创建恶意Pod触发该漏洞。

2. **可能成为漏洞并被分配CVE编号**：按照CVSS 3.1评分标准评估：

   - **攻击向量（AV）**：网络（N）
   - **攻击复杂度（AC）**：低（L）
   - **权限要求（PR）**：低（L）
   - **用户交互（UI）**：无需（N）
   - **影响范围（S）**：未改变（U）
   - **机密性影响（C）**：无（N）
   - **完整性影响（I）**：无（N）
   - **可用性影响（A）**：高（H）

   综合评分为**8.2（High）**。

**Proof of Concept：**

攻击者可以执行以下步骤来利用该漏洞：

1. **创建一个持续崩溃的Pod**：

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: crash-loop-pod
   spec:
     containers:
     - name: crash-container
       image: busybox
       command: ['sh', '-c', 'exit 1']
   ```

2. **应用该Pod配置**：

   ```bash
   kubectl apply -f crash-loop-pod.yaml
   ```

3. **观察效果**：

   - 使用 `kubectl describe pod crash-loop-pod` 可以看到Pod处于CrashLoopBackOff状态。
   - 检查Kubelet日志，会发现大量的错误日志被频繁生成。
   - 监控节点资源，可能会发现CPU、内存使用率升高，磁盘空间因日志增长而减少。

通过上述步骤，攻击者可以导致Kubelet和节点资源耗尽，影响集群的可用性，达到拒绝服务的目的。

---

# 📌 不涉及安全风险的 Issues (59 个)

## Issue #124120 panic with SIGSEGV in kubeadm certs check-expiration

- Issue 链接：[#124120](https://github.com/kubernetes/kubernetes/issues/124120)

### Issue 内容

#### What happened?

```
$ kubeadm certs check-expiration 
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x16c0556]

goroutine 1 [running]:
k8s.io/kubernetes/cmd/kubeadm/app/phases/certs/renewal.fileExists({0xc000501360?, 0x1c1ab60?})
        cmd/kubeadm/app/phases/certs/renewal/readwriter.go:74 +0x56
k8s.io/kubernetes/cmd/kubeadm/app/phases/certs/renewal.(*kubeConfigReadWriter).Exists(0x1c1ab60?)
        cmd/kubeadm/app/phases/certs/renewal/readwriter.go:124 +0x1e
k8s.io/kubernetes/cmd/kubeadm/app/phases/certs/renewal.(*Manager).CertificateExists(0x21f6180?, {0x1f0b722, 0xa})
        cmd/kubeadm/app/phases/certs/renewal/manager.go:325 +0x4a
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdCertsExpiration.func1(0xc0005c1400?, {0x3259d80?, 0x4?, 0x1f03ff0?})
        cmd/kubeadm/app/cmd/certs.go:403 +0x234
github.com/spf13/cobra.(*Command).execute(0xc000004600, {0x3259d80, 0x0, 0x0})
        vendor/github.com/spf13/cobra/command.go:940 +0x87c
github.com/spf13/cobra.(*Command).ExecuteC(0xc00027d800)
        vendor/github.com/spf13/cobra/command.go:1068 +0x3a5
github.com/spf13/cobra.(*Command).Execute(0x21f5ee0?)
        vendor/github.com/spf13/cobra/command.go:992 +0x13
k8s.io/kubernetes/cmd/kubeadm/app.Run()
        cmd/kubeadm/app/kubeadm.go:50 +0x155
main.main()
        cmd/kubeadm/kubeadm.go:25 +0x13
$
```
When the command is run with superuser privileges, it prints the requested information.
#### What did you expect to happen?

Print a warning that files could not be read due to insufficient permission, or that information was missing (e.g., when the /etc/kubernetes directory is not initialized).
When the directory is readable, it works as expected:
```
$ kubeadm certs check-expiration
CERTIFICATE                          EXPIRES   RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
!MISSING! admin.conf                                                                   
!MISSING! apiserver                                                                    
!MISSING! apiserver-etcd-client                                                        
!MISSING! apiserver-kubelet-client                                                     
!MISSING! controller-manager.conf                                                      
!MISSING! etcd-healthcheck-client                                                      
!MISSING! etcd-peer                                                                    
!MISSING! etcd-server                                                                  
!MISSING! front-proxy-client                                                           
!MISSING! scheduler.conf                                                               

CERTIFICATE AUTHORITY      EXPIRES   RESIDUAL TIME   EXTERNALLY MANAGED
!MISSING! ca                                         
!MISSING! etcd-ca                                    
!MISSING! front-proxy-ca                             
$ 
```
where OS and kubeadm version detail was:
```
$ uname -a
Linux z620 6.8.0-11-generic #11-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 14 00:29:05 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/os-release
PRETTY_NAME="Ubuntu Noble Numbat (development branch)"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.8", GitCommit:"fc11ff34c34bc1e6ae6981dc1c7b3faa20b1ac2d", GitTreeState:"clean", BuildDate:"2024-03-15T00:05:37Z", GoVersion:"go1.21.8", Compiler:"gc", Platform:"linux/amd64"}
```

#### How can we reproduce it (as minimally and precisely as possible)?

Issue the command `kubeadm certs check-expiration` as a normal user when the directory /etc/kubernetes/pki exists but not readable.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server my-private-host:6443 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
$ cat /etc/os-release 
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux acs-clb16jm33b70349 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.3", GitCommit:"6813625b7cd706db5bc7388921be03071e1a492d", GitTreeState:"clean", BuildDate:"2024-03-15T00:06:16Z", GoVersion:"go1.21.8", Compiler:"gc", Platform:"linux/amd64"}
```
</details>


#### Install tools

<details>
apt-get install kubeadm (source repo: k8s.io)
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124119 release blocking job ci-kubernetes-e2e-gci-gce-scalability-1-30 needs branch in perf-tests

- Issue 链接：[#124119](https://github.com/kubernetes/kubernetes/issues/124119)

### Issue 内容

#### What happened?

Standing up a new job for k8s 1.30 release needs a new branch `release-1.30` in https://github.com/kubernetes/perf-tests.git repo

```
$ PWD=/home/prow/go/src/k8s.io/perf-tests  git fetch --filter=blob:none https://github.com/kubernetes/perf-tests.git release-1.30 (runtime: 1m5.58005565s)
fatal: couldn't find remote ref release-1.30
# Error: exit status 128
```

#### What did you expect to happen?

CI job to be green

#### How can we reproduce it (as minimally and precisely as possible)?

We already have a CI job - https://testgrid.k8s.io/sig-release-1.30-blocking#gce-cos-1.30-scalability-100&width=20

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124112 Kubelet Stuck Mounting Local PVC

- Issue 链接：[#124112](https://github.com/kubernetes/kubernetes/issues/124112)

### Issue 内容

#### What happened?

On our production cluster, we have 6 pods that use local PVCs on nodes we set up with some host processes.  These local PVCs point to the same underlying disk device.  The pods mount these PVCs using subpaths, if that matters.  These host processes can sometimes use a large portion of the disk utilization, the same disk device that the PVCs point to.  These EBS volumes were created from a snapshot, so read and write speed might be reduced since they are cold.

When the pods startup, they get stuck in the "ContainerCreating" status.  In the kubelet logs, I see the following errors:
```
[pod_workers.go:965] "Error syncing pod, skipping" err="unmounted volumes=[storage], unattached volumes=[...]: timed out waiting for the condition" pod="..." podUID=...
[kubelet.go:1731] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[storage], unattached volumes=[...]: timed out waiting for the condition" pod="..."
```
Other pods that get deployed to the node that doesn't use the same disk device gets deployed perfectly fine.

I also checked the /proc/kubeletProcessId/fd and I noticed that most of the sockets were broken.

#### What did you expect to happen?

I expected that kubelet would eventually recover and the pod would be able to mount the PVCs and start up successfully.  But after waiting for a few hours, it never did.

The only way I was able to recover was restarting kubelet (even when the disk utilization was still high).  Once I did that, all the pods mounted and started up successfully.  I also checked the sockets for the kubelet process after the restart and they were still broken, so that might be a red herring.

#### How can we reproduce it (as minimally and precisely as possible)?

Only some of our sites have ran into this issue, so it is somewhat intermittent.  I'm still working on seeing if I can reproduce this by simulating high disk utilization, but so far, I have been unable to.

#### Anything else we need to know?

I noticed there were some github issues around these feature flags, but they were not enabled, so most likely not related:
kubernetes_feature_enabled{name="NewVolumeManagerReconstruction",stage="BETA"} 0
kubernetes_feature_enabled{name="SELinuxMountReadWriteOncePod",stage="BETA"} 0

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.10-eks-508b6b3
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
# paste output here
Linux site.com 5.15.149-99.162.amzn2.x86_64 #1 SMP Tue Mar 12 11:29:59 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd 1.7.11 64b8a811b07ba6288238eefc14d898ee0b5b99ba
</details>


### 分析结果

不涉及

---

## Issue #124109 Scheduler Cache missed

- Issue 链接：[#124109](https://github.com/kubernetes/kubernetes/issues/124109)

### Issue 内容

#### What happened?

- Pod Scheduler failed with lack resource. Actually, node has enough resource.
- Cache Missed Match，when I dump scheduler cache info
`I0329 17:52:21.473968       1 comparer.go:64] "Cache mismatch" missedPods=[000669f9-8b26-4cd8-98ac-30206645690c 000aa9e5-782b-418e-a2fd-88e158e91e36 001daca0-f738-4830-a60b-7e231b7e0e3b 00374447-d517-4c98-aadc-f1f6d28e07b2 003a063a-9d3e-41a0-8356-7d3158e5ca3f 003d060c-6582-4e11-8ff5-c1d36500451a 0045d507-2feb-4104-a480-050e8a02ba25 0055f20d-30c2-471a-8b11-d87959a1b0bd 00620471-945d-4205-aa73-d4743c5b83f5 00790d14-8e52-41f8-9f49-96674bef0797 (there are hundreds of  pods )]`

#### What did you expect to happen?

- Pod Schedule Successfully

#### How can we reproduce it (as minimally and precisely as possible)?

- correct scheduler cache information

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
scheduler version: 1.27.X
</details>


#### Cloud provider

<details>
commity
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124096 BestEffort pods are using swap

- Issue 链接：[#124096](https://github.com/kubernetes/kubernetes/issues/124096)

### Issue 内容

#### What happened?

I was testing the support for swap and I came to an unexpected behavior. In the documentation it is specified that only pods that fall under the `Burstable` class can use the host's swap memory. However, I created both a deployment with 1 replica of ubuntu belonging to the `Burstable` class, and one belonging to the `BestEffort` class, where I ran the command `stress --vm 1 --vm-bytes 6G --vm-hang 0` to see the consumption of memory made. The host has 4GB RAM memory and 5GB swap. In both situations, the pod started using swap after exceeding the RAM memory requirement. Wasn't the BestEffort pod supposed to be restarted when it reached the limit of the host's RAM memory? I mention that the kubelet is configured to `swapBehavior=LimitedSwap`. I attached two pictures where you can see the normal consumption of host, and consumption after running stress command inside pod
<img width="1180" alt="Screenshot 2024-03-27 at 12 02 30" src="https://github.com/kubernetes/kubernetes/assets/26438926/c6d9bbc6-0ba2-4445-944d-67dfc8eed185">
. 
![screenshot_2024-03-27_at_12 37 02](https://github.com/kubernetes/kubernetes/assets/26438926/1d60c115-46a5-4a4d-9aba-0c34a6411775)


#### What did you expect to happen?

I expected the `BestEffort` pod to be killed when it consumes more RAM memory than the host have available.

#### How can we reproduce it (as minimally and precisely as possible)?

- setup a VM running ubuntu 22.04 with 4GB of RAM memory
- set swap partition to 5GB
- install docker, cri-dockerd and kubernetes packages using the provided versions
- config kubelet with provided config
- install calico cni
- after the cluster is bootstrapped, deploy the following deployment
<details>

```console
$ $ cat test.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ubuntu-deployment
  labels:
    app: ubuntu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ubuntu
  template:
    metadata:
      labels:
        app: ubuntu
    spec:
      containers:
      - name: ubuntu
        image: ubuntu:22.04
        resources:
        command: [ "/bin/bash", "-c", "--" ]
        args: [ "while true; do sleep 30; done;" ]
```

</details>

- this should deploy a BestEffort pod. you can check this by running `kubectl get pod <pod-name> --output=yaml`
- exec into the pod and do `apt update & apt install stress`. Then run `stress --vm 1 --vm-bytes 6G --vm-hang 0`
- check the node where the pod is running with `kubectl get po -o wide` then ssh to that node and run `htop`. Now you should see that the deployed BestEffort pod is consuming swap memory, which according to the Docs, it shouldn't.
- if exec into the pod and check `memory.swap.max`, this is set to max. From what I understand, even if `swapBehavior` was set to `LimitedSwap` in `kubelet`, somehow cri-dockerd may be set the cgroup for `memory.swap.max` to `max`.
<details>

```console
$ cat /sys/fs/cgroup/memory.swap.max 
max
```

</details>

#### Anything else we need to know?

I am using cgroup v2.

Here is my kubelet config.

<details>

```console
$ cat /var/lib/kubelet/config.yaml 
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
 anonymous:
  enabled: false
 webhook:
  cacheTTL: 0s
  enabled: true
 x509:
  clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
 mode: Webhook
 webhook:
  cacheAuthorizedTTL: 0s
  cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
enableServer: true
evictionPressureTransitionPeriod: 0s
failSwapOn: false
featureGates:
 NodeSwap: true
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
 flushFrequency: 0
 options:
  json:
   infoBufferSize: "0"
 verbosity: 0
memorySwap:
 swapBehavior: LimitedSwap
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
```

</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
Hetzner Cloud, but Kubernetes was deployed using `kubeadm`.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux fs-kube-dev-1 5.15.0-100-generic #110-Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

```console
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.3", GitCommit:"6813625b7cd706db5bc7388921be03071e1a492d", GitTreeState:"clean", BuildDate:"2024-03-15T00:06:16Z", GoVersion:"go1.21.8", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
$ cri-dockerd --version
cri-dockerd 0.3.11 (9a8a9fe)
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

calico:
  version: 3.27.2

</details>


### 分析结果

不涉及。

---

## Issue #124095 Even if the container has failed, the DaemonSet pod's phase still shows running

- Issue 链接：[#124095](https://github.com/kubernetes/kubernetes/issues/124095)

### Issue 内容

#### What happened?

In a single container pod, the container has encountered an error, but the pod status is still running, resulting in not being pulled up in a timely manner

#### What did you expect to happen?

When encountering an error in the container, the DaemonSet pod should be refreshed to failed

#### How can we reproduce it (as minimally and precisely as possible)?


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.28.1

```
</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Linux master1 5.10.0-136.12.0.86.h1616.eulerosv2r12.x86_64 #1 SMP Thu Mar 14 04:12:23 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124090 DRA: kubelet:  error preparing devices for claim

- Issue 链接：[#124090](https://github.com/kubernetes/kubernetes/issues/124090)

### Issue 内容

#### What happened?

```
Mar 28 10:29:43 worker-node-1 kubelet[2855486]: E0328 10:29:43.770694 2855486 kuberuntime_manager.go:1152] "Failed to prepare dynamic resources" err="NodePrepareResources failed for claim default/rdma-demo: error preparing devices for claim 71d5aeee-5f6d-47aa-a3d6-98b7fcf3c005: unable to create CDI spec file for claim: failed to set permissions on spec file: chmod /var/run/cdik8s.netresources.spidernet.io-claim_71d5aeee-5f6d-47aa-a3d6-98b7fcf3c005.yaml: no such file or directory" pod="default/rdma-rc-6c5596fdb8-mf95q"

root@worker-node-1:~# ls /var/run/cdi/k8s.netresources.spidernet.io-claim_71d5aeee-5f6d-47aa-a3d6-98b7fcf3c005.yaml
/var/run/cdi/k8s.netresources.spidernet.io-claim_71d5aeee-5f6d-47aa-a3d6-98b7fcf3c005.yaml
```



#### What did you expect to happen?

ok to preparing devices for claim

#### How can we reproduce it (as minimally and precisely as possible)?

create a pod with dra

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124076 podLogsDir validation of default value breaks on windows

- Issue 链接：[#124076](https://github.com/kubernetes/kubernetes/issues/124076)

### Issue 内容

#### What happened?

> Kubelet: a custom root directory for pod logs (instead of default /var/log/pods) can be specified using the podLogsDir key in kubelet configuration. (https://github.com/kubernetes/kubernetes/pull/112957, [@mxpv](https://github.com/mxpv)) [SIG API Machinery, Node, Scalability and Testing]

`podLogsDir` is not added in `cmd/kubelet/app/options/options.go` so I can't use this flag when running kubelet v1.30.0-beta.0.

also, when I ran it in windows (it's fine when I ran it in linux), I got `invalid configuration: pod logs path \"/var/log/pods\" must be absolute path, invalid configuration: pod logs path \"/var/log/pods\" must be normalized`. Notice there are unexpected \ before quotes.



#### What did you expect to happen?

I should be able to run `kubelet` with `--pod-logs-dir`. If I don't use this flag, it should correctly use default value.

#### How can we reproduce it (as minimally and precisely as possible)?

run newest kubelet on windows node with some necessary parameters
```sh
./kubelet --version
> Kubernetes v1.30.0-beta.0
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124073 K8s trace context for APIServer is incorrect

- Issue 链接：[#124073](https://github.com/kubernetes/kubernetes/issues/124073)

### Issue 内容

#### What happened?


In Kubernetes release 1.28, the tracing structure for the API server is depicted as follows:

![image](https://github.com/kubernetes/kubernetes/assets/6135084/f502d19d-0e29-439a-ad47-485669238006)


Within the API server's code structure, the spans "List(recursive=true) etcd3" and "SerializeObject" are expected to be nested under the "List" span. However, "SerializeObject" appears parallel to "List", which is not the intended behavior.



staging/src/k8s.io/apiserver/pkg/endpoints/handlers/get.go
```golang
func ListResource(r rest.Lister, rw rest.Watcher, scope *RequestScope, forceWatch bool, minRequestTimeout time.Duration) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		ctx := req.Context()
		// For performance tracking purposes.

                 // CasperLiu: this span is for high level - "List"
		ctx, span := tracing.Start(ctx, "List", traceFields(req)...)

		namespace, err := scope.Namer.Namespace(req)
		if err != nil {
			scope.err(err, w, req)
			return
		}

                 // omit...
		span.AddEvent("About to List from storage")
                 // CasperLiu: span inside below function is for low level - "List(recursive=true) etcd3"
		result, err := r.List(ctx, &opts)
		if err != nil {
			scope.err(err, w, req)
			return
		}
		span.AddEvent("Listing from storage done")
		defer span.AddEvent("Writing http response done", attribute.Int("count", meta.LenList(result)))
                 // CasperLiu: span inside below function is for low level - "SerializeObject"
		transformResponseObject(ctx, scope, req, w, http.StatusOK, outputMediaType, result)
	}
}
```

#### What did you expect to happen?

Within the API server's code structure, the spans "List(recursive=true) etcd3" and "SerializeObject" are expected to be nested under the "List" span. 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure the Kubernetes API server to enable tracing and direct the emitted spans to a backend. 
2. Illustrate the resulting trace flow. 
3. Invoke a quorum read LIST request for Kubernetes resources; within the trace flow, you may identify instances of incorrect trace context.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```
kuberenetes version:
1.28.3
```

</details>


#### Cloud provider

<details>
Alibaba Cloud Kubernetes (ACK) 
</details>


#### OS version

<details>

Unrelated to the OS version


</details>


#### Install tools

<details>
No need 
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Unrelated to the CRI version

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

Unrelated to the plugins version

</details>


### 分析结果

不涉及。

---

## Issue #124066 kubelet fails to restart with CPUManager policy static when node's some CPU is offline

- Issue 链接：[#124066](https://github.com/kubernetes/kubernetes/issues/124066)

### Issue 内容

#### What happened?

I have a node with Configuration:
a. 96 CPUS with one CPU(ID=95) disabled. 
b. kubelet CPUManager static policy enabled.
c. kubeReserved: cpu: 2, systemReserved: cpu: 2

After start kubelet, the topology kubelet detected shows
`"Detected CPU topoloogy" topology=&{NumCPUs:95 NumCores:48 NumSockets:2 NumNUMANodes:2 CPUDetails:......`
Notes Core 25 contains CPU 25,73。 Core 47 contains CPU 47,95，but 95 is disabled and not shown in the log.

The static Policy process CPU reservation log below. 
`"takeFullCores: clainming core" core=47`
`"takeFullCores: clainming core" core=24`
`"takeRemainingCPUs: claiming CPU" cpu=25`
`"Reserved CPUs not available for exclusive assignment" reservedSize=4 reserved="24-25,47,72"`
`"Updated default CPUSet" cpuSet="0-94"`

The weird thing is when the first Pod admits, the reserved CPU(ID=25) is reused. 
`"Topology Affinity" pod="xxx" containerName="xxx" affinity={NUMANodeAffinity:<nil> Prefered:false}`
`"AllocateCPUs" numCPUs=20 socket=<nil>`
`"takeFullCores: clainming core" core=25`
`"takeRemainingCPUs: claiming CPU" cpu=26`
`"takeRemainingCPUs: claiming CPU" cpu=74`
....
`"Updated default CPUSet" cpuSet="0-24,35-72,83-94"`
`"AllocateCPUs" result="25-34,73-82"`

The confliction would further cause kubelet restart failure because of the static policy validation
`"Static policy invalid state, please drain node and remove policy state file" err="not all reserved cpus: \"24-25,47,72\" are present in defaultCpuSet: \"0-24,35-72,83-94\""`

#### What did you expect to happen?

Pod Cpuset Allocation should not share with Reserved CPUs

#### How can we reproduce it (as minimally and precisely as possible)?

1. use `chcpu -d` to disable one CPU in the node. Check `/sys/devices/system/cpu/online` file to make sure it works.
2. change kubelet configuration
   a. cpuManagerPolicy: static
   b. kubeReserved: cpu: 2, systemReserved: cpu: 2
3. restart kubelet
4. create a guaranteed Pod with some CPU
5. restart kubelet, expects it would fail to startup

#### Anything else we need to know?

it seems that kubelet CPUManager static Policy have not taken CPU offline senario into account. 
1. in the specific case above, this method would result in 1(95/48), not 2
https://github.com/kubernetes/kubernetes/blob/227c2e7c2b2c05a9c8b2885460e28e4da25cf558/pkg/kubelet/cm/cpumanager/topology/topology.go#L49-L54
2. thus result in a Used Core as free (1==1)
https://github.com/kubernetes/kubernetes/blob/227c2e7c2b2c05a9c8b2885460e28e4da25cf558/pkg/kubelet/cm/cpumanager/cpu_assignment.go#L259-L262
3. all logic releated with CPUsPerCore, CPUsPerSocket is affected

#### Kubernetes version

<details>
tested in kubernetes: v1.25, other version is the same.
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124065 StatefulSet pod ends up in state "Completed"

- Issue 链接：[#124065](https://github.com/kubernetes/kubernetes/issues/124065)

### Issue 内容

#### What happened?

A `StatefulSet` pod ends up "`Completed`":

```
NAME                                 READY   STATUS      RESTARTS   AGE
an-sts-pod-0                         0/1     Completed   0          4d12h
```

This pod has a `restartPolicy: Always`.

```
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 22 Mar 2024 05:12:42 -0400
      Finished:     Sun, 24 Mar 2024 12:11:32 -0400
```

The pod terminated due to something external to it (presumably the kubelet) terminating it:
```
[2024-03-24T16:11:29Z INFO  actix_server::server] SIGTERM received; starting graceful shutdown
[2024-03-24T16:11:29Z DEBUG actix_server::accept] paused accepting connections on 0.0.0.0:8080
[2024-03-24T16:11:29Z INFO  actix_server::accept] accept thread stopped
[2024-03-24T16:11:29Z INFO  actix_server::worker] shutting down idle worker
[2024-03-24T16:11:29Z INFO  actix_server::worker] shutting down idle worker
[2024-03-24T16:11:29Z INFO  actix_server::worker] shutting down idle worker
[2024-03-24T16:11:30Z DEBUG tower::buffer::worker] buffer closing; waking pending tasks
```

Shortly prior to this, the `kubelet` is restarted:
```
Mar 24 16:10:41 gke-[snip]-bbzn systemd[1]: kubelet.service: Sent signal SIGTERM to main process 1874 (kubelet) on client request.
```
It then seems to restart, and presumably `SIGTERM`s everything? Its logs are chock full of errors after the restart.
```
Mar 24 16:11:28 gke-[snip]-bbzn kubelet[4021733]: I0324 16:11:28.577436 4021733 eviction_manager.go:174] "Failed to admit pod to node" pod="ns/an-sts-pod-0" nodeCondition=[MemoryPressure]
```
(there are lots of these, presumably one for each pod on the node)
Then it seems like the pod dies, and is attempted to be restarted?
```
Mar 24 16:11:28 gke-[snip]-bbzn kubelet[4021733]: I0324 16:11:28.588544 4021733 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="ns/an-sts-pod-0" event=&{ID:44a879b9-bd14-43ec-8d
23-730abcb24f60 Type:ContainerDied Data:73c54c77f4bdc8f227d7ff45e662d05f7aeb2f5374712f338c66feab2dcd0c38}
Mar 24 16:11:28 gke-[snip]-bbzn kubelet[4021733]: I0324 16:11:28.588564 4021733 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="ns/an-sts-pod-0" event=&{ID:44a879b9-bd14-43ec-8d
23-730abcb24f60 Type:ContainerStarted Data:50348bfeb6c694e1504af763602f94786c47b2cb5299747cb0e6da30b01ddc68}
Mar 24 16:11:28 gke-[snip]-bbzn kubelet[4021733]: I0324 16:11:28.588580 4021733 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="ns/an-sts-pod-0" event=&{ID:44a879b9-bd14-43ec-8d
23-730abcb24f60 Type:ContainerStarted Data:aa9e60de9dd97a3f282c4df133656fa7b6e5cea40fede3c1422b654cb9f438ea}
```
but it never happens:
```
Mar 24 16:11:32 gke-[snip]-bbzn kubelet[4021733]: E0324 16:11:32.666674 4021733 secret.go:194] Couldn't get secret ns/a-secret-this-pod-needs: object "ns"/"a-secret-this-pod-needs" not registered
```
There are *lots* of these, for numerous secrets. These persist for at least the next 8 seconds.

The next time the pod's name shows up in the log is several minute later (so presumably the "object not registered" error abates?)
```
Mar 24 16:16:37 gke-[snip]-bbzn kubelet[4021733]: E0324 16:16:37.047670 4021733 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="44a879b9-bd14-43ec-8d23-730abcb24f60" containerName="an-sts-pod"
Mar 24 16:16:37 gke-[snip]-bbzn kubelet[4021733]: I0324 16:16:37.047682 4021733 state_mem.go:107] "Deleted CPUSet assignment" podUID="44a879b9-bd14-43ec-8d23-730abcb24f60" containerName="an-sts-pod"
```

After that, it seems wedged. The `kubelet` appears to never attempt to restart the pod, despite the restart policy.

#### What did you expect to happen?

The `StatefulSet` controller to restart the pod, or the `kubelet` to restart the pod; I'm not clear which of these two would be responsible.

#### How can we reproduce it (as minimally and precisely as possible)?

I'm not clear on what gets me into this situation.

#### Anything else we need to know?

There's an old SO thread out there that claims that Docker restarts cause this. That bug seems to be in an utterly ancient version (1.x) of Docker; I am past the fixed version for that bug.

Regardless, I checked the logs for `docker`; the Docker daemon does not appear to have restarted when the pod died.

Shortly prior to the `Finished` timestamp, Cloud Logs indicate that the node that pod was on was under Memory Pressure. Presumably, it terminated that pod due to that, but nonetheless, I would expect it to then get evicted.

#### Kubernetes version

```console
$ kubectl version
Server Version: v1.27.7-gke.1121002
```


#### Cloud provider

GCP, GKE

#### OS version

<details>

```console
# On Linux:
# cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_CRASH_ID=Lakitu
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=37c79a2c2008543e2c9a5dc749faa91fb0d806b5
VERSION=105
VERSION_ID=105
BUILD_ID=17412.226.62
# paste output here
# uname -a
Linux gke-[snip] 5.15.133+ #1 SMP Sat Dec 30 13:01:38 UTC 2023 x86_64 Intel(R) Xeon(R) CPU @ 2.20GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

`containerd://1.7.10`


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

(Dunno.)


### 分析结果

不涉及

---

## Issue #124058 Failed to watch *v1.Service: failed to list *v1.Service

- Issue 链接：[#124058](https://github.com/kubernetes/kubernetes/issues/124058)

### Issue 内容

#### What happened?

Installation Debian 12. Prestine, no additional configurations were done to the system aside from installing kubernetes.
For a detailed setup please see https://github.com/kubernetes/kubernetes/issues/123959
CRI containerd 1.6
 cluster is not responsive.
kubectl fails.
This is directly related to 

#### What did you expect to happen?

Cluster is healthy. `kubectl get pods` is returning without an error.

#### How can we reproduce it (as minimally and precisely as possible)?

Please see https://github.com/kubernetes/kubernetes/issues/123959

#### Anything else we need to know?

No OOM event occurred.
No configurations other than providing software to run kubernetes (see related issue) happened.
No explicit manual alterations have been done to the firewall.

[log_without_time.txt](https://github.com/kubernetes/kubernetes/files/14757788/log_without_time.txt)
[iptables.txt](https://github.com/kubernetes/kubernetes/files/14757796/iptables.txt)


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124057 NewVolumeManagerReconstruction: Volumes are reported as unmounted after reboot

- Issue 链接：[#124057](https://github.com/kubernetes/kubernetes/issues/124057)

### Issue 内容

#### What happened?

Kubernetes 1.27 with `NewVolumeManagerReconstruction` feature gate **enabled** and `SELinuxMountReadWriteOncePod` **disabled**, a rebooted node is not able to re-start pods that were running there before the reboot.

The node reports in Pod events:

```
Warning  FailedMount             2m4s (x5 over 11m)  kubelet                  Unable to attach or mount volumes: unmounted volumes=[myvol], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition
```

#### What did you expect to happen?

The Pod should start just fine.

#### How can we reproduce it (as minimally and precisely as possible)?

On Kubernetes 1.27:
1. Enable NewVolumeManagerReconstruction feature gate.
2. Run a StatefulSet that uses a volume.
3. Force-reboot the node which runs the StatefulSet Pod.

#### Anything else we need to know?

The reason is that kubelet's VolumeManager reconstructs the volume as _uncertain_ during startup and it uses a provisional _outer volume name_ for it, because it has not seen the API server and Pods in it yet. This provisional volume ID is then used in ActualStateOfWorld forever.

While kubelet _does_ mount the volume after reboot, it's not able to match it with the Pod that needs it. Kubelet reports `unmounted volumes=[myvol]`, while the volume is mounted with ID `pvc-6357c1ec-bac1-45fd-822f-2d6a014a9721` in my case (= the provisional name, we use PV.name for it) and not `myvol` (=`pod.spec.volumes[*].name`).

It is not reproducible with `SELinuxMountReadWriteOncePod` enabled.

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.1-3530+d715b057d9767f-dirty", GitCommit:"d715b057d9767fd7b9ea4792888e3336e1b36ed1", GitTreeState:"dirty", BuildDate:"2024-03-26T08:51:59Z", GoVersion:"go1.20.12 X:strictfipsruntime", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
AWS + AWS EBS CSI driver, but it should not be relevant.
</details>


#### OS version

<details>

```console
PRETTY_NAME="Red Hat Enterprise Linux CoreOS 414.92.202403251839-0 (Plow)"
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124054 K8s workers NotReady, Kubelet stopped posting node status, kubelet service log "Error getting node" err="node \"workernode\" not found" "failed to ensure lease exists"

- Issue 链接：[#124054](https://github.com/kubernetes/kubernetes/issues/124054)

### Issue 内容

#### What happened?

All of our cluster's worker has gone NotReady status. 
![image](https://github.com/kubernetes/kubernetes/assets/81574913/e25eb4c1-0197-425d-bfc9-472370c8dc26)
![image](https://github.com/kubernetes/kubernetes/assets/81574913/4b68edb2-befd-43c7-b64b-2205f64d73f0)

Inspecting one of the workers kubelet log shows below 

![image](https://github.com/kubernetes/kubernetes/assets/81574913/87ea659f-5f95-49d9-832d-9a41996e2678)


#### What did you expect to happen?

Workers to be working fine as usual.

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure.

#### Anything else we need to know?

A week ago we got an error X509: Certificate Has Expired Or Is Not Yet Valid, we updated the certificate using kubeadm certs renew all and it is working fine

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.12", GitCommit:"b058e1760c79f46a834ba59bd7a3486ecf28237d", GitTreeState:"clean", BuildDate:"2022-07-13T14:59:18Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.12", GitCommit:"b058e1760c79f46a834ba59bd7a3486ecf28237d", GitTreeState:"clean", BuildDate:"2022-07-13T14:53:39Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
On premises machine
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 22.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.1 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
- Containerd v1.5.9-0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
#### ackage Info
- kubernetes v1.22.12
- keepalived v2.2.4
- haproxy v

#### Kubernetes Services Version

- Calico v3.21
- Kubeflow v1.6
</details>


### 分析结果

不涉及

---

## Issue #124052 ResourceQuota error message typo

- Issue 链接：[#124052](https://github.com/kubernetes/kubernetes/issues/124052)

### Issue 内容

#### What happened?

On the document, the operator should be 'Exists'


![スクリーンショット 2024-03-26 昼3 35 18](https://github.com/kubernetes/kubernetes/assets/600238/c180e7a0-b080-41c6-afb9-56e2e6101465)

But in the Error message, it becomes 'Exist'
```
Invalid value: "Exist": must be 'Exist' when scope is any of ResourceQuotaScopeTerminating, ResourceQuotaScopeNotTerminating, ResourceQuotaScopeBestEffort, ResourceQuotaScopeNotBestEffort or ResourceQuotaScopeCrossNamespacePodAffinity
```
![スクリーンショット 2024-03-26 昼3 36 09](https://github.com/kubernetes/kubernetes/assets/600238/267e3126-13e0-4d0f-93c9-7a2a41491274)



#### What did you expect to happen?

the error message should be 
```
Invalid value: "Exist": must be 'Exists' when scope is any of ResourceQuotaScopeTerminating, ResourceQuotaScopeNotTerminating, ResourceQuotaScopeBestEffort, ResourceQuotaScopeNotBestEffort or ResourceQuotaScopeCrossNamespacePodAffinity
```

#### How can we reproduce it (as minimally and precisely as possible)?

create a yaml and apply it
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: user-quota
spec:
  hard:
    pods: "200"
    requests.cpu: "4"
    requests.memory: 4Gi
    limits.cpu: "1200"
    limits.memory: 8Gi
  scopeSelector:
    matchExpressions:
      - scopeName: CrossNamespacePodAffinity
        operator: Exist
```


#### Anything else we need to know?

It's my first time, I am not sure should we change `ResourceQuotaScopeTerminating` to `Terminating`?

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.11+k3s1", GitCommit:"c14436a9ecfffb3be553a06bb0a4fac6122579ce", GitTreeState:"clean", BuildDate:"2023-03-10T22:03:01Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.11+k3s1", GitCommit:"c14436a9ecfffb3be553a06bb0a4fac6122579ce", GitTreeState:"clean", BuildDate:"2023-03-10T22:03:01Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
```


</details>


#### Cloud provider

<details>
K3S
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux mbvm230702 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124050 Server-side apply has trouble with empty map

- Issue 链接：[#124050](https://github.com/kubernetes/kubernetes/issues/124050)

### Issue 内容

#### What happened?

Every time I server-side apply a `Deployment` with `spec.template.metadata.annotations: {}`, the `resourceVersion` updates. Nothing about my manifest changes between applies; if I map `annotations` to the empty map, `kubectl apply --server-side` the same manifest updates the `resourceVersion`. With client-side apply, the `resourceVersion` stays the same as long as my manifest remains constant, even if `annotations: {}`.

I read in #70281 that Kubernetes should treat absent keys, empty values (in this case, an empty map), and `null` values the same. I observe that leaving the `annotations` key out and `annotations: null` behave as expected, not modifying the `Deployment` and leaving `resourceVersion` the same, but empty map `{}` behaves as described above.

If we cannot change this behavior, I'd at least like to understand the rationale behind it.

#### What did you expect to happen?

I expected my `Deployment` object to stay the same between identical server-side applies, where the manifest file content was exactly the same, regardless of the value of `spec.template.metadata.annotations`. A different `resourceVersion` every apply indicates that the object was event type `MODIFIED`. I expect no events to fire on applying the same manifest.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Say `deployment.yaml` contains https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment. In actuality, any `Deployment` is fine.
2. Add `spec.template.metadata.annotatations: {}`.
3. `kubectl apply --server-side deployment.yaml`
4. `kubectl get deployment nginx-deployment -o yaml`

Every successive repeat of steps 3-4 should show different `metadata.resourceVersion` values.

Example of `Deployment` with `spec.template.metadata.annotatations: {}`:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      annotations: {}
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

#### Anything else we need to know?

This is especially a problem when I server-side apply with code in my operator. My operator is built on https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern and watches changes to the child objects it deploys. If I create a `Deployment` with `spec.template.metadata.annotations: {}`, the operator constantly reconciles because every server-side apply of the manifest registers as a type `MODIFIED` event on the object, which triggers the watching operator to reconcile.

#### Kubernetes version

```console
$ kubectl version --short
Client Version: v1.27.11
Kustomize Version: v5.0.1
Server Version: v1.27.8-gke.1067004
```


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
ID=debian
$ uname -a
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124046 1.30 Release Notes: "Known Issues"

- Issue 链接：[#124046](https://github.com/kubernetes/kubernetes/issues/124046)

### Issue 内容

#### What happened?

This issue is a bucket placeholder for collaborating on the "Known Issues" additions for the 1.30 Release Notes. If you know of issues or API changes that are going out in 1.30, please comment here so that we can coordinate incorporating information about these changes in the Release Notes.

/assign @kubernetes/release-team-release-notes

/sig release
/milestone v1.30

### 分析结果

不涉及

---

## Issue #124043 DRA: kubelet: crash when plugin returns nil UnprepareResourceResponse

- Issue 链接：[#124043](https://github.com/kubernetes/kubernetes/issues/124043)

### Issue 内容

#### What happened?

protobuf doesn't require that objects get encoded when they are empty. This can lead to a nil pointer in https://github.com/kubernetes/kubernetes/issues/124042 (although that's probably an error because some information is needed) and in https://github.com/kubernetes/kubernetes/blob/20d0ab7ae808aaddb1556c3c38ca0607663c50ac/staging/src/k8s.io/kubelet/pkg/apis/dra/v1alpha3/api.pb.go#L255 (happened in practice).

The result is a crash of kubelet:
```
E0305 14:50:50.162844   10432 runtime.go:79] Observed a panic: "invalid memory address or nil pointer dereference" (runtime error: invalid memory address or nil pointer dereference)
goroutine 883 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x3de2d20?, 0x6fc0d90})
	/go/src/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x85
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x0?})
	/go/src/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x6b
panic({0x3de2d20?, 0x6fc0d90?})
	/usr/local/go/src/runtime/panic.go:914 +0x21f
k8s.io/kubernetes/pkg/kubelet/cm/dra.(*ManagerImpl).UnprepareResources(0xc00097b998, 0xc000d48900)
	/go/src/kubernetes/pkg/kubelet/cm/dra/manager.go:375 +0x67c
k8s.io/kubernetes/pkg/kubelet/cm.(*containerManagerImpl).UnprepareDynamicResources(0xc00027f5c0?, 0x477a71c?)
	/go/src/kubernetes/pkg/kubelet/cm/container_manager_linux.go:1021 +0x22
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).UnprepareDynamicResources(...)
	/go/src/kubernetes/pkg/kubelet/kubelet.go:3055
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncTerminatingPod(0xc0004e3000, {0xc0010b2510?, 0x24?}, 0xc000d48900, 0xed8569a108?, 0x0, 0x0)
	/go/src/kubernetes/pkg/kubelet/kubelet.go:2082 +0x1a0b
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1({0x1, {0x0, {0x0, 0x0, 0x0}, 0xc000d48900, 0x0, 0x0, 0x0}}, 0xc0004c4dc0, ...)
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:1279 +0x17d
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc0010b2510?, {0xc0010b2510, 0x24}, 0x0?)
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:1288 +0x47b
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:950 +0x115
created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 230
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:945 +0x221b
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x209f8bc]

goroutine 883 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x0?})
	/go/src/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:56 +0xcd
panic({0x3de2d20?, 0x6fc0d90?})
	/usr/local/go/src/runtime/panic.go:914 +0x21f
k8s.io/kubernetes/pkg/kubelet/cm/dra.(*ManagerImpl).UnprepareResources(0xc00097b998, 0xc000d48900)
	/go/src/kubernetes/pkg/kubelet/cm/dra/manager.go:375 +0x67c
k8s.io/kubernetes/pkg/kubelet/cm.(*containerManagerImpl).UnprepareDynamicResources(0xc00027f5c0?, 0x477a71c?)
	/go/src/kubernetes/pkg/kubelet/cm/container_manager_linux.go:1021 +0x22
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).UnprepareDynamicResources(...)
	/go/src/kubernetes/pkg/kubelet/kubelet.go:3055
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncTerminatingPod(0xc0004e3000, {0xc0010b2510?, 0x24?}, 0xc000d48900, 0xed8569a108?, 0x0, 0x0)
	/go/src/kubernetes/pkg/kubelet/kubelet.go:2082 +0x1a0b
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1({0x1, {0x0, {0x0, 0x0, 0x0}, 0xc000d48900, 0x0, 0x0, 0x0}}, 0xc0004c4dc0, ...)
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:1279 +0x17d
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc0010b2510?, {0xc0010b2510, 0x24}, 0x0?)
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:1288 +0x47b
k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:950 +0x115
created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 230
	/go/src/kubernetes/pkg/kubelet/pod_workers.go:945 +0x221b
```

(from 1.29.1).

#### What did you expect to happen?

No crash.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the DRA-enabled Akri plugin.

#### Anything else we need to know?

Reported by @diconico07 in https://kubernetes.slack.com/archives/C0BP8PW9G/p1709647270084129

#### Kubernetes version

Several, but we shouldn't (?) backport because it was alpha.


#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #124039 GracefulNodeShutdown does not trigger on nodes shutdown with `shutdown -h now`

- Issue 链接：[#124039](https://github.com/kubernetes/kubernetes/issues/124039)

### Issue 内容

#### What happened?

Shutting down a node with `shutdown -h now` does not trigger the graceful node drain. 

Despite being indicated in the original proposal
https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2000-graceful-node-shutdown

> In the context of this KEP, shutdown is referred to as shutdown of the underlying machine. On most linux distros shutdown can be initiated via a variety of methods for example:
>
> 1. shutdown -h now
> 2. shutdown -h +30 #schedule a delayed shutdown in 30mins

and explained in the blog:
https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/

> How does it work?
> On Linux, your system can shut down in many different situations. For example:
>
> - A user or script running shutdown -h now or systemctl poweroff or systemctl reboot.

the command seems to not behave the same way as sending a PrepareForShutdown signal on the dbus.

The node does not drain itself from its pods and pods will stay Running until timeout and it becomes stuck at Terminating 

On the other hand, `shutdown -h now` acts similarly as a forced shutdown and as I understand, a forced shutdown on a node doesn't allow a grace period as it is forced; so one can argue that there is nothing wrong with the behavior. However, the need to clarify the documentation on what counts as a shutdown that can trigger GracefulNodeShutdown may be helpful.

#### What did you expect to happen?

Expecting a similar behavior as if executing
`# systemctl poweroff`
or
`#dbus-send --system /org/freedesktop/login1 org.freedesktop.login1.Manager.PrepareForShutdown boolean:true`

where the graceful node drain occurs and terminates the pods.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start a cluster with kubeadm init with this config. I use 2 worker nodes by default but can be reproduced with 1.
```
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
networking:
  podSubnet: "10.244.0.0/16"  #flannel
controlPlaneEndpoint: "XXX.XXX.XXX.XXX"
---
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
featureGates:
  GracefulNodeShutdown: true
shutdownGracePeriod: 90s
shutdownGracePeriodCriticalPods: 30s
kind: KubeletConfiguration                   
```
2.  Make a deployment or a statefulset; I used this statefulset config in my environment: https://github.com/kubernetes/kubernetes/issues/118310#issuecomment-1698823460

3. Monitor the pods on a terminal logged SSHed the control plane:

`kubectl get pods --watch`

4. On a terminal logged in a worker node, execute shutdown with the following command:

`shutdown -h now`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```
tested on kubernetes versions 1.26, 1.27, 1.28, 1.29
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Stream"
VERSION="8"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="8"
PLATFORM_ID="platform:el8"
PRETTY_NAME="CentOS Stream 8"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:8"
HOME_URL="https://centos.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_SUPPORT_PRODUCT_VERSION="CentOS Stream"
$ uname -a
Linux localhost.localdomain 4.18.0-544.el8.x86_64 #1 SMP Thu Feb 22 21:29:23 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124016 node should not be ready when klog flush deamon in kubelet is block in fsync

- Issue 链接：[#124016](https://github.com/kubernetes/kubernetes/issues/124016)

### Issue 内容

#### What happened?

1. some issues cause a part of  fsync syscalls to persist in blocking
2. klog flush deamon hold the lock, call `flushAll` and block in fsync syscall 

https://github.com/kubernetes/kubernetes/blob/a309fadbac3339bc8db9ae0a928a33b8e81ef10f/vendor/k8s.io/klog/v2/klog.go#L1223-L1228

4. other goroutines in kubelet will be block when print log
https://github.com/kubernetes/kubernetes/blob/a309fadbac3339bc8db9ae0a928a33b8e81ef10f/vendor/k8s.io/klog/v2/klog.go#L885-L894

5. kubelet can renew node lease normally because this goroutine do not print log
6. node controller think this node is ready

#### What did you expect to happen?

node should be not ready

#### How can we reproduce it (as minimally and precisely as possible)?

maybe use ptrace to block fsync?

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.26.0
<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

none
<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123992 strictARP Configuration in kube-proxy (IPVS Mode) Does Not Revert sysctl Parameters on Setting to false

- Issue 链接：[#123992](https://github.com/kubernetes/kubernetes/issues/123992)

### Issue 内容

#### What happened?

When `strictARP` is set to `true` in kube-proxy's configuration for IPVS mode, kube-proxy modifies certain `sysctl` parameters as expected. However, if the `strictARP` is later changed to `false`, the modified `sysctl` parameters are not reverted to their original values.

Upon inspecting the source code, I noticed that there is logic to apply changes when `strictARP` is `true`, but there appears to be no corresponding logic to revert these changes when `strictARP` is set to `false`.

#### What did you expect to happen?

revert

#### How can we reproduce it (as minimally and precisely as possible)?

master branch

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #123989 GKE schedules pods that prevent scale down

- Issue 链接：[#123989](https://github.com/kubernetes/kubernetes/issues/123989)

### Issue 内容

#### What happened?

Original issue was closed for no reason: #69696 

GKE still schedules all kinds of pods that prevent scaledown

#### What did you expect to happen?

Pods not to prevent scaledown

#### How can we reproduce it (as minimally and precisely as possible)?

Run a cluster and try to scaledown after scaleup

#### Anything else we need to know?

_No response_

#### Kubernetes version

since 2018 until today

#### Cloud provider

Google

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #123980 restarting a kubelet should never affect the running workload

- Issue 链接：[#123980](https://github.com/kubernetes/kubernetes/issues/123980)

### Issue 内容

#### What happened?

1.Node label changed.
Node label changed is because the operation and maintenance engineer organized the node label, such as hpc=true, and removed this label. When a pod is compatible with this label, restarting the node kubelet will cause the pod to be rebuilt. This actually shouldn’t be the case. It will affect the normal operation of the business.
2.https://github.com/kubernetes/kubernetes/issues/123971
3.https://github.com/kubernetes/kubernetes/issues/123816
After the above three scenarios occur, as long as the kubelet is restarted, the pod will be evicted. Obviously, this is not as expected. It is also an undesirable result for online business.

#### What did you expect to happen?

Restarting a kubelet should not cause any disruption to the running workload (which likely will mean skip admission of running pods, but let's not run ahead of ourselves) and backlink to this issue is probably the best way forward still.

#### How can we reproduce it (as minimally and precisely as possible)?

Before restarting the kubelet, fully check the scenarios mentioned above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123971 pod with initcontainer failed with UnexpectedAdmissionError when restart kubelet

- Issue 链接：[#123971](https://github.com/kubernetes/kubernetes/issues/123971)

### Issue 内容

#### What happened?

Memory manager have bug, restart kubelet (do not delete any checkpoint file ), pod with init-container will failed.

#### What did you expect to happen?

Pod A and Pod B is still running.

#### How can we reproduce it (as minimally and precisely as possible)?

I have a node had 256Gi and 2 NUMA node with 128Gi for one NUMA;
1. Create Pod A, with init-container and container both 100Gi
2. Create Pod B, with init-container and container both 100Gi 
3. Restart kubelet.
4. Both Pod A and Pod B failed with UnexpectedAdmissionError

#### Anything else we need to know?

In `--topology-policy=single-numa-node` mode.
When memory manager caculate memory for topology, it will caculate init-container firstly, when comes to app container, it will reuse the init-container's memory as possible,  and reduce init-container's memory in state, it's as expected;


https://github.com/kubernetes/kubernetes/blob/9df98f3d393e204dd8db02322057ac2e7f3f6c28/pkg/kubelet/cm/memorymanager/policy_static.go#L192
https://github.com/kubernetes/kubernetes/blob/9df98f3d393e204dd8db02322057ac2e7f3f6c28/pkg/kubelet/cm/memorymanager/policy_static.go#L956-L965

But when restart kubelet, memory manager will re calculate memory for init-container,  if now node has not enough memory for init-container, the whole pod will be failed because of Admit failed;

<img width="1060" alt="image" src="https://github.com/kubernetes/kubernetes/assets/151703335/d455fd2e-b7a6-4661-825a-ae661d96d84a">

After restart kubelet:
<img width="1420" alt="image" src="https://github.com/kubernetes/kubernetes/assets/151703335/cd1f196d-b3c7-418f-b670-90bb51051321">



#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123968 Pod hang in terminating when secret/configmap deleted

- Issue 链接：[#123968](https://github.com/kubernetes/kubernetes/issues/123968)

### Issue 内容

#### What happened?

Try to delete pod, but hang
kubelet[28996]: E0318 00:59:17.040631   28996 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/configmap/9493862e-68db-470e-9954-7e43de8a0c75-iscsiconfig podName:9493862e-68db-470e-9954-7e43de8a0c75 nodeName:}" failed. No retries permitted until 2024-03-18 01:01:19.040616429 -0700 -07 m=+498.624350994 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "iscsiconfig" storageclass nil (UniqueName: "kubernetes.io/configmap/9493862e-68db-470e-9954-7e43de8a0c75-iscsiconfig") pod "fio-test-cb57bfd75-2bz89" (UID: "9493862e-68db-470e-9954-7e43de8a0c75") : configmap "iscsi-config" not found

#### What did you expect to happen?

pod successful terminating

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with configmap/secret volume
2. delete the configmap/secret
3. restart kubelet to trigger remount
4. after kubelet report configmap/secret MountVolume.SetUp failed, delete pod

#### Anything else we need to know?

_No response_

#### Kubernetes version

kubelet --version 
Kubernetes v1.23.6

#### Cloud provider

None


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #123959 Debian Bookworm installation not stable

- Issue 链接：[#123959](https://github.com/kubernetes/kubernetes/issues/123959)

### Issue 内容

#### What happened?

Upon Installing Kubernetes from scratch the cluster is briefly reachable via kubectl announcing that there are not pods in the default namespace as is to be expected upon executing `kubectl get pods` .
After some time passes  the installation itself will not be reachable any longer.

What is special about this is that it is a single node, but used to work before version 1.25 (It is a toysetup and since I missed the window for the shift(why are only the latest in the repository anyways and not all the latest supported versions(Debian 11 will not be happy with that given that they ship containerd 1.4.))

This might be related to https://github.com/kubernetes/kubernetes/issues/123837 .




#### What did you expect to happen?

The cluster, like all the installations before is stable and can receive pods upon untainting master to schedule pods.

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
systemctl mask swap.target 
modprobe br_netfilter 
echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf 
sysctl -p /etc/sysctl.conf 

apt-get update && 
apt-get install -y curl gnupg-agent fail2ban &&
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg &&
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list && 
curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -  &&
echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list &&
apt-get update && 
apt-get install -y apt-transport-https ca-certificates curl && 
apt install -y kubeadm kubectl containerd kubelet kubernetes-cni helm

cat << EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF


## this is optional, I ran it once or twice trying to debug, but the results were the same
# mkdir -p /etc/containerd
# containerd config default | sudo tee /etc/containerd/config.toml
# systemctl restart containerd



sysctl --system && 
echo 1 > /proc/sys/net/ipv4/ip_forward &&
systemctl reboot 

kubeadm init --pod-network-cidr=10.217.0.0/16   && 
mkdir $HOME/.kube &&
cat /etc/kubernetes/admin.conf > $HOME/.kube/config &&
kubectl taint node singleNode node-role.kubernetes.io/control-plane:NoSchedule- 
```

#### Anything else we need to know?

Keep in mind, this is a single node only(didn't give me any grief before though).
The installation is fresh so just the installation and the the installation of kubernetes. 

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.29.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
fresh install of debian 12
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.6
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123955 SecretProviderClass should be versioned similar to configmaps

- Issue 链接：[#123955](https://github.com/kubernetes/kubernetes/issues/123955)

### Issue 内容

#### What happened?

Let's say we have a service A, which currently depends on 2 secrets, B & C, provided by a `SecretProviderClass`.

Then, a new version of A removed the usage of secret C, so it now only need one secret B.

When deploying the new version, 2 things will happen:

1. The `SecretProviderClass` being updated to remove C and only provide B
2. The pods for A being _gradually_ replaced by the new version that no longer needs C

But since 1 is almost immediate while 2 is always a gradual rollout, after 1 is replaced the old pods still requiring both B and C will likely start to fail because the update of `SecretProviderClass` that removed C that they still require.

Even if we tweak the order to only do 1 after 2 is fully done, it can still cause the reverse problem (a new version that requires a new secret).

Currently, to avoid this issue we need to be extra careful and do 2-step deploys, first deploy a superset of `SecretProviderClass` that contains the secrets for both new and old version of code with the new code, and after that's fully done, do another deploy to shrink `SecretProviderClass` to only the ones needed by the new version.

But I think to fully address this problem, we should make `SecretProviderClass` versioned (similar to how configmaps are versioned), so every pod can use their matching version of `SecretProviderClass` to provide the secrets they need.

#### What did you expect to happen?

See above.

#### How can we reproduce it (as minimally and precisely as possible)?

See above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.27.10

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123954 Both Endpoints and EndpointSlices not working with Service abstraction for external IPs

- Issue 链接：[#123954](https://github.com/kubernetes/kubernetes/issues/123954)

### Issue 内容

#### What happened?

As recommend in https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors I did try to abstract access to an externally hosted mariaDB. However, both the recommend approach and the legacy approach with Endpoints did not work as expected. There is a slight difference however, in the EndpointSlice case the service description showed `Endpoints: <none>` while in the endpoints case it was empty, i.e. `Endpoints: `.



#### What did you expect to happen?

The external database should be reachable through the service abstraction.

#### How can we reproduce it (as minimally and precisely as possible)?

It is a clean K3s with --disable traefik, nginx ingress as well as cert-manager installed via helm, plus GitLab Agent for Kubernetes also using Helm.

The Service:

``` yaml
apiVersion: v1
kind: Service
metadata:
  name: mariadb
spec:
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
```

I've tried both, EndpointSlice:

``` yaml
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: mariadb-1
  labels:
    kubernetes.io/service-name: mariadb
addressType: IPv4
ports:
  - name: mysql
    port: 3306
    protocol: TCP
endpoints:
  - addresses:
    - 10.0.0.5
```

, and Endpoints:

``` yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: mariadb
subsets:
  - addresses:
      - ip: 10.0.0.5
    ports:
      - name: mysql
        port: 3306
```

Both were successfully deployed in the same namespace as the service and linked to the service.

## case: EndpointSlices

```
kubectl get endpointslice -l kubernetes.io/service-name=mariadb

NAME        ADDRESSTYPE   PORTS   ENDPOINTS   AGE
mariadb-1   IPv4          3306    10.0.0.5    3m29s
```
```
kubectl describe service mariadb

Name:              mariadb
Namespace:         keldysh-projects-workshop
Labels:            app.kubernetes.io/managed-by=Helm
Annotations:       meta.helm.sh/release-name: service
                   meta.helm.sh/release-namespace: keldysh-projects-workshop
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.126.23
IPs:               10.43.126.23
Port:              <unset>  3306/TCP
TargetPort:        3306/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
```

## case: Endpoints

```
kubectl describe endpoints mariadb

Name:         mariadb
Namespace:    keldysh-projects-workshop
Labels:       app.kubernetes.io/managed-by=Helm
Annotations:  meta.helm.sh/release-name: service
              meta.helm.sh/release-namespace: keldysh-projects-workshop
Subsets:
  Addresses:          10.0.0.5
  NotReadyAddresses:  <none>
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    mysql  3306  TCP

Events:  <none>
```
```
kubectl describe service mariadb

Name:              mariadb
Namespace:         keldysh-projects-workshop
Labels:            app.kubernetes.io/managed-by=Helm
Annotations:       meta.helm.sh/release-name: service
                   meta.helm.sh/release-namespace: keldysh-projects-workshop
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.126.23
IPs:               10.43.126.23
Port:              <unset>  3306/TCP
TargetPort:        3306/TCP
Endpoints:         
Session Affinity:  None
Events:            <none>
```

#### Anything else we need to know?

DNS is working in principle:

```
kubectl exec -i -t dnsutils -- nslookup mariadb

Server:         10.43.0.10
Address:        10.43.0.10#53

Name:   mariadb.keldysh-projects-workshop.svc.cluster.local
Address: 10.43.19.67
```


I already submitted a bug report in the K3s repository. They however claim to not change the core kubernetes functionality:
https://github.com/k3s-io/k3s/issues/9725
https://github.com/k3s-io/k3s/discussions/9726

#### Kubernetes version


K3s Version:
k3s version v1.28.7+k3s1 (https://github.com/k3s-io/k3s/commit/051b14b248655896fdfd7ba6c93db6182cde7431)
go version go1.21.7


#### Cloud provider

Self hosted K3s on Hetzner Cloud Servers

#### OS version

Linux kubernetes-test-server 5.15.0-91-generic https://github.com/k3s-io/k3s/issues/101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>



### 分析结果

不涉及。

---

## Issue #123948 Internal dns (svc.cluster.local) doesn't work after applying ipvs mode.

- Issue 链接：[#123948](https://github.com/kubernetes/kubernetes/issues/123948)

### Issue 内容

#### What happened?

Hello, i am new to k8s. First, i apologize for my English skill. :-(

I have trouble with accessing to service object from pods in another namespaces after **applying ipvs mode**. My nginx pod logs like below.

```bash
host not found in upstream "<svc-name>.<namespace-name>.svc.cluster.local:<port>"
```
However, It looks possible that accessing specific service objects from outside of cluster. (e.g. accessing nginx `NodePort` service object, accessing `ClusterIP` service object via portforwarding)


And then, i have installed `ipvsadm`, i loaded some kind of modules to activating ipvs mode.
```bash
user@randomhost:~$ lsmod | grep ip_vs
ip_vs_sh               12288  0
ip_vs_wrr              12288  0
ip_vs_rr               12288  49
ip_vs_lc               12288  0
ip_vs                 221184  57 ip_vs_rr,ip_vs_sh,ip_vs_wrr,ip_vs_lc
nf_conntrack          200704  6 xt_conntrack,nf_nat,xt_nat,nf_conntrack_netlink,xt_MASQUERADE,ip_vs
nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
libcrc32c              12288  4 nf_conntrack,nf_nat,nf_tables,ip_vs
```

```bash
user@randomhost:~$ sudo ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.10.50.6:30010 rr
  -> 10.244.240.231:80            Masq    1      0          0         
TCP  10.10.50.6:30020 rr
TCP  10.10.50.6:30040 rr
  -> 10.244.240.210:8080          Masq    1      0          0         
TCP  10.10.50.6:30100 rr
  -> 10.244.240.203:8080          Masq    1      0          0         
TCP  10.10.50.6:31580 rr
  -> 10.244.240.217:9090          Masq    1      0          0         
TCP  10.96.0.1:443 rr
  -> 10.10.50.6:6443              Masq    1      2          0         
TCP  10.96.0.10:53 rr
  -> 10.244.169.6:53              Masq    1      0          0         
  -> 10.244.240.215:53            Masq    1      0          0         
TCP  10.96.0.10:9153 rr
  -> 10.244.169.6:9153            Masq    1      0          0         
  -> 10.244.240.215:9153          Masq    1      0          0         
TCP  10.96.66.35:8080 rr
  -> 10.244.240.199:8080          Masq    1      0          0         
TCP  10.96.77.112:80 rr
  -> 10.244.240.217:9090          Masq    1      0          0         
TCP  10.96.197.131:8080 rr
  -> 10.244.240.255:8080          Masq    1      0          0         
TCP  10.98.190.64:9400 rr
  -> 10.244.240.205:9400          Masq    1      0          0         
TCP  10.98.211.200:9091 rr
  -> 10.244.240.246:9091          Masq    1      0          0         
TCP  10.99.124.7:8080 rr
  -> 10.244.240.213:8080          Masq    1      0          0         
TCP  10.100.12.75:9093 rr
  -> 10.244.240.220:9093          Masq    1      0          0         
TCP  10.100.42.9:8080 rr
  -> 10.244.240.225:8080          Masq    1      0          0         
TCP  10.100.182.211:6379 rr
  -> 10.244.240.230:6379          Masq    1      0          0         
TCP  10.100.213.41:9100 rr
  -> 10.10.50.5:9100              Masq    1      0          0         
  -> 10.10.50.6:9100              Masq    1      0          0         
TCP  10.100.216.98:8080 rr
  -> 10.244.240.198:8080          Masq    1      0          0         
TCP  10.100.223.0:8080 rr
  -> 10.244.240.209:8080          Masq    1      0          0         
TCP  10.101.100.218:8080 rr
  -> 10.244.240.238:8080          Masq    1      0          0         
TCP  10.101.139.28:9001 rr
  -> 10.244.240.235:9001          Masq    1      0          0         
TCP  10.101.215.114:80 rr
  -> 10.244.240.193:8080          Masq    1      0          0         
TCP  10.101.215.114:443 rr
  -> 10.244.240.193:8080          Masq    1      0          0         
TCP  10.102.197.12:9114 rr
TCP  10.103.254.124:8081 rr
  -> 10.244.240.229:8081          Masq    1      0          0         
TCP  10.104.27.215:8080 rr
  -> 10.244.240.203:8080          Masq    1      0          0         
TCP  10.104.234.123:8080 rr
  -> 10.244.169.4:8080            Masq    1      0          0         
TCP  10.105.53.9:8080 rr
  -> 10.244.240.222:8080          Masq    1      0          0         
  -> 10.244.240.223:8080          Masq    1      0          0         
  -> 10.244.240.224:8080          Masq    1      0          0         
  -> 10.244.240.228:8080          Masq    1      0          0         
  -> 10.244.240.234:8080          Masq    1      0          0         
  -> 10.244.240.237:8080          Masq    1      0          0         
  -> 10.244.240.240:8080          Masq    1      0          0         
  -> 10.244.240.241:8080          Masq    1      0          0         
TCP  10.106.23.135:80 rr
  -> 10.244.240.231:80            Masq    1      0          0         
TCP  10.107.149.189:8080 rr
  -> 10.244.240.194:8080          Masq    1      0          0         
TCP  10.109.119.195:5556 rr
  -> 10.244.240.254:5556          Masq    1      0          0         
TCP  10.109.119.195:5557 rr
  -> 10.244.240.254:5557          Masq    1      0          0         
TCP  10.109.161.105:8080 rr
  -> 10.244.240.210:8080          Masq    1      0          0         
TCP  10.110.137.110:80 rr
TCP  10.110.152.189:8080 rr
  -> 10.244.169.3:8080            Masq    1      0          0         
TCP  10.110.204.48:7000 rr
  -> 10.244.240.216:7000          Masq    1      0          0         
TCP  10.111.130.154:80 rr
  -> 10.244.240.253:3000          Masq    1      0          0         
TCP  10.244.0.0:30010 rr
  -> 10.244.240.231:80            Masq    1      0          0         
TCP  10.244.0.0:30020 rr
TCP  10.244.0.0:30040 rr
  -> 10.244.240.210:8080          Masq    1      0          0         
TCP  10.244.0.0:30100 rr
  -> 10.244.240.203:8080          Masq    1      0          0         
TCP  10.244.0.0:31580 rr
  -> 10.244.240.217:9090          Masq    1      0          0         
TCP  10.244.169.0:30010 rr
  -> 10.244.240.231:80            Masq    1      0          0         
TCP  10.244.169.0:30020 rr
TCP  10.244.169.0:30040 rr
  -> 10.244.240.210:8080          Masq    1      0          0         
TCP  10.244.169.0:30100 rr
  -> 10.244.240.203:8080          Masq    1      0          0         
TCP  10.244.169.0:31580 rr
  -> 10.244.240.217:9090          Masq    1      0          0         
UDP  10.96.0.10:53 rr
  -> 10.244.169.6:53              Masq    1      0          0         
  -> 10.244.240.215:53            Masq    1      0          0
```

Is there something i omitted?
I appreciate that read this issue.

#### What did you expect to happen?

I want to access service object in another namespace via (`svc.cluster.local`) from another pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. initialize k8s cluster via `kubeadm` (version : 1.28.x)
2. install calico CNI (I installed flannel CNI at first time and then migrate to calico CNI)
3. Edit configmap `kube-proxy`
```yaml
mode: "ipvs"
```
4. Delete `kube-proxy` pods and waiting for reproduce.

#### Anything else we need to know?

##### Configmap for kube-proxy
```bash
user@randomhost:~$ kubectl describe configmap kube-proxy -n kube-system

apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
bindAddressHardFail: false
clientConnection:
  acceptContentTypes: ""
  burst: 0
  contentType: ""
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  qps: 0
clusterCIDR: 10.244.0.0/16
configSyncPeriod: 0s
conntrack:
  maxPerCore: null
  min: null
  tcpCloseWaitTimeout: null
  tcpEstablishedTimeout: null
detectLocal:
  bridgeInterface: ""
  interfaceNamePrefix: ""
detectLocalMode: ""
enableProfiling: false
healthzBindAddress: ""
hostnameOverride: ""
iptables:
  localhostNodePorts: null
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: ""
  strictARP: false
  syncPeriod: 0s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
kind: KubeProxyConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
metricsBindAddress: ""
mode: ""
nodePortAddresses: null
oomScoreAdj: null
portRange: ""
showHiddenMetricsForVersion: ""
winkernel:
  enableDSR: false
  forwardHealthCheckVip: false
  networkName: ""
  rootHnsEndpointName: ""
  sourceVip: ""
kubeconfig.conf:
```
##### `/etc/resolve.conf`
```bash
user@randomhost:~$ cat /etc/resolv.conf
# This is /run/systemd/resolve/stub-resolv.conf managed by man:systemd-resolved(8).
# Do not edit.
#
# This file might be symlinked as /etc/resolv.conf. If you're looking at
# /etc/resolv.conf and seeing this text, you have followed the symlink.
#
# This is a dynamic resolv.conf file for connecting local clients to the
# internal DNS stub resolver of systemd-resolved. This file lists all
# configured search domains.
#
# Run "resolvectl status" to see details about the uplink DNS servers
# currently in use.
#
# Third party programs should typically not access this file directly, but only
# through the symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a
# different way, replace this symlink by a static file or a different symlink.
#
# See man:systemd-resolved.service(8) for details about the supported modes of
# operation for /etc/resolv.conf.

nameserver 127.0.0.53
options edns0 trust-ad
search .
search 8.8.4.4
nameserver 8.8.8.8
```

##### Pod log for kube-proxy 
```bash
user@randomhost:~$ kubectl logs -f kube-proxy-ptwtv -n kube-system
I0315 01:08:48.212186       1 node.go:141] Successfully retrieved node IP: 10.10.50.6
I0315 01:08:48.213268       1 conntrack.go:52] "Setting nf_conntrack_max" nfConntrackMax=131072
I0315 01:08:48.223455       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0315 01:08:48.229596       1 server_others.go:218] "Using ipvs Proxier"
I0315 01:08:48.229722       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0315 01:08:48.229743       1 server_others.go:438] "Defaulting to no-op detect-local"
I0315 01:08:48.229928       1 ipset.go:116] "Ipset name truncated" ipSetName="KUBE-6-LOAD-BALANCER-SOURCE-CIDR" truncatedName="KUBE-6-LOAD-BALANCER-SOURCE-CID"
I0315 01:08:48.229955       1 ipset.go:116] "Ipset name truncated" ipSetName="KUBE-6-NODE-PORT-LOCAL-SCTP-HASH" truncatedName="KUBE-6-NODE-PORT-LOCAL-SCTP-HAS"
I0315 01:08:48.229988       1 server.go:846] "Version info" version="v1.28.6"
I0315 01:08:48.229998       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0315 01:08:48.230266       1 config.go:315] "Starting node config controller"
I0315 01:08:48.230292       1 shared_informer.go:311] Waiting for caches to sync for node config
I0315 01:08:48.230403       1 config.go:188] "Starting service config controller"
I0315 01:08:48.230411       1 shared_informer.go:311] Waiting for caches to sync for service config
I0315 01:08:48.230420       1 config.go:97] "Starting endpoint slice config controller"
I0315 01:08:48.230421       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0315 01:08:48.331128       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0315 01:08:48.331140       1 shared_informer.go:318] Caches are synced for node config
I0315 01:08:48.331172       1 shared_informer.go:318] Caches are synced for service config
```

##### iptables
```bash
user@randomhost:~$ sudo iptables --list
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
cali-INPUT  all  --  anywhere             anywhere             /* cali:Cz_u1IQiXIMmKD4c */
KUBE-IPVS-FILTER  all  --  anywhere             anywhere             /* kubernetes ipvs access filter */
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             /* kube-proxy firewall rules */
KUBE-NODE-PORT  all  --  anywhere             anywhere             /* kubernetes health check rules */
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes health check service ports */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         
cali-FORWARD  all  --  anywhere             anywhere             /* cali:wUHhoiAYhphO9Mso */
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             /* kube-proxy firewall rules */
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
FLANNEL-FWD  all  --  anywhere             anywhere             /* flanneld forward */
ACCEPT     all  --  anywhere             anywhere             /* cali:S93hcgKJrXEqnTfs */ /* Policy explicitly accepted packet. */ mark match 0x10000/0x10000
MARK       all  --  anywhere             anywhere             /* cali:mp77cMpurHhyjLrM */ MARK or 0x10000

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
cali-OUTPUT  all  --  anywhere             anywhere             /* cali:tVnHkvAo15HuiPy0 */
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            

Chain FLANNEL-FWD (1 references)
target     prot opt source               destination         
ACCEPT     all  --  randomhost/16          anywhere             /* flanneld forward */
ACCEPT     all  --  anywhere             randomhost/16          /* flanneld forward */

Chain KUBE-EXTERNAL-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-FIREWALL (2 references)
target     prot opt source               destination         
DROP       all  -- !localhost/8          localhost/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT

Chain KUBE-FORWARD (1 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere             ctstate INVALID
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */ mark match 0x4000/0x4000
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED

Chain KUBE-IPVS-FILTER (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             anywhere             match-set KUBE-LOAD-BALANCER dst,dst
RETURN     all  --  anywhere             anywhere             match-set KUBE-CLUSTER-IP dst,dst
RETURN     all  --  anywhere             anywhere             match-set KUBE-EXTERNAL-IP dst,dst
RETURN     all  --  anywhere             anywhere             match-set KUBE-EXTERNAL-IP-LOCAL dst,dst
RETURN     all  --  anywhere             anywhere             match-set KUBE-HEALTH-CHECK-NODE-PORT dst
REJECT     all  --  anywhere             anywhere             ctstate NEW match-set KUBE-IPVS-IPS dst reject-with icmp-port-unreachable

Chain KUBE-KUBELET-CANARY (0 references)
target     prot opt source               destination         

Chain KUBE-NODE-PORT (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* Kubernetes health check node port */ match-set KUBE-HEALTH-CHECK-NODE-PORT dst

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination         

Chain KUBE-PROXY-CANARY (0 references)
target     prot opt source               destination         

Chain KUBE-PROXY-FIREWALL (5 references)
target     prot opt source               destination         

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-SOURCE-RANGES-FIREWALL (0 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere            

Chain cali-FORWARD (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:vjrMJCRpqwy5oRoX */ MARK and 0xfff1ffff
cali-from-hep-forward  all  --  anywhere             anywhere             /* cali:A_sPAO0mcxbT9mOV */ mark match 0x0/0x10000
cali-from-wl-dispatch  all  --  anywhere             anywhere             /* cali:8ZoYfO5HKXWbB3pk */
cali-to-wl-dispatch  all  --  anywhere             anywhere             /* cali:jdEuaPBe14V2hutn */
cali-to-hep-forward  all  --  anywhere             anywhere             /* cali:12bc6HljsMKsmfr- */
cali-cidr-block  all  --  anywhere             anywhere             /* cali:NOSxoaGx8OIstr1z */

Chain cali-INPUT (1 references)
target     prot opt source               destination         
ACCEPT     ipencap--  anywhere             anywhere             /* cali:PajejrV4aFdkZojI */ /* Allow IPIP packets from Calico hosts */ match-set cali40all-hosts-net src ADDRTYPE match dst-type LOCAL
DROP       ipencap--  anywhere             anywhere             /* cali:_wjq-Yrma8Ly1Svo */ /* Drop IPIP packets from non-Calico hosts */
MARK       all  --  anywhere             anywhere             /* cali:ss8lEMQsXi-s6qYT */ MARK and 0xfffff
cali-forward-check  all  --  anywhere             anywhere             /* cali:PgIW-V0nEjwPhF_8 */
RETURN     all  --  anywhere             anywhere             /* cali:QMJlDwlS0OjHyfMN */ mark match ! 0x0/0xfff00000
cali-wl-to-host  all  --  anywhere             anywhere            [goto]  /* cali:nDRe73txrna-aZjG */
ACCEPT     all  --  anywhere             anywhere             /* cali:iX2AYvqGXaVqwkro */ mark match 0x10000/0x10000
MARK       all  --  anywhere             anywhere             /* cali:bhpnxD5IRtBP8KW0 */ MARK and 0xfff0ffff
cali-from-host-endpoint  all  --  anywhere             anywhere             /* cali:H5_bccAbHV0sooVy */
ACCEPT     all  --  anywhere             anywhere             /* cali:inBL01YlfurT0dbI */ /* Host endpoint policy accepted packet. */ mark match 0x10000/0x10000

Chain cali-OUTPUT (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:Mq1_rAdXXH3YkrzW */ mark match 0x10000/0x10000
cali-forward-endpoint-mark  all  --  anywhere             anywhere            [goto]  /* cali:5Z67OUUpTOM7Xa1a */ mark match ! 0x0/0xfff00000
RETURN     all  --  anywhere             anywhere             /* cali:M2Wf0OehNdig8MHR */
ACCEPT     ipencap--  anywhere             anywhere             /* cali:AJBkLho_0Qd8LNr3 */ /* Allow IPIP packets to other Calico hosts */ match-set cali40all-hosts-net dst ADDRTYPE match src-type LOCAL
MARK       all  --  anywhere             anywhere             /* cali:iz2RWXlXJDUfsLpe */ MARK and 0xfff0ffff
cali-to-host-endpoint  all  --  anywhere             anywhere             /* cali:xQqLi8S0sxbiyvjR */ ! ctstate DNAT
ACCEPT     all  --  anywhere             anywhere             /* cali:aSnsxZdmhxm_ilRZ */ /* Host endpoint policy accepted packet. */ mark match 0x10000/0x10000

Chain cali-cidr-block (1 references)
target     prot opt source               destination         

Chain cali-forward-check (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             anywhere             /* cali:Pbldlb4FaULvpdD8 */ ctstate RELATED,ESTABLISHED
cali-set-endpoint-mark  tcp  --  anywhere             anywhere            [goto]  /* cali:ZD-6UxuUtGW-xtzg */ /* To kubernetes NodePort service */ multiport dports 30000:32767 match-set cali40this-host dst
cali-set-endpoint-mark  udp  --  anywhere             anywhere            [goto]  /* cali:CbPfUajQ2bFVnDq4 */ /* To kubernetes NodePort service */ multiport dports 30000:32767 match-set cali40this-host dst
cali-set-endpoint-mark  all  --  anywhere             anywhere             /* cali:jmhU0ODogX-Zfe5g */ /* To kubernetes service */ ! match-set cali40this-host dst

Chain cali-forward-endpoint-mark (1 references)
target     prot opt source               destination         
cali-from-endpoint-mark  all  --  anywhere             anywhere             /* cali:O0SmFDrnm7KggWqW */ mark match ! 0x100000/0xfff00000
cali-to-wl-dispatch  all  --  anywhere             anywhere             /* cali:aFl0WFKRxDqj8oA6 */
cali-to-hep-forward  all  --  anywhere             anywhere             /* cali:AZKVrO3i_8cLai5f */
MARK       all  --  anywhere             anywhere             /* cali:96HaP1sFtb-NYoYA */ MARK and 0xfffff
ACCEPT     all  --  anywhere             anywhere             /* cali:VxO6hyNWz62YEtul */ /* Policy explicitly accepted packet. */ mark match 0x10000/0x10000

Chain cali-from-endpoint-mark (1 references)
target     prot opt source               destination         
cali-fw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  /* cali:_g0DAJdwj0zFTZDd */ mark match 0x40300000/0xfff00000
cali-fw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  /* cali:_xvgcxf_OYnxRPpL */ mark match 0x93400000/0xfff00000
cali-fw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  /* cali:vobAwpu2w-OpfxcW */ mark match 0xf9e00000/0xfff00000
cali-fw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  /* cali:2NCz6Z8qUvBSq-1n */ mark match 0x92300000/0xfff00000
DROP       all  --  anywhere             anywhere             /* cali:Iami5Z3O-giBayN1 */ /* Unknown interface */

Chain cali-from-hep-forward (1 references)
target     prot opt source               destination         

Chain cali-from-host-endpoint (1 references)
target     prot opt source               destination         

Chain cali-from-wl-dispatch (2 references)
target     prot opt source               destination         
cali-fw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  /* cali:K9tdwcxEej5TWJwt */
cali-fw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  /* cali:tCrG_oPsjs7oDjiR */
cali-fw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  /* cali:ty0aZN9DLespD-He */
cali-fw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  /* cali:RuLvWlq3xyYNFBBq */
DROP       all  --  anywhere             anywhere             /* cali:TzHmjW1BcVePGwUs */ /* Unknown interface */

Chain cali-fw-cali1a60675da04 (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:ZGWFfGodtixX8XmQ */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:NsuY-Vd-csUklFg6 */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:HEkNokvoMUqhs_mz */ MARK and 0xfffeffff
DROP       udp  --  anywhere             anywhere             /* cali:yT2TFLX6lbgFvt06 */ /* Drop VXLAN encapped packets originating in workloads */ multiport dports 4789
DROP       ipencap--  anywhere             anywhere             /* cali:JeohvZMA2MUMtka- */ /* Drop IPinIP encapped packets originating in workloads */
cali-pro-kns.kube-system  all  --  anywhere             anywhere             /* cali:Ry3YsktIkN_WGshL */
RETURN     all  --  anywhere             anywhere             /* cali:j42-VxrHhDCsCHKq */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pro-_u2Tn2rSoAPffvE7JO6  all  --  anywhere             anywhere             /* cali:DZeztPbl9GuWcSnS */
RETURN     all  --  anywhere             anywhere             /* cali:QlAcy0LXTxrSFoIZ */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:020SLlST78qhqnSb */ /* Drop if no profiles matched */

Chain cali-fw-cali61bf3705192 (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:HukEhemyC3jUEB2h */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:nS7m2w5F8T8HFKDB */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:SyHL_U3-NYfJnzHU */ MARK and 0xfffeffff
DROP       udp  --  anywhere             anywhere             /* cali:7W0TplglhR5tCDL8 */ /* Drop VXLAN encapped packets originating in workloads */ multiport dports 4789
DROP       ipencap--  anywhere             anywhere             /* cali:-qjAC-m_cklBJkjl */ /* Drop IPinIP encapped packets originating in workloads */
cali-pro-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:6Dm_rdQkT2d-xn0M */
RETURN     all  --  anywhere             anywhere             /* cali:xNfbxZAEEE0ZBeFn */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pro-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             /* cali:umAELb2j4mLttFOx */
RETURN     all  --  anywhere             anywhere             /* cali:FiF3VTCFM8GBIctG */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:9kcFLKXYuJ3kjgED */ /* Drop if no profiles matched */

Chain cali-fw-cali80d4429314e (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:g7SK49eF3d8GQTzM */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:VWWFvGl1dJh0Cblf */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:QXXXDB_vuQ4LM5oU */ MARK and 0xfffeffff
DROP       udp  --  anywhere             anywhere             /* cali:47ZKBD6pl4tSINwl */ /* Drop VXLAN encapped packets originating in workloads */ multiport dports 4789
DROP       ipencap--  anywhere             anywhere             /* cali:bBEbmNiq-HYa4oz8 */ /* Drop IPinIP encapped packets originating in workloads */
cali-pro-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:V-1mwp2arUxEn_dH */
RETURN     all  --  anywhere             anywhere             /* cali:mCCRhepT2W1NMr-Z */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pro-_7Z-cbWmLXlJI5GWsIJ  all  --  anywhere             anywhere             /* cali:DhDPbLnz1bOTKjAj */
RETURN     all  --  anywhere             anywhere             /* cali:-tfrAm7GnlHHz19t */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:wFkb_R24Tl5nk2uM */ /* Drop if no profiles matched */

Chain cali-fw-caliaff77ba4713 (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:zAhgVOdw1_ZwY_qX */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:E_elvvjGDtCyrp2s */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:wYbvB5dlGZFEmI7z */ MARK and 0xfffeffff
DROP       udp  --  anywhere             anywhere             /* cali:Sv2vOgdI-3JOJHV0 */ /* Drop VXLAN encapped packets originating in workloads */ multiport dports 4789
DROP       ipencap--  anywhere             anywhere             /* cali:Ini0PSE7tGhtZvB1 */ /* Drop IPinIP encapped packets originating in workloads */
cali-pro-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:4yzbR1i3_3i5zBej */
RETURN     all  --  anywhere             anywhere             /* cali:seHIJ440rgN8m2Gl */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pro-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             /* cali:G06ddVJcUNN-QcI_ */
RETURN     all  --  anywhere             anywhere             /* cali:0oAp0x-lyWfDbm7q */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:E-X7DQX5M64GSk2C */ /* Drop if no profiles matched */

Chain cali-pri-_4AJPMUgjnkp6PvIZs_ (2 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:6-P0HapqloRNafEU */ /* Profile ksa.gpu-operator.node-feature-discovery ingress */

Chain cali-pri-_7Z-cbWmLXlJI5GWsIJ (1 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:33eST6J3mye-oils */ /* Profile ksa.gpu-operator.gpu-operator ingress */

Chain cali-pri-_u2Tn2rSoAPffvE7JO6 (1 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:WqgznqAQ-uYV0oBx */ /* Profile ksa.kube-system.coredns ingress */

Chain cali-pri-kns.gpu-operator (3 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:MVC_LdvqXMM61ZiS */ /* Profile kns.gpu-operator ingress */ MARK or 0x10000
RETURN     all  --  anywhere             anywhere             /* cali:s0wCV1amxXQsLjX6 */ mark match 0x10000/0x10000

Chain cali-pri-kns.kube-system (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:J1TyxtHWd0qaBGK- */ /* Profile kns.kube-system ingress */ MARK or 0x10000
RETURN     all  --  anywhere             anywhere             /* cali:QIB6k7eEKdIg73Jp */ mark match 0x10000/0x10000

Chain cali-pro-_4AJPMUgjnkp6PvIZs_ (2 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:67xwX_Dluk58rujA */ /* Profile ksa.gpu-operator.node-feature-discovery egress */

Chain cali-pro-_7Z-cbWmLXlJI5GWsIJ (1 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:Jt8hHFPFkxNxFHV9 */ /* Profile ksa.gpu-operator.gpu-operator egress */

Chain cali-pro-_u2Tn2rSoAPffvE7JO6 (1 references)
target     prot opt source               destination         
           all  --  anywhere             anywhere             /* cali:0-_UPh39dt5XfhmJ */ /* Profile ksa.kube-system.coredns egress */

Chain cali-pro-kns.gpu-operator (3 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:dowjNH-r-IuDxByL */ /* Profile kns.gpu-operator egress */ MARK or 0x10000
RETURN     all  --  anywhere             anywhere             /* cali:9xl88GIXoFBqCna9 */ mark match 0x10000/0x10000

Chain cali-pro-kns.kube-system (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:tgOR2S8DVHZW3F1M */ /* Profile kns.kube-system egress */ MARK or 0x10000
RETURN     all  --  anywhere             anywhere             /* cali:HVEEtYPJsiGRXCIt */ mark match 0x10000/0x10000

Chain cali-set-endpoint-mark (3 references)
target     prot opt source               destination         
cali-sm-cali1a60675da04  all  --  anywhere             anywhere            [goto]  /* cali:-46PjhDiNsRWf1FU */
cali-sm-cali61bf3705192  all  --  anywhere             anywhere            [goto]  /* cali:q_vQECIkADutQIWL */
cali-sm-cali80d4429314e  all  --  anywhere             anywhere            [goto]  /* cali:kXqypj13p8lEFDTx */
cali-sm-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  /* cali:KkkLwmGp8wgvlkny */
DROP       all  --  anywhere             anywhere             /* cali:9Lj2OU8ZON1EJ4XC */ /* Unknown endpoint */
MARK       all  --  anywhere             anywhere             /* cali:3mMep0QEGmc9Qt7q */ /* Non-Cali endpoint mark */ MARK xset 0x100000/0xfff00000

Chain cali-sm-cali1a60675da04 (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:cFx4mA6whaURTNOC */ MARK xset 0x40300000/0xfff00000

Chain cali-sm-cali61bf3705192 (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:SQRm6KIr7h0YoMIZ */ MARK xset 0x93400000/0xfff00000

Chain cali-sm-cali80d4429314e (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:msddQTu3_4-8yx_x */ MARK xset 0xf9e00000/0xfff00000

Chain cali-sm-caliaff77ba4713 (1 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             /* cali:RZKZ_MnNUqUKKGiD */ MARK xset 0x92300000/0xfff00000

Chain cali-to-hep-forward (2 references)
target     prot opt source               destination         

Chain cali-to-host-endpoint (1 references)
target     prot opt source               destination         

Chain cali-to-wl-dispatch (2 references)
target     prot opt source               destination         
cali-tw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  /* cali:zIuiWorw_-gqLEXO */
cali-tw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  /* cali:xag75YL5akcG0XLF */
cali-tw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  /* cali:gYjFDNf-SltrRZB7 */
cali-tw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  /* cali:FdkGAd3ATwS-vVVD */
DROP       all  --  anywhere             anywhere             /* cali:fqHovg-WPs4e6zTK */ /* Unknown interface */

Chain cali-tw-cali1a60675da04 (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:w1Cm2ULOSvjTHudq */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:9umnlXX6GO6Vc4_C */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:4hLjdY3hkfGFs4uD */ MARK and 0xfffeffff
cali-pri-kns.kube-system  all  --  anywhere             anywhere             /* cali:gLTSlA3UVavlYqRu */
RETURN     all  --  anywhere             anywhere             /* cali:1pYcsK43THPrR8Qt */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pri-_u2Tn2rSoAPffvE7JO6  all  --  anywhere             anywhere             /* cali:tA0_fCHeaUVu5_kN */
RETURN     all  --  anywhere             anywhere             /* cali:5PoHpiQMvOhdA4jr */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:6P9MHq2lZbGcJMUs */ /* Drop if no profiles matched */

Chain cali-tw-cali61bf3705192 (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:4TsaIXYZcZWbiXXW */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:xlvqDm2nH2zq4V8p */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:GU1FuBxFrFydlHq3 */ MARK and 0xfffeffff
cali-pri-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:1uJveElD9OXuShKZ */
RETURN     all  --  anywhere             anywhere             /* cali:l928XWHGLf6ksfAV */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pri-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             /* cali:n-nBk2VPtnM1xYwe */
RETURN     all  --  anywhere             anywhere             /* cali:wkuEQFdIIQ77Wq7W */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:ElgyHAWClRlWkpqG */ /* Drop if no profiles matched */

Chain cali-tw-cali80d4429314e (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:H1jNyDPmKvDDn5Vk */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:NcCXk1FMcyKvN-0s */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:JlnCwI9rZZNdivZU */ MARK and 0xfffeffff
cali-pri-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:GVRxY9RD3CcrgUk3 */
RETURN     all  --  anywhere             anywhere             /* cali:10QG2G9XDcx2Bwfi */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pri-_7Z-cbWmLXlJI5GWsIJ  all  --  anywhere             anywhere             /* cali:JOmVuKKbAS2BFWnc */
RETURN     all  --  anywhere             anywhere             /* cali:pcYu_mJoNuj2B6Cj */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:AxCTJffEY1zGhysc */ /* Drop if no profiles matched */

Chain cali-tw-caliaff77ba4713 (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             /* cali:Wm5cRViQaTB0AM6_ */ ctstate RELATED,ESTABLISHED
DROP       all  --  anywhere             anywhere             /* cali:3jOmjG1mkPWiBb57 */ ctstate INVALID
MARK       all  --  anywhere             anywhere             /* cali:1AAP5imvUYhZQLJE */ MARK and 0xfffeffff
cali-pri-kns.gpu-operator  all  --  anywhere             anywhere             /* cali:PfCoNSFnCX-GKCGz */
RETURN     all  --  anywhere             anywhere             /* cali:A9mVsPhWlgdu61dl */ /* Return if profile accepted */ mark match 0x10000/0x10000
cali-pri-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             /* cali:fOMutePJ56ab3bD_ */
RETURN     all  --  anywhere             anywhere             /* cali:jNAhD8xZ9K3Laqwi */ /* Return if profile accepted */ mark match 0x10000/0x10000
DROP       all  --  anywhere             anywhere             /* cali:nWbl-mTELMxfckFL */ /* Drop if no profiles matched */

Chain cali-wl-to-host (1 references)
target     prot opt source               destination         
cali-from-wl-dispatch  all  --  anywhere             anywhere             /* cali:Ee9Sbo10IpVujdIY */
ACCEPT     all  --  anywhere             anywhere             /* cali:nSZbcOoG1xPONxb8 */ /* Configured DefaultEndpointToHostAction */
```

#### Kubernetes version

<details>

```console
user@randomhost:~$ kubectl version
Client Version: v1.28.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.6
```
k8s version : `1.28.3`, `1.28.4`
</details>


#### Cloud provider

<details>
Onpremise
</details>


#### OS version

<details>

```bash
user@randomhost:~$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>

### 分析结果

不涉及

---

## Issue #123934 csi-s3 container cant unmount a volume

- Issue 链接：[#123934](https://github.com/kubernetes/kubernetes/issues/123934)

### Issue 内容

#### What happened?

I am using an s3 bucket as volume for my app running in k8s (deployment, 1 replica, rolling update).

When I triggered the deployment of a new revision of my app, the new pod got up and the s3 bucket attached to the new pod.
However, the old pod is failed because it was terminated with an error 137 (probably SIGKILL instead of OOM due to a small gracefull shutdown window, because I don't see any memory related issues right now). 

One big problem is that the old pod is stuck in a terminating state and I guess due to the volume problem.  

Datashim can not unmount the volume from node where was the old pod. csi-s3 pod (csi-s3 container, daemonset) log: 

```bash
2024-03-13T21:04:45.001866708Z stderr F I0313 21:04:45.001667       1 utils.go:98] GRPC request: {}
2024-03-13T21:04:45.001885266Z stderr F I0313 21:04:45.001715       1 utils.go:103] GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}}]}
2024-03-13T21:05:33.635813571Z stderr F I0313 21:05:33.635664       1 utils.go:97] GRPC call: /csi.v1.Node/NodeGetCapabilities
2024-03-13T21:05:33.635874907Z stderr F I0313 21:05:33.635690       1 utils.go:98] GRPC request: {}
2024-03-13T21:05:33.63589224Z stderr F I0313 21:05:33.635736       1 utils.go:103] GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}}]}
2024-03-13T21:05:37.030960679Z stderr F I0313 21:05:37.027972       1 utils.go:97] GRPC call: /csi.v1.Node/NodeUnpublishVolume
2024-03-13T21:05:37.030981533Z stderr F I0313 21:05:37.027993       1 utils.go:98] GRPC request: {"target_path":"/var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount","volume_id":"pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648"}
2024-03-13T21:05:37.035823492Z stderr F I0313 21:05:37.035232       1 util.go:75] Found matching pid 87 on path /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount
2024-03-13T21:05:37.035848546Z stderr F I0313 21:05:37.035254       1 mounter.go:80] Found fuse pid 87 of mount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount, checking if it still runs
2024-03-13T21:05:37.035853313Z stderr F I0313 21:05:37.035273       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:37.135710883Z stderr F I0313 21:05:37.135582       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:37.336121403Z stderr F I0313 21:05:37.335983       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:37.636843237Z stderr F I0313 21:05:37.636538       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:38.037307936Z stderr F I0313 21:05:38.037173       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:38.538075749Z stderr F I0313 21:05:38.537942       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:39.13912303Z stderr F I0313 21:05:39.138960       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:39.839917198Z stderr F I0313 21:05:39.839762       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:40.640972203Z stderr F I0313 21:05:40.640842       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:41.542074541Z stderr F I0313 21:05:41.541932       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:42.542234381Z stderr F I0313 21:05:42.542094       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:43.64277079Z stderr F I0313 21:05:43.642634       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:44.843399965Z stderr F I0313 21:05:44.843259       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:46.14365466Z stderr F I0313 21:05:46.143473       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:47.543820527Z stderr F I0313 21:05:47.543692       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:49.044170561Z stderr F I0313 21:05:49.044067       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:50.645274452Z stderr F I0313 21:05:50.645124       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:52.345495246Z stderr F I0313 21:05:52.345336       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:54.145711584Z stderr F I0313 21:05:54.145589       1 util.go:39] Fuse process with PID 87 still active, waiting...
2024-03-13T21:05:56.046229119Z stderr F E0313 21:05:56.046082       1 utils.go:101] GRPC error: rpc error: code = Internal desc = Timeout waiting for PID 87 to
 end
2024-03-13T21:05:56.625507593Z stderr F I0313 21:05:56.625330       1 utils.go:97] GRPC call: /csi.v1.Node/NodeUnpublishVolume
2024-03-13T21:05:56.625532689Z stderr F I0313 21:05:56.625367       1 utils.go:98] GRPC request: {"target_path":"/var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-
df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount","volume_id":"pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648"}
2024-03-13T21:05:56.626775Z stderr F E0313 21:05:56.626655       1 utils.go:101] GRPC error: rpc error: code = Internal desc = unmount failed: exit status 1
2024-03-13T21:05:56.626785867Z stderr F Unmounting arguments: /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67
-78bd-4501-9dd8-bd2c4cb58648/mount
2024-03-13T21:05:56.626789088Z stderr F Output: umount: can't unmount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-
1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount: Invalid argument
2024-03-13T21:05:57.632955973Z stderr F I0313 21:05:57.632811       1 utils.go:97] GRPC call: /csi.v1.Node/NodeUnpublishVolume
2024-03-13T21:05:57.632982344Z stderr F I0313 21:05:57.632830       1 utils.go:98] GRPC request: {"target_path":"/var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-
df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount","volume_id":"pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648"}
2024-03-13T21:05:57.634391298Z stderr F E0313 21:05:57.634256       1 utils.go:101] GRPC error: rpc error: code = Internal desc = unmount failed: exit status 1
2024-03-13T21:05:57.634405692Z stderr F Unmounting arguments: /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67
-78bd-4501-9dd8-bd2c4cb58648/mount
2024-03-13T21:05:57.634410456Z stderr F Output: umount: can't unmount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-
1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount: Invalid argument
``` 

Onwards, `Output: umount: can't unmount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-
1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount: Invalid argument` repeats indefinitely.

Kubelet produces similar logs on the node where the failed pod is: 
```bash
-- Logs begin at Wed 2024-03-06 20:07:38 UTC, end at Thu 2024-03-14 13:33:03 UTC. --
Mar 14 13:33:03 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: Output: umount: can't unmount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount: Invalid argument
Mar 14 13:33:03 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: Unmounting arguments: /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount
Mar 14 13:33:03 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: E0314 13:33:03.729406    3207 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/ch.ctrox.csi.s3-driver^pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648 podName:36180d66-5fa5-4393-a84d-df95afe5a369 nodeName:}" failed. No retries permitted until 2024-03-14 13:35:05.729378976 +0000 UTC m=+667629.606875374 (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume "documents-storage-s3" (UniqueName: "kubernetes.io/csi/ch.ctrox.csi.s3-driver^pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648") pod "36180d66-5fa5-4393-a84d-df95afe5a369" (UID: "36180d66-5fa5-4393-a84d-df95afe5a369") : kubernetes.io/csi: Unmounter.TearDownAt failed: rpc error: code = Internal desc = unmount failed: exit status 1
Mar 14 13:33:03 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: I0314 13:33:03.651414    3207 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"documents-storage-s3\" (UniqueName: \"kubernetes.io/csi/ch.ctrox.csi.s3-driver^pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648\") pod \"36180d66-5fa5-4393-a84d-df95afe5a369\" (UID: \"36180d66-5fa5-4393-a84d-df95afe5a369\") "
Mar 14 13:31:01 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: Output: umount: can't unmount /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount: Invalid argument
Mar 14 13:31:01 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: Unmounting arguments: /var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount
Mar 14 13:31:01 ip-172-31-30-130.eu-central-1.compute.internal kubelet[3207]: E0314 13:31:01.583136    3207 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/ch.ctrox.csi.s3-driver^pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648 podName:36180d66-5fa5-4393-a84d-df95afe5a369 nodeName:}" failed. No retries permitted until 2024-03-14 13:33:03.583103649 +0000 UTC m=+667507.460600342 (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume "documents-storage-s3" (UniqueName: "kubernetes.io/csi/ch.ctrox.csi.s3-driver^pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648") pod "36180d66-5fa5-4393-a84d-df95afe5a369" (UID: "36180d66-5fa5-4393-a84d-df95afe5a369") : kubernetes.io/csi: Unmounter.TearDownAt failed: rpc error: code = Internal desc = unmount failed: exit status 1
```

Even after I forthfully deleted a failed pod, errors did not disappear. 

Pods description: kubectl get pod survey-service-84cf8d9d49-xhbxq -o yaml                                                                 
```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/restartedAt: "2024-03-06T20:55:56Z"
    <censored>
  creationTimestamp: "2024-03-12T11:19:39Z"
  deletionGracePeriodSeconds: 40
  deletionTimestamp: "2024-03-13T21:05:26Z"
  generateName: survey-service-84cf8d9d49-
  labels:
    app: survey-service
    pod-template-hash: 84cf8d9d49
  name: survey-service-84cf8d9d49-xhbxq
  namespace: test
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: survey-service-84cf8d9d49
    uid: da3867cc-cea3-49bc-b405-4d9d018541ae
  resourceVersion: "125333681"
  uid: 36180d66-5fa5-4393-a84d-df95afe5a369
spec:
  containers:
  - env:
    <censored>
    image: <censored>
    imagePullPolicy: Always
    lifecycle:
      preStop:
        exec:
          command:
          - sh
          - -c
          - sleep 10
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /api/survey/actuator/health/liveness
        port: 8080
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    name: survey-service
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /api/survey/actuator/health/readiness
        port: 8080
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        memory: 2148Mi
      requests:
        memory: 1630Mi
    startupProbe:
      failureThreshold: 40
      httpGet:
        path: /api/survey/actuator/health/liveness
        port: 8080
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /proliance360-s3
      name: documents-storage-s3
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: <censored>
      readOnly: true
    - mountPath: <censored>
      name: <censored>
  - args:
    - echo ${VAULT_CONFIG?} | base64 -d > /home/vault/config.json && vault agent -config=/home/vault/config.json
    command:
    - /bin/sh
    - -ec
    env:
    - name: VAULT_LOG_LEVEL
      value: debug
    - name: VAULT_LOG_FORMAT
      value: standard
    - name: VAULT_CONFIG
      value: <censored>
    image: hashicorp/vault:1.13.1
    imagePullPolicy: IfNotPresent
    lifecycle: {}
    name: vault-agent
    resources:
      limits:
        memory: 64Mi
      requests:
        cpu: 25m
        memory: 16Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 100
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: <censored>
      readOnly: true
    - mountPath: /home/vault
      name: home-sidecar
    - mountPath: /vault/secrets
      name: vault-secrets
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: <censored>
  initContainers:
  - args:
    - echo ${VAULT_CONFIG?} | base64 -d > /home/vault/config.json && vault agent -config=/home/vault/config.json
    command:
    - /bin/sh
    - -ec
    env:
    - name: VAULT_LOG_LEVEL
      value: debug
    - name: VAULT_LOG_FORMAT
      value: standard
    - name: VAULT_CONFIG
      value: <censored>
    image: hashicorp/vault:1.13.1
    imagePullPolicy: IfNotPresent
    name: vault-agent-init
    resources:
      limits:
        memory: 64Mi
      requests:
        cpu: 25m
        memory: 16Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 100
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /home/vault
      name: home-init
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: <censored>
      readOnly: true
    - mountPath: /vault/secrets
      name: vault-secrets
  nodeName: <censored>
  preemptionPolicy: Never
  priority: 1000000
  priorityClassName: default-priority-class
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: <censored>
  serviceAccountName: <censored>
  terminationGracePeriodSeconds: 40
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: documents-storage-s3
    persistentVolumeClaim:
      claimName: survey-service-s3-dataset
  - name: <censored>
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
  - emptyDir:
      medium: Memory
    name: home-init
  - emptyDir:
      medium: Memory
    name: home-sidecar
  - emptyDir:
      medium: Memory
    name: vault-secrets
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-03-12T11:19:43Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-03-13T21:05:36Z"
    reason: PodFailed
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-03-13T21:05:36Z"
    reason: PodFailed
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-03-12T11:19:39Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://54e3be5f2ff3c7e2f672044fcbc979b7d25fed5fadce924fb7917328dfc75713
    image: <censored>
    imageID: <censored>
    lastState: {}
    name: survey-service
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: containerd://54e3be5f2ff3c7e2f672044fcbc979b7d25fed5fadce924fb7917328dfc75713
        exitCode: 137
        finishedAt: "2024-03-13T21:05:36Z"
        reason: Error
        startedAt: "2024-03-12T11:19:46Z"
  - containerID: containerd://5b36804afe1c892c8a8c2b85394dbd790f1beb040e2cc0c3cc59806f85440a96
    image: docker.io/hashicorp/vault:1.13.1
    imageID: docker.io/hashicorp/vault@sha256:b888abc3fc0529550d4a6c87884419e86b8cb736fe556e3e717a6bc50888b3b8
    lastState: {}
    name: vault-agent
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: containerd://5b36804afe1c892c8a8c2b85394dbd790f1beb040e2cc0c3cc59806f85440a96
        exitCode: 0
        finishedAt: "2024-03-13T21:04:46Z"
        reason: Completed
        startedAt: "2024-03-12T11:19:47Z"
  hostIP: 172.31.30.130
  initContainerStatuses:
  - containerID: containerd://1534bbe8943624f5f30534a4fcaa440fc20ca8c5c4a3fea4510738078dbb29b6
    image: docker.io/hashicorp/vault:1.13.1
    imageID: docker.io/hashicorp/vault@sha256:b888abc3fc0529550d4a6c87884419e86b8cb736fe556e3e717a6bc50888b3b8
    lastState: {}
    name: vault-agent-init
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: containerd://1534bbe8943624f5f30534a4fcaa440fc20ca8c5c4a3fea4510738078dbb29b6
        exitCode: 0
        finishedAt: "2024-03-12T11:19:42Z"
        reason: Completed
        startedAt: "2024-03-12T11:19:42Z"
  phase: Failed
  podIP: 172.31.18.155
  podIPs:
  - ip: 172.31.18.155
  qosClass: Burstable
  startTime: "2024-03-12T11:19:39Z"
```
When I go to the node where the failed pod is, there is no active fuse filesystem mounted to  `/var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/*:
```bash
[root@ip-172-31-30-130 csi-s3]# df -HT -t fuse
Filesystem           Type  Size  Used Avail Use% Mounted on
proliance360-staging fuse  1.2P     0  1.2P   0% /var/lib/kubelet/pods/c6ae84a5-208e-4e45-8133-442694c9a91b/volumes/kubernetes.io~csi/pvc-96b824dc-b7af-4b78-b08b-9567c4e52942/mount
proliance360-prod    fuse  1.2P     0  1.2P   0% /var/lib/kubelet/pods/366dcbaf-40f7-491a-aa84-801790cd12f4/volumes/kubernetes.io~csi/pvc-7d2b2c22-7134-421d-b2ad-c1a8b8faf376/mount
proliance360-test    fuse  1.2P     0  1.2P   0% /var/lib/kubelet/pods/df5557ff-b00c-4519-9e59-b7497b3b1ddb/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount`
```

In the csi-s3 container, I may found unfinished goofys process for `/var/lib/kubelet/pods/36180d66-5fa5-439
3-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/` volume:
```bash
PID   USER     TIME  COMMAND
    1 root      1:15 /s3driver --v=5 --endpoint=unix:///csi/csi.sock --nodeid=ip-172-31-30-130.eu-central-1.compute.internal
   20 root      0:22 [goofys]
   40 root      0:26 /bin/goofys --endpoint=https://s3.eu-central-1.amazonaws.com --type-cache-ttl 1s --stat-cache-ttl 1s --dir-mode 0777 --file-mode 0777 --ht
PID   USER     TIME  COMMAND
PID   USER     TIME  COMMAND
    1 root      1:15 /s3driver --v=5 --endpoint=unix:///csi/csi.sock --nodeid=ip-172-31-30-130.eu-central-1.compute.internal
   20 root      0:22 [goofys]
   40 root      0:26 /bin/goofys --endpoint=https://s3.eu-central-1.amazonaws.com --type-cache-ttl 1s --stat-cache-ttl 1s --dir-mode 0777 --file-mode 0777 --ht
tp-timeout 5m -o allow_other --profile=pvc-96b824dc-b7af-4b78-b08b-9567c4e52942 --region eu-central-1 proliance360-staging /var/lib/kubelet/pods/c6ae84a5-208e-
4e45-8133-442694c9a91b/volumes/kubernetes.io~csi/pvc-96b824dc-b7af-4b78-b08b-9567c4e52942/mount
   62 root      3:27 /bin/goofys --endpoint=https://s3.eu-central-1.amazonaws.com --type-cache-ttl 1s --stat-cache-ttl 1s --dir-mode 0777 --file-mode 0777 --ht
tp-timeout 5m -o allow_other --profile=pvc-7d2b2c22-7134-421d-b2ad-c1a8b8faf376 --region eu-central-1 proliance360-prod /var/lib/kubelet/pods/366dcbaf-40f7-491
a-aa84-801790cd12f4/volumes/kubernetes.io~csi/pvc-7d2b2c22-7134-421d-b2ad-c1a8b8faf376/mount
   87 root      0:13 /bin/goofys --endpoint=https://s3.eu-central-1.amazonaws.com --type-cache-ttl 1s --stat-cache-ttl 1s --dir-mode 0777 --file-mode 0777 --ht
tp-timeout 5m -o allow_other --profile=pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648 --region eu-central-1 proliance360-test /var/lib/kubelet/pods/36180d66-5fa5-439
3-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount
  564 root      0:00 /bin/goofys --endpoint=https://s3.eu-central-1.amazonaws.com --type-cache-ttl 1s --stat-cache-ttl 1s --dir-mode 0777 --file-mode 0777 --ht
tp-timeout 5m -o allow_other --profile=pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648 --region eu-central-1 proliance360-test /var/lib/kubelet/pods/df5557ff-b00c-451
9-9e59-b7497b3b1ddb/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount
  607 root      0:00 sh -c clear; (bash || ash || sh)
  614 root      0:00 ash
  616 root      0:00 sh -c clear; (bash || ash || sh)
  624 root      0:00 ash
  652 root      0:00 ps aux
  653 root      0:00 less
```

The pods directory `/var/lib/kubelet/pods/36180d66-5fa5-439 3-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount` actually exists on the node, but It is empty:
```bash
/var/lib/kubelet/pods/36180d66-5fa5-4393-a84d-df95afe5a369/volumes/kubernetes.io~csi/pvc-1ec40a67-78bd-4501-9dd8-bd2c4cb58648/mount # ls -al
total 0
drwxr-x---    2 root     root             6 Mar 12 11:19 .
drwxr-x---    3 root     root            40 Mar 12 11:19 ..
```
I guess that my pod was stuck in a terminating state because Kubelet can not finish some tasks (maybe admissions controllers involved, garbage collector) and it leaves the pod in this state. I want to fix that. 

Worth mentioned that if the pod finished without error (not failed), then no s3 errors/problems occur
Thanks in advance.

#### What did you expect to happen?

Kubelet completely terminates the failed pod. The volume management is healthy. 

#### How can we reproduce it (as minimally and precisely as possible)?

The pod which uses s3 storage volumes should be failed. The rest is in "What happened?" secrion.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version 
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.3", GitCommit:"9e644106593f3f4aa98f8a84b23db5fa378900bd", GitTreeState:"clean", BuildDate:"2023-03-15T13:33:11Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.9-eks-5e0fdde", GitCommit:"3f8ed3d5017d988600f597734a4851930eda35a6", GitTreeState:"clean", BuildDate:"2024-01-02T20:34:38Z", GoVersion:"go1.20.12", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

AWS EKS 1.27


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux  5.10.209-198.858.amzn2.x86_64 #1 SMP Tue Feb 13 18:46:41 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```console
$ ctr --version
ctr github.com/containerd/containerd 1.7.11

```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
DLF Helm chart:
```yaml
apiVersion: v2
name: dlf-chart
description: Dataset Lifecycle Framework chart
type: application
version: 0.1.0
appVersion: 0.1.0

dependencies:
  - name: csi-sidecars-rbac
    version: 0.1.0
    condition: csi-sidecars-rbac.enabled
  - name: csi-nfs-chart
    version: 0.1.0
    condition: csi-nfs-chart.enabled
  - name: csi-s3-chart
    version: 0.1.0
    condition: csi-s3-chart.enabled
  - name: csi-h3-chart
    version: 0.1.0
    condition: csi-h3-chart.enabled
  - name: dataset-operator-chart
    version: 0.1.0
    condition: dataset-operator-chart.enabled
```

csi-attacher-s3 - image registry.k8s.io/sig-storage/csi-attacher:v3.3.0 ;
csi-provisioner-s3 - image registry.k8s.io/sig-storage/csi-provisioner:v2.2.2
csi-s3 - image quay.io/datashim-io/csi-s3:0.3.0
driver-registrar - image registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.3.0
dataset-operator - image quay.io/datashim-io/dataset-operator:0.3.0

</details>


### 分析结果

不涉及

---

## Issue #123933 Code generator is no longer usable via `go run`

- Issue 链接：[#123933](https://github.com/kubernetes/kubernetes/issues/123933)

### Issue 内容

#### What happened?

The code generator was usable via `go run` up until v1.30.0-alpha.3:

```console
$ go version
go version go1.22.1 linux/amd64
```

```console
$ go run k8s.io/code-generator/cmd/client-gen@v0.30.0-alpha.3 --help
Usage of /tmp/go-build2845286753/b001/exe/client-gen:
[...]
```

Since v1.30.0-beta.0:

```console
$ go run k8s.io/code-generator/cmd/client-gen@v0.30.0-beta.0 --help
go: k8s.io/code-generator/cmd/client-gen@v0.30.0-beta.0 (in k8s.io/code-generator@v0.30.0-beta.0):
        The go.mod file for the module providing named packages contains one or
        more replace directives. It must not contain directives that would cause
        it to be interpreted differently than if it were the main module.
```

#### What did you expect to happen?

The code generator works via `go run`.

#### How can we reproduce it (as minimally and precisely as possible)?

See above.

#### Anything else we need to know?

This may not be a supported way to use the code generator at all, but it worked quite well until now, and I thought I'd bring this to attention. IIUC the offending replace directive came in as part of the workspace support (#123529) and I'm not sure there's anything that can be done about it.

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.30.0-beta.0
```

</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
$ cat /etc/os-release
BUG_REPORT_URL="https://github.com/NixOS/nixpkgs/issues"
BUILD_ID="23.11.20240311.ddcd759"
DOCUMENTATION_URL="https://nixos.org/learn.html"
HOME_URL="https://nixos.org/"
ID=nixos
LOGO="nix-snowflake"
NAME=NixOS
PRETTY_NAME="NixOS 23.11 (Tapir)"
SUPPORT_END="2024-06-30"
SUPPORT_URL="https://nixos.org/community.html"
VERSION="23.11 (Tapir)"
VERSION_CODENAME=tapir
VERSION_ID="23.11"
$ uname -srvmo
Linux 6.6.21 #1-NixOS SMP PREEMPT_DYNAMIC Wed Mar  6 14:48:45 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

不涉及

---

## Issue #123931 readiness prober timeout do not run as expected

- Issue 链接：[#123931](https://github.com/kubernetes/kubernetes/issues/123931)

### Issue 内容

#### What happened?

1. We meet the issue that the readiness probe timeout do not run as expected.  It run as 2mins + timeout setting in actual 

2. According  to the below official kubernets doc, it define that the timeoutSeconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.  After I read this infromation from the doc , i think it means if the prober command running time exceeded the timeout period, the kubelet will stop this prober action and start the next prober, butl it not in actual.  The next prober action  will run after the last prober  action completed or last prober action lasted (2mins + timeout setting).

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes

3. I also check the code, and find that it deifine timeous =timeout setting  + default timeout (2 minutes) as timeout to leave some time for  the runtime to do cleanup uless timeout setting is 0s. I think 2 minutes is used for the containerd runtime to prepare necessary action. But the doc description make the customer confusion easily if not add  some details
https://github.com/kubernetes/kubernetes/blob/d1a2a134c532109540025c990697a6900c2e62fc/pkg/kubelet/cri/remote/remote_runtime.go#L469
![image](https://github.com/kubernetes/kubernetes/assets/88937681/3a53ef8c-e01a-40c9-8582-65c56feabb88)




#### What did you expect to happen?

Could you plz modify the deafult timeout in code  or add description in  the officail doc ?

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a configmap  like below. it show the two commd: 1. create a file named with the current time; 2.  sleep 300s
```
apiVersion: v1
data:
  container-readiness-probe-handler.sh: |-
    #!/bin/sh
    touch /tmp/$(date +'%Y-%m-%d-%H-%M-%S')
    sleep 300
kind: ConfigMap
metadata:
  name: tesh
  namespace: default
```


2. create a pod with the nginx image and mount the above configmap as the volume  to the /script path.  Setting the timeoutSeconds as 8s
```
 containers:
  - image: nginx:latest
    imagePullPolicy: Always
    name: nginx-test
    readinessProbe:
      exec:
        command:
        - /bin/sh
        - /script/container-readiness-probe-handler.sh
      failureThreshold: 7
      initialDelaySeconds: 6
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 8
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
    - mountPath: /script
      name: volume-1710223576413
  volumes:
  - configMap:
      defaultMode: 493
      name: tesh
    name: volume-1710223576413
```
    
    
   3.  After the pod running , will find that the new file will be created every (2mins+8s）=128s in /tmp path
![image](https://github.com/kubernetes/kubernetes/assets/88937681/d426d039-81d8-4396-8255-2aca74ed2a0c)

   4.  strace the cri which pull the containerd and find that  it will trigger the prober action every  (2mins+8s）
![image](https://github.com/kubernetes/kubernetes/assets/88937681/6cbc9a21-6e3e-40d2-bf30-70dbe5aea9de)

5. if the  timeout period setting < prober command runiig time< 2 mins.  The new readiness prober will run after last prober command completed, not timeout  period



#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.3-aliyun.1



#### Cloud provider

Alibaba Cloud


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Alibaba Cloud Linux (Aliyun Linux)"
VERSION="2.1903 LTS (Hunting Beagle)"
ID="alinux"
ID_LIKE="rhel fedora centos anolis"
VERSION_ID="2.1903"
PRETTY_NAME="Alibaba Cloud Linux (Aliyun Linux) 2.1903 LTS (Hunting Beagle)"
ANSI_COLOR="0;31"
HOME_URL="https://www.aliyun.com/"

$ uname -a
Linux iZj6c9edi9gpcd5qesfwxvZ 4.19.91-25.6.al7.x86_64 #1 SMP Thu Feb 10 19:15:17 CST 2022 x86_64 x86_64 x86_64 GNU/Linux


#### Install tools

<details>


```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123918 StatefulSet with podManagementPolicy=OrderedReady and minReadySeconds does not scale down correctly

- Issue 链接：[#123918](https://github.com/kubernetes/kubernetes/issues/123918)

### Issue 内容

#### What happened?

1. create the OrderedReady statefulset

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-roll
spec:
  replicas: 2
  minReadySeconds: 30
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: nginx-roll
  template:
    metadata:
      labels:
        app: nginx-roll
    spec:
      containers:
      - name: nginx
        image: ghcr.io/nginxinc/nginx-unprivileged:latest
        ports:
        - containerPort: 80
          name: web
```


2. wait for the 2nd pod to become ready, but not available (can also happen when the pod has lost availability)
3. scale the statefulset to 1 replica
 
```
kubectl scale statefulset nginx-roll --replicas=1
```

4. the 2nd pod should start terminating immediately but instead it hangs until the KCM is fully resynced


#### What did you expect to happen?

the 2nd pod should start terminating immediately

#### How can we reproduce it (as minimally and precisely as possible)?

see `What happened?` 

#### Anything else we need to know?

we can see in the logs that the syncs are happening but the firstUnhealthyPod variable is not resolved, so the progress is stalled

```
stateful_set_control.go:509] "StatefulSet is waiting for Pod to be Available prior to scale down" statefulSet="test/nginx-roll" pod=""
```

We even get  the availability check scheduled and called, but because a prior scheduling already ocurred, the new check is thrown away (should not happen, but a similar issue was reported in https://github.com/kubernetes/kubernetes/issues/119352 and a fix is blocked on https://github.com/kubernetes/kubernetes/pull/112328)

```
stateful_set.go:243] "StatefulSet will be enqueued after minReadySeconds for availability check" statefulSet="test/nginx-roll" minReadySeconds=30
```

the sync will be called too soon and resolves again in 
```
stateful_set_control.go:509] "StatefulSet is waiting for Pod to be Available prior to scale down" statefulSet="test/nginx-roll" pod=""
```

The next sync will ocurr when KCM is fully resynced which can be a long time (depending on KCM resync period)



statefulset controller should scale down the first condemned pod as soon as it can, but keep the predecessor pods running available

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0-rc.1.3813+a0beecc776d492-dirty
```

</details>


#### Cloud provider

NA


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123911 PDB views pods in Terminating state as available

- Issue 链接：[#123911](https://github.com/kubernetes/kubernetes/issues/123911)

### Issue 内容

#### What happened?

1. I have a statefulset with 2 pods. The grace period for these pods is long, so the terminating state can endure over hours, but that's not important.
2. I have a basic PDB of minAvailable: 1
3. I have a service that distributes across both pods.
4. I rollout restart the sts, which first restarts pod-1, then tries to restart pod-0.
5. While pod-0 is still in status "Terminating", I issue a second rollout restart to the sts.
6. K8s restarts pod-1 again, while pod-0 is still Terminating from the first restart.
7. My service is effectively down, as my service returns 503 Service Unavailable, because all pods are Terminating.

#### What did you expect to happen?

6. K8s waits for pod-0 to be restarted and ready again before continuing to restart pod-1
7. My service stays available, as there is always at least one pod that is ready AND Running, which is what I expect from the PDB

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: pdb-fail
  name: pdb-fail
spec:
  podManagementPolicy: Parallel # Without this, the STS stays available regardless of whether there is a PDB in place or not
  replicas: 2
  selector:
    matchLabels:
      app: pdb-fail
  serviceName: pdb-fail
  template:
    metadata:
      labels:
        app: pdb-fail
      name: pdb-fail
    spec:
      containers:
        - name: pdb-fail
          image: nginx
          lifecycle:
            preStop:
              exec:
                command:
                  - sleep
                  - "15"
      terminationGracePeriodSeconds: 1000

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: pdb-fail
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: pdb-fail

```
After successful deployment of the sts, you can use the following script or run these manually to force both pods into "Terminating" state:
```bash
kubectl get pod,sts,pdb
kubectl rollout restart sts pdb-fail
kubectl get pod,sts,pdb
sleep 25
kubectl get pod,sts,pdb
kubectl rollout restart sts pdb-fail
kubectl get pod,sts,pdb
echo Tada
```

Output looks as follows:
```text
$ bash trigger-issue.sh
NAME             READY   STATUS    RESTARTS   AGE
pod/pdb-fail-0   1/1     Running   0          63s
pod/pdb-fail-1   1/1     Running   0          82s

NAME                        READY   AGE
statefulset.apps/pdb-fail   2/2     5m3s

NAME                                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
poddisruptionbudget.policy/pdb-fail   N/A             1                 1                     5m3s
statefulset.apps/pdb-fail restarted
NAME             READY   STATUS        RESTARTS   AGE
pod/pdb-fail-0   1/1     Running       0          63s
pod/pdb-fail-1   1/1     Terminating   0          82s

NAME                        READY   AGE
statefulset.apps/pdb-fail   2/2     5m3s

NAME                                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
poddisruptionbudget.policy/pdb-fail   N/A             1                 0                     5m3s
NAME             READY   STATUS        RESTARTS   AGE
pod/pdb-fail-0   1/1     Terminating   0          84s
pod/pdb-fail-1   1/1     Running       0          5s

NAME                        READY   AGE
statefulset.apps/pdb-fail   2/2     5m24s

NAME                                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
poddisruptionbudget.policy/pdb-fail   N/A             1                 0                     5m24s
statefulset.apps/pdb-fail restarted
NAME             READY   STATUS        RESTARTS   AGE
pod/pdb-fail-0   1/1     Terminating   0          84s
pod/pdb-fail-1   1/1     Terminating   0          5s

NAME                        READY   AGE
statefulset.apps/pdb-fail   2/2     5m24s

NAME                                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
poddisruptionbudget.policy/pdb-fail   N/A             1                 0                     5m24s
Tada
```
#### Anything else we need to know?

I don't want to allow my service to route traffic to terminating pods, though I know that's an option.
I want the PDB to view terminating pods as unavailable, just like the default behavior of a service.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.3
```

</details>


#### Cloud provider

<details>
Azure Kubernetes Service
</details>


#### OS version

<details>

```console
# On Linux (In WSL):
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux DESKTOP-J27URRA 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows (Host for WSL):
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                          OSArchitecture  Version
22631        Microsoft Windows 11 Enterprise  64-bit          10.0.22631
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123885 HPA doesn't work because HPA reads cronjobs resources with a "completed" status.

- Issue 链接：[#123885](https://github.com/kubernetes/kubernetes/issues/123885)

### Issue 内容

#### What happened?


HPA Target is unknown state.
```
❯ k get hpa
NAME             REFERENCE                   TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
{{name}}   Deployment/{{deployment}}   <unknown>/50%   1         10        1          105m

```

But, deployment and pod resource settings are okay.
```
kubectl get -o json deployment {{deployment}} | jq '.spec.template.spec.containers[].resources'
{
  "limits": {
    "cpu": "1",
    "memory": "1500Mi"
  },
  "requests": {
    "cpu": "500m",
    "memory": "1000Mi"
  }
}
```

metric-server  logs are find.

hpa settings are okay.

hpa.yaml
```
apiVersion: v1
items:
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    labels:
      hpa.test: hpa
    name: {{name}}
    namespace: {{namespace}}
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 50
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: {{deployment name}}

```
( cronjobs not have  `hpa.test: hpa` labels but deployment have `hpa.test: hpa` label) 


HPA make some logs

```
 k describe hpa

...
  Type     Reason                   Age                     From                       Message
  ----     ------                   ----                    ----                       -------
  Warning  FailedGetResourceMetric  49m (x13 over 101m)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}} 
  Warning  FailedGetResourceMetric  39m (x72 over 110m)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}} 
  Warning  FailedGetResourceMetric  34m (x7 over 99m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}}of Pod {{cronjob pod name}} 
  Warning  FailedGetResourceMetric  29m (x2 over 82m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}}of Pod {{cronjob pod name}} 
  Warning  FailedGetResourceMetric  19m (x4 over 63m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}}  
  Warning  FailedGetResourceMetric  14m (x5 over 85m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}}  
  Warning  FailedGetResourceMetric  4m50s (x167 over 106m)  horizontal-pod-autoscaler  (combined from similar

```

There are many pod which state are "Completed"

```
❯ k get po
NAME                                                              READY   STATUS      RESTARTS      AGE
{{deployment name}}                                1/1     Running     0             19m
{{cronjob pod }}   0/1     Completed   4             7h43m
{{cronjob pod }} 0/1     Completed   3             5h43m
{{cronjob pod }}  0/1     Completed   3             163m
{{cronjob pod }}          0/1     Completed   4             6h53m

```


So, I think hpa read metric not only deployment also cronjob

#### What did you expect to happen?

I expect hpa to express an exact number

#### How can we reproduce it (as minimally and precisely as possible)?


Deploy the deployment and cronjob together and put the cronjob in completed status.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
v1.28.5 
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123882 pod preemptionPolicy can not set

- Issue 链接：[#123882](https://github.com/kubernetes/kubernetes/issues/123882)

### Issue 内容

#### What happened?

root@29-b:~/k8s-core-teaching/pod# kubectl apply -f preemptionPolicy-priority-Never-without-priority.yaml  -n k8s
Error from server (Forbidden): error when creating "preemptionPolicy-priority-Never-without-priority.yaml": pods "nginx-pod" is forbidden: the string value of PreemptionPolicy (Never) must not be provided in pod spec; priority admission controller computed PreemptLowerPriority from the given PriorityClass name

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: container
    image: nginx
  preemptionPolicy: Never
```

#### What did you expect to happen?

apply ok

#### How can we reproduce it (as minimally and precisely as possible)?

apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
root@29-b:~/k8s-core-teaching/pod# kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
</details>


#### Cloud provider

<details>
wmare
</details>


#### OS version

<details>
root@29-b:~/k8s-core-teaching/pod# cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```
wmare

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123859 Pods are incorrectly rejected after kubelet restart due to NUMA and node label changes

- Issue 链接：[#123859](https://github.com/kubernetes/kubernetes/issues/123859)

### Issue 内容

#### What happened?

1.NUMA causing rejection:
Node configuration: --cpu-manager-policy=static --memory-manager-policy=Static --topology-manager-policy=single-numa-node
InitContainer and main container's NUMA are calculated one by one.
CPU affinity is present, but in reality, the NUMA alignment of the InitContainer might not match the resource requirements of the main container.
Restarting kubelet leads to Pod rejection.
![WX20240311-175733@2x](https://github.com/kubernetes/kubernetes/assets/46776140/4ca5624f-3caa-4ba3-9518-6fb5148fe376)

2.Node Label change causing rejection:
Node has a label hpc=true, and the Pod has nodeAffinity based on this label.
Due to specific reasons, the label is removed.
Restarting kubelet results in Pod rejection with the error message: NodeAffinity failed

#### What did you expect to happen?

Regardless of the situation, kubelet should not reject Pods that are running and were created before the kubelet restart.

#### How can we reproduce it (as minimally and precisely as possible)?

I mentioned the steps in the 'what happened' section, and by following those steps, the issue can be reproduced consistently, 100% of the time.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
$ uname -a
Linux mjq-k8stestarsenalmaster-1 5.16.20-1.el7.x86_64 #3 SMP PREEMPT Thu Aug 18 13:53:10 CST 2022 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd v1.6.16 31aa4358a36870b21a992d3ad2bef29e1d693bec
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123840 StructuredAuthenticationConfiguration does not support ES256 algorithm

- Issue 链接：[#123840](https://github.com/kubernetes/kubernetes/issues/123840)

### Issue 内容

#### What happened?

I'm using [Pinniped](https://pinniped.dev/docs/background/architecture/) as an OIDC provider to authenticate users. following the Pinniped documentation we need to add the following flags to the api-server to trust supervisor as OICD provider
```
# Make this exactly match the spec.issuer of your Supervisor's FederationDomain.
# Note: It seems like the kube-apiserver pod cannot resolve `cluster.local`
# DNS names, so don't use one of those DNS names as the issuer.
--oidc-issuer-url="https://my-supervisor.example.com/my-issuer"

# This is only required if the kube-apiserver pod is not going to trust your
# Supervisor's FederationDomain's TLS certificates, e.g. if you used a
# self-signed CA. Make this match where you mounted the CA PEM file into
# your control plane node's filesystem, which must be under a directory
# that the kube-apiserver container is going to volume mount.
--oidc-ca-file="/etc/ca-certificates/supervisor/root-ca.pem"

# Choose a unique value for each cluster here. By making this unique, the
# Supervisor will be able to issue ID tokens for this cluster that cannot
# be used on any other cluster, which improves security. Do not use the
# special value "pinniped-cli" or any value that contains the substring
# ".pinniped.dev", because these special values are reserved for other
# purposes.
--oidc-client-id="my-cluster-342klb7h"

# Use these exact values. These are based on how the Supervisor issues ID
# tokens. Do not change these values.
--oidc-signing-algs="ES256"
--oidc-username-claim="username"
--oidc-groups-claim="groups"
```
which works fine. However when I tried to implement the same thing using the new featureGates `StructuredAuthenticationConfiguration ` in the apiserver as follow
```
- --feature-gates=StructuredAuthenticationConfiguration=true
- --authentication-config=/etc/kubernetes/auth-config.yaml
```

The output of `auth-config.yaml`
```
apiVersion: apiserver.config.k8s.io/v1alpha1
kind: AuthenticationConfiguration
jwt:
  - issuer:
      url: https://my-supervisor.example.com/my-issuer
      audiences:
        - my-cluster-342klb7h
       certificateAuthority: |
         xxxxxxxx
    claimMappings:
      username:
        expression: 'claims.email'
      groups:
        expression: 'claims.groups'
```

I'm getting ```Unable to authenticate the request err=[invalid bearer token, oidc; verify token: oidc: id token signed with unsupported algortithm, expected [\"RSA256\"] got \"ES256\"]"```

According to the quote from the KEP, all asymmetric algorithms are allowed, which is not the case
```
Goals:
Easy migration from existing OIDC flags
Note: we intend to drop the --oidc-signing-algs flag because this configuration provides no benefit (we will always allow all asymmetric algorithms)
```
https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3331-structured-authentication-configuration#goals

cc @cfryanr  to follow 

Any help, please? 

#### What did you expect to happen?

ES256 is an asymmetric algorithm so it should be allowed

#### How can we reproduce it (as minimally and precisely as possible)?

Allow all asymmetric algorithm 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.29.1

#### Cloud provider

<details>

</details>
On premise

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
Ubuntu 22

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>
Containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123839 Static pods with restartPolicy: OnFailure remain pending if kubelet restarts after container create but before start.

- Issue 链接：[#123839](https://github.com/kubernetes/kubernetes/issues/123839)

### Issue 内容

#### What happened?

Given a static pod with `restartPolicy: OnFailure`, and  a Kubelet crash while re-creating a failed container in the static pod, the pod may remain pending indefinitely.

1. Run a cluster with the patch at https://github.com/hoskeri/kubernetes/commit/60b103a1df145a688a72ebe1473deddec57109e0. The patch causes kubelet to crash when attempting to start a container that's been created after a failure.
2. Start a static pod with `restartPolicy: OnFailure`.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: staticpod-failure-tester
spec: 
  restartPolicy: OnFailure
  hostNetwork: true
  initContainers:
  - name: staticpod-failure-init-1
    image: "debian:unstable"
    command:
    - echo 
    - "staticpod-failure-tester-init"
  containers:
  - name: staticpod-failure-tester-main
    image: "redis:latest"
    command:
    - "redis-server"
```


4. Arm the crash to occur on the next StartContainer of our container.

echo -n staticpod-failure-tester-main > /run/crash-container-names

6. Kill the shim process and `crictl stopp $pod`. 

(Careful, modify the command to be more specific if other containers are running in your env)

`kill -9 $SHIM_PID; crictl pods -q|xargs crictl stopp`

7. Kubelet should create a replacement container for the killed container in the same sandbox.

9. Kubelet crashes and restarts on StartContainer.

10. After the first crash, kubelet will not trigger the crash again, because it will not attempt a StartContainer again. 
Instead, `computePodActions` wants to stop the existing sandbox, but this does not converge because containerd ignores the `StopContainer` request. 

```
info msg="Container to stop \"469b9ca1059f90783892692b7ecf22b967d015a390446b44ee29894026b7e46a\" must be in running or unknown state, current state \"CONTAINER_CREATED\""
```

```
kuberuntime_manager.go:1058] "computePodActions got for pod" podActions="KillPod: true, CreateSandbox: false, UpdatePodResources: false, Attempt: 3, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="default/staticpod-failure-tester-i-ea7ab8cd-6939-4748-b544-21d797194f5e"
```

11. The pod remains `Pending` indefinitely.

The real-world version of the above sequence involves a reboot of control plane host vm while Kubelet is still recovering pods from a previous crash - so two reboots in quick succession. 

Based on my reading of the code, it seems to me that the core issue is in the `containerSucceeded` function treats an uninitialized container as having succeeded. We incorrectly read the golang zero value of `containerStatus.ExitCode` as a successful exit code of `0` from the container. 

https://github.com/kubernetes/kubernetes/blob/8f80e0146726c42edefdfaeda6123872a5ec0981/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L535

This is not an issue if the restartPolicy is `Always` because https://github.com/kubernetes/kubernetes/blob/8f80e0146726c42edefdfaeda6123872a5ec0981/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L851 is not reached in that case.



#### What did you expect to happen?

The static pod should either end at a terminal phase, or restart the successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

See reproduction steps in the main report. 

Here's a strawman change that fixes the issue for my test case. 

https://github.com/hoskeri/kubernetes/commit/5270c2916f78b78aab6845de94cd4d06b271ecd3

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0-alpha.3.320+19df52c41a25ce
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release

PRETTY_NAME="Debian GNU/Linux trixie/sid"
NAME="Debian GNU/Linux"
VERSION_CODENAME=trixie
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux i-ea7ab8cd-6939-4748-b544-21d797194f5e 6.6.15-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.6.15-2 (2024-02-04) x86_64 GNU/Linux
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
Type:          io.containerd.grpc.v1
ID:            cri
Requires:     
               io.containerd.event.v1
               io.containerd.service.v1
Platforms:     linux/amd64
Exports:      
               CRIVersionAlpha      v1alpha2
               CRIVersion           v1
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
NA
</details>


### 分析结果

不涉及

---

## Issue #123837 kube-proxy does not appear to be creating iptables entries

- Issue 链接：[#123837](https://github.com/kubernetes/kubernetes/issues/123837)

### Issue 内容

#### What happened?

When setting up a new cluster using Kubernetes 1.29.2 on Debian 12.5 ("bookworm"), it appears that the necessary iptables entries to permit access to services, etc., are not being created by kube-proxy. Upon reaching the step in setting up the first control-plane node, post `kubeadm init`, at which it is necessary to add a networking option, the pods of the networking option invariably fail complaining that it is impossible to reach the Kubernetes API server via the Kubernetes service.

Things at this point appear normal except for the failed networking option pod:

```
❯ kubectl get node
NAME                STATUS   ROLES           AGE   VERSION
princess-celestia   Ready    control-plane   55m   v1.29.2

❯ kubectl get pod -A
NAMESPACE     NAME                                        READY   STATUS              RESTARTS       AGE
kube-system   coredns-76f75df574-62dx4                    0/1     ContainerCreating   0              55m
kube-system   coredns-76f75df574-m6dhg                    0/1     ContainerCreating   0              55m
kube-system   etcd-princess-celestia                      1/1     Running             3 (23m ago)    55m
kube-system   kube-apiserver-princess-celestia            1/1     Running             3 (22m ago)    55m
kube-system   kube-controller-manager-princess-celestia   1/1     Running             2 (23m ago)    55m
kube-system   kube-proxy-7pwnn                            1/1     Running             0              21m
kube-system   kube-scheduler-princess-celestia            1/1     Running             2 (23m ago)    55m
kube-system   weave-net-mnm4v                             1/2     CrashLoopBackOff    24 (25s ago)   50m
```

This example is from Weave, but the equivalent error also occurs in Flannel, leading me to conclude that the issue is not with them:

```
FATA: 2024/03/09 23:25:51.801940 [kube-peers] Could not get peers: Get "https://[fdc9:b01a:cafe:60::1]:443/api/v1/nodes": dial tcp [fdc9:b01a:cafe:60::1]:443: connect: network is unreachable
Failed to get peers
```

The kube-proxy pod log shows no calls to iptables:

```
I0309 23:20:38.208141       1 server_others.go:72] "Using iptables proxy"
I0309 23:20:38.212206       1 server.go:1050] "Successfully retrieved node IP(s)" IPs=["172.16.0.129"]
I0309 23:20:38.213216       1 conntrack.go:58] "Setting nf_conntrack_max" nfConntrackMax=262144
I0309 23:20:38.220948       1 server.go:652] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0309 23:20:38.220959       1 server_others.go:168] "Using iptables Proxier"
I0309 23:20:38.221908       1 proxier.go:245] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0309 23:20:38.221976       1 server.go:865] "Version info" version="v1.29.2"
I0309 23:20:38.221981       1 server.go:867] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0309 23:20:38.222805       1 config.go:315] "Starting node config controller"
I0309 23:20:38.222817       1 shared_informer.go:311] Waiting for caches to sync for node config
I0309 23:20:38.222829       1 config.go:188] "Starting service config controller"
I0309 23:20:38.222837       1 shared_informer.go:311] Waiting for caches to sync for service config
I0309 23:20:38.222939       1 config.go:97] "Starting endpoint slice config controller"
I0309 23:20:38.222943       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0309 23:20:38.323006       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0309 23:20:38.323015       1 shared_informer.go:318] Caches are synced for node config
I0309 23:20:38.323020       1 shared_informer.go:318] Caches are synced for service config
```

and while the chains and some relevant entries are seen, the essential ones appear to be missing, per the following output from _ip6tables-save_ and _iptables-save_:

```
root@princess-celestia:~# ip6tables-save
# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-IPTABLES-HINT - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-PROXY-CANARY - [0:0]
COMMIT
# Completed on Sat Mar  9 17:33:03 2024
# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-PROXY-FIREWALL - [0:0]
:KUBE-SERVICES - [0:0]
-A INPUT -j KUBE-FIREWALL
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A INPUT -m comment --comment "kubernetes health check service ports" -j KUBE-NODEPORTS
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a/128 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp has no endpoints" -m tcp --dport 53 -j REJECT --reject-with icmp6-port-unreachable
-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a/128 -p tcp -m comment --comment "kube-system/kube-dns:metrics has no endpoints" -m tcp --dport 9153 -j REJECT --reject-with icmp6-port-unreachable
-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a/128 -p udp -m comment --comment "kube-system/kube-dns:dns has no endpoints" -m udp --dport 53 -j REJECT --reject-with icmp6-port-unreachable
COMMIT
# Completed on Sat Mar  9 17:33:03 2024
# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-SEP-ZW3YEZJQTUKK7ANJ - [0:0]
:KUBE-SERVICES - [0:0]
:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully
-A KUBE-SEP-ZW3YEZJQTUKK7ANJ -s fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8/128 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ
-A KUBE-SEP-ZW3YEZJQTUKK7ANJ -p tcp -m comment --comment "default/kubernetes:https" -m tcp -j DNAT --to-destination [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443
-A KUBE-SERVICES -d fdc9:b01a:cafe:60::1/128 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES ! -d ::1/128 -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-NPX46M4PTMTKRN6Y ! -s fdc9:b01a:cafe::/56 -d fdc9:b01a:cafe:60::1/128 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-MARK-MASQ
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https -> [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443" -j KUBE-SEP-ZW3YEZJQTUKK7ANJ
COMMIT
# Completed on Sat Mar  9 17:33:03 2024
```

```
root@princess-celestia:~# iptables-save
# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-IPTABLES-HINT - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-PROXY-CANARY - [0:0]
COMMIT
# Completed on Sat Mar  9 17:33:35 2024
# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-PROXY-FIREWALL - [0:0]
:KUBE-SERVICES - [0:0]
:WEAVE-NPC - [0:0]
:WEAVE-NPC-DEFAULT - [0:0]
:WEAVE-NPC-EGRESS - [0:0]
:WEAVE-NPC-EGRESS-ACCEPT - [0:0]
:WEAVE-NPC-EGRESS-CUSTOM - [0:0]
:WEAVE-NPC-EGRESS-DEFAULT - [0:0]
:WEAVE-NPC-INGRESS - [0:0]
-A INPUT -j KUBE-FIREWALL
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A INPUT -m comment --comment "kubernetes health check service ports" -j KUBE-NODEPORTS
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment "block incoming localnet connections" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP
-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC -d 224.0.0.0/4 -j ACCEPT
-A WEAVE-NPC -m physdev --physdev-out vethwe-bridge --physdev-is-bridged -j ACCEPT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-DEFAULT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-INGRESS
-A WEAVE-NPC-EGRESS -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC-EGRESS -m physdev --physdev-in vethwe-bridge --physdev-is-bridged -j RETURN
-A WEAVE-NPC-EGRESS -m addrtype --dst-type LOCAL -j RETURN
-A WEAVE-NPC-EGRESS -d 224.0.0.0/4 -j RETURN
-A WEAVE-NPC-EGRESS -m state --state NEW -j WEAVE-NPC-EGRESS-DEFAULT
-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000/0x40000 -j WEAVE-NPC-EGRESS-CUSTOM
-A WEAVE-NPC-EGRESS-ACCEPT -j MARK --set-xmark 0x40000/0x40000
COMMIT
# Completed on Sat Mar  9 17:33:35 2024
# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-SERVICES - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
COMMIT
# Completed on Sat Mar  9 17:33:35 2024
```


#### What did you expect to happen?

Once `kubeadm init` had completed, installation of a network option should proceed and complete normally; it (and other pods) should be able to access the _kubernetes_ service.


#### How can we reproduce it (as minimally and precisely as possible)?

Rather than repeat the details of every command:

Take a vanilla, minimal Debian 12.5 installation, add containerd as the runtime, and then `kubeadm init`. Specifically, I use the cluster configuration file:

```
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration

localAPIEndpoint:
  advertiseAddress: fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8

nodeRegistration:
  taints: []

---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: 1.29.2

clusterName: harmony

apiServer:
  certSANs:
    - "princess-celestia.arkane-systems.lan"
    - "172.16.0.129"
  timeoutForControlPlane: 4m0s

etcd:
  local:
    dataDir: /var/lib/etcd

networking:
  dnsDomain: cluster.local
  serviceSubnet: fdc9:b01a:cafe:60::/112,10.96.0.0/16
  podSubnet: fdc9:b01a:cafe:f4::/56,10.244.0.0/16
```

to set up for IPv6 networking, using the command `kubeadm init --config ./cluster.conf`, although using different subnet configurations makes no difference.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
None.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux princess-celestia 6.1.0-18-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
None except `kubeadm`.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
# containerd --version
containerd containerd.io 1.6.28 ae07eda36dd25f8a1b98dfbf587313b99c0190bb
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123835 CRD validation allows invalid CABundles that will fail setting up handlers

- Issue 链接：[#123835](https://github.com/kubernetes/kubernetes/issues/123835)

### Issue 内容

#### What happened?

A CRDs with an invalid conversion webhook CABundle failed to serve requests not requiring conversion.


#### What did you expect to happen?

Since creating cr-1.yaml doesn't require conversion, I would have expected either:

1. An error response when attempting to create/update the CRD with an invalid CABundle
2. Only an error when conversion is required to server a request

#### How can we reproduce it (as minimally and precisely as possible)?


Create a mult-version CRD with a conversion webhook configured with an invalid CABundle:

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: replicant.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                myfield:
                  type: integer
    - name: v2
      served: true
      storage: false
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                myfield:
                  type: integer
  scope: Namespaced
  names:
    plural: replicant
    singular: replicants
    kind: Replicant
  conversion:
    strategy: Webhook
    webhook:
      conversionReviewVersions: [v1]
      clientConfig:
        service:
          namespace: default
          name: example-conversion-webhook-server
          path: /convert
        caBundle: "Cg=="
```

```
$ kubectl apply -f crd.yaml
customresourcedefinition.apiextensions.k8s.io/replicant.stable.example.com created
```

Read the CRD back:

```
$ kubectl get crd replicant.stable.example.com -oyaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
...
```

Attempt to create a CR using the storage version:

```yaml
apiVersion: stable.example.com/v1
kind: Replicant
metadata:
  name: cr1
spec:
  myfield: 1
```

```
$ kubectl apply cr1.yaml
Error from server (InternalError): error when retrieving current configuration of:
Resource: "stable.example.com/v1, Resource=replicant", GroupVersionKind: "stable.example.com/v1, Kind=Replicant"
Name: "cr1", Namespace: "default"
from server for: "cr-1.yaml": Internal error occurred: error resolving resource

$ grep "customresource_handler" /tmp/local-kube-apiserver.log
E0308 12:57:41.892888 1158130 customresource_handler.go:301] unable to load root certificates: unable to parse bytes as PEM block

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

All babsically. master...{introduction of CRD conversion}




#### Cloud provider

All

#### OS version

All

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #123832 [apisnoop-conformance-gate][conformance] ValidatingAdmissionPolicy - You have 17 untested endpoints

- Issue 链接：[#123832](https://github.com/kubernetes/kubernetes/issues/123832)

### Issue 内容

from: https://testgrid.k8s.io/sig-arch-conformance#apisnoop-conformance-gate&width=20

<img width="902" alt="image" src="https://github.com/kubernetes/kubernetes/assets/23304/1a304e9d-8a7c-4d42-a759-1eceab341471">

from: https://storage.googleapis.com/kubernetes-jenkins/logs/apisnoop-conformance-gate/1765238577685336064/build-log.txt
```
==================
UNTESTED ENDPOINTS
listAdmissionregistrationV1ValidatingAdmissionPolicyBinding
patchAdmissionregistrationV1ValidatingAdmissionPolicyStatus
patchAdmissionregistrationV1ValidatingAdmissionPolicy
patchAdmissionregistrationV1ValidatingAdmissionPolicyBinding
createAdmissionregistrationV1ValidatingAdmissionPolicyBinding
replaceAdmissionregistrationV1ValidatingAdmissionPolicy
deleteAdmissionregistrationV1CollectionValidatingAdmissionPolicy
deleteAdmissionregistrationV1CollectionValidatingAdmissionPolicyBinding
deleteAdmissionregistrationV1ValidatingAdmissionPolicy
deleteAdmissionregistrationV1ValidatingAdmissionPolicyBinding
readAdmissionregistrationV1ValidatingAdmissionPolicyStatus
replaceAdmissionregistrationV1ValidatingAdmissionPolicyStatus
readAdmissionregistrationV1ValidatingAdmissionPolicyBinding
replaceAdmissionregistrationV1ValidatingAdmissionPolicyBinding
readAdmissionregistrationV1ValidatingAdmissionPolicy
listAdmissionregistrationV1ValidatingAdmissionPolicy
createAdmissionregistrationV1ValidatingAdmissionPolicy

==================
You have 17 untested endpoints
==================
```


### 分析结果

不涉及。

---

## Issue #123816 Pod Failed with TopologyAffinityError because of init-container CPU NUMA Topology BUG

- Issue 链接：[#123816](https://github.com/kubernetes/kubernetes/issues/123816)

### Issue 内容

#### What happened?

My Node is 250Gi memory and 64 Core CPU, which has two NUMA node, both is 125Gi and 32 Core CPU, topology policy is single-numa-node.
Create 2 pod,  with init-container 1 Core CPU and 1Gi memory, app container 26 Core and 32Gi memory.
The first pod create success.
The second pod will failed with TopologyAffinityError.

Result from `lscpu`:
```
NUMA node0 CPU(s):     0-15,32-47
NUMA node1 CPU(s):     16-31,48-63
```

In `/var/lib/kubelet/cpu_manager_state` file: 
```
{
  "policyName": "static",
  "defaultCpuSet": "1,3,15-31,47-63",
  "entries": {
    "69aa6124-4e0e-4c66-94ea-feae7df25b68": {  # the second Pod, init-container use NUMA 0
      "init-container": "35"
    },
    "e49ac32c-22a3-4c8c-8be3-721bd1fd9eaa": {   # the first Pod, use NUMA 0
      "centos": "0,2,4-14,32,34,36-46",
      "init-container": "33"
    }
  },
  "checksum": 3748757820
}
```



#### What did you expect to happen?

Both two pods can be created successfully.
First pod use NUMA 0;
The sceond pod init-container can use either NUMA 0 or NUMA 1, but app container use NUMA 1;

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create one pod, which use CPU, for example 10 of 32(Numa Node) CPU of the node
2. Create another pod, which need init-container 1Core CPU, and app container need most of NUMA Node CPU,  like 25 Core (greater than the left of the first NUMA)
3.  The second pod will failed with TopologyAffinityError.

#### Anything else we need to know?

I think the bug is in `generateCPUTopologyHints ` function:  https://github.com/kubernetes/kubernetes/blob/7ea3d0245a63fbbba698f1cb939831fe8143db3e/pkg/kubelet/cm/cpumanager/policy_static.go#L623 
1. when allocate init-container CPU, CPUManager will record as cpuToReuse for pod. https://github.com/kubernetes/kubernetes/blob/7ea3d0245a63fbbba698f1cb939831fe8143db3e/pkg/kubelet/cm/cpumanager/policy_static.go#L345  
2. before call `generateCPUTopologyHints`, get cpuToReuse of the pod, as reusable https://github.com/kubernetes/kubernetes/blob/7ea3d0245a63fbbba698f1cb939831fe8143db3e/pkg/kubelet/cm/cpumanager/policy_static.go#L587
3. when caculate app container CPU, CPUManager will filter the NUMA Node which is not equal to reusableCPU's NUMA Node
4. Even if NUMA 1 is the only one that fits CPU resource acquirement, it still passed in calculating TopologyHint; NUMA 0 fits with reusableCPU, but CPU left is not enough. 

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123787  conn.go:254] Error on socket receive: read tcp 127.0.0.1:33421->127.0.0.1:54136: use of closed network connection

- Issue 链接：[#123787](https://github.com/kubernetes/kubernetes/issues/123787)

### Issue 内容

#### What happened?

I used the jenkins k8s plugin to dynamically schedule the pod .This error occurred when the pod was connected to execute the command after successful scheduling

#### What did you expect to happen?

Why is socket closed before to connecting? 

#### How can we reproduce it (as minimally and precisely as possible)?

Jenkins ERROR: 
[2024-03-06T13:20:56.162Z]  java.net.SocketTimeoutException: connect timed out

K8s node  /var/log/message  ERROR:
Mar  6 21:18:56 k8s-node-11 kubelet: E0306 21:18:56.398023  214481 conn.go:254] Error on socket receive: read tcp 127.0.0.1:33421->127.0.0.1:54136: use of closed network connection：

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.3-1+db95af67a8a660", GitCommit:"db95af67a8a660dc53ac09b8475161de2f5c711e", GitTreeState:"clean", BuildDate:"2021-01-22T07:54:01Z", GoVersion:"go1.15.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.3-1+db95af67a8a660", GitCommit:"db95af67a8a660dc53ac09b8475161de2f5c711e", GitTreeState:"clean", BuildDate:"2021-01-22T07:24:32Z", GoVersion:"go1.15.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
# paste output here
$ uname -a
# paste output here
Linux cambricon-test-ai74 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadmin
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
docker  Version: 20.10.12
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.8.4
</details>


### 分析结果

不涉及

---

## Issue #123784 client-go: reflector name defaulting seems broken by Go modules

- Issue 链接：[#123784](https://github.com/kubernetes/kubernetes/issues/123784)

### Issue 内容

#### What happened?

Client-go emits logs like
```
Listing and watching *v1.Namespace from pkg/mod/k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229
```


#### What did you expect to happen?

Log says "from <somewhere more useful>"

#### How can we reproduce it (as minimally and precisely as possible)?

Increase verbosity of anything using client-go, check logs

#### Anything else we need to know?

Code is from https://github.com/kubernetes/kubernetes/blame/05cb0a55c88e0cdcfe2fb184328ad9be53e94d5c/staging/src/k8s.io/client-go/tools/cache/reflector.go#L290. `client-go/tools/cache/` is no longer accurate, with go modules this will actually be something like `client-go@v0.29.1/tools/cache/` generally. The blame shows its 6 years old which was before go modules, which would make sense.

This also means this has probably been an issue for many years, and it only impacts logs that are generally disabled, so this is certainly not a high priority

#### Kubernetes version

<details>

```console
N/A
```

</details>


#### Cloud provider

<details>

</details>
N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123778 kublet prober infinite Readiness check - no Liveness probe defeating self-heal 

- Issue 链接：[#123778](https://github.com/kubernetes/kubernetes/issues/123778)

### Issue 内容

#### What happened?

pod(container) Readiness and Liveness probe are non-blocking routines. And if readiness probe is failing, a liveness probe can trigger restart and possibly self-heal.

However, encountered a case where;
- coredns pod starts, but an external automation causes IP removal on node. the cni IPAM is forced to sync the resource state and the coredns pod network ns is torn down and rebuilt - container ID change, but pod remains ID unchanged
```
Feb 28 16:33:53 ... kubelet.go:2456] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-f88c6698d-zmjgk" event={"ID":"001d42a0-1729-44fc-9959-b6e751ee44d9","Type":"ContainerStarted","Data":"9e324b5e26ac15640355f3dd86bcdb80f81f380827b435abc85365cd67fcc1f2"}
Feb 28 16:33:54 ... kubelet.go:2456] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-f88c6698d-zmjgk" event={"ID":"001d42a0-1729-44fc-9959-b6e751ee44d9","Type":"ContainerStarted","Data":"86d1a5bcff3978fcbaae12fc6259adade96747cc14c3aa3374102d41b34c1636"}
```

-  no startUp probe in coredns, so container ready, and doProbe readiness probe is sent
```
Feb 28 16:33:54 ... kubelet.go:2528] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/coredns-f88c6698d-zmjgk"
```

- this http probe fails with a http status code 503, aand a liveness probe is never issued and self-heal/restart triggered
```
Feb 28 16:33:54 ... prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-f88c6698d-zmjgk" podUID="001d42a0-1729-44fc-9959-b6e751ee44d9" containerName="coredns" probeResult="failure" output="HTTP probe failed with statuscode: 503"
Feb 28 16:33:56 ... prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-f88c6698d-zmjgk" podUID="001d42a0-1729-44fc-9959-b6e751ee44d9" containerName="coredns" probeResult="failure" output="HTTP probe failed with statuscode: 503"
Feb 29 00:18:22 ... prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-f88c6698d-zmjgk" podUID="001d42a0-1729-44fc-9959-b6e751ee44d9" containerName="coredns" probeResult="failure" output="HTTP probe failed with statuscode: 503"
```
---
- It is just unclear why the liveness probe on coredns spec is never sent
is the getWorker here to [UpdatePodStatus](https://github.com/kubernetes/kubernetes/blob/546f7c30860dcdecb75c544230a1b7cdf5bd5958/pkg/kubelet/prober/prober_manager.go#L289) after checking startup probe not introducing an inadvertent wait for readiness?

#### What did you expect to happen?

- kubernetes self-heal attempt of a pod that is stuck/failing probe

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness
    args:
    - /server
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 60 || 300
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
  restartPolicy: Always
```
- test pod above
- remove the IP assigned to a pod externally, after starting and readiness probe(unimportant), but before the liveness probe 
- forcing the node IPAM to re-sync and a similar error is encountered;
```
Mar 06 20:18:37 ... prober.go:107] "Probe failed" probeType="Readiness" pod="gateway-ns/liveness-http" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName="liveness" probeResult=failure output="HTTP probe failed with statuscode: 500"
Mar 06 20:18:47 ... prober.go:107] "Probe failed" probeType="Readiness" pod="gateway-ns/liveness-http" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName="liveness" probeResult=failure output="HTTP probe failed with statuscode: 500"
Mar 06 20:18:57 ... prober.go:107] "Probe failed" probeType="Readiness" pod="gateway-ns/liveness-http" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName="liveness" probeResult=failure output="HTTP probe failed with statuscode: 500"
Mar 06 20:19:07 ... prober.go:107] "Probe failed" probeType="Readiness" pod="gateway-ns/liveness-http" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName="liveness" probeResult=failure output="HTTP probe failed with statuscode: 500"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4...
Server Version: v1.29.1...-eks-...
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"

$ uname -a
inux ....compute.internal 5.10.198-187.748.amzn2.x86_64 #1 SMP Tue Oct 24 19:49:54 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123775 A Job might finish with ready!=0, terminating!=0

- Issue 链接：[#123775](https://github.com/kubernetes/kubernetes/issues/123775)

### Issue 内容

#### What happened?

When a Job is declared Failed, the running Pods still count as ready.
This causes problems for higher level controllers that use the Failed/Completed conditions to do usage accounting. If the job is marked as finished before all the pods finish, the accounting is inaccurate.

#### What did you expect to happen?

A few options (non-necessarily exclusive), in order of my preference:
1. The job not to be declared Completed/Failed until all the Pods have finished. This is possibly a breaking change. As a mitigation, we can use the `FailureTarget` or `SuccessCriteriaMet` conditions to provide early feedback to controllers that a Job is marked for failure.
2. ~Do not count Pods with a deletionTimestamp as ready. (Possibly a breaking change)~
3. ~We just consider ready=0 if setting a finished condition~
4. Continue syncing a Job even after the Failed condition is added, until ready/terminating fields are zero.

Options 2 and 3 don't satisfy the requirement of proper accounting.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Use this Job (note backoffLimit: 0):
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-job
  labels:
    jobgroup: indexedjob
spec:
  completions: 4
  parallelism: 4
  backoffLimit: 0
  completionMode: Indexed
  podReplacementPolicy: Failed
  template:
    metadata:
      labels:
        jobgroup: indexedjob
    spec:
      restartPolicy: Never
      containers:
      - name: 'worker'
        image: 'centos:7'
        command:
        - "sh"
        - "-c"
        - "echo 'hello' && sleep 120 && echo 'bye'
```
2. Delete one of the pods with kubectl

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
ANY
</details>


#### Cloud provider

<details>
ANY
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #123766 failed to delete cgroup paths

- Issue 链接：[#123766](https://github.com/kubernetes/kubernetes/issues/123766)

### Issue 内容

#### What happened?

Pods stuck in terminating state

lots of log entries like this one
```
journalctl -u kubelet --since -1m -f | grep "failed to delete cgroup paths"
Feb 28 05:57:26 worker kubelet[592400]: E0228 05:57:26.352096  592400 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to delete cgroup paths for [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : unable to destroy cgroup paths for cgroup [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : Failed to remove paths: map[:/sys/fs/cgroup/unified/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice blkio:/sys/fs/cgroup/blkio/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpu:/sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpuacct:/sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpuset:/sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice devices:/sys/fs/cgroup/devices/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice freezer:/sys/fs/cgroup/freezer/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice hugetlb:/sys/fs/cgroup/hugetlb/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice memory:/sys/fs/cgroup/memory/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice misc:/sys/fs/cgroup/misc/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice name=systemd:/sys/fs/cgroup/systemd/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice net_cls:/sys/fs/cgroup/net_cls,net_prio/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice net_prio:/sys/fs/cgroup/net_cls,net_prio/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice perf_event:/sys/fs/cgroup/perf_event/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice pids:/sys/fs/cgroup/pids/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice rdma:/sys/fs/cgroup/rdma/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice]" pod="nams-prod-de/attachment-upload-service-cb986ddbc-zz85t" podUID="c5659845-8967-408a-aa34-1223384f0ade"
```

#### What did you expect to happen?

Pods terminating and cgroups paths deleted

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure. In one cluster I have seen the issue multiple times. Other clusters with the same configuration do not show the same symptoms. 

#### Anything else we need to know?

Unable to delete cgroup scope
```
rmdir /sys/fs/cgroup/unified/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice/cri-containerd-1495ffbe17ae4d061ef354162d59a10fa7536e6512f8d280ceb1a21fa31fe684.scope
rmdir: failed to remove '/sys/fs/cgroup/unified/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice/cri-containerd-1495ffbe17ae4d061ef354162d59a10fa7536e6512f8d280ceb1a21fa31fe684.scope': Device or resource busy
```

Workaround
```
apt install cgroup-tools
lscgroup | grep <part-of-pod-id>
for cgroup in $(lscgroup | grep <part-of-pod-id>); do cgdelete -r $cgroup; done
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.10", GitCommit:"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99", GitTreeState:"clean", BuildDate:"2023-10-18T11:44:31Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.5", GitCommit:"506050d61cf291218dfbd41ac93913945c9aa0da", GitTreeState:"clean", BuildDate:"2023-12-19T13:32:53Z", GoVersion:"go1.20.12", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
none, kubeadm based cluster on VMware VMs
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux gtloclvu14373 5.15.0-94-generic #104~20.04.1-Ubuntu SMP Tue Jan 16 13:34:09 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
Kubermatic Kubernetes Platform v2.24
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.6.28 ae07eda36dd25f8a1b98dfbf587313b99c0190bb

runc version 1.1.12
commit: v1.1.12-0-g51d5e94
spec: 1.0.2-dev
go: go1.20.13
libseccomp: 2.5.1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Canal v3.26.1
</details>


### 分析结果

不涉及

---

## Issue #123761 patching a service with the same value ipFamilyPolicy: "RequireDualStack" causes an error

- Issue 链接：[#123761](https://github.com/kubernetes/kubernetes/issues/123761)

### Issue 内容

#### What happened?

1. Used the [ingress-nginx](https://github.com/kubernetes/ingress-nginx) helm chart to deploy a dual stack ingress.
2. Ran helm upgrade --install with the exact same values again to gain idempotancy
3. Got an error that describes, that ipFamilyPolicy cannot be patched, even though the same value "RequireDualStack" was used.
4. Created a [bug](https://github.com/kubernetes/ingress-nginx/issues/11068) on ingress-nginx because I initially thought this was an issue with the helm-chart
5. The folks over there were kind enough to route me here with a more comprehensive point to look at: https://github.com/kubernetes/kubernetes/blob/dc3f5ec6ccb9855dfa99f4c1078625df5fdfab6a/pkg/registry/core/service/storage/alloc.go#L184

#### What did you expect to happen?

A patch with the same value in the service will not result in an error.

#### How can we reproduce it (as minimally and precisely as possible)?

```sh
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

# 1st installation on a fresh cluster, no issues
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace --reuse-values --set resources=null --set controller.service.ipFamilyPolicy=RequireDualStack --wait

# 2nd try, same cluster, this time the error happens
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace --reuse-values --set resources=null --set controller.service.ipFamilyPolicy=RequireDualStack --wait
```

#### Anything else we need to know?

Independent of cloudprovider or used loadbalancers, this issue comes directly from kubernetes itself.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Server Version: v1.28.2
```

</details>


#### Cloud provider

<details>
Doesn't matter
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux beta 5.15.0-1052-oracle #58-Ubuntu SMP Tue Feb 13 19:44:34 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux

```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
runc
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
probably not of interest, but
- calico
- longhorn
- metallb
</details>


### 分析结果

不涉及

---

## Issue #123746 pod topologySpreadConstraints nodeAffinityPolicy: Ignore not work？

- Issue 链接：[#123746](https://github.com/kubernetes/kubernetes/issues/123746)

### Issue 内容

#### What happened?

crd
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 6
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: nginx
```

```
apiVersion: v1
kind: Pod
metadata:
  name: web-server
  labels:
    app: myapp
spec:
  containers:
  - name: web-server
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - web
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: myapp
    nodeAffinityPolicy: Ignore
```

```
root@29-b:~# kubectl get pod -n k8s  -owide -w
NAME                                READY   STATUS    RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES
myapp-deployment-7546cdb555-d55cf   1/1     Running   0          57m   10.0.1.118   worker-03   <none>           <none>
myapp-deployment-7546cdb555-dt4vs   1/1     Running   0          57m   10.0.0.129   worker-01   <none>           <none>
myapp-deployment-7546cdb555-j9v5z   1/1     Running   0          57m   10.0.1.61    worker-03   <none>           <none>
myapp-deployment-7546cdb555-kc9zf   1/1     Running   0          57m   10.0.3.79    master-01   <none>           <none>
myapp-deployment-7546cdb555-tzbnj   1/1     Running   0          57m   10.0.2.182   worker-02   <none>           <none>
myapp-deployment-7546cdb555-zjmsw   1/1     Running   0          57m   10.0.0.94    worker-01   <none>           <none>
web-server                          0/1     Pending   0          12m   <none>       <none>      <none>           <none>
```

2m16s       Warning   FailedScheduling    pod/web-server                           0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.


nodeTaintsPolicy has the same problem

#### What did you expect to happen?

ignore nodeaffinity

#### How can we reproduce it (as minimally and precisely as possible)?

apply my crd,node without that label

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
^Croot@29-b:~ kubectl versionon
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
vmware
</details>


#### OS version

<details>

```console
root@29-b:~# cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #123743 ReadWriteMany access mode should not be allowed with hostPath volumes.

- Issue 链接：[#123743](https://github.com/kubernetes/kubernetes/issues/123743)

### Issue 内容

#### What happened?

We know that `hostPath` doesn't support `ReadWriteMany` access mode from the docs[1]. But if we attempt to create a PV and PVC with `hostPath` option enabled and the access mode with `ReadWriteMany`. Kubernetes allows this to be created but when the end goal is to have a common volume shared by many pods of a deployment running on multiple nodes. This setup causes confusion to the users, as they assumed everything is setup perfectly since there was no error while creating the PV and PVC. 
When the user tries to write some files from one of the pods, the data will not be accessible to the other pods, since it is mounting the `hostpath` and writing onto it.

This was observed with deployment that was deployed on GKE.
 
[1] https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes:~:text=%2D-,HostPath,-%E2%9C%93`

#### What did you expect to happen?

Kubernetes should not allow creating PV's with `hostPath` option enabled along with Access mode set to `ReadWriteMany`. We should provide some kind of error message or have a web-hook that blocks the creation of these PV's.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a PV and PVC with following configuration.

```
--- PV---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  storageClassName: standard
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: "/tmp/data"

---PVC---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  volumeName: test-pv

``` 

- Create a deployment that mounts the above volume at `/tmp/data` path like below
```
volumeMounts:
        - name: test-data
          mountPath: /tmp/data
      volumes:
        - name: test-data
          persistentVolumeClaim:
            claimName: test-pvc
```

- This will be successfully created but the user will not be able to access the shared volume since the `hostPath` option is mounting the host filesystem path of the node the pod is running. 







#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.9-dispatcher", GitCommit:"8b508a33aafcd3ba51641b6b2ef203adbdd9de1e", GitTreeState:"clean", BuildDate:"2023-12-21T23:22:51Z", GoVersion:"go1.20.12", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.13-gke.1052000", GitCommit:"c88fda4c3b7ef204c20602c368811d8ba2fdd2e6", GitTreeState:"clean", BuildDate:"2024-01-23T17:45:47Z", GoVersion:"go1.20.13 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
Google Cloud Platform 
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux cs-779332008923-default 6.1.58+ #1 SMP PREEMPT_DYNAMIC Mon Jan 29 15:19:25 UTC 2024 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123704 Completed Job leaves orphaned pod

- Issue 链接：[#123704](https://github.com/kubernetes/kubernetes/issues/123704)

### Issue 内容

#### What happened?

We have a job template:
```
apiVersion: batch/v1
kind: Job
metadata:
  name: REPLACE_ME
spec:
  backoffLimit: 6
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      name: test
    spec:
      serviceAccountName: job-sa
      containers:
      - name: document-test
        resources:
          limits:
            memory: 2000Mi
            cpu: 2000m
          requests:
            memory: 500Mi
            cpu: 500m
        image: REPLACE_ME
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: secrets-volume
            mountPath: /app/secrets
      restartPolicy: Never
      volumes:
        - name: secrets-volume
          secret:
            secretName: ****-secrets
```

After launching a random number of jobs (10-600), all jobs are deleted after completion, whether the status is succeeded or failed. However, this does not remove all associated pods and leaves a random number of orphaned pods.

The problem arose suddenly, there were no changes to the teamplate. There are also no changes in the args for the container.

#### What did you expect to happen?

As before, I expected that all associated pods would be deleted after deleting the jobs.

#### How can we reproduce it (as minimally and precisely as possible)?

If there needed i can share my full job part.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
serverVersion:
  buildDate: "2024-01-02T20:34:38Z"
  compiler: gc
  gitCommit: 3f8ed3d5017d988600f597734a4851930eda35a6
  gitTreeState: clean
  gitVersion: v1.27.9-eks-5e0fdde
  goVersion: go1.20.12
  major: "1"
  minor: 27+
  platform: linux/amd64
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux 5.10.209-198.858.amzn2.x86_64 #1 SMP Tue Feb 13 18:46:41 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123683 CustomResourceDefinition Watch test is flaky when run multiple times on a same cluster

- Issue 链接：[#123683](https://github.com/kubernetes/kubernetes/issues/123683)

### Issue 内容

#### What happened?

We run conformance test on a same cluster multiple times. The test failed initially with

```
  INFO: Unexpected error: failed to list events in namespace "crd-watch-7911": 
      <*url.Error | 0xc004873770>: 
      Get "https://<CP_IP_AND_PORT>/api/v1/namespaces/crd-watch-7911/events": dial tcp <CP_IP_AND_PORT>: connect: connection refused
      {
          Op: "Get",
          URL: "https://<CP_IP_AND_PORT>/api/v1/namespaces/crd-watch-7911/events",
          Err: <*net.OpError | 0xc0006c3220>{
              Op: "dial",
              Net: "tcp",
              Source: nil,
              Addr: <*net.TCPAddr | 0xc003832750>{
                  <redacted>
                  Zone: "",
              },
              Err: <*os.SyscallError | 0xc002a6f680>{
                  Syscall: "connect",
                  Err: <syscall.Errno>0x6f,
              },
          },
      }
  [FAILED] in [DeferCleanup (Each)] - test/e2e/framework/debug/dump.go:44 @ 02/20/24 23:38:32.653
```

Then, all the subsequent tests failed with the following error

```
  [FAILED] failed to create CustomResourceDefinition: customresourcedefinitions.apiextensions.k8s.io "noxus.mygroup.example.com" already exists
  In [It] at: test/e2e/apimachinery/crd_watch.go:71 @ 02/21/24 23:36:15.799
```

****

#### What did you expect to happen?

The conformance test should withstand transient errors and crd should be cleaned up after the failed run

#### How can we reproduce it (as minimally and precisely as possible)?

The same issue can be reproduced by creating the same CRD on the cluster and run the conformance test. 

(Fix can also be verified by running conformance tests at least twice after CRD creation - the first run can fail, but the second run should pass)

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: noxus.mygroup.example.com
spec:
  conversion:
    strategy: None
  group: mygroup.example.com
  names:
    categories:
    - all
    kind: WishIHadChosenNoxu
    listKind: NoxuItemList
    plural: noxus
    shortNames:
    - foo
    - bar
    - abc
    - def
    singular: nonenglishnoxu
  scope: Cluster
  versions:
  - name: v1beta1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.5", GitCommit:"6b1d87acf3c8253c123756b9e61dac642678305f", GitTreeState:"clean", BuildDate:"2021-03-18T01:10:43Z", GoVersion:"go1.15.8", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
This is a bare metal cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123677 SetFeatureGateDuringTest doesn't work if test is run in pararell. 

- Issue 链接：[#123677](https://github.com/kubernetes/kubernetes/issues/123677)

### Issue 内容

#### What happened?

`defer featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.ConsistentListFromCache, true)()` should change the feature flag to true, however feature flag is a global state, so if somewhere in the test we call `t.Parallel()` there is a risk that some other test changes the flag state in the mean time.

Found this when we launched broken ConsistentListFromCache to Alpha and only noticed issues in Beta as the test didn't really test anything. https://github.com/kubernetes/kubernetes/pull/123674/files#r1511492942.



#### What did you expect to happen?

We should protect against usage of `featuregatetesting.SetFeatureGateDuringTest` and `t.Parallel` in a single test. 

#### How can we reproduce it (as minimally and precisely as possible)?

```
func TestFeatureGateOn(t *testing.T) {
	defer featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.ConsistentListFromCache, true)()
	runFeatureGateTest(t, true)
}

func TestFeatureGateOff(t *testing.T) {
	defer featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.ConsistentListFromCache, false)()
	runFeatureGateTest(t, false)
}

func runFeatureGateTest(t *testing.T, expectEnabled bool) {
	t.Parallel()
	time.Sleep(time.Second)
	gotEnabled := utilfeature.DefaultFeatureGate.Enabled(features.ConsistentListFromCache)
	if gotEnabled != expectEnabled {
		t.Errorf("unexpected feature gate state, got: %v, want: %v", gotEnabled, expectEnabled)
	}
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123651 [kubeadam][structured authz] can't use structured authz due to default `--authorization-mode` flag

- Issue 链接：[#123651](https://github.com/kubernetes/kubernetes/issues/123651)

### Issue 内容

#### What happened?

**TL;DR**
https://github.com/kubernetes/kubernetes/blob/7c11cc9cfcdc54a7ca4efdc10609bf421bfa54d4/cmd/kubeadm/app/phases/controlplane/manifests.go#L232 prevents us from using structured authz (setting `--authorization-config` flag) as those flags are mutually exclusive

/cc @palnabarun (as the author of https://github.com/kubernetes/kubernetes/pull/120154)
/cc @liggitt 

FYI @stealthybox @robscott 

To use structured authz you need to define the authorization configuration using the `--authorization-config` flag.

However this flag is mutually exclusive with `--authorization-modes` and `--authorization-webhook-*` flags.  (refer to [1](https://github.com/kubernetes/kubernetes/blob/7c11cc9cfcdc54a7ca4efdc10609bf421bfa54d4/pkg/kubeapiserver/options/authorization.go#L113) & [2](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#configuring-the-api-server-using-an-authorization-config-file))

Kubeadam adds `--authorization-mode` by default ([1](https://github.com/kubernetes/kubernetes/blob/7c11cc9cfcdc54a7ca4efdc10609bf421bfa54d4/cmd/kubeadm/app/phases/controlplane/manifests.go#L232) & [2](https://github.com/kubernetes/kubernetes/blob/7c11cc9cfcdc54a7ca4efdc10609bf421bfa54d4/cmd/kubeadm/app/phases/controlplane/manifests.go#L241)) which prevents us from using `--authorization-config`

#### What did you expect to happen?

I should be able to define the `authorization-config`  flag and have structured authz working.

#### How can we reproduce it (as minimally and precisely as possible)?

Spin up a kind cluster using this kind config (make sure the images are > 1.29 for structured authz to be avaliable)

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  StructuredAuthorizationConfiguration: true
kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
      extraArgs:
        authorization-config: "/files/authorization_config.yaml" 
      extraVolumes:
      - name: api-server-basic-auth-files
        hostPath: "/files"
        mountPath: "/files"
        readOnly: true
nodes:
- role: control-plane
  # add a mount from /path/to/my/files on the host to /files on the node
  extraMounts:
  - hostPath: <some-path-on-my-host>/refgrant-poc/files
    containerPath: /files
```

(For this bug, the content of `/files/authorization_config.yaml` doesn't matter.)

However, the cluster become unhealthy.
docker exec to the kind cluster container, where we can see the inner container of the apiserver has exited. 
Running `crictl logs <container>` we can see this error

<img width="1720" alt="image" src="https://github.com/kubernetes/kubernetes/assets/76940994/efeee626-7b6d-42de-a5e8-cee546b60194">


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
The kind cluster version image: `kindest/node:v1.29.2`
```

</details>


#### Cloud provider

<details>
N/A -I am using kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123631 The image of a running container was deleted by the image garbage collection

- Issue 链接：[#123631](https://github.com/kubernetes/kubernetes/issues/123631)

### Issue 内容

#### What happened?

The image of the container was deleted while pod was running.
When I noticed it, it was already almost two weeks after the pod was created, and by prometheus metrics, I could see at the time that the pod was created the kubelet performed image garbage collection on the node and images were deleted.

It might be worth mentioning that nothing other than the kubelet could have deleted the image from the node.

This situation happen a while ago, and I couldn't deal with it for too long while it happened, So unfortunately I don't have data to share. But by looking in the code the following scenario seems to make sense:
1. At the start of the garbage collection process the image was already in the node (for a while), but not in used yet. 
2. The garbage collection get the current images and pods on that node from the runtime, because this image is not in use by any pod, it is considered unused.  Ref: https://github.com/kubernetes/kubernetes/blob/b340ef2e29b1694464bee60487160654d4bf8bbc/pkg/kubelet/images/image_gc_manager.go#L232-L247
3. Before the image is actually deleted, A pod using this image is scheduling at this node. since the image is already on the node and the `imagePullPolicy` is `IfNotPresent` there is no need for pulling and the container is start to running. There is no knowledge about that the image is about to be deleted. Ref: https://github.com/kubernetes/kubernetes/blob/b340ef2e29b1694464bee60487160654d4bf8bbc/pkg/kubelet/images/image_manager.go#L126-L143
4. The garbage collection is still running, and it eventually comes to delete this image. There is no knowledge about that the image is in use now.

In short, I think that in the time between pods are detected from runtime to find the unused image to the time those images are  actually deleted (the time between step 2 to step 4), new pods could be scheduling on that node, and make use of the images that will be deleted by the garbage collection immediately afterwards. Although the chances of this scenario are rare, the troublesome time I described would be bigger when the node could contain a lot of images and pods, and the difference between `HighThresholdPercent` and `LowThresholdPercent` have an effect as well.



#### What did you expect to happen?

The image garbage collection should not delete used images. 

#### How can we reproduce it (as minimally and precisely as possible)?

Suppose an unused image X has already been on the node for some time.
The timeline is designed as following,
![image](https://github.com/kubernetes/kubernetes/assets/34262680/01a8992a-95d8-44cb-bbca-8bcf249496a8)
Any pod that uses image X and is scheduling on the node within the orange zone will cause this issue.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-15T00:36:28Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
local
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>
CRI-O v1.27.3 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及。

---

## Issue #123618 warning message for `get` subcommand with `--watch`

- Issue 链接：[#123618](https://github.com/kubernetes/kubernetes/issues/123618)

### Issue 内容

#### What happened?

When running subcommand `get` like this: `kubernetes get events --sort-by="{.lastTimestamp}" -w` a warning message will display: `warning: --watch or --watch-only requested, --sort-by will be ignored` which is incorrect in `watch` mode. It is only correct in `--watch-only` mode.

#### What did you expect to happen?

Warning message to only display when `--watch-only` flag is passed in. 

#### How can we reproduce it (as minimally and precisely as possible)?

Run `kubernetes get events --sort-by="{.lastTimestamp}" -w`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.27.1
Kustomize Version: v5.0.1
Server Version: v1.27.8-gke.1067004
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123616 how to connect specific Pods of Api and asp.net core MVC in Kubernetes?

- Issue 链接：[#123616](https://github.com/kubernetes/kubernetes/issues/123616)

### Issue 内容

#### What happened?

I have Two projects asp .net core Api and asp .net core MVC. In the asp .net core Api have a Two Replicas(instance) and asp .net core MVC have a three Replicas(instances).

So i want connect one pod of Api to one pod of MVC and than connect second pod of Api to second pod of MVC and than connect third pod of MVC to both pod of Api.

So i want when i Run the one pod of Api and MVC connected the Api data get and show in the MVC project.

So i want Pod's url for get data from Api.

#### What did you expect to happen?

I want to use with specific pod connection of Api and MVC to Get data through Api and show in MVC.

#### How can we reproduce it (as minimally and precisely as possible)?

I trying to pass pod url (Base Url) like this below image but this url is not working in MVC

I want to use with specific pod connection of Api and MVC to Get data through Api and show in MVC.



#### Anything else we need to know?

_No response_

#### Kubernetes version

![kub](https://github.com/kubernetes/kubernetes/assets/102901250/c3eb0ed0-72c0-46e2-8dd6-7c3b58193b86)


#### Cloud provider

    `    
   
      [HttpGet]

     public async Task<ActionResult> GetMySqlData()

    {

        string Baseurl = "http://webapiservice-0.webapiservice.default.pod.cluster.local:8081/";
        
        List<Customer> EmpInfo = new List<Customer>();

        using (var client = new HttpClient())
        {
            //Passing service base url  
            client.BaseAddress = new Uri(Baseurl);

            //Sending request to find web api REST service resource GetAllEmployees using HttpClient  
            HttpResponseMessage Res = await client.GetAsync("home/MySqlData");

            //Checking the response is successful or not which is sent using HttpClient  
            if (Res.IsSuccessStatusCode)
            {
                //Storing the response details recieved from web api   
                var EmpResponse = Res.Content.ReadAsStringAsync().Result;

                //Deserializing the response recieved from web api and storing into the Employee list  
                EmpInfo = JsonConvert.DeserializeObject<List<Customer>>(EmpResponse);

            }
            //returning the employee list to view  
            return View(EmpInfo);

        }
    }
`


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123612 When the node is in the NotReady state for 5 minutes, the DamonSet Pod on the node is still in the Running state.

- Issue 链接：[#123612](https://github.com/kubernetes/kubernetes/issues/123612)

### Issue 内容

#### What happened?

When the kubelet on the node server2 stopped for 5 minutes, it was found that the Pod of DamonSet on the server2 node was still in the Running state. as follows:
![9bb35de7f03cfaf2a9a16225b7e3c4b](https://github.com/kubernetes/kubernetes/assets/110345347/7b8c6a8e-7ac0-4cfc-b8b9-ecce896408d3)


#### What did you expect to happen?

When the node is in the NotReady state for 5 minutes, the Pod of the DamonSet on the node should be in the NodeLost state.

#### How can we reproduce it (as minimally and precisely as possible)?

1. In a k8s cluster, stop the kubelet on one of the nodes.
2. After 5 minutes, observe the running status of the DamonSet Pod on this node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version v1.28.2
# paste output here
```

</details>


#### Cloud provider

<details>
local
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
CRI-O v1.28.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico 
</details>



### 分析结果

不涉及

---

