# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (7 个)

## Issue #125220 AppArmor profile parser error

- Issue 链接：[#125220](https://github.com/kubernetes/kubernetes/issues/125220)

### Issue 内容

#### What happened?

I0530 12:13:05.231090       1 loader.go:97] Polling /profiles every 30s
W0530 12:13:05.234334       1 loader.go:174] AppArmor parser error for /profiles/k8s-nginx in /etc/apparmor.d/tunables/etc at line 25: Could not open 'if'
W0530 12:13:05.234370       1 loader.go:144] Error reading /profiles/k8s-nginx: error reading profiles from /profiles/k8s-nginx: exit status 1

#### What did you expect to happen?

profile should be loaded without any issue. I am trying in Azure aks

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader
tried above link to setups on Azure aks

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

@:/mnt/c/poc/AppArmor/apparmorAzure$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.9", GitCommit:"1c9860e7360c3f8147ae068e867eaab73b4a6257", GitTreeState:"clean", BuildDate:"2024-04-12T23:21:51Z", GoVersion:"go1.20.12", Compiler:"gc", Platform:"linux/amd64"}

</details>


#### Cloud provider

<details>
Azure AKS
</details>


#### OS version

<details>

```
root@aks-devapp-25368695-vmss000000:/#  cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
root@aks-devapp-25368695-vmss000000:/# uname -a
Linux aks-devapp-25368695-vmss000000 5.15.0-1061-azure #70-Ubuntu SMP Wed Apr 3 02:05:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
root@aks-devapp-25368695-vmss000000:/#

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因及可能的影响：**

在该Issue中，AppArmor配置文件解析错误导致配置文件未能正确加载。这可能导致应用程序未受到预期的安全策略保护，以默认或“unconfined”的模式运行。攻击者可以利用这一点，执行未受限制的操作，如访问敏感数据、执行任意代码或提升权限等。

由于AppArmor未能应用预期的限制，应用程序的攻击面增大，可能被攻击者利用，从而对系统造成严重影响。

**Proof of Concept：**

1. 按照提供的链接（https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader）在Azure AKS上部署AppArmor配置。
2. 观察到AppArmor解析器在加载`/profiles/k8s-nginx`配置文件时出错，具体错误为：

   ```
   W0530 12:13:05.234334       1 loader.go:174] AppArmor parser error for /profiles/k8s-nginx in /etc/apparmor.d/tunables/etc at line 25: Could not open 'if'
   ```

3. 由于解析错误，AppArmor配置未能正确应用，目标应用程序可能在未受限制的模式下运行。
4. 攻击者可以利用应用程序缺乏AppArmor限制的漏洞，执行超出预期权限的操作，例如读取系统文件、访问其他容器资源或执行恶意代码。

**CVSS 评分：**

根据CVSS 3.1标准，此漏洞的评分可能在高风险范围内，具体评估如下：

- **攻击向量（AV）：** 网络（N）
- **攻击复杂度（AC）：** 低（L）
- **权限要求（PR）：** 无（N）
- **用户交互（UI）：** 无（N）
- **作用范围（S）：** 未改变（U）或已改变（C），取决于具体情况
- **机密性影响（C）：** 高（H）
- **完整性影响（I）：** 高（H）
- **可用性影响（A）：** 高（H）

综合评分可能为高于7.0，属于高风险级别。

**建议：**

- 检查并修复AppArmor配置文件中的语法错误，确保配置文件能够正确解析和加载。
- 在部署之前，使用`apparmor_parser`工具对配置文件进行语法验证。
- 实施配置文件的完整性校验，防止配置文件在传输或部署过程中被篡改。

---

## Issue #125053 ValidatingAdmissionPolicy objects have different runtime type compared to CRDValidationRules

- Issue 链接：[#125053](https://github.com/kubernetes/kubernetes/issues/125053)

### Issue 内容

#### What happened?

Applied CRD:

```yaml
openAPIV3Schema:
        type: object
        properties:
          duration:
            type: string
            format: duration
            "x-kubernetes-validations":
            - rule: "self >= duration(\"60m\")"
              message: "duration must be at least 1 hour"
```

Created policy with following rule:

```yaml
  validations:
    - expression: "object.duration >= duration(\"2h\")"
      message: "duration must be at least 2 hours"
```

Applied instances of CRD:

```yaml
apiVersion: example.com/v1
kind: Thing
metadata:
  name: example-thing
duration: 2h
---
apiVersion: example.com/v1
kind: Thing
metadata:
  name: bad-thing
duration: 5m
```

Correctly received error from CRD rule for `bad-thing`, incorrectly received error for `good-thing` on policy due to missing overload for `>=` operator:

```console
❯ kubectl apply -f ./instance.yaml
Error from server (Invalid): error when creating "./instance.yaml": things.example.com "example-thing" is forbidden: ValidatingAdmissionPolicy 'example-policy.example.com' with binding 'example-binding.example.com' denied request: expression 'object.duration >= duration("2h")' resulted in error: no such overload
Error from server (Invalid): error when creating "./instance.yaml": Thing.example.com "bad-thing" is invalid: duration: Invalid value: "string": duration must be at least 1 hour
```


This is because CRDValidationRules consider the schema when representing the object in CEL, while VAP directly exposes the unstructured serialization. i.e.  a `duration`-format `string` in a CRD is represented in CEL using a CEL `duration`, but in VAP it is represented using a `string`

https://github.com/kubernetes/kubernetes/blob/e1b0bc3d0a7fb89a1e60f4ec1ee34b10de22d00a/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter.go#L158-L174

https://github.com/kubernetes/kubernetes/blob/1453085b44ff96b10ab25b68ce34929829114521/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go#L786-L804

#### What did you expect to happen?

VAP expressions and CRD rules to work the same way on the same objects in CEL

#### How can we reproduce it (as minimally and precisely as possible)?

https://gist.github.com/alexzielenski/db4faa6cb26a6b31dc967fa18dc79199

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30

#### Cloud provider

N/A

#### OS version

Tested using kind 1.30

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

存在潜在的安全风险。

**原因分析：**

在Kubernetes中，ValidatingAdmissionPolicy（VAP）和CRDValidationRules在使用CEL（Common Expression Language）表达式时，对对象属性的类型处理方式存在不一致。

- **CRDValidationRules**：根据OpenAPI V3 Schema的定义，会将具有`format`（如`duration`）的`string`类型字段在CEL中表示为对应的类型（如`duration`）。因此，表达式`self >= duration("60m")`中的`self`被视为`duration`类型，可以正确比较。

- **ValidatingAdmissionPolicy（VAP）**：直接暴露未经类型转换的对象属性，即使属性在OpenAPI Schema中指定了`format`，在CEL中仍被视为`string`类型。因此，`object.duration`在CEL中是`string`类型，而`duration("2h")`是`duration`类型，导致表达式`object.duration >= duration("2h")`类型不匹配，出现错误。

这种不一致导致VAP的验证规则无法按预期执行，可能出现以下安全问题：

1. **验证规则被绕过**：由于类型不匹配，验证表达式可能抛出错误或始终返回`false`，导致本应被拒绝的请求被接受。

2. **拒绝服务攻击**：攻击者可以通过提交特制的请求，触发验证表达式的错误，导致API服务器拒绝服务或性能下降。

**可能的影响：**

- **安全策略失效**：攻击者可以利用此漏洞，绕过VAP的验证规则，创建或修改不符合安全要求的资源，危及集群的安全性和完整性。

- **数据篡改和泄露**：未经验证或不合规的资源可能导致数据被篡改或敏感信息泄露。

- **权限提升**：攻击者可能通过创建特定资源，提升自身权限或获取未经授权的访问。

**CVSS 3.1评分：**

- 攻击向量（AV）：网络（N）——漏洞可通过网络远程利用。
- 攻击复杂度（AC）：低（L）——利用该漏洞不需要特殊条件。
- 必要权限（PR）：低（L）——攻击者需要能够向API服务器提交请求的权限。
- 用户交互（UI）：无（N）——不需要其他用户的参与。
- 作用域（S）：未改变（U）——仅影响Kubernetes自身组件。
- 机密性（C）：高（H）——可能导致敏感信息泄露。
- 完整性（I）：高（H）——可能导致数据被未授权修改。
- 可用性（A）：中（M）——可能导致服务性能下降或拒绝服务。

**综合得分：9.1（高）**

**Proof of Concept：**

1. **定义CRD和ValidatingAdmissionPolicy：**

   ```yaml
   # CRD定义
   openAPIV3Schema:
     type: object
     properties:
       duration:
         type: string
         format: duration
         x-kubernetes-validations:
         - rule: "self >= duration(\"60m\")"
           message: "duration must be at least 1 hour"

   # ValidatingAdmissionPolicy
   validations:
     - expression: "object.duration >= duration(\"2h\")"
       message: "duration must be at least 2 hours"
   ```

2. **攻击者创建不合规的资源：**

   ```yaml
   apiVersion: example.com/v1
   kind: Thing
   metadata:
     name: malicious-thing
   duration: "5m"
   ```

3. **预期结果：**

   - **按照安全策略**，该资源应被拒绝，因为`duration`小于`2h`。
   - **实际结果**，由于类型不匹配，验证表达式未正确执行，导致资源被创建成功。

4. **验证效果：**

   - **攻击者验证资源已被创建：**

     ```shell
     kubectl get thing malicious-thing
     ```

   - **资源存在，说明验证规则被绕过。**

**总结：**

由于VAP和CRDValidationRules在处理对象属性类型上的不一致，导致验证规则可能被绕过，攻击者可以利用该漏洞创建不符合安全策略的资源，危及集群安全。建议对VAP的实现进行修复，使其与CRDValidationRules在类型处理上保持一致，确保验证规则能够正确执行。

---

## Issue #124944 Non existing localhostProfile Seccomp profile is not applied on Kubernetes nodes >= 1.28

- Issue 链接：[#124944](https://github.com/kubernetes/kubernetes/issues/124944)

### Issue 内容

#### What happened?

I am running 2 clusters, 1 is still on 1.26 and the other is on 1.29.5. I tried to apply a custom seccomp profile to a pod on 1.29.5 and noticed it did not seem to work.  While trying to pin-point the issue, I found out that applying a custom seccomp profile that does not exist (i.e. the file does not exist on the node) to a pod on a kubernetes node below 1.28 fails to start the pod, with an error: 
```
Error: failed to generate security options for container "test-container": failed to generate seccomp security options for container: cannot load seccomp profile "/var/lib/kubelet/seccomp/profiles/audit.json": open /var/lib/kubelet/seccomp/profiles/audit.json: no such file or directory
```

However, when trying to do the same on a kubernetes node >= 1.28 incorrectly starts the pod, with no mention of the seccomp profile file not being found or being invalid. Although I used minikube to reproduce this bug, I observed it first on a bare metal installation. 

#### What did you expect to happen?

I expect a pod not to start when the seccomp profile cannot be loaded.

#### How can we reproduce it (as minimally and precisely as possible)?

```
minikube start --kubernetes-version 1.27
kubectl apply -f audit-pod.yaml
```

Observe that the pod does not start and gives an error: 
```
Error: failed to generate security options for container "test-container": failed to generate seccomp security options for container: cannot load seccomp profile "/var/lib/kubelet/seccomp/profiles/audit.json": open /var/lib/kubelet/seccomp/profiles/audit.json: no such file or directory
```

```
minikube delete
minikube start --kubernetes-version 1.28\
kubectl apply -f audit-pod.yaml\
```

Observe that the pod has started without any errors.



audit-pod.yaml:
```apiVersion: v1
kind: Pod
metadata:
  name: audit-pod
  labels:
    app: audit-pod
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json
  containers:
  - name: test-container
    image: hashicorp/http-echo:1.0
    args:
    - "-text=just made some syscalls!"
    securityContext:
      allowPrivilegeEscalation: false
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

Version that errors on invalid seccomp profile:
<details>

```console
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.7
WARNING: version difference between client (1.29) and server (1.27) exceeds the supported minor version skew of +/-1
```

</details>

Version that does not error:
<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=Buildroot
VERSION=2021.02.12-1-gb75713b-dirty
ID=buildroot
VERSION_ID=2021.02.12
PRETTY_NAME="Buildroot 2021.02.12"
$ uname -a
Linux minikube 5.10.57 #1 SMP Tue Nov 7 06:51:54 UTC 2023 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该Issue涉及潜在的安全风险，具体原因和可能影响如下：

**问题描述：**

在Kubernetes 1.28及以上版本，当Pod配置了不存在的本地Seccomp profile（`localhostProfile`），即使该Seccomp profile文件在节点上不存在，Pod仍然能够正常启动且没有任何错误提示。然而，在1.28以下的版本中，如果指定的Seccomp profile文件不存在，Pod会无法启动并报错。

**安全风险分析：**

1. **Seccomp隔离失效：** Seccomp是一种用于限制容器内进程可以调用的系统调用的安全机制。如果Seccomp profile未正确加载，意味着Pod缺乏应有的系统调用限制，可能允许Pod内的进程执行更多的系统调用，从而增加攻击面。

2. **绕过安全策略：** Kubernetes集群管理员或安全策略可能依赖于Seccomp来实现系统调用级别的限制。如果Seccomp profile未被正确应用，攻击者可以利用这一漏洞绕过安全策略，执行原本被禁止的操作。

3. **权限提升和系统破坏：** 攻击者可以利用缺失的Seccomp限制，发起系统调用级别的攻击，例如提权、访问敏感信息、干扰主机系统等，可能导致集群节点被攻陷。

**攻击场景：**

- **多租户环境下的风险：** 在共享的Kubernetes集群中，具有创建Pod权限的用户可能是不同的团队或租户。如果他们的Pod未正确应用Seccomp策略，恶意用户可以利用该漏洞对集群进行攻击。

- **低权限攻击者的利用：** 攻击者只需要具有创建Pod的权限（Privileges Required为Low），即可通过指定不存在的Seccomp profile，使Pod在未受限的情况下运行。

**CVSS v3.1评分：**

根据上述分析，按照CVSS v3.1标准，评分如下：

- **攻击向量（AV）：** 网络（N）——攻击者可以通过网络访问集群API服务器创建Pod。
- **攻击复杂度（AC）：** 低（L）——攻击不需要特殊条件，容易实施。
- **所需特权（PR）：** 低（L）——仅需要创建Pod的权限。
- **用户交互（UI）：** 无（N）——不需要其他用户的参与。
- **范围（S）：** 改变（C）——攻击可能影响到超出攻击者权限的资源。
- **机密性影响（C）：** 高（H）——可能访问到敏感信息。
- **完整性影响（I）：** 高（H）——可能篡改系统数据。
- **可用性影响（A）：** 高（H）——可能导致系统服务中断。

**CVSS Base Score计算：** 9.6（严重）

**PoC（概念验证）：**

按照Issue中提供的步骤，可以重现该问题：

1. 启动Kubernetes 1.28或更高版本的集群（例如使用Minikube）：

   ```bash
   minikube start --kubernetes-version 1.28
   ```

2. 应用配置了不存在的Seccomp profile的Pod：

   ```bash
   kubectl apply -f audit-pod.yaml
   ```

   其中`audit-pod.yaml`的内容指定了不存在的Seccomp profile：

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: audit-pod
     labels:
       app: audit-pod
   spec:
     securityContext:
       seccompProfile:
         type: Localhost
         localhostProfile: profiles/audit.json
     containers:
     - name: test-container
       image: hashicorp/http-echo:1.0
       args:
       - "-text=just made some syscalls!"
       securityContext:
         allowPrivilegeEscalation: false
   ```

3. 观察Pod成功启动且没有错误信息：

   ```bash
   kubectl get pods
   # 应该看到audit-pod处于Running状态
   ```

4. 验证Seccomp未正确应用，可以在Pod内部执行原本应被限制的系统调用。

**可能的缓解措施：**

- **验证Seccomp profile存在性：** 在Pod启动前，确保指定的Seccomp profile文件存在于节点上的正确路径。
- **升级或修复Kubernetes版本：** 关注Kubernetes的更新，如果该问题被认定为漏洞，官方可能会发布修复版本。
- **审计和监控：** 实施对Pod和节点的安全审计，监控异常的系统调用行为。

综上所述，该Issue涉及一个潜在的安全漏洞，可能导致Seccomp安全策略未被正确应用，允许攻击者绕过系统调用级别的限制，造成严重的安全后果。

---

## Issue #124863 NFS PV mountOptions does not work when Volume SubPath option is set

- Issue 链接：[#124863](https://github.com/kubernetes/kubernetes/issues/124863)

### Issue 内容

#### What happened?

Setting the Volume subpath option at deployment with an NFS PV with a mount option (e.g. nosuid) does not mount accordingly. If no SubPath is set and the NFS PV is set to the /nfs_share/directory the mount works with the nosuid.

#### What did you expect to happen?

should be mounted with correct mountoptions

#### How can we reproduce it (as minimally and precisely as possible)?

- create KIND cluster and install nfs client packages
- create a PV and PVC with below options: 


```
spec:
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
    - ReadWriteMany
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: pvc01
    namespace: default
    resourceVersion: '53505'
    uid: 0b3736ac-bbb6-4539-95c1-854ddcb36114
  mountOptions:
    - nfsvers=4.2
    - sec=sys
    - nosuid
  nfs:
    path: /nfs
    server: 172.31.40.82
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem  
```
-  Update the deployment with the volume and mount information: 

```
          volumeMounts:
            - mountPath: /data
              name: vol-oirnd
              subPath: share01

      volumes:
        - name: vol-oirnd
          persistentVolumeClaim:
            claimName: pvc01
          __newPvc:
            type: persistentvolumeclaim
            metadata:
              namespace: default
            spec:
              storageClassName: ''
              volumeName: ''
              resources:
                requests: {} 
```

#### Anything else we need to know?

```
**Not using SubPath**

root@kind-2-control-plane:/# mount | grep nfs
192.168.100.217:/nfs on /var/lib/kubelet/pods/c29b33d8-3709-4082-892c-def5efbba86f/volumes/kubernetes.io~nfs/pv type nfs4 (rw,nosuid,relatime,vers=4.2,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.2,local_lock=none,addr=192.168.100.217)

**When not using SubPath**


192.168.100.217:/nfs/hello/world on /var/lib/kubelet/pods/c29b33d8-3709-4082-892c-def5efbba86f/volume-subpaths/pv/httpd/0 type nfs4 (rw,relatime,vers=4.2,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.2,local_lock=none,addr=192.168.100.217)
root@kind-2-control-plane:/# 

root@kind-2-control-plane:/# kubectl get nodes
NAME                   STATUS   ROLES           AGE   VERSION
kind-2-control-plane   Ready    control-plane   49m   v1.29.2
root@kind-2-control-plane:/# 
```

#### Kubernetes version

<details>

```console
$ kubectl version
 v1.29.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Ubuntu

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该 Issue 存在潜在的安全风险。

**原因：**

在 Kubernetes 中，当使用 NFS 类型的 PersistentVolume（PV）时，管理员可以通过 `mountOptions` 来指定挂载选项，例如 `nosuid`，以防止执行具有 SUID 位的二进制文件，从而提高安全性。

然而，该 Issue 指出，当在 Pod 的配置中使用了 `subPath` 选项时，PV 上指定的 `mountOptions`（如 `nosuid`）未被正确应用。这意味着：

- 挂载到容器内的子路径可能未禁用 SUID。
- 攻击者如果能在 NFS 共享目录中放置恶意的 SUID 程序，可能利用此漏洞在容器内获得提升的权限。
- 在某些情况下，甚至可能影响宿主机的安全。

**可能的影响：**

- **权限提升：** 攻击者可以在 NFS 共享上放置恶意的 SUID 二进制文件，容器内的进程执行该文件后，可获得更高的系统权限。
- **容器逃逸：** 在某些配置下，攻击者可能突破容器的隔离，访问宿主机或其他容器的资源。
- **数据泄露或篡改：** 攻击者可能访问或修改敏感数据。

**符合风险判断标准：**

1. **该风险能被攻击者利用：** 是，攻击者可以利用未正确应用的 `nosuid` 选项执行恶意的 SUID 程序。
2. **可能成为漏洞，CVSS 评分高于 High：** 是，根据以下评分，CVSS 3.1 基本分为 9.8（Critical）。
3. **不属于提交者的问题：** 是，问题源于 Kubernetes 对 `mountOptions` 在使用 `subPath` 时未正确应用，是项目的问题。
4. **无需额外权限即可攻击：** 攻击者可能只需要对 NFS 共享的写权限，或者利用其他漏洞获取写权限。

**CVSS 3.1 评分：**

- **攻击向量（AV）：** 网络（N）——攻击者可通过网络访问 NFS 共享。
- **攻击复杂度（AC）：** 低（L）——攻击不需要高复杂度。
- **特权要求（PR）：** 低（L）——需要对 NFS 共享有写权限。
- **用户交互（UI）：** 无（N）——不需要用户交互。
- **作用范围（S）：** 改变（C）——影响从容器到宿主机的安全。
- **机密性（C）：** 高（H）——可能访问敏感信息。
- **完整性（I）：** 高（H）——可能修改系统数据。
- **可用性（A）：** 高（H）——可能导致服务中断。

**CVSS 基本分：** 9.8（严重）

**Proof of Concept：**

1. **环境准备：**
   - 部署一个使用 NFS PV 的 Kubernetes 集群。
   - PV 配置了 `mountOptions`，包括 `nosuid`。
   - Pod 配置了 `volumeMounts`，使用了 `subPath`。

2. **攻击步骤：**
   - 攻击者在 NFS 共享的子路径下放置一个恶意的 SUID 二进制文件，例如一个简单的提权程序，设置 SUID 位：
     ```
     #include <unistd.h>
     int main() {
         setuid(0);
         system("/bin/sh");
         return 0;
     }
     ```
     编译后，将二进制文件的权限设置为 4755（设置 SUID 位）：
     ```
     chmod 4755 evil_suid_binary
     ```
   - 由于 `nosuid` 选项未生效，容器内的用户执行该二进制文件后，将以 root 权限运行，获得提升的权限。

3. **验证：**
   - 在容器内，执行 `evil_suid_binary`，如果成功获得 root 权限，证明漏洞存在。

**建议：**

- 修复 Kubernetes 对 `mountOptions` 在使用 `subPath` 时的处理，确保选项被正确应用。
- 临时解决方案：避免在敏感环境下使用 `subPath`，或在 NFS 服务端限制 SUID。

---

## Issue #124759 CVE-2024-3744: azure-file-csi-driver discloses service account tokens in logs

- Issue 链接：[#124759](https://github.com/kubernetes/kubernetes/issues/124759)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N) - **MEDIUM** (6.5)

A security issue was discovered in azure-file-csi-driver where an actor with access to the driver logs could observe service account tokens. These tokens could then potentially be exchanged with external cloud providers to access secrets stored in cloud vault solutions.  Tokens are only logged when [TokenRequests is configured in the CSIDriver object](https://kubernetes-csi.github.io/docs/token-requests.html) and the driver is set to run at log level 2 or greater via the -v flag.

This issue has been rated **MEDIUM** [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N) (6.5), and assigned **CVE-2024-3744**

#### Am I vulnerable?

You may be vulnerable if [TokenRequests is configured in the CSIDriver object](https://kubernetes-csi.github.io/docs/token-requests.html) and the driver is set to run at log level 2 or greater via the -v flag.

To check if token requests are configured, run the following command:

kubectl get csidriver file.csi.azure.com -o jsonpath="{.spec.tokenRequests}"

To check if tokens are being logged, examine the secrets-store container log:

kubectl logs csi-azurefile-controller-56bfddd689-dh5tk -c azurefile -f | grep --line-buffered "csi.storage.k8s.io/serviceAccount.tokens"

##### Affected Versions

- azure-file-csi-driver <= v1.29.3
- azure-file-csi-driver v1.30.0

#### How do I mitigate this vulnerability?

Prior to upgrading, this vulnerability can be mitigated by running azure-file-csi-driver at log level 0 or 1 via the -v flag.

##### Fixed Versions

- azure-file-csi-driver v1.29.4
- azure-file-csi-driver v1.30.1

To upgrade, refer to the documentation: https://github.com/kubernetes-sigs/azurefile-csi-driver?tab=readme-ov-file#install-driver-on-a-kubernetes-cluster 

#### Detection

Examine cloud provider logs for unexpected token exchanges, as well as unexpected access to cloud resources.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was patched by Weizhi Chen @cvvz from Microsoft.

Thank You,
Rita Zhang on behalf of the Kubernetes Security Response Committee

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

在**azure-file-csi-driver**中，如果在CSIDriver对象中配置了`TokenRequests`，并且驱动程序以`-v`标志设置了日志级别为2或更高，那么服务账户令牌可能会被记录在日志中。

这意味着具有访问驱动程序日志权限的攻击者，可以从日志中获取服务账户令牌。这些令牌可能被用于与外部云提供商交互，从而访问存储在云密钥库（cloud vault solutions）中的机密信息。

根据风险判断标准第5条：

- **对于日志中泄露凭据的风险，如果攻击者可以利用比泄露凭据更低的权限从日志中读取该凭据，或者泄露的凭据与攻击者使用的凭据不是一类凭据，导致攻击者可以利用泄露凭据访问其他系统，则应适当提高风险评级判断为高风险。**

因此，攻击者可能利用较低的权限（仅需访问日志的权限），获取高权限的服务账户令牌，并进一步访问敏感的云资源。

**CVSS 3.1评分：**

虽然Issue中将该漏洞评分为**Medium（6.5）**，但根据上述风险判断，应重新评估为**High**或更高。

**Proof of Concept：**

1. **前提条件：**
   - **TokenRequests**已在CSIDriver对象中配置。
   - 驱动程序以`-v`标志设置了日志级别为2或更高。
   - 攻击者拥有对azure-file-csi-driver日志的读取权限（可能是低权限用户）。

2. **攻击步骤：**
   - 攻击者访问并读取azure-file-csi-driver的日志文件。
   - 在日志中搜索包含`"csi.storage.k8s.io/serviceAccount.tokens"`的日志条目。
   - 提取日志中记录的服务账户令牌信息。
   - 使用提取的服务账户令牌，访问云提供商的API或其他受保护的资源。
   - 获取存储在云密钥库等位置的机密信息。

**可能的影响：**

- **数据泄露：** 攻击者获取敏感的服务账户令牌后，可能访问到存储在云中的机密数据。
- **权限提升：** 攻击者利用服务账户令牌，获得比原有权限更高的访问能力。
- **横向移动：** 攻击者可能利用获取的令牌，访问其他系统或服务，扩大攻击范围。

因此，该问题确实涉及潜在的安全风险，需要引起重视，并及时采取措施进行修复。

---

## Issue #124701 kubelet should alway get lastest secret/configmap resource in pod add event

- Issue 链接：[#124701](https://github.com/kubernetes/kubernetes/issues/124701)

### Issue 内容

#### What happened?

The logic is as follows:
1. Manually update secret resources.
2. kubelet listens to resources and updates the watch cache.
3. Start the pod and query the volume attached to the secret resource.

but probabilistically, especially at lots of operating pressures. The third step is performed before the second step. As a result, the old secret is mounted when the pod is started.

![image](https://github.com/kubernetes/kubernetes/assets/17514799/e72a9968-6467-4436-803d-002df05b5df7)

----------

![image](https://github.com/kubernetes/kubernetes/assets/17514799/b81c5384-4b47-420a-9995-6ce46ce275c5)

https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/



#### What did you expect to happen?

![image](https://github.com/kubernetes/kubernetes/assets/17514799/e8fd8400-b3c4-4072-8399-322548183c3c)

solution: kubelet should alway get lastest secret/configmap resource in pod add event instead of using cache

#### How can we reproduce it (as minimally and precisely as possible)?

see What happend

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

安全风险分析：

问题描述中指出，当手动更新了 Secret 资源后，kubelet 监听资源并更新缓存（watch cache）。但是在高负载情况下，可能出现 Pod 启动并查询附加的 Secret 资源的操作发生在 kubelet 更新缓存之前，导致 Pod 启动时挂载了旧的 Secret。

**潜在的安全风险：**

1. **敏感信息泄露与未授权访问：**
   - **场景描述：** 如果管理员更新了 Secret，以撤销某些凭据或更换敏感信息，以防止未授权的访问。但由于 kubelet 未及时获取最新的 Secret，新的 Pod 可能仍然使用旧的 Secret。
   - **攻击者利用：**
     - 攻击者如果具备启动 Pod 的权限，可以在缓存未更新前迅速创建新的 Pod，获取并使用旧的 Secret。
     - 利用旧的凭据，攻击者可能继续访问原本应被撤销访问权限的服务或数据。

2. **权限维持与提升：**
   - **场景描述：** 在安全事件发生后，管理员更新 Secret 以阻止攻击者的进一步行动。
   - **攻击者利用：**
     - 攻击者利用 kubelet 的缓存机制，在 Secret 更新后但缓存未刷新前，启动新的 Pod，获取旧的 Secret，实现权限维持。
     - 通过旧的 Secret，攻击者可能提升权限、横向移动，造成更大的安全威胁。

3. **一致性与合规性风险：**
   - **场景描述：** 企业可能有合规要求，确保所有系统组件使用最新的安全配置和凭据。
   - **风险体现：**
     - Pod 使用旧的 Secret，可能违反合规要求，带来法律和监管风险。

**风险评级：**

根据 **CVSS 3.1** 评分标准：

- **攻击向量（AV）：** 网络（N）
- **攻击复杂度（AC）：** 低（L）
- **特权要求（PR）：** 低（L）——需要有创建 Pod 的权限
- **用户交互（UI）：** 无（N）
- **机密性影响（C）：** 高（H）
- **完整性影响（I）：** 高（H）
- **可用性影响（A）：** 低（L）

综合评分可能达到 **High（7.5 - 8.8）**。

**概念验证（Proof of Concept）：**

1. **前提条件：**
   - 攻击者具备在集群中创建 Pod 的权限（可能是被攻陷的低权限账户）。
   - 管理员更新了 Secret，以撤销旧的凭据。

2. **攻击步骤：**
   - 管理员执行更新操作，更换 Secret 中的敏感信息。
   - 在 kubelet 缓存尚未更新的短时间窗口内，攻击者迅速创建一个新的 Pod，指定挂载目标 Secret。
   - 由于 kubelet 使用了过期的缓存，Pod 挂载了旧的 Secret。
   - 攻击者在 Pod 内获取旧的 Secret，继续对敏感资源进行未授权的访问。

3. **结果：**
   - 攻击者成功绕过了 Secret 更新带来的权限控制，维持了对敏感资源的访问。
   - 安全控制措施被绕过，可能导致数据泄露、服务破坏等严重后果。

**建议措施：**

- **在 Pod 启动时，kubelet 应直接获取最新的 Secret/ConfigMap 资源，而非依赖缓存。**
- **增加缓存刷新机制的及时性，确保在资源更新后，缓存能够立即反映最新状态。**
- **加强权限控制，限制谁可以创建 Pod，减少潜在攻击者利用该漏洞的机会。**

---

## Issue #124680 Watch request for CRs costs about 10-15x more memory in k8s-apiserver than in-tree resource watches

- Issue 链接：[#124680](https://github.com/kubernetes/kubernetes/issues/124680)

### Issue 内容

#### What happened?

I was running some load testing related to flux. When creating 10.000 kustomization custom resources (about 1KiB), the k8s apiserver consumes about 1GiB of memory. When checking with 100.000k and 300.00k, the k8s apiserver scales linearly.
When doing the same thing for 1KiB conifgmaps, creating 10.000 resources, the k8s apiserver consumes about 100 MiB of memory. 
Memory pprof for 10k kustomizations:
![10k kustomizations](https://github.com/kubernetes/kubernetes/assets/6106093/de508042-fe96-4ccc-a1ef-a6d535e611fa)

Memory pprof for 10k configmaps:
![image](https://github.com/kubernetes/kubernetes/assets/6106093/d710dae6-4818-460e-9384-b319938df92d)

The memory usage stays the same as long as the resources are existing. After looking a bit into what might force this, it seems that the kube-controller-manager sets up a watch for the kustomizations/configmaps resources.
```json
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"a4187168-a301-4fd1-907a-09da8fc3b587","stage":"RequestReceived","requestURI":"/apis/kustomize.toolkit.fluxcd.io/v1/kustomizations?allowWatchBookmarks=true\u0026resourceVersion=727\u0026timeout=5m37s\u0026timeoutSeconds=337\u0026watch=true","verb":"watch","user":{"username":"system:kube-controller-manager","groups":["system:authenticated"]},"sourceIPs":["172.18.0.2"],"userAgent":"kube-controller-manager/v1.29.2 (linux/amd64) kubernetes/4b8e819/metadata-informers","objectRef":{"resource":"kustomizations","apiGroup":"kustomize.toolkit.fluxcd.io","apiVersion":"v1"},"requestReceivedTimestamp":"2024-04-29T14:33:32.979279Z","stageTimestamp":"2024-04-29T14:33:32.979279Z"}
```

This is needed because garbage collector that runs in kube-controller-manager needs to walk the ownership reference map, and it wants to do that in cache: https://github.com/kubernetes/kubernetes/blob/a9eded097d092350a9f8de80a153b3f96841f012/pkg/controller/garbagecollector/garbagecollector.go#L253

#### What did you expect to happen?

Expectation would've been that there is similar memory usage for in-tree and custom resources.

Also, the current garbage collector seems to force k8s-apiserver to cache the full contents of etcd. Is that a correct implementation?

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a cluster
`kind create cluster`

2. Add the kustomize CRD
`curl -L https://raw.githubusercontent.com/fluxcd/kustomize-controller/main/config/crd/bases/kustomize.toolkit.fluxcd.io_kustomizations.yaml | kubectl apply -f -`

3. Create 10k of the following
```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: podinfo
spec:
  interval: 10m
  targetNamespace: default
  sourceRef:
    kind: GitRepository
    name: podinfo
  path: "./kustomize"
  prune: true
  timeout: 1m
  patches:
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: not-used
      spec:
        template:
          metadata:
            annotations:
              cluster-autoscaler.kubernetes.io/safe-to-evict: "true"        
    target:
      kind: Deployment
      labelSelector: "app.kubernetes.io/part-of=my-app"
  - patch: |
      - op: add
        path: /spec/template/spec/securityContext
        value:
          runAsUser: 10000
          fsGroup: 1337
      - op: add
        path: /spec/template/spec/containers/0/securityContext
        value:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL        
    target:
      kind: Deployment
      name: podinfo
      namespace: apps
```

#### Anything else we need to know?



#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该 Issue 存在潜在的安全风险，具体分析如下：

**1. 风险描述：**
在 Kubernetes 集群中，攻击者可以通过创建大量的自定义资源（CRs），如 `Kustomization` 资源，导致 `kube-apiserver` 消耗大量内存。由于垃圾收集器需要监视这些资源，`kube-apiserver` 会缓存所有这些资源的信息，内存使用量随之线性增长。这可能导致服务器的内存耗尽，引发拒绝服务（DoS）攻击。

**2. 风险可利用性：**
攻击者只需具备创建自定义资源实例的低级权限即可实施攻击。很多情况下，普通用户可能被授予创建某些命名空间下资源的权限。如果攻击者能够批量创建大量的自定义资源，就能触发该漏洞。

**3. 可能的影响：**
- **可用性影响（A）：高（H）**  
攻击者可以导致 `kube-apiserver` 内存耗尽，导致其崩溃或无法响应请求，影响整个 Kubernetes 集群的可用性。
- **业务影响：**  
集群管理和业务应用可能受到严重影响，无法正常调度和管理工作负载。

**4. CVSS 3.1 评分：**

| 向量项                | 评级  |
|-----------------------|-------|
| 攻击向量 (AV)         | 网络 (N)   |
| 攻击复杂度 (AC)       | 低 (L)     |
| 所需特权 (PR)         | 低 (L)     |
| 用户交互 (UI)         | 无 (N)     |
| 作用范围 (S)          | 未变 (U)   |
| 机密性影响 (C)        | 无 (N)     |
| 完整性影响 (I)        | 无 (N)     |
| 可用性影响 (A)        | 高 (H)     |

**综合评分：7.5（高）**

**5. POC（概念验证）：**

按照 Issue 中提供的步骤，攻击者可以执行以下操作：

**步骤1：创建 Kubernetes 集群**
```bash
kind create cluster
```

**步骤2：添加自定义资源定义（CRD）**
```bash
curl -L https://raw.githubusercontent.com/fluxcd/kustomize-controller/main/config/crd/bases/kustomize.toolkit.fluxcd.io_kustomizations.yaml | kubectl apply -f -
```

**步骤3：批量创建大量的 Kustomization 资源**
编写一个脚本，创建例如 10,000 个 `Kustomization` 资源：

```bash
for i in {1..10000}
do
cat <<EOF | kubectl apply -f -
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: podinfo-$i
spec:
  interval: 10m
  targetNamespace: default
  sourceRef:
    kind: GitRepository
    name: podinfo
  path: "./kustomize"
  prune: true
  timeout: 1m
EOF
done
```

**预期结果：**
- `kube-apiserver` 的内存使用量将显著增加，可能达到数 GiB。
- 如果继续增加资源数量，`kube-apiserver` 可能会出现内存耗尽，导致服务崩溃或无法响应。

**6. 总结：**
该漏洞允许具有低权限的攻击者通过创建大量自定义资源，导致 `kube-apiserver` 内存耗尽，从而实施拒绝服务攻击。根据风险判断标准和 CVSS 评分，风险等级为高（High），应予以重视，可能需要分配 CVE 编号并及时修复。

---

# 📌 不涉及安全风险的 Issues (58 个)

## Issue #125242 runtime: failed to create new OS thread

- Issue 链接：[#125242](https://github.com/kubernetes/kubernetes/issues/125242)

### Issue 内容

#### What happened?

[root@m01 log]# kubectl   get  pod  -A
runtime: failed to create new OS thread (have 3 already; errno=11)
runtime: may need to increase max user processes (ulimit -u)
fatal error: newosproc

runtime stack:
runtime.throw(0x1c1d651, 0x9)
        /usr/local/go/src/runtime/panic.go:1116 +0x72
runtime.newosproc(0xc0000b6400)
        /usr/local/go/src/runtime/os_linux.go:161 +0x1c5
runtime.newm1(0xc0000b6400)
        /usr/local/go/src/runtime/proc.go:1843 +0xdd
runtime.newm(0x1d07a08, 0xc00004a800, 0x2)
        /usr/local/go/src/runtime/proc.go:1822 +0x9b
runtime.startm(0x0, 0xc000048001)
        /usr/local/go/src/runtime/proc.go:1979 +0xc9
runtime.wakep()
        /usr/local/go/src/runtime/proc.go:2067 +0x66
runtime.newproc.func1()
        /usr/local/go/src/runtime/proc.go:3561 +0x97
runtime.systemstack(0x46c414)
        /usr/local/go/src/runtime/asm_amd64.s:370 +0x66
runtime.mstart()
        /usr/local/go/src/runtime/proc.go:1116

goroutine 1 [running, locked to thread]:
runtime.systemstack_switch()
        /usr/local/go/src/runtime/asm_amd64.s:330 fp=0xc0000b26f0 sp=0xc0000b26e8 pc=0x46c540
runtime.newproc(0x0, 0x1d078c8)
        /usr/local/go/src/runtime/proc.go:3554 +0x6e fp=0xc0000b2738 sp=0xc0000b26f0 pc=0x44214e
runtime.init.6()
        /usr/local/go/src/runtime/proc.go:243 +0x35 fp=0xc0000b2758 sp=0xc0000b2738 pc=0x439e35
runtime.doInit(0x2b17e80)
        /usr/local/go/src/runtime/proc.go:5652 +0x8a fp=0xc0000b2788 sp=0xc0000b2758 pc=0x446fca
runtime.main()
        /usr/local/go/src/runtime/proc.go:151 +0xd9 fp=0xc0000b27e0 sp=0xc0000b2788 pc=0x439b59
runtime.goexit()
        /usr/local/go/src/runtime/asm_amd64.s:1374 +0x1 fp=0xc0000b27e8 sp=0xc0000b27e0 pc=0x46e181

#### What did you expect to happen?

Output Pod information.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how to repeat the problem.

#### Anything else we need to know?

$ ulimit -u
655350
$ ulimit -n
655350

systemctl edit kubelet.service
[Service]
LimitNOFILE=infinity
LimitNPROC=infinity

#### Kubernetes version

<details>

```console
$ kubectl version
v 1.19.16
```

</details>


#### Cloud provider

<details>
Physical server
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
CentOS Linux 7 (Core)

$ uname -a
linux m01 4.20.3-1.el7.elrepo.x86_64  x86_64  GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker v24.0.6

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### 分析结果

不涉及

---

## Issue #125226 Listen tcp :53: bind: permission denied ERROR!!

- Issue 链接：[#125226](https://github.com/kubernetes/kubernetes/issues/125226)

### Issue 内容

#### What happened?

```
kubectl get po -A
NAMESPACE      NAME                                   READY   STATUS             RESTARTS      AGE
kube-flannel   kube-flannel-ds-962vp                  1/1     Running            0             15m
kube-flannel   kube-flannel-ds-qs6xr                  1/1     Running            0             15m
kube-system    coredns-7db6d8ff4d-6w776               0/1     CrashLoopBackOff   1 (10s ago)   12s
kube-system    coredns-7db6d8ff4d-99tng               0/1     CrashLoopBackOff   1 (9s ago)    12s
kube-system    etcd-controlplane                      1/1     Running            0             12m
kube-system    kube-apiserver-controlplane            1/1     Running            0             12m
kube-system    kube-controller-manager-controlplane   1/1     Running            0             12m
kube-system    kube-proxy-hbt5b                       1/1     Running            0             12m
kube-system    kube-proxy-x96js                       1/1     Running            0             11m
kube-system    kube-scheduler-controlplane            1/1     Running            0             12m
```

I was setting up the single-node k8s cluster (1 controlplane and 1 worker node). After going through all the installation process from the official [k8s](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) site -> at last, after deploying the network plugin on the k8s cluster. CoreDNS pods went into a Crashloopbackoff state. I did check for the container logs and found the following error message:

```
 listen tcp :53: bind: permission denied
```

Please look into this and provide some insights. I have faced same issue while upgrading the cluster from v1.29 to v1.30. 


Thanks & Regards,
Tej Singh Rana

#### What did you expect to happen?

Both coreDNS pods should be in the running state, after deploying the network plugin. 

#### How can we reproduce it (as minimally and precisely as possible)?

Simply follow the steps from the official k8s site. 

#### Anything else we need to know?

I did some tests, and I used 1024 instead of 53 port, and it started to work. (AFAIK, using below port ~1000 was not working)
 
```console
kubectl logs -n kube-system coredns-7db6d8ff4d-wchnq 
.:1024
[INFO] plugin/reload: Running configuration SHA512 = e20da72760199c1bc59098f3ae16621ae48df8f7756e50bd0dfa5553ccb7be57af61562fff46a43fdcce51ac086b26aa19929386004908ad3afe3aea9b06316a
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
```
**Environment**:

- the version of CoreDNS: registry.k8s.io/coredns/coredns:v1.11.1
- Corefile: The below content is from the `coredns` configMap. 

```console
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
```
- logs:
```console
 listen tcp :53: bind: permission denied
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0 <br>
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 <br>
Server Version: v1.30.1

```

</details>


#### Cloud provider


N/A



#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release

PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux controlplane 5.4.0-1106-gcp #115~18.04.1-Ubuntu SMP Mon May 22 20:46:39 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

```console
kubectl version 
Client Version: v1.30.0 
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 
Server Version: v1.30.1
```

```console
kubelet --version

Kubernetes v1.30.0
```

```console
kubeadm version 

kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0",  GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-04-17T17:34:08Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd --version <br>
containerd containerd.io 1.6.26 3dd1e886e55dd695541fdcd67420c2888645a495
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
<pre>
   Service Account:  flannel
  Init Containers:
   install-cni-plugin:
    Image:      docker.io/flannel/flannel-cni-plugin:v1.4.1-flannel1
    Port:       <none>
    Host Port:  <none>
    Command:
      cp
    Args:
      -f
      /flannel
      /opt/cni/bin/flannel
    Environment:  <none>
    Mounts:
      /opt/cni/bin from cni-plugin (rw)
   install-cni:
    Image:      docker.io/flannel/flannel:v0.25.2
    Port:       <none>
</pre>
</details>


### 分析结果

不涉及

---

## Issue #125217 Persistent 'Terminating' State for Old Pod after Migration to New Node

- Issue 链接：[#125217](https://github.com/kubernetes/kubernetes/issues/125217)

### Issue 内容

#### What happened?

After a node is shut down, the pod on it migrates to another node and a new pod is established. However, the old pod remains in the Terminating state, continuously displayed, and has not been deleted promptly.

The screenshot is as follows：

![原来节点被关闭后，pod迁移到节点后，旧的节点信息一直显示，没有被及时删除](https://github.com/kubernetes/kubernetes/assets/168079959/7660b781-0fa8-4784-a615-d6ef8bde0694)


#### What did you expect to happen?

Can the terminating pod be  deleted promptly ? 

#### How can we reproduce it (as minimally and precisely as possible)?

no idea

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.2

</details>


#### Cloud provider

<details>
no 
</details>


#### OS version

<details>

NAME="CentOS Linux"
VERSION="8"


</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125216 ResourceClaimParametersRef.APIGroup allows APIversion values

- Issue 链接：[#125216](https://github.com/kubernetes/kubernetes/issues/125216)

### Issue 内容

#### What happened?

Following YAML file is accepted without error. Notice the `spec.parametersRef.apiGroup` field, it is an `APIVersion` instead of `APIGroup`, containing `/` character. This can lead to an issue when resource driver generates structured parameters' in-tree `ResourceClaimParameters` object with `generatedFrom.apiGroup` being just `APIGroup` without version. The DRA scheduler plugin will try to match the parameters object based on the reference here: https://github.com/kubernetes/kubernetes/blob/be4afb9ef90b19ccb6f7e595cbdb247e088b2347/pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go#L1066

In latest master the code appears to use an indexer, and the issue might be hidden, if applicable at all. I did not check the reproducibility in master yet. However, main question is whether the `APIGroup` should allow `APIVersion` value with `/` character in general.

```YAML
apiVersion: gpu.resource.intel.com/v1alpha2
kind: GpuClaimParameters
metadata:
  name: delayed-claim-external-gpu
spec:
  count: 1
  type: "gpu"
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: delayed-claim-external-gpu
spec:
  resourceClassName: intel-gpu-structured
  parametersRef:
    apiGroup: gpu.resource.intel.com/v1alpha2
    kind: GpuClaimParameters
    name: delayed-claim-external-gpu
```

#### What did you expect to happen?

An error is expected from the apiserver upon object creation request, reporting that APIGroup containes APIVersion. Maybe.

#### How can we reproduce it (as minimally and precisely as possible)?

Have any resource driver that supports structured parameters, [dra-example-driver](https://github.com/kubernetes-sigs/dra-example-driver/pull/47/files#diff-df3210fefbab9f26fa97ba679b29869e25ce897d0ba713ce461da521f10ff7d8), for instance, also puts `APIGroup` (gpucrd.APIGroup) value into `generatedFrom.apiGroup`.

#### Anything else we need to know?

#### Kubernetes version

v1.30.1

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
scheduler's dynamicresources plugin
</details>


### 分析结果

该 Issue 不涉及安全风险。

---

## Issue #125205 [FG:InPlacePodVerticalScaling] Slow reconcile when quickly reverting resize patch

- Issue 链接：[#125205](https://github.com/kubernetes/kubernetes/issues/125205)

### Issue 内容

#### What happened?

While working on https://github.com/kubernetes/kubernetes/pull/125202 and testing the second case (patching a pod to perform an in-place resize and then quickly reverting the patch before the resize has been actuated), I've discovered some unexpected behavior. In this case the pod eventually reconciles but it takes about 3 minutes, with the following test output:

```
[sig-node] Pod InPlace Resize Container [Feature:InPlacePodVerticalScaling] Burstable QoS pod, three containers - no change for c1, increase c2 resources, decrease c3 (net decrease for pod) [sig-node, Feature:InPlacePodVerticalScaling]
k8s.io/kubernetes/test/e2e/node/pod_resize.go:1281
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.391
  I0529 18:13:28.391963 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.393961 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize @ 05/29/24 18:13:28.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.406
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.408
  I0529 18:13:28.408474 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.409838 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize-resource-quota @ 05/29/24 18:13:28.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.417
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.419
  I0529 18:13:28.419504 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.420717 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize-errors @ 05/29/24 18:13:28.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.427                                                                                                                                                                                                                                                                        
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.428                                                                                                                                                                                                                                                                                 
  STEP: creating pod @ 05/29/24 18:13:28.429                                                                                                                                                                                                                                                                                                                                
  STEP: verifying the pod is in kubernetes @ 05/29/24 18:13:30.448                                                                                                                                                                                                                                                                                                          
  STEP: verifying initial pod resources, allocations, and policy are as expected @ 05/29/24 18:13:30.453                                                                                                                                                                                                                                                                    
  STEP: verifying initial pod status resources and cgroup config are as expected @ 05/29/24 18:13:30.453                                                                                                                                                                                                                                                                    
  STEP: patching pod for resize @ 05/29/24 18:13:30.457                                                                                                                                                                                                                                                                                                                     
  STEP: verifying pod patched for resize @ 05/29/24 18:13:30.47                                                                                                                                                                                                                                                                                                             
  STEP: patching pod for rollback @ 05/29/24 18:13:30.516                                                                                                                                                                                                                                                                                                                   
  STEP: verifying pod patched for rollback @ 05/29/24 18:13:30.529                                                                                                                                                                                                                                                                                                          
  STEP: waiting for rollback to be actuated @ 05/29/24 18:13:30.529                                                                                                                                                                                                                                                                                                         
  I0529 18:13:30.534186 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 -- 
 kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod -- ls 
  /sys/fs/cgroup/cgroup.controllers'
  I0529 18:13:30.679258 3291 builder.go:146] stderr: "Defaulted container \"c1\" out of: c1, c2, c3\n"
  I0529 18:13:30.679299 3291 builder.go:147] stdout: "/sys/fs/cgroup/cgroup.controllers\n"
  I0529 18:13:30.679331 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 209715200 in path /sys/fs/cgroup/memory.max
  I0529 18:13:30.679379 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:30.819936 3291 builder.go:146] stderr: ""
  I0529 18:13:30.820015 3291 builder.go:147] stdout: "209715200\n"
  I0529 18:13:30.820050 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 20000 100000 in path /sys/fs/cgroup/cpu.max
  I0529 18:13:30.820117 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/cpu.max'
  I0529 18:13:30.907604 3291 builder.go:146] stderr: ""
  I0529 18:13:30.907650 3291 builder.go:147] stdout: "20000 100000\n"
  I0529 18:13:30.907669 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 4 in path /sys/fs/cgroup/cpu.weight
  I0529 18:13:30.907714 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/cpu.weight'
  I0529 18:13:30.993819 3291 builder.go:146] stderr: ""
  I0529 18:13:30.993869 3291 builder.go:147] stdout: "4\n"
  I0529 18:13:30.993895 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c2 - looking for cgroup value 314572800 in path /sys/fs/cgroup/memory.max
  I0529 18:13:30.993951 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:31.078830 3291 builder.go:146] stderr: ""
  I0529 18:13:31.078876 3291 builder.go:147] stdout: "367001600\n"
  I0529 18:13:33.080354 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:33.205904 3291 builder.go:146] stderr: ""
  I0529 18:13:33.205947 3291 builder.go:147] stdout: "367001600\n"
  I0529 18:13:35.207569 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:35.335993 3291 builder.go:146] stderr: ""
  I0529 18:13:35.336037 3291 builder.go:147] stdout: "367001600\n"
```

With the last couple of lines continuing indefinitely until it reads the "rolled back" value from `memory.max` in `c2` (3 mins in my case). This also happens inconsistently, you may have to rerun the test case a couple of times to hit it. You can also run the full suite (`-ginkgo.focus="Feature:InPlacePodVerticalScaling"`) to hit it instead of the specific case provided below.


#### What did you expect to happen?

After patching forwards and patching backwards, the pod should reach its initial state, and `memory.max` in `c2` should be set to its initial value.

#### How can we reproduce it (as minimally and precisely as possible)?

- Check out https://github.com/kubernetes/kubernetes/pull/125202
- Uncomment [this line](https://github.com/kubernetes/kubernetes/pull/125202/files#diff-44a1da1c31a8cd8913e1b8cfb2893c5436bb7c60a4d51c9707f9368bf755ef8fR1391) - `patchAndVerifyAborted`
- Comment both `patchAndVerify` lines (so we only run the "aborted" case), and comment [this line](https://github.com/kubernetes/kubernetes/pull/125202/files#diff-44a1da1c31a8cd8913e1b8cfb2893c5436bb7c60a4d51c9707f9368bf755ef8fR1362)
- Run `e2e.test` with `-ginkgo.focus="Burstable QoS pod, three containers - decrease c1 resources" -num-nodes 2` (you can also run the full suite (`-ginkgo.focus="Feature:InPlacePodVerticalScaling"`) - you may have to run it a couple of times)

#### Anything else we need to know?

This could also be an issue with my test changes, so please flag anything that seems suspicious. If you leave `patchAndVerify` uncommented (so patch -> wait for resize -> patch back -> wait for resize) and then run `patchAndVerifyAborted` (on the same case), you will hit a different bug, which I will file separately. 

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"31+", GitVersion:"v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty", GitCommit:"1ff1207d22ab5cf442c8dafdf5bded1e32519873", GitTreeState:"dirty", BuildDate:"2024-05-28T19:28:33Z", GoVersion:"go1.22.3", Compiler:"gc", Platform:"linux/arm64"}
WARNING: version difference between client (1.25) and server (1.31) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux docker-desktop 5.15.49-linuxkit #1 SMP PREEMPT Tue Sep 13 07:51:32 UTC 2022 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind version 0.17.0
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
NAME                 STATUS   ROLES           AGE     VERSION                                    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane   6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.4    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
kind-worker          Ready    <none>          6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.2    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
kind-worker2         Ready    <none>          6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.3    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125194 kubelet option ImageMaximumGCAge not accepting value in format day and hour (1d1h)

- Issue 链接：[#125194](https://github.com/kubernetes/kubernetes/issues/125194)

### Issue 内容

#### What happened?

kubelet option **imageMaximumGCAge** which allows an admin to specify a time after which unused images will be garbage collected by the Kubelet, regardless of disk usage. The value is specified as a Kubernetes duration; for example, you can set the configuration field to 3d12h, which means 3 days and 12 hours.
We set the value and kubelet is failing to start and below error is showing in journalctl logs

**May 29 15:46:05 node-10-120-127-63 kubelet[31112]: E0529 15:46:05.091483   31112 run.go:74] "command failed" err="failed to load kubelet config file, path: /var/lib/kubelet/config.yaml, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to decode: time: unknown unit \"d\" in duration \"1d1h\**

This is how option is set in kubelet config
imageMaximumGCAge: 1d1h

If I change the duration to only hour e.g. 1h then it works without any issue

Here is the description about this option

https://kubernetes.io/docs/concepts/architecture/garbage-collection/





#### What did you expect to happen?

option **imageMaximumGCAge** should accept the value as mentioned in the documentation in the format day and hours (1d1h) 

#### How can we reproduce it (as minimally and precisely as possible)?

Edit the kubelet configuration file _/var/lib/kubelet/config.yaml_ and append the option _imageMaximumGCAge: 1d1h_ 
vi /var/lib/kubelet/config.yaml
Append below line
imageMaximumGCAge: 1d1h

Restart the kubelet service
sytemctl restart kubelet


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1



```
</details>


#### Cloud provider

<details>
Virtual machines are deployed on KVM environment
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="SLES"
VERSION="15-SP3"
VERSION_ID="15.3"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP3"
ID="sles"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:15:sp3"
DOCUMENTATION_URL="https://documentation.suse.com/"

$ uname -a
Linux node-10-120-127-62 5.3.18-57-default #1 SMP Wed Apr 28 10:54:41 UTC 2021 (ba3c2e9) x86_64 x86_64 x86_64 GNU/Linux




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
#crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.6.28
RuntimeApiVersion:  v1


</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125191 The APIServer memory usage is high.

- Issue 链接：[#125191](https://github.com/kubernetes/kubernetes/issues/125191)

### Issue 内容

#### What happened?

![image](https://github.com/kubernetes/kubernetes/assets/54977497/2541c771-b727-4031-8cfd-f093ecbeea3c)
The apiserver memory usage is high. The pprof command output shows that the processEvent method occupies a large amount of memory.The processEvent method takes up about 40% of the memory

#### What did you expect to happen?

The APIServer memory usage is not high when the number of requests is stable.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how it happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125188 Resource Quotas does not work correctly for services.nodeports

- Issue 链接：[#125188](https://github.com/kubernetes/kubernetes/issues/125188)

### Issue 内容

#### What happened?

We have Resources Quotes for **services.nodeports** with an allowed limit of 5 and for **services.loadbalancers** with an allowed limit of 1.

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: network-resources
spec:
  hard:
    services.nodeports: "5"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    services.loadbalancers: "1"
```

We deploy a **NodePort** type service in the amount of 1 with the publication of 4 ports, one **ClusterIP** type service and the use of the services.nodeports 4/5 resource quota.

```
k get Resourcequotas -n test
NAME                AGE   REQUEST                       LIMIT
compute-resources   18m   services.loadbalancers: 0/3   
network-resources   95s   services.nodeports: 4/5       

k get svc -n test
NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                     AGE
my-service-cip              ClusterIP   10.36.39.25    <none>        80/TCP                                                      31m
my-service-np   NodePort    10.36.36.151   <none>        80:30626/TCP,8081:25225/TCP,8083:28082/TCP,9000:26419/TCP   2m33s
```

We deploy a service of the **LoadBalancers** type in the amount of 1 with the publication of 1 port and get the use of the resource quota services.loadbalancers: 1/3   
services.nodeports: 5/5.

```
k get resourcequotas -n test
NAME                AGE     REQUEST                       LIMIT
compute-resources   20m     services.loadbalancers: 1/3   
network-resources   3m24s   services.nodeports: 5/5       

❯ k get svc -n test
NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                     AGE
my-service-lb                  LoadBalancer   10.36.32.236   <pending>     80:32268/TCP                                                16s
my-service-cip              ClusterIP      10.36.39.25    <none>        80/TCP                                                      33m
my-service-np   NodePort       10.36.36.151   <none>        80:30626/TCP,8081:25225/TCP,8083:28082/TCP,9000:26419/TCP   4m17s
```

The described behavior has been tested on versions Kubernetes 1.20, 1.27-1.30, and the result is the same for all.

#### What did you expect to happen?

In  the documentation https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota the use of the resource quota for **services.nodeports** is described as **"The total number of Services of type NodePort that can exist in the namespace."**

In fact, each published port is considered for the services.nodeports resource quota, regardless of the specified type of Load Balancer or NodePort service.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Resource Quota

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: network-resources
spec:
  hard:
    services.nodeports: "5"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    services.loadbalancers: "1"
```

Create service

```
apiVersion: v1
kind: Service
metadata:
  name: my-service-cip
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: my-service-np
spec:
  externalTrafficPolicy: Local
  ports:
  - name: api
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: probe
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: prof
    port: 8083
    protocol: TCP
    targetPort: 8083
  - name: metrics
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app.kubernetes.io/name: my-service-np
  sessionAffinity: None
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: my-service-lb
spec:
  externalTrafficPolicy: Local
  ports:
  - name: api
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: probe
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: prof
    port: 8083
    protocol: TCP
    targetPort: 8083
  - name: metrics
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app.kubernetes.io/name: my-service-lb
  sessionAffinity: None
  type: LoadBalancer
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

The described behavior has been tested on versions Kubernetes 1.20, 1.27-1.30, and the result is the same for all.

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/

$ uname -a
Linux k8s-xc-01.stage.xxx.ru 6.1.0-18-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU/Linu
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
conteinerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI calico
</details>


### 分析结果

不涉及。

---

## Issue #125185 Job controller reports the count of ready pods with unnecessary delay

- Issue 链接：[#125185](https://github.com/kubernetes/kubernetes/issues/125185)

### Issue 内容

#### What happened?

When Job controller is deleting pods the counter in the status.ready field does not reflect this properly (there is a delay until the cache is refreshed). This affects the scenarios of terminating job, suspended job, and excess pods deleted.

#### What did you expect to happen?

The ready field is updated as soon as the delete requests are sent successfully, in sync with the status.active field.

#### How can we reproduce it (as minimally and precisely as possible)?

1. To see this you can scale down a job, use the following yaml:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: simple-job
spec:
  completions: 2
  parallelism: 2
  completionMode: Indexed
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: 'worker'
        image: python
        command:
        - python3
        - -c
        - |
          import time
          time.sleep(300)
```

2. Watch for the job updates with the following watcher:
```
kubectl get jobs -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,READY:.object.status.ready,ACTIVE:.object.status.active,TERMINATING:.object.status.terminating,CONDITIONS:.object.status.conditions | ts "%Y-%m-%d %H:%M:%.S"`
```
3. Scale down the job by `kubectl edit job` and decrease parallelism and completions to 1.

We observe events like the following
```
2024-05-29 12:49:58.790101 ADDED     simple-job   <none>   <none>   <none>        <none>
2024-05-29 12:49:58.810008 MODIFIED   simple-job   0        2        0             <none>
2024-05-29 12:50:02.065710 MODIFIED   simple-job   2        2        0             <none>
2024-05-29 12:50:17.526562 MODIFIED   simple-job   2        2        0             <none>
2024-05-29 12:50:17.542893 MODIFIED   simple-job   2        1        0             <none>
2024-05-29 12:50:18.537404 MODIFIED   simple-job   1        1        1             <none>
2024-05-29 12:50:48.785143 MODIFIED   simple-job   1        1        0             <none>
2024-05-29 12:55:01.515515 MODIFIED   simple-job   0        1        0             <none>
2024-05-29 12:55:02.519571 MODIFIED   simple-job   0        <none>   0             <none>
2024-05-29 12:55:02.529433 MODIFIED   simple-job   0        <none>   0             [map[lastProbeTime:2024-05-29T10:55:02Z lastTransitionTime:2024-05-29T10:55:02Z status:True type:Complete]]
```
Note that for one update at `2024-05-29 12:50:17.542893` ready is `2` while `active` is already `1`. We can set `ready` to `1` at this point already. Note that the pod was terminating for around 30s. 

Also note, that the event at `2024-05-29 12:50:17.542893` has `terminating=0`. This value could be set to `1` already, and this is being addressed in https://github.com/kubernetes/kubernetes/issues/125089.

#### Anything else we need to know?

This is related to https://github.com/kubernetes/kubernetes/issues/123775. We need this fix to be able to re-introduce the validation rule that `ready <= active` which was withdrawn here: https://github.com/kubernetes/kubernetes/pull/123792.


The culprit of the issue is that we determine the count of ready pods by filtering active pods [here](https://github.com/kubernetes/kubernetes/blob/afebfdc5d411859468153e2b9921915f3e4f9ff4/pkg/controller/job/job_controller.go#L832). However, the count of active pods might be changed later. 

**Note that**: This issue is slightly more complex than https://github.com/kubernetes/kubernetes/pull/123792. We cannot unconditionally decrement the counter for ready pods based on the number of successfully deleted pods. The functions deleting pods should probably also return the counters of successfully deleted ready pods (along with the count of successfully deleted pods).

#### Kubernetes version

<details>
1.30

</details>


#### Cloud provider

<details>
Kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125184 DNS resolution fails within the cluster, and it can only resolve the Pods deployed on the same host

- Issue 链接：[#125184](https://github.com/kubernetes/kubernetes/issues/125184)

### Issue 内容

#### What happened?


---

I have set up a cluster and deployed our production application. However, there is an issue with connectivity between microservices. Upon investigation, I found that our services cannot access the Eureka registry service using the internal cluster domain name.

The Eureka service is deployed on the nb-17-8 server. Pods deployed on the same server, nb-17-8, can access Eureka using DNS. For example, the three replicas of Eureka need to communicate with each other for synchronization (which works fine):

```
ndsp          eureka-0                                       1/1     Running                 1          71m     10.244.86.42    nb-17-8     <none>           <none>
ndsp          eureka-1                                       1/1     Running                 1          71m     10.244.86.44    nb-17-8     <none>           <none>
ndsp          eureka-2                                       1/1     Running                 1          71m     10.244.86.45    nb-17-8     <none>           <none>
```

Within the Eureka pods on nb-17-8, they cannot access 10.96.0.1 or 10.96.0.10:

```
kubectl exec -it -n ndsp eureka-0 -- sh
/ # nc -v 10.96.0.1 443
10.96.0.1 (10.96.0.1:443) open
^Cpunt!

/ # nc -v  10.96.0.10 53
10.96.0.10 (10.96.0.10:53) open
^Cpunt!
```

On other hosts, the Service IPs are accessible:

```
[root@nb-17-57 ~]#  nc -v 10.96.0.1 443
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.1:443.
^C
[root@nb-17-57 ~]#  nc -v 10.96.0.10 53
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.10:53.
```

On the hosts, accessing the Eureka pod via domain name is not possible, but using the Pod IP works (from all hosts and pods):

```
[root@nb-17-57 ~]# curl -I 10.244.86.42:8761
HTTP/1.1 200 
Content-Type: text/html;charset=UTF-8
Content-Language: en-US
Transfer-Encoding: chunked
Date: Wed, 29 May 2024 11:14:14 GMT
```

In the CoreDNS pod, there are the following error messages:

```
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:44838->8.8.8.8:53: i/o timeout
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:51686->8.8.4.4:53: i/o timeout
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:54068->8.8.4.4:53: i/o timeout
```

All hosts have the following `resolv.conf` file content (since it's a closed internal network, domain name resolution uses the hosts file and there is no internal DNS server):

```
cat /etc/resolv.conf
; generated by /usr/sbin/dhclient-script
```

Does anyone have any good solutions?

---

#### What did you expect to happen?

Good DNS resolution



#### How can we reproduce it (as minimally and precisely as possible)?

I don’t know, the environment is a big problem

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes version: 1.20.0

#### Cloud provider

<details>

</details>


#### OS version

Host OS: BigCloud Enterprise Linux release 8.2.2107 (Core) （Like Centos）


#### Install tools

Kubeadm

#### Container runtime (CRI) and version (if applicable)

Docker  19.03.5

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI and version: flannel v0.11.0

### 分析结果

不涉及

---

## Issue #125172 Kube-system pods are in crashlookbackup state when the OS is upgraded from RHEL 7.9 to 9.3.

- Issue 链接：[#125172](https://github.com/kubernetes/kubernetes/issues/125172)

### Issue 内容

#### What happened?

We have a cluster with 3 master and 2 worker nodes. One the master node went through RHEL OS upgrade from 7.9V to 9.3V. after this upgrade of OS the kubelet packages got upgrade to 1.28.15 automatically and resulted in Node failure for which we had to downgrade the kubelet and kubeadm packages to 1.22.15 which is our current version of kubernetes. On doing so we had the node back in ready state. but after this the KUBE-SYSTEM pods running on this server are failing with crashloopback. 

#### What did you expect to happen?

After the downgrade of the Kubelet and Kubeadm packages we expected the kube-system pods to run fine. 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run kubernetes 1.22.15 on a RHEL server 7.9v 
2. Upgrade the OS to 9.3 RHEL version
3. You can see the Kubelet and kubeadm along with other kubenetes packages will be upgraded to 1.28V
4. now donwgrade the related k8S packages to 1.22.15V
5. now you can reproduce that Kube-system pods running this particular servers wont come up.
6. You can leave the other2 master nodes running on 1.22.15v of k8s on RHEl 7.9V

#### Anything else we need to know?

I wanted to know if we can get a list of kubernetes packages or its needed dependency packages which can be avoid during the OS upgrade of RHEL from 7.9 to 9.3V.

I believe if you can give the list of packages that will get affected during the RHEL OS upgrade, would be of great help. 

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
``` Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.15", GitCommit:"1d79bc3bcccfba7466c44cc2055d6e7442e140ea", GitTreeState:"clean", BuildDate:"2022-09-21T12:18:10Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>

</details>
NA

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125170 k8s future build failure with Golang tip (1.23), testDeps interface needs updating

- Issue 链接：[#125170](https://github.com/kubernetes/kubernetes/issues/125170)

### Issue 内容

#### What happened?

Hello k8s folks:

This code here:

https://github.com/kubernetes/kubernetes/blob/cb9844915686832cf58add8d4b76d2fec9857fd1/pkg/util/coverage/coverage.go#L85

is making a call to the function `testing.MainStart`, which is documented as not maintaining the Go 1 compatibility guarantee. Here is the header comment for MainStart:

```
// MainStart is meant for use by tests generated by 'go test'.
// It is not meant to be called directly and is not subject to the Go 1 compatibility document.
// It may change signature from release to release.
```

Filing this issue to let you know that in the upcoming Go release (1.23) the signature of MainStart is indeed changing; there is a new method on the testDeps interface.

If you could please add this new method to your dummy testDeps interface (as in the previous patch  https://github.com/kubernetes/kubernetes/commit/cb9844915686832cf58add8d4b76d2fec9857fd1 that would be great.

Thanks.

@thanm from the golang team.


#### What did you expect to happen?

N/A

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
./kubectl version
Client Version: v0.0.0-master+$Format:%H$
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125167 ExternalTrafficPolicy: Local does not work for NodePort Services

- Issue 链接：[#125167](https://github.com/kubernetes/kubernetes/issues/125167)

### Issue 内容

#### What happened?

I have a 3-node Kubernetes cluster. When I create a deployment with 2 nginx pods and a NodePort service with externalTrafficPolicy: Local, I find that one of the servers is not correctly forwarding traffic with ipvsadm.

Here is the nginx.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  externalTrafficPolicy: Local
  type: NodePort
  ports:
    - port: 80
      name: http
  selector:
    app: nginx
```
pods status
```bash
root@qt-10-106-120-216:~# kubectl get po -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
nginx-deployment-77d8468669-5dbrg   1/1     Running   0          13m   10.244.1.28   qt-10-106-120-43    <none>           <none>
nginx-deployment-77d8468669-747gb   1/1     Running   0          20m   10.244.2.2    qt-10-106-120-210   <none>           <none>
```

On vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
  -> 10.244.2.2:80                Masq    1      0          0
```

On vm-2: qt-10-106-120-210

Output of ipvsadm -Ln:
```console
TCP  10.106.120.43:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
```

On vm-3: qt-10-106-120-43

Output of ipvsadm -Ln:
```console
TCP  10.106.120.210:31764 rr
  -> 10.244.2.2:80                Masq    1      0          0
```

When I update the nginx deployment to have 3 pods:
```console
root@qt-10-106-120-216:~# kubectl get po -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE                     NOMINATED NODE   READINESS GATES
nginx-deployment-77d8468669-5dbrg   1/1     Running   0          21m   10.244.1.28   qt-10-106-120-43         <none>           <none>
nginx-deployment-77d8468669-747gb   1/1     Running   0          28m   10.244.2.2    qt-10-106-120-210        <none>           <none>
nginx-deployment-77d8468669-g68qg   1/1     Running   0          62s   10.244.0.4    qt-core-10-106-120-216   <none>           <none>
```
On vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
  -> 10.244.0.4:80                Masq    1      0          0
```

On vm-2: qt-10-106-120-210

Output of ipvsadm -Ln:
```console
TCP  10.106.120.43:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
```

On vm-3: qt-10-106-120-43

Output of ipvsadm -Ln:
```console
TCP  10.106.120.210:31764 rr
  -> 10.244.2.2:80                Masq    1      0          0
```


#### What did you expect to happen?

When the nginx deployment has only 2 pods, the node without a pod should directly drop the traffic to ip:nodeport, rather than forwarding it to other nodes.

vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
```

#### How can we reproduce it (as minimally and precisely as possible)?
kubeadm.yaml
<details>

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.106.120.216
  bindPort: 6443
nodeRegistration:
  criSocket: /run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: qt-core-10-106-120-216
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.30.1
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs 
ipvs:
  strictARP: true
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```

</details>

I have an initialization script that runs on vm-1: qt-10-106-120-216.

<details>

```console
#!/bin/bash

# Temporarily disable swap
swapoff -a
# Permanently disable swap
sed -i.bak '/swap/s/^/#/' /etc/fstab

systemctl stop ufw

# Load modules
cat <<EOF> /etc/modules-load.d/k8s.conf
br_netfilter
overlay
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
EOF

modprobe br_netfilter
modprobe overlay
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh

cat <<EOF> /etc/sysctl.d/99-kubernetes-cri.conf
arp_ignore=1
arp_announce=2
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

apt update -y
apt install -y apt-transport-https
mkdir /etc/apt/keyrings
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install -y kubelet kubeadm kubectl containerd

mkdir /etc/containerd -p

containerd config default > /etc/containerd/config.toml

# Change cgroups to systemd
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml

# Change the base infrastructure image
sed -i 's#sandbox_image = "registry.k8s.io/pause"#sandbox_image = "registry.aliyuncs.com/google_containers/pause"#' /etc/containerd/config.toml

systemctl daemon-reload
systemctl restart containerd
systemctl enable containerd

cat <<EOF> /var/lib/kubelet/config.yaml
KUBELET_KUBEADM_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9"
EOF
systemctl enable kubelet.service
systemctl restart kubelet.service

# Download required images
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
# Check if images are downloaded
kubeadm config images list --config kubeadm.yaml

# Initialize the cluster
kubeadm init --config kubeadm.yaml # kubeadm.yaml needs to be modified with hostname and IP address

# Get the real user's home directory under sudo
ORIGINAL_HOME=""

if [ -n "$SUDO_USER" ]; then
    ORIGINAL_HOME=$(getent passwd "$SUDO_USER" | cut -d: -f6)
    # Get the original user's user ID and group ID
    ORIGINAL_UID=$(getent passwd "$SUDO_USER" | cut -d: -f3)
    ORIGINAL_GID=$(getent passwd "$SUDO_USER" | cut -d: -f4)
else
    ORIGINAL_HOME="$HOME"
    ORIGINAL_UID=$(id -u)
    ORIGINAL_GID=$(id -g)
fi

mkdir -p $ORIGINAL_HOME/.kube
cp -i /etc/kubernetes/admin.conf $ORIGINAL_HOME/.kube/config
# Use the original user's user ID and group ID
chown $ORIGINAL_UID:$ORIGINAL_GID $ORIGINAL_HOME/.kube/config

# Remove taints
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes --all node-role.kubernetes.io/master-
```

</details>

Run the following script on other nodes and execute kubeadm join to join the cluster
vm-2: qt-10-106-120-210.
vm-3: qt-10-106-120-43.

<details>

```console
#!/bin/bash

# Temporarily disable swap
swapoff -a
# Permanently disable swap
sed -i.bak '/swap/s/^/#/' /etc/fstab

systemctl stop ufw

# Load modules
cat <<EOF> /etc/modules-load.d/k8s.conf
br_netfilter
overlay
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
EOF

modprobe br_netfilter
modprobe overlay
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh

cat <<EOF> /etc/sysctl.d/99-kubernetes-cri.conf
arp_ignore=1
arp_announce=2
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

apt update -y
apt install -y apt-transport-https
mkdir /etc/apt/keyrings
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install -y kubelet kubeadm kubectl containerd

mkdir /etc/containerd -p

containerd config default > /etc/containerd/config.toml

# Change cgroups to systemd
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml

# Change the base infrastructure image
sed -i 's#sandbox_image = "registry.k8s.io/pause"#sandbox_image = "registry.aliyuncs.com/google_containers/pause"#' /etc/containerd/config.toml

systemctl daemon-reload
systemctl restart containerd
systemctl enable containerd

cat <<EOF> /var/lib/kubelet/config.yaml
KUBELET_KUBEADM_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9"
EOF
systemctl enable kubelet.service
systemctl restart kubelet.service

# Download required images
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
```

</details>

Install the flannel plugin.

<details>

```yaml
---
kind: Namespace
apiVersion: v1
metadata:
  name: kube-flannel
  labels:
    pod-security.kubernetes.io/enforce: privileged
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - "networking.k8s.io"
  resources:
  - clustercidrs
  verbs:
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-flannel
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-flannel
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni-plugin
        image: docker.io/flannel/flannel-cni-plugin:v1.1.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2
        command:
        - cp
        args:
        - -f
        - /flannel
        - /opt/cni/bin/flannel
        volumeMounts:
        - name: cni-plugin
          mountPath: /opt/cni/bin
      - name: install-cni
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: EVENT_QUEUE_DEPTH
          value: "5000"
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
        - name: xtables-lock
          mountPath: /run/xtables.lock
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni-plugin
        hostPath:
          path: /opt/cni/bin
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
```

</details>

Install the nginx.

<details>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  externalTrafficPolicy: Local
  type: NodePort
  ports:
    - port: 80
      name: http
  selector:
    app: nginx
```

</details>

Use the curl command to access the IP:nodeport of a node where the nginx pod does not exist.




#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.3 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.3 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
# Linux qt-10-106-120-216 5.4.0-99-generic #112-Ubuntu SMP Thu Feb 3 13:50:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

```console
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.1", GitCommit:"6911225c3f747e1cd9d109c305436d08b668f086", GitTreeState:"clean", BuildDate:"2024-05-14T10:49:05Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
containerd github.com/containerd/containerd 1.7.2
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

flannel:v0.21.2

<details>

```yaml
---
kind: Namespace
apiVersion: v1
metadata:
  name: kube-flannel
  labels:
    pod-security.kubernetes.io/enforce: privileged
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - "networking.k8s.io"
  resources:
  - clustercidrs
  verbs:
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-flannel
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-flannel
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni-plugin
        image: docker.io/flannel/flannel-cni-plugin:v1.1.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2
        command:
        - cp
        args:
        - -f
        - /flannel
        - /opt/cni/bin/flannel
        volumeMounts:
        - name: cni-plugin
          mountPath: /opt/cni/bin
      - name: install-cni
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: EVENT_QUEUE_DEPTH
          value: "5000"
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
        - name: xtables-lock
          mountPath: /run/xtables.lock
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni-plugin
        hostPath:
          path: /opt/cni/bin
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
```
</details>


### 分析结果

不涉及

---

## Issue #125164 Parallel image pull should pull unique images only once

- Issue 链接：[#125164](https://github.com/kubernetes/kubernetes/issues/125164)

### Issue 内容

#### What would you like to be added?

Creating large amount of pods that all use the same image causes the kubelet to start pull per each pod, when parallel pull is enabled. This causes the reigstry QPS to be used, slowing down other pods with other images and also wastes the bandwidth for downloading the same images multiple times.

If same image is being already downloaded in parallel, other pods should just wait for it, not starting new download and not using up registry QPS. 

#### Why is this needed?

We have use case where we start large number (500+) pods at the same time where many pods gets scheduled on single node (100+), and most pods have same init container with the same image (given the image on the container is provided with full SHA digest).

This causes lot of bandwidth on the node to be used during the startup of those pods, and also some pods are delayed due to `ImagePullBackoff` caused by `pull QPS exceeded`.

### 分析结果

不涉及

---

## Issue #125155 problem when calling the API interface to create or delete  PVC resources

- Issue 链接：[#125155](https://github.com/kubernetes/kubernetes/issues/125155)

### Issue 内容

#### What happened?

When calling the API interface to create PVC resources, PVC is not yet in a bound state (still in a pending state), The API interface call has returned. When calling the API interface to delete PVC resources, PVC is still in the terminating state and has not been completely removed, but the API interface has returned success

#### What did you expect to happen?

When calling the API interface to create PVC resources, when PVC is in a bound state, The API interface call returned. When calling the API interface to delete PVC resources, PVC  has been completely removed, the API interface call returned success.

#### How can we reproduce it (as minimally and precisely as possible)?

calling the API interface to create or delete  PVC resources

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.23


#### Cloud provider

none

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125134 ResourceQuota computed used resources for extended resources is wrong

- Issue 链接：[#125134](https://github.com/kubernetes/kubernetes/issues/125134)

### Issue 内容

#### What happened?

When using the `ResourceQuotas` admission controller for extended resources (`nvidia.com/gpu`) the reported used resources are inconsistently and wrong.

#### What did you expect to happen?

The reported used extended resources should respect the current status in the Namespace.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the following ResourceQuota

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: capsule-skg00000-2
spec:
  hard:
    requests.nvidia.com/gpu: "10"
```

Check the RQ is applied correctly and reported the expected status:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ResourceQuota","metadata":{"annotations":{},"name":"capsule-skg00000-2","namespace":"skg00000-gpu"},"spec":{"hard":{"requests.nvidia.com/gpu":"10"}}}
  creationTimestamp: "2024-05-26T17:00:28Z"
  labels:
    capsule.clastix.io/managed-by: skg00000
  name: capsule-skg00000-2
  namespace: skg00000-gpu
  resourceVersion: "8225530"
  uid: 3badf2c2-9959-4882-9315-7ffff127e08e
spec:
  hard:
    requests.nvidia.com/gpu: "10"
status:
  hard:
    requests.nvidia.com/gpu: "10"
  used:
    requests.nvidia.com/gpu: "0"
```

Create a Deployment with a template consuming the extended resource (GPU).

```yaml
spec:        
  automountServiceAccountToken: false   
  containers:
  - args:    
    - sleep 3600      
    command: 
    - /bin/bash       
    - -c     
    - --     
    image: nvidia/cuda:11.0.3-base-ubuntu20.04   
    imagePullPolicy: Always    
    name: nvidia      
    resources:        
      limits:
        cpu: "1"      
        memory: 1Gi   
        nvidia.com/gpu: "1"    
      requests:       
        cpu: "1"      
        memory: 1Gi   
        nvidia.com/gpu: "1"
```

After the Pod is deployed, the ResourceQuota reports the wrong amount of used resources:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ResourceQuota","metadata":{"annotations":{},"name":"capsule-skg00000-2","namespace":"skg00000-gpu"},"spec":{"hard":{"requests.nvidia.com/gpu":"10"}}}
  creationTimestamp: "2024-05-26T17:00:28Z"
  labels:
    capsule.clastix.io/managed-by: skg00000
  name: capsule-skg00000-2
  namespace: skg00000-gpu
  resourceVersion: "8225807"
  uid: 3badf2c2-9959-4882-9315-7ffff127e08e
spec:
  hard:
    requests.nvidia.com/gpu: "10"
status:
  hard:
    requests.nvidia.com/gpu: "10"
  used:
    requests.nvidia.com/gpu: "2"
```

The same applies when performing scale out of the Deployment, with inconsistent reporting.

```
$: kubectl scale deployment gpu-workload --replicas=2
$ RQ reports 4 used instances
$: kubectl scale deployment gpu-workload --replicas=3
$ RQ reports 6 used instances
$: kubectl scale deployment gpu-workload --replicas=4
$ RQ reports 8 used instances
```

Upon scaling down, the used instances are still inconsistent from time to time.

```
$: kubectl scale deployment gpu-workload --replicas=3
$ RQ reports 3 used instances
$: kubectl scale deployment gpu-workload --replicas=2
$ RQ reports 3 used instances
$: kubectl scale deployment gpu-workload --replicas=1
$ RQ reports 1 used instances
$: kubectl scale deployment gpu-workload --replicas=0
$ RQ reports 0 used instances
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux REDACTED 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N.R.
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N.R.
</details>


### 分析结果

不涉及

---

## Issue #125133 Watch of a single namespace using /api/v1/watch/namespaces/$name missing all events in 1.27+

- Issue 链接：[#125133](https://github.com/kubernetes/kubernetes/issues/125133)

### Issue 内容

#### What happened?

We upgraded our test cluster to `1.28.8` (GKE `1.28.8-gke.1095000`) and noticed that some of our tests began to time out when watching for events that never arrived. 

Upon investigation, it appears that there is a regression in the deprecated path-based watch API for at least `namespaces` e.g. `http://localhost:8001/api/v1/watch/namespaces/<name>`. We have not been able to reproduce this with any other resource type.

When creating a watch for a namespace (see `how to reproduce`) we see the initial `added` event, but then we do not see subsequent events if we modify the namespace e.g. add a label or annotation. 

#### What did you expect to happen?

I would expect to see events when the namespace is altered. E.g. adding a label should produce a `MODIFIED` event with the contents.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a new test namespace e.g. `kubectl create namespace test-namespace`
2. Create a proxy to the master `kubectl proxy`
3. Curl the watch path of the namespace that has been created e.g. `http://localhost:8001/api/v1/watch/namespaces/test-namespace`
4. In another shell, modify the namespace to add a label to it.
5. Observe that there is no modified event in the watch.

#### Anything else we need to know?

It is understood that the `/watch` based paths are and have been for some time deprecated, we are working on moving to use the watch query param instead.

#### Kubernetes version

```
K8s Version: v.1.28.8
GKE Version: 1.28.8-gke.1095000
Kubectl Version: v1.28.7
```

#### Cloud provider

<details>
We are using Google Kubernetes Engine, please see version above.
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125130 Regarding the issue of certificate expiration when accessing links with Https in kubernetes through pod access

- Issue 链接：[#125130](https://github.com/kubernetes/kubernetes/issues/125130)

### Issue 内容

#### What happened?

![image](https://github.com/kubernetes/kubernetes/assets/46311200/5f7f6d63-c825-47ca-80a0-7784a370f86d)
I curl any link with HTTPS in the ingress nginx controller pod in Kubernetes, and the HTTPS certificate will expire at the same time. However, I have no problem curling on the node, and I have no problem starting an nginx container curl using Docker

#### What did you expect to happen?

I curl any link with HTTPS in the ingress nginx controller pod in Kubernetes, and the HTTPS certificate will expire at the same time. However, I have no problem curling on the node, and I have no problem starting an nginx container curl using Docker

#### How can we reproduce it (as minimally and precisely as possible)?

Use Proxmox Virtual Environment and start running a kubernetes cluster, then run an nginx container curl https link

#### Anything else we need to know?

_No response_

#### Kubernetes version

[root@Rocky ~]#  kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.0
WARNING: version difference between client (1.29) and server (1.24) exceeds the supported minor version skew of +/-1



#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
![image](https://github.com/kubernetes/kubernetes/assets/46311200/6023ab68-775f-487a-a3fb-c84c0c068bc0)
![image](https://github.com/kubernetes/kubernetes/assets/46311200/6c9a2fd8-30f1-4421-9660-7657dcd66ddb)


#### Install tools

<details>
Kind  kubeadm
</details>
I have installed both Kind and Kubeadm and encountered the same problem

#### Container runtime (CRI) and version (if applicable)

<details>
![image](https://github.com/kubernetes/kubernetes/assets/46311200/0b862854-970d-4e82-9eb7-a555187731a8)

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125098 kubernetes-sigs / scheduler-plugins  go.mod Error 

- Issue 链接：[#125098](https://github.com/kubernetes/kubernetes/issues/125098)

### Issue 内容

#### What happened?

Trying to create scheduler plugin by referring link: https://github.com/kubernetes-sigs/scheduler-plugins.git 
Getting error while executing "go mod tidy"

go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1/fake
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/scheme
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/internalinterfaces
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/listers/scheduling/v1alpha1
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/fake imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/fake imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1/fake: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1 imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/scheme: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/internalinterfaces: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1 imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/listers/scheduling/v1alpha1: no matching versions for query "latest"
  

#### What did you expect to happen?

no error

#### How can we reproduce it (as minimally and precisely as possible)?

How to update mod.go

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
not applicable

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
Not applicable

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125089 Job controller reports the count of terminating pods with unnecessary delay

- Issue 链接：[#125089](https://github.com/kubernetes/kubernetes/issues/125089)

### Issue 内容

#### What happened?

When Job controller is terminating and deletes all pods the counter in the `status.terminating` field does not reflect this properly. 

#### What did you expect to happen?

The counter for the terminating pods is updated as soon as Job controller has this information. Similarly as for the counter of active pods.

#### How can we reproduce it (as minimally and precisely as possible)?

0. watch the job & pod updates with the commands:
`kubectl get jobs -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,STATUS:.object.status | ts "%Y-%m-%d %H:%M:%.S"`
`kubectl get pods -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,PHASE:.object.status.phase,FINALIZERS:.object.metadata.finalizers,DELETION:object.metadata.deletionTimestamp,CONTAINERS:.object.status.containerStatuses | ts "%Y-%m-%d %H:%M:%.S"`
1. Run the Job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-job
  labels:
    jobgroup: indexedjob
spec:
  completions: 2
  parallelism: 2
  backoffLimit: 0
  completionMode: Indexed
  podReplacementPolicy: Failed
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: 'worker'
        image: python
        readinessProbe:
          httpGet:
            path: /non-existent-endpoint
            port: 80
          initialDelaySeconds: 1
          periodSeconds: 10
          failureThreshold: 10
        command:
        - python3
        - -c
        - |
          import os, sys, time
          print("Hello world")
          if int(os.environ.get("JOB_COMPLETION_INDEX")) == 0:
            time.sleep(60)
            sys.exit(1)
          time.sleep(300)
          sys.exit(0)
```
3. When the job transitions you can observe updates like this:
For Job:
```
2024-05-23 13:52:11.053575 EVENT   NAME          STATUS
2024-05-23 13:52:11.053833 ADDED   indexed-job   map[]
2024-05-23 13:52:11.076666 MODIFIED   indexed-job   map[active:2 ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[]]
2024-05-23 13:53:15.315997 MODIFIED   indexed-job   map[ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[failed:[b237e4f0-dc57-4349-8aa6-7bc381266aed d4dfe535-4b6d-4374-8e9d-b73c9823cd35]]]
2024-05-23 13:53:15.328668 MODIFIED   indexed-job   map[conditions:[map[lastProbeTime:2024-05-23T11:53:15Z lastTransitionTime:2024-05-23T11:53:15Z message:Job has reached the specified backoff limit reason:BackoffLimitExceeded status:True type:Failed]] failed:2 ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[]]
```
**Issue**: the job reports `terminating: 0` even though it already deleted the second pod, because it is enumerated in the 
`uncountedTerminatedPods`. Also, it is inconsistent, `active` is reported as `0` and `terminating` is 0 as well, while setting `Failed` condition, the pods are in fact still terminating. This can be visible from the pod updates:
```
2024-05-23 13:52:11.063000 EVENT   NAME                  PHASE     FINALIZERS                           DELETION   CONTAINERS
2024-05-23 13:52:11.063188 ADDED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.066376 MODIFIED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.069335 ADDED      indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.076759 MODIFIED   indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.079168 MODIFIED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     [map[image:python imageID: lastState:map[] name:worker ready:false restartCount:0 started:false state:map[waiting:map[reason:ContainerCreating]]]]
2024-05-23 13:52:11.089661 MODIFIED   indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     [map[image:python imageID: lastState:map[] name:worker ready:false restartCount:0 started:false state:map[waiting:map[reason:ContainerCreating]]]]
2024-05-23 13:52:13.031692 MODIFIED   indexed-job-0-wz49q   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:52:14.034589 MODIFIED   indexed-job-1-jhl9t   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:13.140294 MODIFIED   indexed-job-0-wz49q   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:14.300875 MODIFIED   indexed-job-0-wz49q   Failed    [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.142013 MODIFIED   indexed-job-0-wz49q   Failed    [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.308553 MODIFIED   indexed-job-1-jhl9t   Running   [batch.kubernetes.io/job-tracking]   2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:15.324258 MODIFIED   indexed-job-0-wz49q   Failed    <none>                               <none>                 [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.324497 MODIFIED   indexed-job-1-jhl9t   Running   <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:45.569196 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.198066 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.205463 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:15Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.207674 DELETED    indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:15Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
```
Note that the second pod gets the deletionTimestamp at `2024-05-23 13:53:15.324497` which is prior to the Job controller setting the `Failed` condition.

I think when fix we should have the `terminating>0` for the last status updates for the Job controller above.

#### Anything else we need to know?

I think the proper solution is to increment the counter of `terminating` pods [here](https://github.com/kubernetes/kubernetes/blob/c9a1a0a3b8acb70cba30d6f4ea59505b4564b4d9/pkg/controller/job/job_controller.go#L895-L902), where we decrement the `active` pods.

Also, this fix is related to https://github.com/kubernetes/kubernetes/issues/123775. If we have the fix we could delay setting the `Failed` condition until `terminating=0`. Otherwise we may observe `terminating=0` because the update of the counter is delayed.

#### Kubernetes version

<details>
Server Version: v1.30.0
</details>


#### Cloud provider

<details>
Kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125079 The old pod log file is not deleted from the /var/log/pods/ directory

- Issue 链接：[#125079](https://github.com/kubernetes/kubernetes/issues/125079)

### Issue 内容

#### What happened?

In the /var/log/pods/ directory, many old log files are not deleted in a timely manner. As a result, the disk space is used up.
![1709001460262](https://github.com/kubernetes/kubernetes/assets/54977497/e541ec52-d6c4-4b7a-bcb6-f7bc88969d24)


#### What did you expect to happen?

I think there should be only 1 old log file in this path.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how this happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.28
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125069 Bug: securityContext appArmorProfile unconfined not working with containerd

- Issue 链接：[#125069](https://github.com/kubernetes/kubernetes/issues/125069)

### Issue 内容

#### What happened?

If I define the podSecurityContext with appArmorProfile unconfined, containerd does not take it into account and use the default cri-containerd.apparmor.d profile.
I don't have the problem if I use the deprecated annotations.

#### What did you expect to happen?

The securityContext should give the same result as the annotations. According to #123811 containerd uses both methods.

#### How can we reproduce it (as minimally and precisely as possible)?

This deployment works with annotations:
```
apiVersion: apps/v1                                                                                                                                          
kind: Deployment                                                                                                                                             
metadata:                                                                                                                                                    
  name: test                                                                                                                                                 
  namespace: test                                                                                                                                            
spec:                                                                                                                                                        
  selector:                                                                                                                                                  
    matchLabels:                                                                                                                                             
      mytest: test
  template:
    metadata:
      labels:
        mytest: test
      annotations:
        container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
    spec:
      initContainers:
      - name: mount-cgroup
        image: quay.io/cilium/cilium:v1.15.4
        env:
        - name: CGROUP_ROOT
          value: /run/cilium/cgroupv2
        - name: BIN_PATH
          value: /opt/cni/bin
        securityContext:
          appArmorProfile:
            type: Unconfined
          capabilities:
            add:
            - SYS_ADMIN
            - SYS_CHROOT
            - SYS_PTRACE
            drop:
            - ALL
          seLinuxOptions:
            level: s0
            type: spc_t
        command:
        - sh
        - -ec
        - ls -la /hostproc/1/ns/cgroup
        volumeMounts:
        - name: hostproc
          mountPath: /hostproc
      containers:
      - name: busybox
        image: busybox
        command:
        - sleep
        - "300"
        securityContext:
          appArmorProfile:
            type: Unconfined
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
          type: Directory
```
But this one doesn't.
```
apiVersion: apps/v1                                                                                                                                          
kind: Deployment                                                                                                                                             
metadata:                                                                                                                                                    
  name: test                                                                                                                                                 
  namespace: test                                                                                                                                            
spec:                                                                                                                                                        
  selector:                                                                                                                                                  
    matchLabels:                                                                                                                                             
      mytest: test
  template:
    metadata:
      labels:
        mytest: test
    spec:
      initContainers:
      - name: mount-cgroup
        image: quay.io/cilium/cilium:v1.15.4
        env:
        - name: CGROUP_ROOT
          value: /run/cilium/cgroupv2
        - name: BIN_PATH
          value: /opt/cni/bin
        securityContext:
          appArmorProfile:
            type: Unconfined
          capabilities:
            add:
            - SYS_ADMIN
            - SYS_CHROOT
            - SYS_PTRACE
            drop:
            - ALL
          seLinuxOptions:
            level: s0
            type: spc_t
        command:
        - sh
        - -ec
        - ls -la /hostproc/1/ns/cgroup
        volumeMounts:
        - name: hostproc
          mountPath: /hostproc
      containers:
      - name: busybox
        image: busybox
        command:
        - sleep
        - "300"
        securityContext:
          appArmorProfile:
            type: Unconfined
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
          type: Directory
```
dmesg on the node where the container is launched gives:
`type=1400 audit(1716404572.354:231): apparmor="DENIED" operation="ptrace" profile="cri-containerd.apparmor.d" pid=1852196 comm="ls" requested_mask="read" denied_mask="read" peer="unconfined"`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
Bare Metal K8s on Ubuntu 22.04 servers
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
# Linux k8s-master-01 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.7.17
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
cilium-cli: v0.16.7 compiled with go1.22.2 on linux/amd64
cilium image (default): v1.15.4
cilium image (stable): v1.15.5
cilium image (running): 1.15.4
</details>


### 分析结果

不涉及

---

## Issue #125011 Failed to create the reconcile looper: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded

- Issue 链接：[#125011](https://github.com/kubernetes/kubernetes/issues/125011)

### Issue 内容

#### What happened?

While we tried to scale in/out pods, reconciler failure was reported . Reconciler job scheduled for every 5 minutes but failed to execute with the given error: 
`[error] failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded.`
 `[error] failed to create the reconcile looper: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded`
`[verbose] reconciler failure: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded.`

The deployment of pod replicas scaled out to 500, the same for ipam podreferences. But when we scale in replicas to 1, pods are scaled in successfully multiple times but more that 100 podreferences are left. Sometimes pods are staying in terminating state. We did multiple series of scale in/out and then uninstall and redeployment - the same issue faced every time: more than 100 pod references left undeleted after scale in. The number of pod references increase as we do multiple times scale in.

Pod reference left undeleled :
```
kubectl get ippools.whereabouts.cni.cncf.io -n kube-system  197.0.0.0-8 -o yaml | grep -c podref
143
```
Error when pod is in terminating state: 
`Warning  FailedKillPod   46s    kubelet            error killing pod: failed to "KillPodSandbox" for "f48e47e5-a4c7-44df-ad4a-b420844dcacc" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"`

Issue reported also in [whereabouts](https://github.com/k8snetworkplumbingwg/whereabouts/issues/389)

#### What did you expect to happen?

We expect all podreferences should be deleted during scale in and it shouldn't generate any error in terminating state or podreference issue.



#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction can be done using the [test](https://github.com/k8snetworkplumbingwg/whereabouts?tab=readme-ov-file#running-whereabouts-cni-in-a-local-kind-cluster) where we have used 8 NetworkAttachmentDefinition and do the following steps:

1. The deployment of pod replicas to 500, the same for ipam pod references.
2. Scale in replicas to 1, pods are scaled successfully but more than 100 pod references are left undeleted.
3. Do these 2 series of scale in/out and then release uninstall and redeployment - the same issue every time: more than 100 pod references left undeleted after a scale in.

#### Anything else we need to know?

Would like to explain the issue here in steps for better view:

1. IPs are stored as the ippools CRD of whereabouts, There is an overlapping IP ranges CRD used to store all IPs.[Which creates object for each IP] 
4. The issue is when reconciler code tries to fetch all the objects in under the overlappingipranges. When the number of IPs grow, the number of overlappingipranges objects grow(one for each IP)
5. Reconciler on the initialization of the the first executor objects tries to list all objects under the overlappingipranges CRD, and hits a context-deadline.

#### Kubernetes version

<details>

```console
Issue started with 1.26.1 version and also reproduced in later versions.
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Whereabouts version : 0.6.2 and also with upgraded version
</details>


### 分析结果

不涉及。

---

## Issue #125010 `GOTOOLCHAIN=go1.22.1 make verify WHAT=codegen` fails with error deleting tempdir

- Issue 链接：[#125010](https://github.com/kubernetes/kubernetes/issues/125010)

### Issue 内容

#### What happened?

While debugging https://github.com/kubernetes/kubernetes/issues/124932 I discovered that setting `GOTOOLCHAIN` causes `make verify WHAT=codegen` @ 765e7ef0d21 (recent master branch commit) to fail on cleanup at the end.

This doesn't happen if you don't set `GOTOOLCHAIN`. It's not immediately apparent why those are related.

#### What did you expect to happen?

This make target should not fail on a clean branch with no diff.

#### How can we reproduce it (as minimally and precisely as possible)?

set GOTOOLCHAIN to some valid go version and then run `make verify WHAT=codegen`

#### Anything else we need to know?

/sig testing release
/triage accepted

#### Kubernetes version

765e7ef0d21

#### Cloud provider

n/a

#### OS version

Linux. ~Debian. But this is happening inside our build container.

#### Install tools

n/a

#### Container runtime (CRI) and version (if applicable)

docker 20.10.21

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

### 分析结果

不涉及

---

## Issue #124962 DRA: no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/

- Issue 链接：[#124962](https://github.com/kubernetes/kubernetes/issues/124962)

### Issue 内容

#### What happened?

```
E0520 07:49:57.337809       1 nonblockinggrpcserver.go:125] "handling request failed" err="failed registration process: RegisterPlugin error -- no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/netresources.spidernet.io.sock" logger="registrar" requestID=8 request="&RegistrationStatus{PluginRegistered:false,Error:RegisterPlugin error -- no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/netresources.spidernet.io.sock,}"
```

#### What did you expect to happen?

handling request well

#### How can we reproduce it (as minimally and precisely as possible)?

write a dra kubelet-plugin with `kubeletplugin.Start()`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
root@controller-node-1:~# kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Linux controller-node-1 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124940 last applied annotations are not getting updated

- Issue 链接：[#124940](https://github.com/kubernetes/kubernetes/issues/124940)

### Issue 内容

#### What happened?

i was trying to update configmap and after editing successfully configmap annotation didn't update with the last values. still, it holds 1st-time data only.
basically, I am trying to write one code to capture if someone changes anything in CM and we can identify it.

and also in some configmap I am not able to find last-applied annotation.

#### What did you expect to happen?

annotation should update

#### How can we reproduce it (as minimally and precisely as possible)?

create cm with below 
`apiVersion: v1
kind: ConfigMap
metadata:
  name: game-config-env-file
data:
  allowed: '"true"'
  enemies: aliens
  lives: "31"`

and now edit this cm game-config-env-file and let say change any values last-applied annotation is not getting chnaged

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.28.8-eks-ae9a62a
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8-eks-adc7111
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124938 [Sidecar Containers] Eviction message should account for the sidecar containers

- Issue 链接：[#124938](https://github.com/kubernetes/kubernetes/issues/124938)

### Issue 内容

#### What happened?

The `evictionMessage` is not accounting for the restartable init containers (sidecars). 

/kind bug

#### What did you expect to happen?

When pod is evicted, requests of the sidecar containers must be added to the annotations.

#### How can we reproduce it (as minimally and precisely as possible)?

Evict pod with the sidecars and check out annotation.

#### Anything else we need to know?

/sig node
/priority backlog
KEP: https://github.com/kubernetes/enhancements/issues/753

### 分析结果

不涉及。

---

## Issue #124937 [Sidecar Containers] Sidecar containers finish time needs to be accounted for in job controller

- Issue 链接：[#124937](https://github.com/kubernetes/kubernetes/issues/124937)

### Issue 内容

#### What happened?

Today the function `getFinishTimeFromContainers` (pkg/controller/job/backoff_utils.go) only account for the regular containers finish time,
presumably assuming that init containers have completed before before.
However, with the sidecar (restartable init) containers, sidecar containers will always finish later than
regular containers. And those needs to be accounted for the calculation.

/kind bug

#### What did you expect to happen?

The sidecar finish time will be accounted when calculating the job's finish time.

#### How can we reproduce it (as minimally and precisely as possible)?

Sidecar container with prolonged termination logic on a job will result in incorrect finish time reporting.

#### Anything else we need to know?

/sig apps
/priority backlog
KEP: https://github.com/kubernetes/enhancements/issues/753

### 分析结果

不涉及。

---

## Issue #124936 [Sidecar Containers] Pods comparison by maxContainerRestarts should account for sidecar containers

- Issue 链接：[#124936](https://github.com/kubernetes/kubernetes/issues/124936)

### Issue 内容

#### What happened?

Today, there are a few uses of the function `maxContainerRestarts` - mostly to compare pods to decide which one is
better to delete or which logs to get. This is not a huge issue, mostly a quality of life improvement.

The code only look at Container Statuses, but likely need to look at init container statuses as well.
Especially in case of sidecar containers that may behave exactly as regular containers.

There are 2 implementations and 5 comparison interfaces.

Implementatons in:

- pkg/controller/controller_utils.go
- staging/src/k8s.io/kubectl/pkg/util/podutils/podutils.go

```
func maxContainerRestarts(pod *v1.Pod) int {
	maxRestarts := 0
	for _, c := range pod.Status.ContainerStatuses {
		maxRestarts = max(maxRestarts, int(c.RestartCount))
	}
	return maxRestarts
}
```

We may need to be careful including all init container statuses. If a Pod was failing to start for a while
because of Init container failures and now it is running OK, it is likely not important. However, including
the restartable containers (sidecars) restart count is important.

I think the desireable behavior will be to check regular containers max restart count first. And compare this.
Then compare max restart count for restarteable init containers.

/kind bug

#### What did you expect to happen?

Comparison of max container restarts account for init containers as well as regular containers.

#### How can we reproduce it (as minimally and precisely as possible)?

Two pods - one with the sidecar container in constant restart loop and one is running sucessfully. Random one will be picked to get logs from.

#### Anything else we need to know?

/sig apps
/priority backlog

KEP: https://github.com/kubernetes/enhancements/issues/753

### 分析结果

不涉及

---

## Issue #124932 containerized protobuf codegen does not handle .go-version / GOTOOLCHAIN properly

- Issue 链接：[#124932](https://github.com/kubernetes/kubernetes/issues/124932)

### Issue 内容

Seen in https://github.com/kubernetes/kubernetes/pull/124922

```
+++ command: bash "hack/make-rules/../../hack/verify-codegen.sh"
go version go1.22.3 linux/amd64
+++ [0517 06:22:42] Generating protobufs for 67 targets
+++ [0517 06:22:42] protoc 23.4 not found (can install with hack/install-protoc.sh); generating containerized...
+++ [0517 06:22:42] Verifying Prerequisites....
+++ [0517 06:22:43] Building Docker image kube-build:build-71fc6bd7d6-5-v1.28.0-go1.21.10-bullseye.0
+++ [0517 06:24:57] Creating data container kube-build-data-71fc6bd7d6-5-v1.28.0-go1.21.10-bullseye.0
+++ [0517 06:24:58] Syncing sources to container
+++ [0517 06:25:03] Output from this container will be rsynced out upon completion. Set KUBE_RUN_COPY_OUTPUT=n to disable.
+++ [0517 06:25:03] Running build command...
go: downloading go1.22.3 (linux/amd64)
go: download go1.22.3 for linux/amd64: toolchain not available
!!! [0517 06:25:04] Call tree:
!!! [0517 06:25:04]  1: build/../build/common.sh:489 kube::build::run_build_command_ex(...)
!!! [0517 06:25:04]  2: build/run.sh:39 kube::build::run_build_command(...)
!!! [0517 06:25:04] Call tree:
!!! [0517 06:25:04]  1: hack/update-codegen.sh:919 codegen::protobuf(...)
+++ exit code: 1
+++ error: 1
[0;31mFAILED[0m   verify-codegen.sh	148s
```

Something is not working properly between .go-version → GOTOOLCHAIN when the go version inside this specific container doesn't match

/assign 
/cc @BenTheElder @MadhavJivrajani

### 分析结果

不涉及

---

## Issue #124930 v1.30: kube-scheduler crashes with: Observed a panic: "integer divide by zero"

- Issue 链接：[#124930](https://github.com/kubernetes/kubernetes/issues/124930)

### Issue 内容

#### What happened?

On Kubernetes v1.30.0 (and v1.30.1), `kube-scheduler` can crash with the following panic if a pod is defined in a certain way:

```
W0514 09:09:41.391780       1 feature_gate.go:246] Setting GA feature gate MinDomainsInPodTopologySpread=true. It will be removed in a future release.
I0514 09:09:43.191448       1 serving.go:380] Generated self-signed cert in-memory
W0514 09:09:43.574824       1 authentication.go:446] failed to read in-cluster kubeconfig for delegated authentication: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
W0514 09:09:43.574862       1 authentication.go:339] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0514 09:09:43.574871       1 authentication.go:363] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0514 09:09:43.574885       1 authorization.go:225] failed to read in-cluster kubeconfig for delegated authorization: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
W0514 09:09:43.574992       1 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
I0514 09:09:43.586307       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0-zalando-master-117-dirty"
I0514 09:09:43.586327       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0514 09:09:43.587758       1 secure_serving.go:213] Serving securely on [::]:10259
I0514 09:09:43.587944       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0514 09:09:43.688733       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I0514 09:09:59.030033       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
E0514 09:09:59.122043       1 runtime.go:79] Observed a panic: "integer divide by zero" (runtime error: integer divide by zero)
goroutine 330 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x1b68bc0, 0x35e1c40})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x7c
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x4000be8e00?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x78
panic({0x1b68bc0?, 0x35e1c40?})
	runtime/panic.go:770 +0x124
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatFitPod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:502 +0x88c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:402 +0x25c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulingCycle(0x40000d6900, {0x22aae08, 0x40012b9630}, 0x4000347dc0, {0x22d2bb8, 0x4000320fc8}, 0x4000c92e10, {0x2?, 0x2?, 0x36352c0?}, ...)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:149 +0xb8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne(0x40000d6900, {0x22aae08, 0x40012b83c0})
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:111 +0x4c0
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext.func1()
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x2c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400161dec8?)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400161df68, {0x2286fe0, 0x4001398b70}, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0x90
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x4000669f68, 0x0, 0x0, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x80
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0x22aae08, 0x40012b83c0}, 0x4001385720, 0x0, 0x0, 0x1)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x80
k8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:170
created by k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run in goroutine 352
	k8s.io/kubernetes/pkg/scheduler/scheduler.go:445 +0x104
panic: runtime error: integer divide by zero [recovered]
	panic: runtime error: integer divide by zero

goroutine 330 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x4000be8e00?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:56 +0xe0
panic({0x1b68bc0?, 0x35e1c40?})
	runtime/panic.go:770 +0x124
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatFitPod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:502 +0x88c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:402 +0x25c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulingCycle(0x40000d6900, {0x22aae08, 0x40012b9630}, 0x4000347dc0, {0x22d2bb8, 0x4000320fc8}, 0x4000c92e10, {0x2?, 0x2?, 0x36352c0?}, ...)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:149 +0xb8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne(0x40000d6900, {0x22aae08, 0x40012b83c0})
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:111 +0x4c0
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext.func1()
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x2c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400161dec8?)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400161df68, {0x2286fe0, 0x4001398b70}, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0x90
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x4000669f68, 0x0, 0x0, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x80
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0x22aae08, 0x40012b83c0}, 0x4001385720, 0x0, 0x0, 0x1)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x80
k8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:170
created by k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run in goroutine 352
	k8s.io/kubernetes/pkg/scheduler/scheduler.go:445 +0x104
```

The crash happens [here](https://github.com/kubernetes/kubernetes/blob/7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a/pkg/scheduler/schedule_one.go#L502) because the `len(nodes)` is `0` in certain cases. 

#### What did you expect to happen?

`kube-scheduler` should not crash, on v1.29.4 it doesn't happen.

On v1.29.4 the `kube-scheduler` is just printing these error logs, but doesn't crash:

```
E0517 13:19:26.611504       1 schedule_one.go:1003] "Error scheduling pod; retrying" err="nodeinfo not found for node name \"invalid-node\"" pod="default/break-kube-scheduler"
```

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: break-kube-scheduler
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - invalid-node # a node that doesn't exist
  containers:
  - name: main
    image: alpine
    command: ["cat"]
    stdin: true
```
The important part is that the affinity doesn't match a real/valid node.

Once this pod is being processed by `kube-scheduler` it will crash and continue to do so until the pod is deleted.

#### Anything else we need to know?

This issue was triggered during the rotation of control-plane nodes. In our setup we update the control plane by scaling from 1 to 2 instances. Once both a ready, the old is being terminated via EC2 API. At this stage the kube-controller-manager _sometimes_ manages to create a replacement daemonset pod once the old node is being deleted. This results in a pod targeting a no longer existing node via `affinity` as illustrated in the example above. For some reason, when the `kube-scheduler` is crashing the `kube-controller-manager` doesn't delete the extra/invalid daemonset pod. Not sure if this is another issue or it has always happened in our setup, but only v1.30 makes `kube-scheduler` crash which causes an actual issue.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
We run a custom Kubernetes setup on AWS EC2 (not EKS).
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux ip-10-149-91-92 6.5.0-1020-aws #20~22.04.1-Ubuntu SMP Wed May  1 16:38:06 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Custom tooling: https://github.com/zalando-incubator/kubernetes-on-aws
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该 Issue 存在潜在的安全风险。

**原因：**

攻击者如果具备创建 Pod 的权限，可以通过构造特殊的 Pod 定义（在 `nodeAffinity` 中指定不存在的节点），触发 kube-scheduler 中的整数除零错误，导致 kube-scheduler 崩溃。这会导致整个 Kubernetes 集群的新 Pod 无法被调度，造成拒绝服务（DoS）攻击。

**根据 CVSS 3.1 评分标准进行评估：**

- **攻击向量（AV）**：网络（N），值为 0.85（攻击者可以通过网络接口远程发起攻击）
- **攻击复杂度（AC）**：低（L），值为 0.77（攻击不需要特殊条件，易于实施）
- **权限要求（PR）**：低（L），值为 0.68（需要具备在集群中创建 Pod 的权限）
- **用户交互（UI）**：无（N），值为 0.85（不需要其他用户的交互）
- **作用范围（S）**：已改变（C）（攻击者的操作影响到了超过其权限范围的组件）
- **机密性影响（C）**：无（N），值为 0.00（不涉及机密性信息泄露）
- **完整性影响（I）**：无（N），值为 0.00（不涉及数据篡改）
- **可用性影响（A）**：高（H），值为 0.56（导致 kube-scheduler 崩溃，影响集群可用性）

**CVSS 计算结果：**

- **基本评分 (Base Score)**：7.1（高）
- **向量字符串**：CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H

**可能的影响：**

- **拒绝服务攻击**：攻击者可以导致 kube-scheduler 持续崩溃，阻止新 Pod 的调度，影响集群的正常运行。
- **影响范围广泛**：由于 kube-scheduler 是集群关键组件，其崩溃会影响整个集群，而非单一命名空间或节点。

**Proof of Concept：**

攻击者可以创建以下恶意 Pod 来触发漏洞：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: malicious-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchFields:
              - key: metadata.name
                operator: In
                values:
                  - non-existent-node # 指定不存在的节点
  containers:
    - name: exploit
      image: alpine
      command: ["sleep", "3600"]
```

**说明：**

- `nodeAffinity` 中引用了一个不存在的节点 `non-existent-node`。
- 当 kube-scheduler 处理该 Pod 时，由于未找到匹配的节点列表，导致 `len(nodes)` 为 0。
- 在后续计算中发生除以 `len(nodes)` 的操作，触发整数除零错误，导致 kube-scheduler 崩溃。

**结论：**

该问题允许具有创建 Pod 权限的攻击者通过发送特制请求，导致 kube-scheduler 崩溃，影响整个集群的可用性，属于高危安全漏洞，应及时修复和处理。

---

## Issue #125012 kubelet on Windows fails if a pod has SecurityContext with RunAsUser.

- Issue 链接：[#125012](https://github.com/kubernetes/kubernetes/issues/125012)

### Issue 内容

#### What happened?

Install a cluster with v1.29.4 with linux node as master and worker node as windows
kubeadm upgrade apply v1.30.0 fails with

`[ERROR CreateJob]: Job \"upgrade-health-check-4rxpv\" in the namespace \"kube-system\" did not complete in 15s: no condition of type Complete [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors`

Describe Pod events:

`mountvolume.setup failed for volume "kube-api-access-rncjh" : chown c:\var\lib\kubelet\pods\c5a3db55-4ee9-4b99-8fd6-17da5799f34a\volumes\kubernetes.io~projected\kube-api-access-rncjh\..2024_05_08_20_28_34.3908400707\token: not supported by windows`

It seems that previously the pod was Pending as well, but this was ignored, because the jobs was successfully deleted in defer and return value was overridden with nil.

```
defer func() {
    lastError = deleteHealthCheckJob(client, ns, jobName)
}()
```

https://github.com/kubernetes/kubernetes/blob/v1.29.1/cmd/kubeadm/app/phases/upgrade/health.go#L151



#### What did you expect to happen?

Upgrade health check job expected to be scheduled only on Linux nodes using node selectors, rather than on Windows nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

See **What happened?**

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kubeadm version (use kubeadm version): v1.30.0

Environment:

Kubernetes version (use kubectl version): v1.30.0
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): Ubuntu 22.04.2 LTS, Windows Server 2022 Datacenter
Kernel (e.g. uname -a): 5.19.0-1025-aws
Container networking plugin (CNI): calico

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124915 Ignore and potentially prevent reporting container status for not-existing containers

- Issue 链接：[#124915](https://github.com/kubernetes/kubernetes/issues/124915)

### Issue 内容

#### What happened?

Some third party controllers may report the Container Status for containers that are not defined in a pod spec. This may lead to inconsistencies in codebase and ideally needs to be blocked.

We see this with the https://github.com/admiraltyio/admiralty/pull/206, but there may be more examples like this since k8s never checked for consistency of statuses to specs.

#### What did you expect to happen?

As mentioned: https://github.com/kubernetes/kubernetes/pull/124906#pullrequestreview-2061200945 usages of container statuses needs to be reviewed and in most places we should start ignoring statuses for non-existing containers.

#### How can we reproduce it (as minimally and precisely as possible)?

Update the Pod Status with the container status for the container that doesn't exist:

```json
"apiVersion": "v1",
"kind": "Pod",
"metadata": {
            .....
},
"spec": {
  "containers": [],
  "initContainers": [
    {
      ....
      "name": "tester",
      ....
    },
    {
        ....
      "name": "init-proxy",
      ....
    }
  ],
  "nodeName": "foo",
},
"status": {
  "containerStatuses": [
    ....
  ],
  "initContainerStatuses": [
    {
      ....
      "imageID": "",
      "lastState": {},
      "name": "not-existing-container",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    },
    {
        ......
      "imageID": "",
      "lastState": {},
      "name": "tester",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    },
    {
      ....
      "imageID": "",
      "lastState": {},
      "name": "init-proxy",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    }
  ],
  "phase": "Pending",
  "qosClass": "Burstable",
  "startTime": "2024-05-07T23:55:23Z"
}
```

#### Anything else we need to know?

/sig node

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Any
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

不涉及

---

## Issue #124903 One Node all pods got crashloopbackoff

- Issue 链接：[#124903](https://github.com/kubernetes/kubernetes/issues/124903)

### Issue 内容

#### What happened?

Hello 
I  have installed three nodes on my k8s 
one is master node and two nodes are slave node
but one slave node pods works normally but one slave node pods got crashloopback error continously

#### What did you expect to happen?

I want to run all pods norml

#### How can we reproduce it (as minimally and precisely as possible)?

I have installed self kubernetes on my ubuntu servers. if I add new server to it, it doesn't work 

#### Anything else we need to know?

this is just the log from master node 
![image](https://github.com/kubernetes/kubernetes/assets/165806253/2812f5d5-d27c-4462-a522-104d1ab718f9)
this is kubelet log from error node 
![image](https://github.com/kubernetes/kubernetes/assets/165806253/ba1ff7ce-df5f-4219-a526-d8b3da86c717)
this is containerd log from error node
![image](https://github.com/kubernetes/kubernetes/assets/165806253/b4936d4b-e155-405b-9b7c-c009adc1b14b)



#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.28.9



#### Cloud provider

self hosting

#### OS version

ubuntu 22.0.4


#### Install tools

docker 
container
kubectl 
helm 


#### Container runtime (CRI) and version (if applicable)

containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

flannel
![image](https://github.com/kubernetes/kubernetes/assets/165806253/db458c22-8200-4093-8a0d-f94d206d78c2)

```


### 分析结果

不涉及。

---

## Issue #124900 1.30 tag also breaks PodIP.IP (which should be marked required)

- Issue 链接：[#124900](https://github.com/kubernetes/kubernetes/issues/124900)

### Issue 内容

#### What happened?

We have a CRD that contains PodStatus.  After generating CRD with k8s v1.30.1, applying it failed with the message:

> spec.validation.openAPIV3Schema.properties[status].properties[podIPs].items.properties[ip].default: Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property

Similar issue reported in https://github.com/kubernetes/kubernetes/issues/124540 but different struct field.

#### What did you expect to happen?

CRD is applied successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Create CRD that has PodStatus and then apply it.

#### Anything else we need to know?

This is a leftover of https://github.com/kubernetes/kubernetes/pull/124553
PodIP.IP should also be marked required because it is +listMapKey=ip in PodStatus.PodIPs.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124879  Linux 6.6 EEVDF scheduler on Kubernetes: openat2 /sys/fs/cgroup/kubepods.slice/cpu.weight: no such file or directory

- Issue 链接：[#124879](https://github.com/kubernetes/kubernetes/issues/124879)

### Issue 内容

#### What happened?

Linux 6.6 comes with a new job scheduler called EEVDF (Earliest Eligible Virtual Deadline First), replacing the old CFS.

The kubelet/containerd failed to create cgroup

#### What did you expect to happen?

Kubelet/containerd is compatible with linux EEVDF scheduler

#### How can we reproduce it (as minimally and precisely as possible)?

Run kubernetes on Ubuntu 24.04 LTS

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.28.3-aliyun.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9-aliyun.1
```

</details>


#### Cloud provider

<details>
Aliyun
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124873 Status manager does not normalize ephemeral container statuses

- Issue 链接：[#124873](https://github.com/kubernetes/kubernetes/issues/124873)

### Issue 内容

#### What happened?

`EphemeralContainerStatuses` is not normalized in 	`normalizeStatus()` in the status manager:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L1026-L1043

This does not seem to cause any user facing problem because `EphemeralContainerStatuses` is sorted before passed to the status manager and timestamps are normalized at marshaling to create a patch:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/kubelet_pods.go#L2336

https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L873

However, an unexpected behavior is caused internally because timestamps are not normalized. If the log verbosity is 3 or larger, the following messages are logged periodically in kubelet when an ephemeral container exists:

```
May 14 20:10:33 kind-control-plane kubelet[720]: I0514 20:10:33.437155 	720 status_manager.go:230] "Syncing all statuses"
May 14 20:10:33 kind-control-plane kubelet[720]: I0514 20:10:33.438512 	720 status_manager.go:984] "Pod status is inconsistent with cached status for pod, a reconciliation should be triggered" pod="default/ephemeral-demo" statusDiff=<
May 14 20:10:33 kind-control-plane kubelet[720]:     	  &v1.PodStatus{
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	... // 8 identical fields
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	PodIPs:            	{{IP: "10.244.0.5"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	StartTime:         	s"2024-05-14 20:09:21 +0000 UTC",
May 14 20:10:33 kind-control-plane kubelet[720]:     	-     	InitContainerStatuses: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	+     	InitContainerStatuses: []v1.ContainerStatus{},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	ContainerStatuses: 	{{Name: "ephemeral-demo", State: {Running: &{StartedAt: {Time: s"2024-05-14 20:09:23 +0000 UTC"}}}, Ready: true, Image: "registry.k8s.io/pause:3.1", ...}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	QOSClass:          	"BestEffort",
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	EphemeralContainerStatuses: []v1.ContainerStatus{
May 14 20:10:33 kind-control-plane kubelet[720]:     	              	{
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	Name: "debugger-qk87g",
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	State: v1.ContainerState{
May 14 20:10:33 kind-control-plane kubelet[720]:     	                              	Waiting:	nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	-                             	Running:	&v1.ContainerStateRunning{StartedAt: v1.Time{Time: s"2024-05-14 20:09:35 +0000 UTC"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	+                             	Running:	&v1.ContainerStateRunning{StartedAt: v1.Time{Time: s"2024-05-14 20:09:35.697513091 +0000 UTC"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                              	Terminated: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	LastTerminationState: {},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	Ready:            	false,
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	... // 7 identical fields
May 14 20:10:33 kind-control-plane kubelet[720]:     	              	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	Resize:            	"",
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	ResourceClaimStatuses: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	  }
May 14 20:10:33 kind-control-plane kubelet[720]:  >

```

This message is emitted here:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L984-L988

After `needsReconcile()` returns `true`, `syncPod()` is called. This does not look so harmful because `unchaged` gets `true` eventually:

https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L873-L881

API is called once unnecessarily in `syncPod()`, though:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L843


#### What did you expect to happen?

It might be better to normalize ephemeral container statuses, at least timestamps, in the status manager to avoid this unexpected behavior.


#### How can we reproduce it (as minimally and precisely as possible)?

Set the log verbosity of kubelet to 3 or larger. Then, create an ephemeral container like [this](https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container-example).

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124871 kubelet crash loop panic with SIGSEGV

- Issue 链接：[#124871](https://github.com/kubernetes/kubernetes/issues/124871)

### Issue 内容

#### What happened?

I was running some pods on my node as usual.
At some point (23:41:35.127989 in the log), the kubelet crashed and then went into a crash loop.

Log showing the first crash and several thereafter: [kubelet-crash.txt](https://github.com/kubernetes/kubernetes/files/15312195/kubelet-crash.txt)


The errors I see shortly before the crash are:
```
projected.go:292] Couldn't get configMap default/kube-root-ca.crt: object "default"/"kube-root-ca.crt" not registered
projected.go:198] Error preparing data for projected volume kube-api-access-lwp5v for pod default/run-fluid-wtw5n-99kwf: object "default"/"kube-root-ca.crt" not registered
nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/e6cfc866-a5b2-4e16-9b83-884de8552d45-kube-api-access-lwp5v podName:e6cfc866-a5b2-4e16-9b83-884de8552d45 nodeName:}" failed. No retries permitted until 2024-05-13 23:41:38.8756138 +0000 UTC m=+9176.959811707 (durationBeforeRet
ry 2m2s). Error: MountVolume.SetUp failed for volume "kube-api-access-lwp5v" (UniqueName: "kubernetes.io/projected/e6cfc866-a5b2-4e16-9b83-884de8552d45-kube-api-access-lwp5v") pod "run-fluid-wtw5n-99kwf" (UID: "e6cfc866-a5b2-4e16-9b83-884de8552d45") : object "default"/"kube-root-ca.crt" not registered
```

Stack trace from the first crash (later stack traces are a bit different):
```
goroutine 401 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x3d005c0?, 0x6d89410})
        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x480e338?})
        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x75
panic({0x3d005c0, 0x6d89410})
        /usr/local/go/src/runtime/panic.go:884 +0x213
errors.As({0x12, 0x0}, {0x3eac4c0?, 0xc0014e6590?})
        /usr/local/go/src/errors/wrap.go:109 +0x215
k8s.io/apimachinery/pkg/util/net.IsConnectionReset(...)
        vendor/k8s.io/apimachinery/pkg/util/net/util.go:45
k8s.io/client-go/rest.(*Request).request.func2(0x6d2e8a0?, {0x12, 0x0})
        vendor/k8s.io/client-go/rest/request.go:1007 +0x79
k8s.io/client-go/rest.IsRetryableErrorFunc.IsErrorRetryable(...)
        vendor/k8s.io/client-go/rest/with_retry.go:43
k8s.io/client-go/rest.(*withRetry).IsNextRetry(0xc001587f40, {0x0?, 0x0?}, 0x0?, 0xc001ef5f00, 0xc001bb1560, {0x12, 0x0}, 0x480e328)
        vendor/k8s.io/client-go/rest/with_retry.go:169 +0x170
k8s.io/client-go/rest.(*Request).request.func3(0xc001bb1560, 0xc002913af8, {0x4c44840?, 0xc001587f40?}, 0x0?, 0x0?, 0x39efc40?, {0x12?, 0x0?}, 0x480e328)
        vendor/k8s.io/client-go/rest/request.go:1042 +0xba
k8s.io/client-go/rest.(*Request).request(0xc00199f200, {0x4c42e00, 0xc00227b0e0}, 0x2?)
        vendor/k8s.io/client-go/rest/request.go:1048 +0x4e5
k8s.io/client-go/rest.(*Request).Do(0xc00199f200, {0x4c42dc8, 0xc000196010})
        vendor/k8s.io/client-go/rest/request.go:1063 +0xc9
k8s.io/client-go/kubernetes/typed/core/v1.(*nodes).Get(0xc001db6740, {0x4c42dc8, 0xc000196010}, {0x7ffe6a355c42, 0x1d}, {{{0x0, 0x0}, {0x0, 0x0}}, {0x4c03920, ...}})
        vendor/k8s.io/client-go/kubernetes/typed/core/v1/node.go:77 +0x145
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).tryUpdateNodeStatus(0xc000289400, {0x4c42dc8, 0xc000196010}, 0x44ead4?)
        pkg/kubelet/kubelet_node_status.go:561 +0xf6
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).updateNodeStatus(0xc000289400, {0x4c42dc8, 0xc000196010})
        pkg/kubelet/kubelet_node_status.go:536 +0xfc
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncNodeStatus(0xc000289400)
        pkg/kubelet/kubelet_node_status.go:526 +0x105
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc002913f28?)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x4c19ae0, 0xc0008f2000}, 0x1, 0xc000180360)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x2540be400, 0x3fa47ae147ae147b, 0x0?, 0x0?)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x89
created by k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run
        pkg/kubelet/kubelet.go:1606 +0x58a
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x1a pc=0x47a935]
```

#### What did you expect to happen?

Ideally no crash, but at least a useful error message.

#### How can we reproduce it (as minimally and precisely as possible)?

Really not sure.

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.6

#### Cloud provider

Bare metal.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

kubespray

#### Container runtime (CRI) and version (if applicable)

containerd

containerd github.com/containerd/containerd v1.7.13 7c3aca7a610df76212171d200ca3811ff6096eb8

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI = calico v3.26.4


### 分析结果

不涉及

---

## Issue #124868 image-gc-high-threshold should be lower than value causing hard eviction nodefs.available or imagefs.available

- Issue 链接：[#124868](https://github.com/kubernetes/kubernetes/issues/124868)

### Issue 内容

#### What happened?

when disk fills, it hits the hard eviction threshold, causing node disk pressure in the same moment imagegc spots it should prune something and start acting. this casues node going into disk pressure and evicting pods and not just start imagegc soon enough

#### What did you expect to happen?

i expect imagegc spots the filling disk soon enough, to start garbage collection and empty disk before node hits disk pressure.

from current documentation:

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/

```
--image-gc-high-threshold int32     Default: 85
--image-gc-low-threshold int32     Default: 80
```

and

https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
```
    "imageGCHighThresholdPercent": 85,
    "imageGCLowThresholdPercent": 80,
```

https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#hard-eviction-thresholds

vs.

```
nodefs.available<10%
imagefs.available<15%
```

... this does not make sense having the same percentage for imagefs.available (100-15 = 85 :-)

better approach would be having default values a little bit shifted with eqivalent to setting

```
  - "--image-gc-high-threshold=80"  
  - "--image-gc-low-threshold=75"   
```

... after setting this, i almost never get node disk pressure, because garbage collection and pruning disk happens soon enough


#### How can we reproduce it (as minimally and precisely as possible)?

fill disk, spot node disk pressure state at the same moment it starts garbage collecting.

#### Anything else we need to know?

_No response_

#### Kubernetes version

this is not version specific as checked documentation at the moment, its for years the same.

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

none - kubeadm installation
<details>

</details>


#### OS version

not relevant

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124839 Kubelet crashes on concurrent map iteration and map write

- Issue 链接：[#124839](https://github.com/kubernetes/kubernetes/issues/124839)

### Issue 内容

#### What happened?

When running e2e, the following fatal error can be encountered:

```
  30715 I0513 08:25:43.792330  285839 reflector.go:289] Starting reflector *v1.ConfigMap (0s) from object-"flexvolume-445 9"/"kube-root-ca.crt"
  30716 I0513 08:25:43.797355  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/attachable-with-long-mount-flexvolume-4459"
  30717 I0513 08:25:43.797400  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/dummy-attachable"
  30718 I0513 08:25:43.851107  285839 reconciler.go:352] "attacherDetacher.AttachVolume started" volumeName="k8s/attachable-with-long-mount-flexvolume-4459/test-long-detach-flex" nodeName="master" scheduledPods=["        flexvolume-4459/flexvolume-detach-test-client"]
  30719 I0513 08:25:43.854533  285839 operation_generator.go:400] AttachVolume.Attach succeeded for volume "test-long-detach-flex" (UniqueName: "k8s/attachable-with-long-mount-flexvolume-4459/test-long-detach-flex        ") from node "master"
  30720 I0513 08:25:43.854661  285839 event.go:307] "Event occurred" object="flexvolume-4459/flexvolume-detach-test-client" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SuccessfulAttachVolume" mes        sage="AttachVolume.Attach succeeded for volume \"test-long-detach-flex\" "
  30721 I0513 08:25:43.864876  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/attachable-with-long-mount-flexvolume-4459"
  30722 I0513 08:25:43.866879  285839 desired_state_of_world.go:351] "volume add to dsw" volume="test-long-detach-flex" podName="ee90c357-99a0-467c-b829-1525c220da21"
  30723 I0513 08:25:43.866933  285839 desired_state_of_world.go:351] "volume add to dsw" volume="kube-api-access-4525n" podName="ee90c357-99a0-467c-b829-1525c220da21"
  30724 I0513 08:25:43.971490  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/dummy-attachable"
  30725 fatal error: concurrent map iteration and map write
  30726 
  30727 goroutine 4078 [running]:
  30728 k8s.io/kubernetes/pkg/volume.(*VolumePluginMgr).FindPluginBySpec(0xc00b0bc008, 0xc00b944168)
  30729         pkg/volume/plugins.go:676 +0x37f
  30730 k8s.io/kubernetes/pkg/volume/util/operationexecutor.(*operationGenerator).GenerateMountVolumeFunc(0xc00b08ef50, 0x8bb2c97000, {{0xc0150614a0, 0x4c}, {0xc0135d7a70, 0x24}, 0xc00b944168, {0xc00c9bfac0, 0xf},         0xc015846488, ...}, ...)
```
The related code is as follows:
Added read lock on line 634:
https://github.com/kubernetes/kubernetes/blob/a07d3c49b301acd3426e985c3ad54a56fdb5b925/pkg/volume/plugins.go#L633-L666
delete map on line 714:
https://github.com/kubernetes/kubernetes/blob/a07d3c49b301acd3426e985c3ad54a56fdb5b925/pkg/volume/plugins.go#L694-L720





#### What did you expect to happen?

What's a good way for the community to solve this problem?

#### How can we reproduce it (as minimally and precisely as possible)?

None at present

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.28.1
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
false
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124797 app Container can't reuse its init Container cpuset in a specific condition

- Issue 链接：[#124797](https://github.com/kubernetes/kubernetes/issues/124797)

### Issue 内容

#### What happened?

We can't make sure app Container always reuses init Container cpuset, which may lead to the waste of CPU(init container has already exited but the cpuset can not be reused by other containers) and `not enough cpus available to satis
fy request` error.

<img width="1421" alt="image" src="https://github.com/kubernetes/kubernetes/assets/14137033/1135377d-dde3-48d5-84aa-c940f9bf4992">


#### What did you expect to happen?

app Container always reuses init Container cpuset after init Container exits.

#### How can we reproduce it (as minimally and precisely as possible)?

This is one of the spefic condition that might cause the issue:
- Pod A ready to allocate cpuset with `init container` and `app container` both request 92 cpu.
- Pod B already running on the node and ready to be deleted.

1. Pod A's init container starts to allocate cpuset（4-24,48-60,73-84,100-120,144-156,169-180）：
```
I0510 16:40:21.232949   20266 state_mem.go:80] "Updated desired CPUSet" podUID="2f9922ce-df66-4b58-abd8-01187b813318" containerName="init-container" cpuSet="4-24,48-60,73-84,100-120,144-156,169-180"
```
2. Pod A's init container exits.
3. Before Pod A's app container starts to allocate cpuset, Pod B gets deleted and release it's cpuset（cpuSet="0-3,25-47,61-72,85-99,121-143,157-168,181-191）：
```
I0510 16:40:27.759335   20266 state_mem.go:107] "Deleted CPUSet assignment" podUID="74510e24-48ba-4fd7-ab85-80dd99c6df5d" containerName="deleted-container"
I0510 16:40:27.759714   20266 state_mem.go:88] "Updated default CPUSet" cpuSet="0-3,25-47,61-72,85-99,121-143,157-168,181-191"
```
4. Pod A's app container starts to allocate cpuset. 
What we expect is that Pod A's app container reuses its init container's cpuset.
But due to Pod B's deletion, it won't allocate the same cpuset as its init container.（4-49,100-145）：
```
 I0510 16:40:27.989453   20266 state_mem.go:80] "Updated desired CPUSet" podUID="2f9922ce-df66-4b58-abd8-01187b813318" containerName="app-container" cpuSet="4-49,100-145"
```

Now we have Pod A's init container taking cpuset: 4-24,48-60,73-84,100-120,144-156,169-180
And  Pod A's app container  taking cpuset: 4-49,100-145
The init container cpuset won't be reused as expected.

A new Pod C starts to allocate cpuset, it may get `not enough cpus available to satis
fy request` error

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.30
```

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124796 bug: kubelet panic & crash if `--config-dir` is used

- Issue 链接：[#124796](https://github.com/kubernetes/kubernetes/issues/124796)

### Issue 内容

#### What happened?

I created a v1.30.0 k8s cluster with kubeadm and created a  drop-in directory for kubelet configuration under `/etc/kubernetes/kubelet.conf.d`. The normal conf file created by kubeadm is in ` /var/lib/kubelet/config.yaml`

According to the [docs](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/#kubelet-conf-d) the kubelet should merge all configs in specified order and start. But the kubelet crashes under two setups:

**Setup 1**:

The `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is completely empty the kubelet crashes with:
```
>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
E0510 13:47:28.749138    9050 run.go:74] "command failed" err="failed to merge kubelet configs: failed to walk through kubelet dropin directory \"/etc/kubernetes/kubelet.conf.d\": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: kubelet config file \"/etc/kubernetes/kubelet.conf.d/20-kubelet.conf\" was empty"
```
If there is just one space or an newline in that file it crashes with:
```
>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
E0510 13:49:26.243517    9062 run.go:74] "command failed" err=<
        failed to merge kubelet configs: failed to walk through kubelet dropin directory "/etc/kubernetes/kubelet.conf.d": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: Object 'Kind' is missing in '
        '
```
Even when the docs say:

> These files may contain partial configurations and might not be valid config files by themselves. Validation is only performed on the final resulting configuration structure stored internally in the kubelet. 

If there is nothing or only whitspaces in  `/etc/kubernetes/kubelet.conf.d` the merged config should not be invalid and not every file in `/etc/kubernetes/kubelet.conf.d` must contain a `kind` property.

**Setup 2**:
The `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is the same as the `/var/lib/kubelet/config.yaml` the kubelet panics and crashes with:
```
> kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
panic: non-positive interval for NewTicker

goroutine 1 [running]:
time.NewTicker(0xc00098f7a0?)
        time/tick.go:22 +0xe5
k8s.io/klog/v2/internal/clock.RealClock.NewTicker(...)
        k8s.io/klog/v2@v2.120.1/internal/clock/clock.go:111
k8s.io/klog/v2.(*flushDaemon).run(0xc000229650, 0x0)
        k8s.io/klog/v2@v2.120.1/klog.go:1169 +0x10f
k8s.io/klog/v2.StartFlushDaemon(0x0)
        k8s.io/klog/v2@v2.120.1/klog.go:1220 +0x36
k8s.io/component-base/logs/api/v1.apply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130})
        k8s.io/component-base/logs/api/v1/options.go:285 +0x890
k8s.io/component-base/logs/api/v1.validateAndApply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130}, 0x4?)
        k8s.io/component-base/logs/api/v1/options.go:132 +0x71
k8s.io/component-base/logs/api/v1.ValidateAndApplyAsField(...)
        k8s.io/component-base/logs/api/v1/options.go:124
k8s.io/kubernetes/cmd/kubelet/app.NewKubeletCommand.func1(0xc0000d8908, {0xc000100060, 0x4, 0x4})
        k8s.io/kubernetes/cmd/kubelet/app/server.go:238 +0x4bc
github.com/spf13/cobra.(*Command).execute(0xc0000d8908, {0xc000100060, 0x4, 0x4})
        github.com/spf13/cobra@v1.7.0/command.go:940 +0x882
github.com/spf13/cobra.(*Command).ExecuteC(0xc0000d8908)
        github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5
github.com/spf13/cobra.(*Command).Execute(...)
        github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/component-base/cli.run(0xc0000d8908)
        k8s.io/component-base/cli/run.go:146 +0x290
k8s.io/component-base/cli.Run(0xc0000061c0?)
        k8s.io/component-base/cli/run.go:46 +0x17
main.main()
        k8s.io/kubernetes/cmd/kubelet/kubelet.go:36 +0x18
```
Using the same config only via `--config` works.

#### What did you expect to happen?

The kubelet should start with empty files in `/etc/kubernetes/kubelet.conf.d` or if the files in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` are the same.

If this is not the expected usage, the docs have to be adjusted. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create a cluster with kubeadm v1.30 with kublet config in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` or setup kubelet by hand. 

```yaml
# kubeadm config
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration

nodeRegistration:
  kubeletExtraArgs:
    config: /var/lib/kubelet/config.yaml
    config-dir: /etc/kubernetes/kubelet.conf.d
``` 


#### Anything else we need to know?

The kublet conf api version is `apiVersion: kubelet.config.k8s.io/v1beta1` and is was with and without setting `KUBELET_CONFIG_DROPIN_DIR_ALPHA`

Functionality was introduced in #119390

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-04-17T17:34:08Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
Hetzner/None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux controlplane-node-amd64-vcxkzu 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd v1.7.16
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
not relevant
</details>


### 分析结果

不涉及

---

## Issue #124880 Kubectl auth can-i doesn't know about the `approve` verb

- Issue 链接：[#124880](https://github.com/kubernetes/kubernetes/issues/124880)

### Issue 内容

**What happened**:

If you use `kubectl auth can-i` to test whether a user can approve certificate signing requests, you get a response indicating that `approve` is not a valid verb

```bash
kubectl auth can-i approve certificatesigningrequests.certificates.k8s.io
Warning: resource 'certificatesigningrequests' is not namespace scoped in group 'certificates.k8s.io'

Warning: verb 'approve' is not a known verb
```

An example of the `approve` verb being used can be seen [here](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection-kubectl)

**What you expected to happen**:

`approve` should be recognized as a valid verb on `certificatesigningrequest` objects.


**How to reproduce it (as minimally and precisely as possible)**:
Run

```bash
kubectl auth can-i approve certificatesigningrequests.certificates.k8s.io
```

**Anything else we need to know?**:

I think that `approve` needs to be added to the kubectl auth can-i code [here](https://github.com/kubernetes/kubectl/blob/514f46729f82412dd9cc41f206058bc4ae9b62b0/pkg/cmd/auth/cani.go#L106)

**Environment**:
- Kubernetes client and server versions (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`):



### 分析结果

不涉及。

---

## Issue #124786 InPlacePodVerticalScaling does not meet the requirement of qosClass being equal to Guaranteed after shrinking the memory

- Issue 链接：[#124786](https://github.com/kubernetes/kubernetes/issues/124786)

### Issue 内容

#### What happened?

InPlacePodVerticalScaling does not meet the requirement of qosClass being equal to Guaranteed after shrinking the memory
![image](https://github.com/kubernetes/kubernetes/assets/13641341/0c5d19fb-b124-4cbf-a520-b574d3fe0656)


#### What did you expect to happen?

InPlacePodVerticalScaling maintains the same qosclass type of Pod before and after scaling

#### How can we reproduce it (as minimally and precisely as possible)?

After enabling the InPlacePodVerticalScaling feature, the patch modifies the request and limit of the container's resource to a value smaller than usage

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.1", GitCommit:"3ddd0f45aa91e2f30c70734b175631bec5b5825a", GitTreeState:"clean", BuildDate:"2022-05-24T12:17:11Z", GoVersion:"go1.18.3", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T22:24:00Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124781 kubectl cp a file from a Windows Pod to Windows 11 localhost failed

- Issue 链接：[#124781](https://github.com/kubernetes/kubernetes/issues/124781)

### Issue 内容

#### What happened?

I was trying to cp a from a Windows Pod to the Windows localhost. But it failed.


```
kubectl.exe cp --kubeconfig .\kubconfig -n namespace-win pod-20240509-120334-779546rrfsc:C:\\data\\0509-1065.log 0509-1065.log
```

It showed

```
tar: Removing leading drive letter from member names
error: tar contents corrupted
```

#### What did you expect to happen?

I want to copy the file to the localhost.

#### How can we reproduce it (as minimally and precisely as possible)?

Windows 11 localhost
Windows Pod.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$(powershell) kubectl version
# paste output here

Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Unable to connect to the server: dial tcp 127.0.0.1:6443: connectex: No connection could be made because the target machine actively refused it.
```
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here

BuildNumber  Caption                      OSArchitecture  Version
22621        Microsoft Windows 11 Professional  64-bit          10.0.22621
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124760 Open API v3 requirement for `PATCH` verb support (introduced PR #115119)

- Issue 链接：[#124760](https://github.com/kubernetes/kubernetes/issues/124760)

### Issue 内容

#### What happened?

Aggregated API backend:  Call to `CREATE` endpoint consults `PATCH` field validation properties, even if the resource does not support the `PATCH` verb..

#### What did you expect to happen?

Expected `CREATE` to be called with `CREATE` field validation without looking at the `PATCH` endpoint.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Open API v3 specification with a field validator.  Storage for API server (aggregated API server) advertises support only for `CREATE`, `GET`, `DELETE`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9-gke.1000000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

    architecture: amd64
    bootID: f993a6ef-ee9e-457b-856e-bfd51f9cdd22
    containerRuntimeVersion: containerd://1.7.13
    kernelVersion: 5.15.0-1054-gke
    kubeProxyVersion: v1.27.12-gke.1190000
    kubeletVersion: v1.27.12-gke.1190000
    machineID: 4da31cf9d84cf8d82e680abcb09130ae
    operatingSystem: linux
    osImage: Ubuntu 22.04.4 LTS
    systemUUID: 4da31cf9-d84c-f8d8-2e68-0abcb09130ae

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124749 Volumeattachment deletion in a detach operation should carry the resourceVersion

- Issue 链接：[#124749](https://github.com/kubernetes/kubernetes/issues/124749)

### Issue 内容

#### What happened?

There is a use case in the flow test that creates a pod which uses pvc and then waits about 2 minutes and then delete the pod. Later it was discovered that the pv referenced by the pod had been attached on the node and had not been detached.

Combining the csi plugin and k8s component logs, we found that the csi plugin took a long time to attach, and it was very late before it succeeded, and then it patched finalizers on the volumeattachment resource. At the same time, the deletion of the pod triggered the k8s detach operation, which will delete the The volumeattachment resource.

Due to the multiple instances of apiserver, when the volumeattachment delete operation reaches an apiserver, it does not realize that finalizers have been patched on the va resource, resulting in a successful delete operation that deletes the va.

csi plugin log:
```
opdisk_sts_attacher.log:I0319 16:58:00.344601       1 round_trippers.go:454] PATCH [https://123.123.0.1:443/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea](https://123.123.0.1/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea) 200 OK in 23 milliseconds

opdisk_sts_attacher.log:I0319 17:00:08.945137       1 csi_handler.go:275] Attached "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:00:08.945141       1 util.go:37] Marking as attached "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:00:08.949561       1 round_trippers.go:454] PATCH [https://123.123.0.1:443/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea/status](https://123.123.0.1/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea/status) 404 Not Found in 4 milliseconds

opdisk_sts_attacher.log:I0319 17:00:08.949646       1 csi_handler.go:236] Error processing "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea": failed to mark as attached: volumeattachments.storage.k8s.io "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea" not found

opdisk_sts_attacher.log:I0319 17:01:25.093522       1 controller.go:198] Started VA processing "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:01:25.093538       1 controller.go:205] VA "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea" deleted, ignoring
```

kube-controller-manager log:
```
kube-controller-manager.klog:I0319 16:58:00.333212      11 operation_generator.go:1665] Verified volume is safe to detach for volume "pvc-03cb8ebe-d2b5-410c-9db5-0a6b131d8f03" (UniqueName: "kubernetes.io/csi/opdisk.csi.openpalette.org^9df25125-5cb6-4965-9f88-bbceb277224a") on node "minion-0-0"

kube-controller-manager.klog:I0319 16:58:00.885712      11 operation_generator.go:526] DetachVolume.Detach succeeded for volume "pvc-03cb8ebe-d2b5-410c-9db5-0a6b131d8f03" (UniqueName: "kubernetes.io/csi/opdisk.csi.openpalette.org^9df25125-5cb6-4965-9f88-bbceb277224a") on node "minion-0-0"
```

#### What did you expect to happen?

Solve the problem of concurrent operations of finalizers patch and va deletion to ensure the safe deletion of va.

in pkg/volume/csi/csi_attacher.go

we can use c.plugin.volumeAttachmentLister.Get(attachID) to get va resourceVersion and pass to Delete function bellow to solve the problem.
```
c.k8s.StorageV1().VolumeAttachments().Delete(context.TODO(), attachID, metav1.DeleteOptions{Preconditions: &metav1.Preconditions{ResourceVersion: &resourceVersion}})
```

#### How can we reproduce it (as minimally and precisely as possible)?

Low probability of recurrence

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
v1.28.3
```

</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124745 HPAContainerMetrics Version V2Beta2 Cannot Implement on Kubernetes 1.30 as "V2Beta2" was remove since Kubernetes 1.26

- Issue 链接：[#124745](https://github.com/kubernetes/kubernetes/issues/124745)

### Issue 内容

#### What happened?

Base on HPA/v2beta2 was removed since Kubernetes 1.26 and current HPA/V2 is not support "unknown field "spec.containerMetrics", unknown field "spec.metrics[0].resource.container". So just want suggestion for implement HPAContainerMetrics on cluster.

<img width="1424" alt="328572042-781024d1-7647-43a7-8481-4503f0a9c404" src="https://github.com/kubernetes/kubernetes/assets/16981299/cef4406f-e717-4d74-88f3-69ce6319b5cc">


#### What did you expect to happen?

HPAContainerMetrics should working fine on Kubernetes 1.30 as GA feature gate

#### How can we reproduce it (as minimally and precisely as possible)?

Yes base on issue of syntact api version v2beta2 is not avaliable on Kubernetes 1.30 for create this hpa

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30.0

#### Cloud provider

AWS Cloud Controller

#### OS version

Ubuntu 22.04

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

Containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124734 CPUStats.UsageCoreNanoSeconds is inaccurate on windows - 1/10 of the real value.

- Issue 链接：[#124734](https://github.com/kubernetes/kubernetes/issues/124734)

### Issue 内容

#### What happened?

The CPU utilization metrics from Kubelet summary API  [`CPUStats.UsageCoreNanoSeconds`](https://github.com/kubernetes/kubernetes/blob/79470526896f8b6ca744c29109bc3455c1a9d199/staging/src/k8s.io/kubelet/pkg/apis/stats/v1alpha1/types.go#L224C2-L224C22) is not accurate on Windows. The value is only 1/10 of the real value which I have other VM level metrics as a proof.

## summary API CPU utilization metric

![image](https://github.com/kubernetes/kubernetes/assets/14968376/f54a5e13-f6f7-4c6f-afc5-8e9704397600)

## VM CPU utilization metric

![image](https://github.com/kubernetes/kubernetes/assets/14968376/ddc08d96-4ef3-4a98-bda5-328ad864ce2e)



#### What did you expect to happen?

The `CPUStats.UsageCoreNanoSeconds` should be accurate on Windows too.

#### How can we reproduce it (as minimally and precisely as possible)?

```
$ kubectl get --raw "/api/v1/nodes/<NODE>/proxy/stats/summary"
```

T1
```
  "cpu": {
   "time": "2023-12-06T21:23:48Z",
   "usageNanoCores": 8000000,
   "usageCoreNanoSeconds": 20985800000000
  },
```

T2
```
  "cpu": {
   "time": "2023-12-06T21:24:38Z",
   "usageNanoCores": 8000000,
   "usageCoreNanoSeconds": 20986320000000
  },
```

(20986320000000 - 20985800000000)ns / 50s = 0.01 s/s
This matches the magnitude of the metric chart.

#### Anything else we need to know?

This problem doesn't exist on Linux.

Call stack
- metric is set here by kubelet - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/winstats.go#L118-L129
- the data collected by using windows perf counters - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/perfcounter_nodestats.go#L206-L212
- query here - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/perfcounters.go#L32

#### Kubernetes version

<details>

```console
$ kubectl version
1.27
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124733 Confusing doc on name of object of kind defined by CRD

- Issue 链接：[#124733](https://github.com/kubernetes/kubernetes/issues/124733)

### Issue 内容

#### What happened?

I wondered what are the restrictions (if any) that Kubernetes imposes on the names of objects of a kind that gets defined by a CustomResourceDefinition. I am not talking about the name of the CRD itself. I tried to find this in the documentation, and couldn't. I asked on the `sig-api-machinery` channel on the Kubernetes Slack, and was referred to https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions . The text there says

> The name of a CRD object must be a valid [DNS subdomain name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

Since a CRD is itself an object, I thought this was clearly talking about the name of the CRD itself (I temporarily forgot that I know that the name of the CRD itself is constrained to be `${resouce}.${apiGroup}`). But I was assured that this text is trying to talk about the objects whose kind is defined by the CRD.

#### What did you expect to happen?

The Kubernetes documentation clearly answers my question.

#### How can we reproduce it (as minimally and precisely as possible)?

Explained above.

#### Anything else we need to know?

No.

#### Kubernetes version

<details>

This doc has been this way for a long time.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

不涉及。

---

## Issue #124716 Kubelet memory leak when a plugin is registered twice

- Issue 链接：[#124716](https://github.com/kubernetes/kubernetes/issues/124716)

### Issue 内容

#### What happened?

We found that the kubelet memory kept increasing，and we exported the pprof of the goroutine. The grpc goroutine leaks, causing memory leakage.
![image](https://github.com/kubernetes/kubernetes/assets/54977497/76a1d622-2129-49e0-bd4c-79b6bad786b1)
We found out that the reason was because one of the pluginThe following code causes this situation. One client is lost.s kept registering twice and using the same name for both.
https://github.com/kubernetes/kubernetes/blob/1dc30bf90fd6a729d226b4e942118110b0a73e65/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L90-L96
When two requests are registered at the same time, only one client is reserved in s.clients.
https://github.com/kubernetes/kubernetes/blob/1dc30bf90fd6a729d226b4e942118110b0a73e65/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L106-L117
Therefore, after the c.Run () method in the runClient method is executed, s.getClient obtains only one registered client. As a result, the c.grpc.Close () method is not invoked, causing memory and coroutine leakage.


#### What did you expect to happen?

Even in this case, kubelet should not leak memory.

#### How can we reproduce it (as minimally and precisely as possible)?

1、The plug-in is registered every 5 seconds and two registration requests are sent at the same time.
2、The kubelet memory usage keeps increasing.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.28
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124709 Throughput degradation scheduling daemonset pods

- Issue 链接：[#124709](https://github.com/kubernetes/kubernetes/issues/124709)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/pull/119779 added some map queries and creations that add non-negligible latency.

The pprof reveals the following lines inside `findNodesThatFitPod` as too expensive:
- 22.2% https://github.com/kubernetes/kubernetes/blob/44bd04c0cbddde69aaeb7a90d3bd3de4e417f27f/pkg/scheduler/schedule_one.go#L492
- 21.7% https://github.com/kubernetes/kubernetes/blob/44bd04c0cbddde69aaeb7a90d3bd3de4e417f27f/pkg/scheduler/schedule_one.go#L489

![image](https://github.com/kubernetes/kubernetes/assets/1299064/d82a658e-ae01-4bcd-9e0d-6ab892bee6c2)


#### What did you expect to happen?

Scheduler to keep a throughput of 300 pods/s

#### How can we reproduce it (as minimally and precisely as possible)?

Schedule 5k daemonset pods giving 300 qps to the scheduler

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.30
</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124704 Pod phase transition is slower when EventedPLEG is enabled

- Issue 链接：[#124704](https://github.com/kubernetes/kubernetes/issues/124704)

### Issue 内容

#### What happened?
After #124297, which fixes timestamps set by Evented PLEG, was merged, a pod phase transition is slower especially at deletion when the EventedPLEG is enabled.

EventedPLEG is enabled:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
pod/simple-pod created
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	3s
pod/simple-pod condition met
simple-pod   1/1 	Terminating     	0      	4s
pod "simple-pod" deleted
simple-pod   0/1 	Terminating     	0      	13s
simple-pod   0/1 	Terminating     	0      	18s
simple-pod   0/1 	Terminating     	0      	18s
simple-pod   0/1 	Terminating     	0      	18s
```

EventedPLEG is disabled:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
pod/simple-pod created
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
```

As described here, a pod worker is blocked at `cache.GetNewerThan()` when it is woken up by an update without a cache update in a PLEG:
https://github.com/kubernetes/kubernetes/blob/ade0d2140a68a69f5343c865792393a83c76ca6a/pkg/kubelet/pod_workers.go#L1244-L1253

The worker is unblocked when another event is delivered or the PLEG calls `cache.UpdateTime()`. At the latter case, while the genericPLEG calls `cache.UpdateTime()` every one second along with `Relist()`, the EventPLEG calls `cache.UpdateTime()` every five seconds. Because of this difference, the Evented PLEG spends more time to get pods into another phase.

https://github.com/kubernetes/kubernetes/blob/ade0d2140a68a69f5343c865792393a83c76ca6a/pkg/kubelet/pleg/evented.go#L34-L37

Even if there is a cache update, a worker can be blocked when the cache is updated by an asynchronous event before the worker finishes `SyncPod()`. For instance, when a runtime starts a container, the PLEG gets an event and caches the container status(`running`). If this event is received after a pod worker finishes `SyncPod()` to start the container, the worker gets the new status at `GetNewerThan()` soon and runs `SyncPod()` again to update the pod phase to `running`. However, if the event arrives before the worker finishes `SyncPod()` to start the container, the worker is blocked at `GetNewerThan()` because the cached status is older than `lastSyncTime`.

#### What did you expect to happen?

The pod phase transition should be as fast as GenericPLEG. It would be better to set `globalCacheUpdatePeriod` to 1 second.

#### How can we reproduce it (as minimally and precisely as possible)?
This issue can be observed in the `pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e` job as described in https://github.com/kubernetes/kubernetes/pull/124297#issuecomment-2340279198:
```
E2eNode Suite: [It] [sig-node] [NodeFeature:SidecarContainers] Containers Lifecycle [It] should not hang in termination if terminated during initialization [sig-node, NodeFeature:SidecarContainers]
{ failed [FAILED] should delete in < 5 seconds, took 10.071795
Expected
    <float64>: 10.071794756
to be <
    <int64>: 5
In [It] at: k8s.io/kubernetes/test/e2e_node/container_lifecycle_test.go:3470 @ 09/10/24 08:54:31.819
}
```

This can be also reproduced manually as follows:

Build kubernetes locally with applying PR #124297 and run with enabling `EventedPLEG` feature gate.

Use this simple-pod.yaml
```
apiVersion: v1
kind: Pod
metadata:
  labels:
	run: simple-pod
  name: simple-pod
spec:
  containers:
  - command:
	- sh
	- -c
	- trap "exit 0" SIGTERM; while true; do sleep 1; done
	image: busybox
	name: simple-container
```

Run the command:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
```

This is the result I tried the command ten times:
<details>

```
$ for i in `seq 10`; do kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!; done
pod/simple-pod created
[1] 17875
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	12s
simple-pod   0/1 	Terminating     	0      	17s
simple-pod   0/1 	Terminating     	0      	17s
simple-pod   0/1 	Terminating     	0      	17s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 18248
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 18583
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19057
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
simple-pod   1/1 	Terminating     	0      	6s
pod "simple-pod" deleted
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19386
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19718
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20052
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20586
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20914
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 21294
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
```

</details>

This is the result when `EventedPLEG` is disabled:

<details>

```
$ for i in `seq 10`; do kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!; done
pod/simple-pod created
[1] 34236
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 34506
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 34820
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
pod/simple-pod condition met
simple-pod   1/1 	Running         	0      	2s
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35084
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35373
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35640
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35900
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36173
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36430
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36694
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
```

</details>


#### Anything else we need to know?

_No response_

#### Kubernetes version

master + PR #124297

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.582+8ec29a96e91e40
WARNING: version difference between client (1.28) and server (1.31) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124700 "kubectl top nodes" reports "unknown" when executing multiple concurrent “kubectl exec” requests against a pod running on a Windows node

- Issue 链接：[#124700](https://github.com/kubernetes/kubernetes/issues/124700)

### Issue 内容

#### What happened?

"kubectl top nodes" reports "unknown" when executing multiple concurrent “kubectl exec” requests against a pod

#### What did you expect to happen?

"kubectl top nodes" reports the status of nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create an AKS Windows node akswin22000000.
2. deploy a pod to this node with this yaml (kubectl apply -f hpc.yaml):
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod: hpc
  name: hpc
spec:
  securityContext:
    windowsOptions:
      hostProcess: true
      runAsUserName: "NT AUTHORITY\\SYSTEM"
  hostNetwork: true
  containers:
    - name: test
      image: mcr.microsoft.com/windows/servercore:ltsc2022
      imagePullPolicy: IfNotPresent
      command:
        - powershell.exe
        - -Command
        - "Start-Sleep 2147483"
  nodeSelector:
    kubernetes.io/hostname: "akswin22000000"
```
3. run the following script (test.py) and wait for several minutes.
```
import subprocess
import threading

def run_command(pod_name, command):
  subprocess.run(["kubectl", "exec", pod_name, "--", "powershell", "-command", command])

threads = []
for i in range(0, 50):
  thread = threading.Thread(target=run_command, args=("hpc", "& { Start-Sleep 0 }"))
  thread.start()
  threads.append(thread)

for thread in threads:
  thread.join()
```
4. run ```kubectl top nodes``` and get the following result:
```
NAME                                CPU(cores)   CPU%        MEMORY(bytes)   MEMORY% 
aks-nodepool1-66451635-vmss000000   170m         8%          1877Mi          41%  
akswin22000000                      <unknown>    <unknown>   <unknown>       <unknown> 
```

#### Anything else we need to know?

Restarting node can recover it. 

But we still have the following questions:
1. Why is “kubectl exec” causing problems? This looks like a bug: if the API is not designed to handle some load, we expect it to return an error, not hang.
2. Why is the node not recovering its metrics, even after the “hanging” pod is restarted?

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.1-eks-2f008fe", GitCommit:"abfec7d7e55d56346a5259c9379dea9f56ba2926", GitTreeState:"clean", BuildDate:"2023-04-14T20:43:13Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.3", GitCommit:"5214c5ddb5785ed9d8e47e79e67181e205555067", GitTreeState:"clean", BuildDate:"2024-04-12T23:22:53Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124699 Topology Aware Routing not working

- Issue 链接：[#124699](https://github.com/kubernetes/kubernetes/issues/124699)

### Issue 内容

#### What happened?

Despite enabling topology-aware routing, I'm still seeing traffic being routed from one AZ to another. This is not the expected behavior, as it should prioritize in-zone traffic.

Current setup: 

3 AZ eu-west
3 pods in each AZ 
3 endpoints per each AZ 



#### What did you expect to happen?

When i enable topology aware routing I expect that traffic will stay in AZ where traffic was generated to maximize cost savings.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create nginx deployment with 9 replicas with :
spec:
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: nginx
2. create Service 
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.kubernetes.io/topology-aware-hints: Auto
  name: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: ClusterIP
3. check for endpointslice if hints were generated
4. Label every node with : topology.kubernetes.io/zone

#### Anything else we need to know?

kubectl get events  --field-selector involvedObject.kind=Service,involvedObject.name=nginx
16m         Normal    TopologyAwareHintsEnabled    service/nginx   Topology Aware Hints has been enabled, addressType: IPv4



Kube-Proxy logs:

I0505 16:46:38.759957      10 topology.go:171] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint" 
I0505 16:46:38.105331      10 topology.go:171] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"    
I0505 16:49:27.896972      10 topology.go:181] "Skipping topology aware endpoint filtering since no hints were provided for zone" zone="eu-west-1c" 
I0505 16:49:28.432488      10 topology.go:181] "Skipping topology aware endpoint filtering since no hints were provided for zone" zone="eu-west-1c" 

Hints were assigned:
addressType: IPv4
apiVersion: discovery.k8s.io/v1
endpoints:
- addresses:
  - 100.96.11.100
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-jdnd9
    namespace: default
    uid: a142d4d9-e078-44e9-9ce1-3956faa1e5ce
  zone: eu-west-1c
- addresses:
  - 100.96.11.155
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-cr7f2
    namespace: default
    uid: a2bd185c-86d0-47a8-a5ab-45ff4e32a85d
  zone: eu-west-1c
- addresses:
  - 100.96.9.173
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-znbbm
    namespace: default
    uid: 204eaf01-79bb-4993-a37c-b7bb90fb2094
  zone: eu-west-1b
- addresses:
  - 100.96.9.26
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-lfxqp
    namespace: default
    uid: 47909cec-d536-4bee-ac95-5b021a29041b
  zone: eu-west-1b
- addresses:
  - 100.96.10.164
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-05ab65b1b0ba1ab03
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-mjvd5
    namespace: default
    uid: 777dceea-a87f-483d-ac7f-d30cd7812bb1
  zone: eu-west-1a
- addresses:
  - 100.96.10.225
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-05ab65b1b0ba1ab03
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-ltj2v
    namespace: default
    uid: 2a4d680c-8822-4a2c-b3b8-a56ceaff96e7
  zone: eu-west-1a
- addresses:
  - 100.96.11.62
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-xwqlm
    namespace: default
    uid: b4fb49ed-630b-4f7c-8ea0-22cc11ab2c9d
  zone: eu-west-1c
- addresses:
  - 100.96.9.138
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-p92c5
    namespace: default
    uid: 48c29d22-98f4-4316-bc96-958d608a8ffc
  zone: eu-west-1b
- addresses:
  - 100.96.12.233
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-0272547448c4bb6b7
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-sqb79
    namespace: default
    uid: 3f206f30-184c-4c83-986f-29b7b4ad5f77
  zone: eu-west-1a
kind: EndpointSlice

#### Kubernetes version

1.26.12

#### Cloud provider

AWS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #124691 When creating two replicas in a deployment with volume mount, one pod become Error or CrashLoopBackOff stage.

- Issue 链接：[#124691](https://github.com/kubernetes/kubernetes/issues/124691)

### Issue 内容

#### What happened?

When creating a MySQL deployment with two replicas also volume mount added, after some time one pod becomes `CrashLoopBackOff` stage and another is running successfully.

<img width="446" alt="Screenshot 2024-05-04 at 14 14 22" src="https://github.com/kubernetes/kubernetes/assets/51718908/c9095968-ead0-41bb-92cb-dfa172a1caa9">

Also, when we delete the volume mount and try, both replicas are working the same. But the common volume can not be met.

#### What did you expect to happen?

Both pods should be in the same state.

#### How can we reproduce it (as minimally and precisely as possible)?

Create PV and PVC
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    type: local
spec:
  persistentVolumeReclaimPolicy: Retain
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```
Create the mysql deployment 
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:latest
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: pwd
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
```
After some seconds, you can see both pods running successfully.(`k get pods`)
Then open the bash of from the first pod. `kubectl exec --stdin --tty mysql-pod-name-0 -- /bin/bash`. Then open the mysql in that bash. `mysql -u root -ppwd`. Now you can access the MySQL. Play with it and create a db for reference.

Then exit from the pod-0 and open the bash of the second pod by `kubectl exec --stdin --tty mysql-pod-name-1 -- /bin/bash`. You can access the bash of pod-1. Now open the MySQL there.  `mysql -u root -ppwd`. You will get an error like
```
bash-4.4# mysql -u root -ppwd
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2
```

In other way, try delete the pod-0(the workable pod).(`kubectl delete pod pod_name_0`). Then you can see another new pod(named pod-2) created. Now do the same procedure to access the mysql from both pods, you can observe that accessing the mysql from pod-1 will works with the already created db. and the pod-2 will expose the same error we observed in the pod-1 early.

#### Anything else we need to know?

-

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
uname -a
Darwin Sivakajans-MacBook-Pro.local 23.3.0 Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #124686 Kubelet should not add IPv6 entries to /etc/hosts on systems where IPv6 is disabled

- Issue 链接：[#124686](https://github.com/kubernetes/kubernetes/issues/124686)

### Issue 内容

#### What happened?

Kubelet unconditionally adds IPv6 entries to `/etc/hosts` even when system has IPv6 totally disabled. This results in strange errors due to "happy eyeballs" fallback behavior: when clients see both an IPv4 and an IPv6 address for a host, they first try IPv4 and then fall back to IPv6, but if IPv6 is disabled in the kernel, they then get an "Address family not supported by protocol" exception instead of "Connection refused".

This happens with standard HTTP clients, etc, but here's an example with netcat:
```
# nc -v 127.0.0.1 1111
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection refused.
```
vs
```
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection to 127.0.0.1 failed: Connection refused.
Ncat: Trying next address...
libnsock nsock_make_socket(): Socket trouble: Address family not supported by protocol
Ncat: Address family not supported by protocol.
```

That results in clients needing to handle a different exception. However, even if IPv6 were enabled on the system, it would still be invalid to put loopback addresses in `/etc/hosts` for localhost when they are not actually configured on the loopback interface.

Code here: https://github.com/kubernetes/kubernetes/blob/v1.29.4/pkg/kubelet/kubelet_pods.go#L373-L377

#### What did you expect to happen?

Kubelet should not add localhost entries to `/etc/hosts` for addresses that do not actually exist on the pod's loopback interface.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't think you need a reproducer for this one since the code is just unconditionally doing this. However, if you're looking to test this on a system with ipv6 disabled such that you get the "Address family not supported by protocol" error, or guarantee that CNI does not add an IPv6 loopback address, you can boot your kernel with `ipv6.disable=1` on the kernel command line.

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A all versions impacted


#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

不涉及。

---

## Issue #124655 The Version Skew Policy does not mention aggregated apiservers

- Issue 链接：[#124655](https://github.com/kubernetes/kubernetes/issues/124655)

### Issue 内容

#### What happened?

Someone opened #124533 saying

> I expected 1.29 libraries to still be compatible with Kubernetes 1.28 based on the kube-apiserver version skew.

I think that is not what the version skew policy (https://kubernetes.io/releases/version-skew-policy/) is trying to say. I think that the guiding principle is that you must first upgrade the servers that store objects of a given API group and resource/kind, then upgrade the clients of those servers.

In particular, the existing version skew policy document explicitly discusses several kinds of components, but does not mention aggregated apiservers. I think that these should be explicitly mentioned.

#### What did you expect to happen?

Everybody agrees on what to expect.

#### How can we reproduce it (as minimally and precisely as possible)?

Read the cited doc and Issue.

#### Anything else we need to know?

Nope

#### Kubernetes version

<details>

All of them.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

