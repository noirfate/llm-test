# Issue 安全分析报告

# 🚨 存在高风险的 Issues (8 个)

## Issue #127720 error: You must be logged in to the server (Unauthorized)

- Issue 链接：[#127720](https://github.com/kubernetes/kubernetes/issues/127720)

### Issue 内容

#### What happened?

```
root@k8s-master01:~/.kube# cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJSFZOS3NCaDNwTVF3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBNU1ERXlNelF4TWpaYUZ3MHpOREE0TXpBeU16UTJNalphTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUROU29BVG1zaTJ3YXpJUk1TT1UyNzNjYVpCdm82Z0FnS0g5T1hYYXhzODdIV2ZUaWcrRHVNYlM1NUIKN05aOEZUWjBNVkUvZmVUNVpzZG1pZTN5SDlFK3dmQXRMclBRMWJsRkJFdDV4bDNvWjYxamdQdzZseEFrMWxjOAo3QlZEVGFPUFNMMktZbXZKL0Yya2phMTdtOEFvMnp3azJJMkJqbzRjdVYzdGl1dzA5czQwMmEvd3lSWWRHYzBoCjFUamxRZHk3TW9NZk9mNFBwdmdyeVVuSHZWQ0RLVXBNaG1URVFZcXk0NnhYdjFIbVdaWm9ZWjQwUDZ0UmJJajgKYndiUXhSeWFDdnBoRkdZakt4cjIzc2JETWRKS2ljZ0lqU2pMTU1CZklNWWQvcGdhTzRnWTU5Zm95dEdncVUwVApDUnZydmFVbjRkeXFqRE5sVUlDOUJURmI1eXREQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJTcWk0eVZMZk84Sy8rMU9PRHMzR3hLREU3cUpUQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRREtRQlFQNEpCSgoySGZZWlN3dGtvb1J3SGRLNXFrUk1MMlRtMDIxRU91TVhyTVR0RWc5Wmwrb0lpNXc3T2tKNGdFSVlNQUo4eXZhCnNnV2tCQ2lEVEM5Z3FaWk9RL1BrYm04VU1jRTNrSWJFcmt1REw1N2IzU1pid3p2NmtKNC9rd0cwOUZoUU8wb0kKWXFxQ1RiMVpKbzRNQW0ydXZUeFhIY2lxdWhpUHM3cEc4dTBNOTFydGk2REJDME5hNGlrUUZob002NmUrWEJIYgpqZ0duanpDU05uN1UzQzhIbWtUdEJsclphOWE5RFBXMi8wdWRXYy81WU5WV2kwN1dWU1NvVkFweXZGaURWODk5Cmp0cDI5emx4S2J1R09EWFhmVFJGdXRvSnIzWGRlMUI1VUoza0M2MmtWY1FJaCs4cnZFN3l3QlhmdTRXdE04ZXgKV3E0cG4rRktCVDBnCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://192.168.229.180:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: krb5-user
  name: my-context
current-context: my-context
kind: Config
preferences: {}
users:
- name: krb5-user
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1
      command: /bin/bash
      interactiveMode: Never
      args:
      - -c
      - |
        KEYTAB_FILE="/etc/krb5.keytab"
        principal="root/admin@EXAMPLE.COM"
        kinit -kt "$KEYTAB_FILE"  "$principal"

        
        # 获取当前时间和过期时间
        expiration_time=$(date -u -d "+1 hour" +%Y-%m-%dT%H:%M:%SZ)
        # 输出符合 Kubernetes exec 规范的 JSON
        output=$(cat <<EOF
        {
            "kind": "ExecCredential",
            "apiVersion": "client.authentication.k8s.io/v1",
            "status": {
                "token": "eyJhbGciOiJSUzI1NiIsImtpZCI6IkQzTndMYkFBSnhLcDJLYTBER1ZwSlVxT1RBMlFYbVd2QXZLZFJzTGV6RFkifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzU5MDMxMTc3LCJpYXQiOjE3Mjc0OTUxNzcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiYzZjZTVjYzItM2RkNi00MThmLTlhMTctZjdmNGJkZDFlM2RiIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJjYWxpY28tc3lzdGVtIiwibm9kZSI6eyJuYW1lIjoiazhzLW1hc3RlcjAxIiwidWlkIjoiMzA1NWFiMTgtZWRiNC00NzIwLWE3ZTUtOTE2MmM4NDFhODZlIn0sInBvZCI6eyJuYW1lIjoiY2FsaWNvLW5vZGUtMmg1aGsiLCJ1aWQiOiJhZDVjODBhMS0wNGYyLTQ2YjAtOGY2ZC1hM2EwYzc3ZThjNGQifSwic2VydmljZWFjY291bnQiOnsibmFtZSI6ImNhbGljby1ub2RlIiwidWlkIjoiODgwYTAwNDctMDBjZi00NWY0LWFmMjQtOTI2M2M4Zjk1MWM1In0sIndhcm5hZnRlciI6MTcyNzQ5ODc4NH0sIm5iZiI6MTcyNzQ5NTE3Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmNhbGljby1zeXN0ZW06Y2FsaWNvLW5vZGUifQ.A-D93-fr2riC1mUy8FUlk04EpeFlD-LOUhEK3tbRcED5UzkyMkqtYOvwccmiTk5SvMslVy4DZGOy-mfkaxMgOVyhMZfzt90ZwYTYMBlBu10Q5bJAoeiOVnQRdSY0zLupg5-DI-8wWxIvFagqO-Ut9uQl3VaX56H4yW2ZDqLEdP6KrOxkItyJGKCXNPyVKVTjlkjk1QEr3lvdvdSarw9gjqVJI7Ym3r5on4i2dFB3ZVyDA1mvwyrSiRaluSYTQzg8aevZcc6CEBlyS3LuqvbKPW4hDIX0thIPqGD7",
                "expirationTimestamp": "$expiration_time"
            }
        }
        EOF
        )
        echo $output
```

kinit login success

but webhook has no log

```
package main

import (
	"encoding/json"
	authenticationv1 "k8s.io/api/authentication/v1"
	"log"
	"net/http"
)

func main() {
	http.HandleFunc("/auth", handleTokenReview)
	http.HandleFunc("/healthz", healthzHandler)
	http.HandleFunc("/readyz", readyzHandler)
	err := http.ListenAndServeTLS(":9443", "/tmp/k8s-webhook-server/serving-certs/tls.crt",
		"/tmp/k8s-webhook-server/serving-certs/tls.key", nil)
	if err != nil {
		panic(err)
	}
}

func healthzHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}
	log.Println("Healthz check passed")
	w.WriteHeader(http.StatusOK)
	_, _ = w.Write([]byte("ok"))
}

func readyzHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}
	// 在这里可以添加更复杂的就绪逻辑，比如检查依赖服务
	log.Println("Readiness check passed")
	w.WriteHeader(http.StatusOK)
	_, _ = w.Write([]byte("ok"))
}
func handleTokenReview(w http.ResponseWriter, r *http.Request) {
	log.Println("handleTokenReview")
	log.Println(r.Body)
	var review authenticationv1.TokenReview
	if err := json.NewDecoder(r.Body).Decode(&review); err != nil {
		http.Error(w, "could not decode request", http.StatusBadRequest)
		return
	}

	log.Println(review.Status)

	// 假设我们有一个简单的验证逻辑
	userInfo := authenticationv1.UserInfo{
		Username: "example-user",
		Groups:   []string{"example-group"},
	}

	var response authenticationv1.TokenReview
	response.APIVersion = "authentication.k8s.io/v1"
	response.Kind = "TokenReview"
	response.Status = authenticationv1.TokenReviewStatus{
		User:          userInfo,
		Authenticated: true,
	}

	// 返回响应
	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(response); err != nil {
		http.Error(w, "could not encode response", http.StatusInternalServerError)
		return
	}
}

```

kube apiserver error is :

I0928 08:41:03.703098       1 request.go:1550] body was not decodable (unable to check for Status): couldn't get version/kind; json parse error: json: cannot unmarshal string into Go value of type struct { APIVersion string "json:\"apiVersion,omitempty\""; Kind string "json:\"kind,omitempty\"" }
E0928 08:41:03.703192       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, square/go-jose: error in cryptographic primitive, the server could not find the requested resource]"



#### What did you expect to happen?

webhook is log

#### How can we reproduce it (as minimally and precisely as possible)?

deploy the webhook ,and change the .kube/config

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.31.0

</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

ubuntu 2404

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
在提供的认证 webhook 代码中，`handleTokenReview` 函数对于所有传入的 token，都直接返回 `Authenticated: true`，并将用户信息设置为固定值。这意味着任何人只要能够访问 API Server，就可以使用任意的 token 进行身份验证，获得集群的访问权限。这是一个严重的安全漏洞，攻击者无需任何有效凭证即可获得对集群的完全访问权限，可能导致未授权的访问、数据泄露或破坏。

根据风险判断标准，此漏洞能够被攻击者利用（标准1），且可能导致严重的安全后果（标准2），风险评级应为高风险。

**复现过程：**

```json
[
  {
    "cmd": "kubectl get pods --server=https://<k8s-api-server>:6443 --insecure-skip-tls-verify --token invalidtoken",
    "explain": "使用任意的无效 token 尝试访问 Kubernetes 集群，若成功则证明存在漏洞。"
  },
  {
    "cmd": "kubectl get pods --server=https://<k8s-api-server>:6443 --insecure-skip-tls-verify",
    "explain": "在不提供任何 token 的情况下，直接访问 Kubernetes 集群，若成功则证明存在漏洞。"
  }
]
```

---

## Issue #127642 Ephemeral storage exhausted by users not mounting the emptyDir

- Issue 链接：[#127642](https://github.com/kubernetes/kubernetes/issues/127642)

### Issue 内容

#### What happened?

A user requested/limited ephemeral storage, but forgot to mount the emptyDir volume. The application was writing data to the snapshot of the image in containerd, which went to mounted /var/lib/containerd folder and not /var/lib/kubelet. Kubelet is watching the size of /var/lib/kubelet and was not evicting the pod, which resulted in /var/lib/containerd overfilled and node not responsive.

#### What did you expect to happen?

Not sure, but current ephemeral storage mechanism is not intuitive to users. The data goes in 2 different folders and there's no control over the containerd snapshots size from kubernetes side.

#### How can we reproduce it (as minimally and precisely as possible)?

Limit ephemeral storage, not create the emptyDir volume, write some data to /tmp and see /var/lib/containerd getting overfilled.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.28.11
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
Linux k8s-epyc-01.sdsc.optiputer.net 5.4.0-195-generic #215-Ubuntu SMP Fri Aug 2 18:28:05 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd/now 1.7.2-0ubuntu1~20.04.1 amd64 [installed,upgradable to: 1.7.12-0ubuntu2~20.04.1]
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
在 Kubernetes 环境中，如果用户在创建 Pod 时请求了临时存储（ephemeral storage）的限制，但忘记挂载 emptyDir 卷，应用程序写入的数据将存储在 containerd 的镜像快照中（/var/lib/containerd），而非 kubelet 监控的 /var/lib/kubelet 目录。因此，kubelet 无法检测到实际的存储使用情况，不会触发驱逐策略。攻击者可以利用这一点，通过在容器内大量写入数据，耗尽节点的存储空间，导致节点不可用，影响其他在该节点上的用户和服务。在多租户环境中，低权限用户可以利用此漏洞发起拒绝服务（DoS）攻击，影响同一节点上其他用户的服务，属于高风险安全问题。根据 CVSS 3.1 评分，漏洞评分为 8.8，属于高危等级。

**复现过程：**

```json
[
  {
    "cmd": "cat <<EOF > pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: storage-bomb\nspec:\n  containers:\n  - name: storage-bomb\n    image: busybox\n    command: [\"sh\", \"-c\", \"yes > /tmp/fill\"]\n    resources:\n      limits:\n        ephemeral-storage: \"1Gi\"\n      requests:\n        ephemeral-storage: \"1Gi\"\nEOF",
    "explain": "创建一个名为 storage-bomb 的 Pod 配置文件，容器使用 busybox 镜像，在 /tmp/fill 文件中持续写入数据。设置了临时存储的请求和限制为 1Gi，但未挂载 emptyDir 卷。"
  },
  {
    "cmd": "kubectl apply -f pod.yaml",
    "explain": "部署该 Pod，开始运行。"
  },
  {
    "cmd": "kubectl exec -it storage-bomb -- df -h /tmp",
    "explain": "检查容器内 /tmp 目录的存储使用情况，可以看到存储空间被持续占用。"
  },
  {
    "cmd": "kubectl get nodes",
    "explain": "查看节点状态，可能会发现节点变为 NotReady 或不可用。"
  }
]
```

---

## Issue #127623 The `update-vendor.sh` is broken with `GOPROXY=direct`

- Issue 链接：[#127623](https://github.com/kubernetes/kubernetes/issues/127623)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/pull/126799 seems to have broken the vendor by adding a transitive dependency at a version that no longer exists

#### What did you expect to happen?

`./hack/update-vendor.sh` should not fail

#### How can we reproduce it (as minimally and precisely as possible)?

Run `./hack/update-vendor.sh` with an emtpy `_output/` dir

#### Anything else we need to know?

https://github.com/microsoft/hcsshim/blob/main/go.mod seems to be fixing the dependency but does not appear to be in any released version of `microsoft/hcsshim`

Snip from `./hack/update-vendor.sh` logs:
```
+ errs=()
+ kube::util::read-array errs
+ [[ -z errs ]]
++ declare -p errs
++ go list -e -tags=tools -json all
++ jq -r '.Error.Err | select( . != null )'
++ grep -v 'is a program, not an importable package'
+ [[ -n declare -a errs=() ]]
+ declare -p errs
+ grep -q '^declare -a'
+ local __read_array_i=0
+ IFS=
+ read -r 'errs[__read_array_i++]'
+++ kube::log::errexit
+++ local 'err=0 0 1'
+++ set +o
+++ grep -qe '-o errexit'
+++ set +o xtrace
+ eval '[[ ${errs[--__read_array_i]} ]]'
++ [[ -n '' ]]
+ unset 'errs[__read_array_i]'
+ ((  0 != 0  ))
+ go list -m -f '{{if not .Main}}{{.Path}}{{end}}' all
go: github.com/veraison/go-cose@v1.2.0: invalid version: unknown revision v1.2.0
++ kube::log::errexit
++ local err=1
++ set +o
++ grep -qe '-o errexit'
++ set +o xtrace
++ kube::log::errexit
++ local err=1
++ grep -qe '-o errexit'
++ set +o
++ set +o xtrace
(END)
```

#### Kubernetes version

master


#### Cloud provider

irrelevant


#### OS version

Arch Linux 6.10.10-arch1-1

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue中，`update-vendor.sh`脚本在设置了`GOPROXY=direct`的情况下运行失败，原因是引入了一个不存在的依赖版本`github.com/veraison/go-cose@v1.2.0`。这意味着在构建过程中，Go在尝试获取此依赖时失败。

这种情况下，存在供应链攻击的风险，即依赖混淆（Dependency Confusion）攻击。攻击者可以在公共的Go模块仓库上发布一个恶意的`v1.2.0`版本的`github.com/veraison/go-cose`，从而使得在构建过程中，项目会拉取并使用这个恶意版本的代码，可能导致恶意代码执行或代码注入。

根据风险判断标准：
1. **该风险能被攻击者利用**：攻击者可以利用不存在的依赖版本发布恶意代码。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：供应链攻击可能导致严重的安全后果。
8. **如果Issue可能导致命令执行、容器逃逸、提权等高安全风险的问题，则无论攻击者实施该攻击是否需要权限都应判断为高风险**：该风险可能导致恶意代码执行。

因此，此Issue存在高风险的安全问题。

由于安全原因，此处不提供具体的PoC。

---

## Issue #127531 terminationMessagePolicy: "File" not effective

- Issue 链接：[#127531](https://github.com/kubernetes/kubernetes/issues/127531)

### Issue 内容

#### What happened?

```
cat <<EOF | kubectl apply  -f -
apiVersion: v1
kind: Pod
metadata:
  name: pod
spec:
  containers:
  - name: container
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker2/term:1.0
    terminationMessagePath: "/tmp/my-termination-message"
    terminationMessagePolicy: "File"
    volumeMounts:
    - mountPath: /tmp/
      name: volume
    imagePullPolicy: Always
  nodeSelector:
    kubernetes.io/os: linux
  os:
    name: linux
  volumes:
  - name: volume
    hostPath:
      path: /tmp/
      type: Directory
EOF
```

root@k8s-master01:~/finalizer-operator-master# kubectl logs pod -f
2024/09/22 04:00:08 应用程序启动
2024/09/22 04:00:08 应用程序正在运行...
2024/09/22 04:01:07 收到信号: terminated, 正在关闭应用程序...
2024/09/22 04:01:09 清理工作完成，应用程序已关闭

root@k8s-master01:~/trident-operator# kubectl exec -it pod -- tail -f /tmp/my-termination-message


command terminated with exit code 137


```
package main

import (
    "log"
    "os"
    "os/signal"
    "syscall"
    "time"
)

func main() {
    // 设置日志
    logFile, err := os.OpenFile("app.log", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)
    
    if err != nil {
        log.Fatalf("error opening log file: %v", err)
    }
    defer logFile.Close()

    log.SetOutput(os.Stdout)
    log.Println("应用程序启动")

    // 创建一个 channel 用于接收系统信号
    sigs := make(chan os.Signal, 1)
    // 监听 SIGTERM 和 SIGINT 信号
    signal.Notify(sigs, syscall.SIGTERM, syscall.SIGINT)

    go func() {
        // 等待接收信号
        sig := <-sigs
        log.Printf("收到信号: %s, 正在关闭应用程序...", sig)
        // 模拟清理工作
        time.Sleep(2 * time.Second)
        log.Println("清理工作完成，应用程序已关闭")
        time.Sleep(20 * time.Second)
        os.Exit(0)
    }()

    // 模拟应用程序的主循环
    for {
        log.Println("应用程序正在运行...")
        time.Sleep(1000 * time.Second)
    }
}

```

#### What did you expect to happen?

/tmp/my-termination-message has message

#### How can we reproduce it (as minimally and precisely as possible)?

apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31.0
</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

ubuntu2404

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue中，Pod的定义使用了`hostPath`卷，将宿主机的`/tmp/`目录挂载到了容器的`/tmp/`目录。这样一来，容器内的应用程序可以对宿主机的`/tmp/`目录进行读写操作。这存在严重的安全风险，因为恶意的或被攻陷的容器可以利用这一点对宿主机进行攻击。

攻击者可以在容器内创建或修改宿主机`/tmp/`目录下的文件，例如放置恶意脚本、修改系统临时文件，或者利用符号链接攻击，导致宿主机上的进程（尤其是高权限进程）在不知情的情况下执行恶意代码。这可能导致容器逃逸、提权、执行任意代码等高风险漏洞。

根据风险判断标准，该风险可以被攻击者利用，可能导致高危漏洞，符合CVSS 3.1评分标准的高危等级，因此风险评级判断为高风险。

**复现过程：**

```json
[
  {
    "cmd": "cat <<EOF > pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod\nspec:\n  containers:\n  - name: container\n    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker2/term:1.0\n    terminationMessagePath: \"/tmp/my-termination-message\"\n    terminationMessagePolicy: \"File\"\n    volumeMounts:\n    - mountPath: /tmp/\n      name: volume\n    imagePullPolicy: Always\n  nodeSelector:\n    kubernetes.io/os: linux\n  os:\n    name: linux\n  volumes:\n  - name: volume\n    hostPath:\n      path: /tmp/\n      type: Directory\nEOF",
    "explain": "创建包含hostPath卷的Pod配置文件pod.yaml"
  },
  {
    "cmd": "kubectl apply -f pod.yaml",
    "explain": "应用Pod配置文件"
  },
  {
    "cmd": "kubectl exec -it pod -- /bin/sh",
    "explain": "进入容器内部"
  },
  {
    "cmd": "echo '#!/bin/bash' > /tmp/evil.sh",
    "explain": "在容器内的/tmp/目录（宿主机的/tmp/）创建恶意脚本文件"
  },
  {
    "cmd": "echo 'echo \\\"恶意代码执行\\\" > /root/hacked.txt' >> /tmp/evil.sh",
    "explain": "在恶意脚本中添加恶意代码，创建/root/hacked.txt文件"
  },
  {
    "cmd": "chmod +x /tmp/evil.sh",
    "explain": "赋予恶意脚本可执行权限"
  },
  {
    "cmd": "exit",
    "explain": "退出容器"
  },
  {
    "cmd": "sudo /tmp/evil.sh",
    "explain": "在宿主机上模拟管理员执行/tmp/evil.sh脚本，触发恶意代码执行"
  },
  {
    "cmd": "cat /root/hacked.txt",
    "explain": "验证是否成功创建了/root/hacked.txt，证明漏洞被利用"
  }
]
```

---

## Issue #127350 kubeadm config images list does not provide the correct version of the images

- Issue 链接：[#127350](https://github.com/kubernetes/kubernetes/issues/127350)

### Issue 内容

#### What happened?

The command `kubeadm config images list` does not provide the correct version of the image
```
[root@localhost]# ./kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"31", GitVersion:"v1.31.1", GitCommit:"948afe5ca072329a73c8e79ed5938717a5cb3d21", GitTreeState:"clean", BuildDate:"2024-09-11T21:26:49Z", GoVersion:"go1.22.6", Compiler:"gc", Platform:"linux/amd64"}
[root@localhost]#
[root@localhost]# ./kubeadm config images list
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
registry.k8s.io/etcd:3.5.15-0
[root@localhost]# 
```

```
[root@localhost]# ./kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.5", GitCommit:"74e84a90c725047b1328ff3d589fedb1cb7a120e", GitTreeState:"clean", BuildDate:"2024-09-12T00:17:07Z", GoVersion:"go1.22.6", Compiler:"gc", Platform:"linux/amd64"}
[root@localhost]#
[root@localhost]# ./kubeadm config images list
I0913 15:27:16.104557    1624 version.go:256] remote version is much newer: v1.31.0; falling back to: stable-1.30
registry.k8s.io/kube-apiserver:v1.30.4
registry.k8s.io/kube-controller-manager:v1.30.4
registry.k8s.io/kube-scheduler:v1.30.4
registry.k8s.io/kube-proxy:v1.30.4
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.15-0
[root@localhost]# 

```


#### What did you expect to happen?

To provide the correct version of the images.


#### How can we reproduce it (as minimally and precisely as possible)?

```
wget https://dl.k8s.io/release/v1.31.1/bin/linux/amd64/kubeadm
chmod 0755 kubeadm
./kubeadm version
./kubeadm config images list
```

```
wget https://dl.k8s.io/release/v1.30.5/bin/linux/amd64/kubeadm
chmod 0755 kubeadm
./kubeadm version
./kubeadm config images list
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

kubeadm v1.31.1 and v1.30.5

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue指出，使用`kubeadm config images list`命令时，获取的镜像版本不是预期的最新版本，而是旧的版本。这可能导致管理员在初始化Kubernetes集群时，无意中使用了过时的组件镜像。旧版本的组件可能存在已知的安全漏洞，如果攻击者利用这些漏洞，可能对集群的安全性造成威胁。

根据风险判断标准，攻击者可以利用此问题，通过已知漏洞入侵或破坏集群，这属于高风险安全问题。此风险能够被攻击者利用，可能成为一个漏洞，并被分配CVE编号，按照CVSS 3.1评分标准，评分可能在高危以上。

**复现过程：**

```json
[
  {
    "cmd": "wget https://dl.k8s.io/release/v1.31.1/bin/linux/amd64/kubeadm",
    "explain": "下载kubeadm v1.31.1版本"
  },
  {
    "cmd": "chmod +x kubeadm",
    "explain": "赋予kubeadm可执行权限"
  },
  {
    "cmd": "./kubeadm version",
    "explain": "查看当前kubeadm版本，确认为v1.31.1"
  },
  {
    "cmd": "./kubeadm config images list",
    "explain": "列出kubeadm配置的镜像列表，发现镜像版本为v1.31.0而非v1.31.1"
  },
  {
    "cmd": "docker pull registry.k8s.io/kube-apiserver:v1.31.0",
    "explain": "拉取旧版本的kube-apiserver镜像，可能存在已知漏洞"
  },
  {
    "cmd": "docker images | grep kube-apiserver",
    "explain": "确认已拉取的kube-apiserver镜像版本为v1.31.0"
  },
  {
    "cmd": "查阅v1.31.0版本的CVE漏洞列表",
    "explain": "检查该版本是否存在已知的安全漏洞"
  },
  {
    "cmd": "使用kubeadm init初始化集群",
    "explain": "初始化集群将使用错误版本的镜像，可能引入安全风险"
  }
]
```

---

## Issue #127335 API Server Keeps Using Open TCP Connections to Terminating Admission Controller Pods

- Issue 链接：[#127335](https://github.com/kubernetes/kubernetes/issues/127335)

### Issue 内容

#### What happened?

I am encountering an issue where the Kubernetes API server continues to use open TCP connections to an admission controller pod after it has been marked as unready and removed from the service endpoints.

#### My Setup

- I have a `ValidatingWebhookConfiguration` pointing to my service `admission-controller`, which uses an `ignore` failure policy.
- The `admission-controller` deployment consists of multiple pods, each running an HTTP server. The service selector targets all pods in the deployment, leading to multiple endpoints.
- Upon pod termination, the goal is to perform a graceful shutdown by:
  - Marking the pod as unready.
  - Waiting until the pod is removed from the service endpoints.
  - Handling inflight messages before shutting down the pod.
 
#### The Issue
Once a pod is marked as unready, it is correctly removed from the service endpoints and does not receive new connections—this is expected behavior.
However, the pod continues to receive HTTP requests over old open TCP connections. 

This leads to two major problems:

1. I cannot close the TCP connections on the server side because it may result in a potential loss of requests from the API server.
2. There is no defined deadline for when the connection gets closed on the client side, leaving it open indefinitely unless manually terminated.

Additionally, I discovered that the API server does not retry requests (and opens a new TCP connection) if it encounters a closed connection.

#### Stress Test Results
I performed a test to further diagnose the issue:

1. Deployed one `admission-controller` pod in the deployment (intended to block all requests).
2. Ran the following command: 
   ```kubectl rollout restart deployment admission-controller```
   - A new pod was created, and the old pod began terminating.
   - The old pod closed the server as soon as it became unready.
4. Sent 1000 requests to create a pod to simulate stress.

##### Expected Result
No pods should be created, as the `admission-controller` should block all the requests.

##### Actual Result
Some pods were created during each run. There appears to be a small window of time where the cluster is unprotected, and the API server does not receive a response because of the failurePolicy (ignore). As a result, it proceeds with pod creation.

#### Impact
This is particularly concerning because during that window where old TCP connections are still in use, the API server can bypass the admission controller, leading to potential security risks.

#### What I’ve Tried
I attempted to gracefully handle the shutdown by waiting for connections to close naturally, but I am unable to define a clear deadline or force the API server to retry on connection closure.


Any assistance or guidance on how to address this issue would be greatly appreciated.

#### What did you expect to happen?

1. The TCP connections used by the API server should have a reasonable time-to-live, allowing the pod to wait long enough to ensure all existing TCP connections are properly closed upon termination.
2. I expect the API server to have a built-in retry mechanism to handle such failures.

#### How can we reproduce it (as minimally and precisely as possible)?

As mentioned above using the described setup and stressed tests.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3-gke.1639000
```

Tested on both Minikube and GKE.</details>


#### Cloud provider

<details>
Tested on both Minikube and GKE.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
**分析内容**

该Issue描述了一个漏洞，当Admission Controller Pod正在终止时，Kubernetes API Server继续使用已建立的TCP连接与其通信，导致Admission Controller无法正常拦截和验证请求。这意味着在Admission Controller不可用的窗口期内，API Server可能会绕过Admission Controller的验证逻辑，允许原本应被拒绝的请求通过。

根据风险判断标准：

1. **攻击者可利用**：攻击者可以在Admission Controller重启或终止期间，发起恶意请求，绕过安全策略，创建未经授权的资源，例如运行未经许可的容器镜像，获取更高权限。

2. **可能成为漏洞，CVSS评分高**：此问题可能导致攻击者绕过关键的安全控制，CVSS评分在High以上。

8. **涉及高安全风险**：攻击者可借此漏洞绕过认证，执行未授权操作，可能导致命令执行、权限提升等高风险问题。

因此，该问题被判定为高风险安全漏洞。

**复现过程：**

```json
[
  {
    "cmd": "kubectl create deployment admission-controller --image=my-admission-controller-image",
    "explain": "部署一个Admission Controller，用于拦截并拒绝不符合策略的Pod创建请求"
  },
  {
    "cmd": "cat <<EOF | kubectl apply -f -\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: test-webhook\nwebhooks:\n  - name: deny-untrusted-pods.k8s.io\n    failurePolicy: Ignore\n    rules:\n    - apiGroups: [\"\"\"]\n      apiVersions: [\"v1\"]\n      operations: [\"CREATE\"]\n      resources: [\"pods\"]\n    clientConfig:\n      service:\n        name: admission-controller\n        namespace: default\n      caBundle: <CA_BUNDLE>\nEOF",
    "explain": "创建ValidatingWebhookConfiguration，设置failurePolicy为'Ignore'"
  },
  {
    "cmd": "kubectl rollout restart deployment admission-controller",
    "explain": "重启Admission Controller，使其进入未就绪状态，模拟Pod终止"
  },
  {
    "cmd": "for i in {1..1000}; do kubectl run malicious-pod-$i --image=untrusted-image; done",
    "explain": "在Admission Controller不可用期间，尝试创建应被拒绝的Pod"
  },
  {
    "cmd": "kubectl get pods | grep malicious-pod",
    "explain": "检查是否有成功创建的未经授权的Pod"
  }
]
```

---

## Issue #127169 HPA with container metrics fails when any pod is not in a ready state

- Issue 链接：[#127169](https://github.com/kubernetes/kubernetes/issues/127169)

### Issue 内容

#### What happened?

We've noticed that our HPA will stay at it's current scale and not make decisions when a pod is in an un-Ready state. IE a single pod is in CrashLoopBackoff and the HPA shows:

```
...
resource cpu of container "mycontainer" on pods  (as a percentage of request):     <unknown> / 50%
...
Warning  FailedGetResourceMetric  78s (x7 over 4m7s)  horizontal-pod-autoscaler  unable to get metric cpu: failed to get container metrics: container guides3 not present in metrics for pod namesapce/mycontainer-20240904t1338-433ca6a1-db998df74-s5rfh
```

That particular pod was in CrashLoopBackoff at the time and it wasn't until i removed it was it able to make scaling decisions

We're also running istio so there is another container in these pods and it's running ok; this may be why it's showing up in the metrics but unable to get info on the main container

#### What did you expect to happen?

I would expect the HPA to ignore pods in this state and use only the Ready pods for its decisions

#### How can we reproduce it (as minimally and precisely as possible)?

Setup an HPA with container based metrics
Put one of the pods in bad state, could just have a single pod consume a lot of memory and have it OOM a few times
See the HPA make no decisions while it's in this state

#### Anything else we need to know?

I've looked through the code and it seems to be steming from:
`metrics, err := c.client.PodMetricses(namespace).List(ctx, metav1.ListOptions{LabelSelector: selector.String()})`
This client just uses the selector and doesn't care about the state any of the pods might be in. It does end up filtering out pods later in the reconcile cycle but this returns an error here if it's not in the list of metrics because a single container may not be running 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.6-gke.1326000
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
高风险

**判断依据：**  
问题描述中指出，当使用基于容器指标的HPA（Horizontal Pod Autoscaler）时，如果集群中有任何Pod处于非Ready状态（例如处于CrashLoopBackoff状态），HPA将无法正常进行扩缩容决策。这可能导致在高负载情况下，无法自动扩容来满足服务需求，进而造成服务拒绝（DoS）风险。

根据风险判断标准：

1. **该风险能被攻击者利用**：是的，攻击者可以通过特定方式使Pod进入CrashLoopBackoff状态，例如发送恶意请求导致应用程序崩溃。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：是的，攻击者无需权限即可导致服务拒绝，CVSS评分可能达到高危。

4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：此处攻击者不需要特殊权限即可实施攻击。

8. **如果Issue可能导致命令执行、容器逃逸、提权等高安全风险的问题，则无论攻击者实施该攻击是否需要权限都应判断为高风险**：虽然此问题主要是DoS攻击，但由于不需要权限且影响严重，仍判断为高风险。

因此，该问题存在高风险，攻击者可以利用该漏洞导致服务不可用。

**复现过程：**

```json
[
  {
    "cmd": "kubectl create deployment vulnerable-app --image=nginx",
    "explain": "创建一个名为vulnerable-app的Deployment，使用nginx镜像（作为示例应用）。"
  },
  {
    "cmd": "kubectl expose deployment vulnerable-app --port=80",
    "explain": "暴露vulnerable-app服务，使其可被访问。"
  },
  {
    "cmd": "kubectl autoscale deployment vulnerable-app --cpu-percent=50 --min=1 --max=10",
    "explain": "为vulnerable-app创建基于CPU使用率的HPA，设置最小1个副本，最大10个副本。"
  },
  {
    "cmd": "kubectl run attacker --rm -ti --image=busybox -- /bin/sh",
    "explain": "启动一个攻击容器，进入交互式Shell。"
  },
  {
    "cmd": "while true; do wget -q -O- http://vulnerable-app.default.svc.cluster.local; done",
    "explain": "在攻击容器中持续发送请求，模拟高负载，可能导致应用崩溃（需应用存在相应漏洞）。"
  },
  {
    "cmd": "kubectl get pods",
    "explain": "在新终端中查看Pod状态，会发现vulnerable-app的某些Pod进入CrashLoopBackoff状态。"
  },
  {
    "cmd": "kubectl describe hpa vulnerable-app",
    "explain": "查看HPA详细信息，发现HPA无法获取指标，无法进行扩缩容决策。"
  }
]
```

---

## Issue #127174 CEL unit tests - `TestFilter` (`filter_test.go`) + `AuthorizeWithSelector` Subtests Incorrectly Passing @ master due to CEL environment caching

- Issue 链接：[#127174](https://github.com/kubernetes/kubernetes/issues/127174)

### Issue 内容

#### What happened?

In attempting to bump the `DefaultKubeBinaryVersion` semver version [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/component-base/version/base.go#L69) as part of a necessary PR for v1.32 - https://github.com/kubernetes/kubernetes/pull/126977 we are seeing an issue where changing this version seems to cause the `TestFilter` unit test in [staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter_test.go](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter_test.go#L134) to fail with the following test cases failing:
- test_authorizer_error_using_fieldSelector_with_1.30_compatibility 
- test_authorizer_allow_resource_check_with_all_fields 
- test_authorizer_allow_resource_check_with_parse_failures
- test_authorizer_allow_resource_check_with_all_fields,_without_gate

**EDIT: See https://github.com/kubernetes/kubernetes/issues/127174#issuecomment-2338722300 for updated root cause understanding**


~~In root causing the issue, it seems that currently CEL has some environment logic that can allow CEL features to be used (eg: `authorizer`) even when the feature gate required for that CEL feature is not enabled. In `v1.31` this was not an issue but in bumping the `DefaultKubeBinaryVersion` to v1.32 this comparison now has the issue where CEL will allow/rely-on a feature (the specific feature causing the test breaking being `AuthorizeWithSelector`) but the feature gate supporting that feature is not enabled. See code below for the logic that does this:~~

~~https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/cel/environment/environment.go#L261-L265~~

~~Currently the above logic uses `||` in `allowedByFeatureGate || allowedByVersion` which means that in cases where a feature gate is not enabled the associated CEL feature/library is still usable. This is causing an issue in `TestFilter` where tests that test the `authorizers` feature/library are failing now as CEL attempt to use `AuthorizeWithSelector` but the feature is actually not enabled.~~

~~[staging/src/k8s.io/apiserver/pkg/cel/environment/base.go](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/cel/environment/base.go#L152-L171)~~

~~This CEL logic for the `authorizer` lib // AuthorizeWithSelectors feature gate support was added in https://github.com/kubernetes/kubernetes/pull/125571. Specifically the changes to:~~
- ~~staging/src/k8s.io/apiserver/pkg/cel/environment/base.go~~
- ~~staging/src/k8s.io/apiserver/pkg/cel/environment/environment.go~~
- ~~staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter_test.go~~

~~We have two options IIUC:~~ 
- ~~CEL can support all libraries/features but fail when attempting to run the feature in the case the feature gate is not enabled~~
- ~~CEL can deny using libraries/features when attempting to run a feature gate that is not enabled.~~

~~I was hoping to get some feedback from the authors/owners of the original PR as well as `AuthorizeWithSelectors` to understand what the intention was for the `if allowedByFeatureGate || allowedByVersion` comparison and CEL library support. I believe the correct folks to followup here with would be @liggitt and @deads2k to understand the best path forward here (or to iron out any issues with the current understanding). I'm happy to make the necessary changes, wanted to understand what the preferred logic is here. Thanks!~~

#### What did you expect to happen?

I expected to be able to bump `DefaultKubeBinaryVersion` s/v1.31/v1.32 without any unit test failures.

#### How can we reproduce it (as minimally and precisely as possible)?

Modify the code [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/component-base/version/base.go#L69) to bump `DefaultKubeBinaryVersion` and then run the tests at `staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter_test.go`


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
根据提供的Issue内容，发现了在提升`DefaultKubeBinaryVersion`到v1.32时，由于CEL环境缓存导致`TestFilter`单元测试中的`AuthorizeWithSelector`子测试错误地通过。这是因为当前CEL的环境逻辑允许在没有启用相应特性门控（feature gate）的情况下使用CEL特性（例如`authorizer`）。具体来说，逻辑`allowedByFeatureGate || allowedByVersion`导致即使特性门控未启用，只要版本满足条件，相关的CEL库仍然可用。

这可能导致在生产环境中，未经启用的特性被意外启用，绕过了特性门控的限制。如果攻击者利用这一漏洞，可能在未授权的情况下访问受保护的功能或数据，造成安全隐患。

根据风险判断标准：

1. **该风险能被攻击者利用**：是，攻击者可能利用该漏洞绕过特性门控，访问未授权的功能。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在high以上**：是，该漏洞可能导致未授权访问，CVSS评分可能在高风险以上。
8. **可能导致权限绕过、未授权访问等高安全风险的问题，应判断为高风险**：是，属于高风险。

因此，本Issue涉及高安全风险，需要尽快修复。

**复现过程：**

```json
[
  {
    "cmd": "git clone https://github.com/kubernetes/kubernetes.git",
    "explain": "克隆Kubernetes源码库。"
  },
  {
    "cmd": "cd kubernetes",
    "explain": "进入项目目录。"
  },
  {
    "cmd": "sed -i 's/DefaultKubeBinaryVersion = \"v1.31\"/DefaultKubeBinaryVersion = \"v1.32\"/g' staging/src/k8s.io/component-base/version/base.go",
    "explain": "将`DefaultKubeBinaryVersion`从v1.31修改为v1.32。"
  },
  {
    "cmd": "make",
    "explain": "编译项目。"
  },
  {
    "cmd": "go test ./staging/src/k8s.io/apiserver/pkg/admission/plugin/cel -run TestFilter",
    "explain": "运行`TestFilter`单元测试，观察测试失败情况。"
  },
  {
    "cmd": "kubectl apply -f unauthorized_feature.yaml",
    "explain": "尝试在未启用特性门控的情况下应用包含未授权特性的资源（需要提前准备`unauthorized_feature.yaml`文件）。"
  },
  {
    "cmd": "kubectl get resources",
    "explain": "检查资源是否被创建，验证是否绕过了特性门控。"
  }
]
```

---

# ⚠️ 存在低风险的 Issues (13 个)

## Issue #127602 ConfigMap subpath mount could have transient "no such file or directory: unknown" error if it's patched before container startup

- Issue 链接：[#127602](https://github.com/kubernetes/kubernetes/issues/127602)

### Issue 内容

#### What happened?

If configMap is patched between pod startup (volume mount) and container startup, there's a chance that the container startup will fail with error "no such file or directory: unknown". The error is transient and is recovered on container restart. However, if the container can't be restarted or the pod is deleted on container startup failure, it will appear as a final error.

#### What did you expect to happen?

Ideally we want to avoid this mount error from happening.

#### How can we reproduce it (as minimally and precisely as possible)?

ok I think I got a reliable repro:

first, save the following to `subpath.yaml`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-{{number}}
spec:
  volumes:
  - configMap:
      name: extra-cfg
    name: extra-cfg
  containers:
  - name: test
    image: ubuntu:latest
    command: ["bash", "-c"]
    args:
    - |
      echo "test test-pod-{{number}} running"
      sleep 25
    resources:
      requests:
        cpu: 10m
    volumeMounts:
      - name: extra-cfg
        mountPath: /etc/extra.ini
        subPath: extra.ini
---
apiVersion: v1
data:
  extra.ini: |
    somedata-{{number}}
kind: ConfigMap
metadata:
  name: extra-cfg
```

then, run the following script:

```sh
for i in {1..20}
do
  scp subpath.yaml tmp.yaml
  sed -i -e "s@{{number}}@$i@g" "tmp.yaml"
  k apply -f tmp.yaml
done
```

this will reliably reproduce the issue. However, from what I can see, this issue is transient and the pods will recover on the next container restart.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
```

</details>


#### Cloud provider

<details>
reproducible on GKE but not KIND, assuming that's because it's harder to hit this race condition on kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes中，如果在Pod启动（卷挂载）和容器启动之间，ConfigMap被patch更新，可能会导致容器启动失败，出现`no such file or directory: unknown`错误。该错误是暂时性的，容器可以通过重启恢复。但是，如果容器无法重启，或在启动失败时Pod被删除，则可能成为最终错误。

从安全角度分析：

- 攻击者需要具备修改ConfigMap的权限，才能在容器启动前patch ConfigMap，以触发该错误。
- 根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。
- 此问题可能导致拒绝服务（DoS）攻击，但需要一定权限，故风险评级不为高。
- 此问题不存在命令执行、容器逃逸、提权等高安全风险。

因此，该Issue存在安全风险，但风险评级在high以下，属于低风险。

---

## Issue #127666 Cloud provider detection functions are innaccurate and could lead to undefined behavior

- Issue 链接：[#127666](https://github.com/kubernetes/kubernetes/issues/127666)

### Issue 内容

#### What happened?

During the course of the work for [KEP-2395 (Removing in-tree Cloud Providers)](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2395-removing-in-tree-cloud-providers) the internal cloud provider loops have been removed but some of the warning and detection helper functions are no longer accurate and could theoretically cause undefined behaviors or panics.

For example, this function for detecting if a provider is deprecated:
```go
func IsDeprecatedInternal(name string) bool {
	for _, provider := range deprecatedCloudProviders {
		if provider.name == name {
			return true
		}
	}

	return false
}
```
relies on the `deprecatedCloudProviders` containing a list, but the actual definition is empty:
```go
deprecatedCloudProviders = []struct {
		name     string
		external bool
		detail   string
	}{}
```

This is one example of a systemic issue where several commands (kubelet, kube-controller-manager, and kube-apiserver) rely on this function to gate behavior, and warnings, about the in-tree controller loops. But, given the logic, these functions will not clearly advertise the state of the removed providers.

#### What did you expect to happen?

I would expect the functions related to detecting and warning about in-tree cloud provider loops to be relevant for any value of the `--cloud-provider` flag which is not `external`. The controller loops have been removed and there is technical debt surrounding the detection code that should be removed for the health of the project.

#### How can we reproduce it (as minimally and precisely as possible)?

Passing an incorrect value to the `--cloud-provider` flag for kubelet, kube-controller-manager, and kube-apiserver would be the primary way to see this problem, but we have done a fairly good job in preventing bad values from making it to these internal functions.

There are some conditions underwhich the kube-controller-manager has been observed to panic when a malformed value is passed, but this is not consistent and it is not clear what combination of flags is leading to the panic.

#### Anything else we need to know?

There is related code around the detection and use of the internal cloud provider loops that needs to be cleaned up. It does not appear to be negatively affecting the functioning of kubernetes, but this code is effectively not in use and could pose a security weakness as it continues to bit rot.

#### Kubernetes version

<details>

1.31 is the completion of KEP-2395 and as such all the internal loops have been removed, this change should be affected from 1.31 onwards.


</details>


#### Cloud provider

<details>

this affects any value of cloud provider that is not "external"
</details>


#### OS version

n/a

#### Install tools

n/a

#### Container runtime (CRI) and version (if applicable)

n/a

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在移除内置云提供商的过程中，一些用于检测和警告的辅助函数不再准确，可能导致未定义的行为或panic。例如，当向`--cloud-provider`参数传递不正确的值时，`kube-controller-manager`可能会在某些情况下发生panic。这可能导致服务的意外中断，属于拒绝服务（DoS）风险。

然而，利用此问题需要攻击者能够修改或配置`kubelet`、`kube-controller-manager`、`kube-apiserver`的启动参数，这通常需要管理员权限。根据风险判断标准的第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

因此，此问题存在安全风险，但风险评级在high以下，属于低风险。

---

## Issue #127576 Differing apt package dependencies between 1.29.8-1.29.9 and 1.28.13-1.28.14

- Issue 链接：[#127576](https://github.com/kubernetes/kubernetes/issues/127576)

### Issue 内容

#### What happened?

Installing a cluster from the official apt repos with Ansible and got a warning that socat was not installed. 
```
[init] Using Kubernetes version: v1.28.14
[preflight] Running pre-flight checks
        [WARNING FileExisting-socat]: socat not found in system path
```

Looking at the apt packages in the repos, we found that 1.29.8 had socat as a dependency and 1.29.9 does not. The same difference exists between 1.28.13 and 1.28.14.

(apt -a show kubelet on a host with 1.28 repos enabled)
```
Package: kubelet
Version: 1.28.14-2.1
Priority: optional
Section: net
Maintainer: Kubernetes Authors <dev@kubernetes.io>
Installed-Size: 111 MB
Depends: iptables (>= 1.4.21),kubernetes-cni (>= 1.2.0),iproute2,mount,conntrack,util-linux,ethtool,libc6
Homepage: https://kubernetes.io
Download-Size: 19.6 MB
APT-Manual-Installed: yes
APT-Sources: https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages
Description: Node agent for Kubernetes clusters
 Node agent for Kubernetes clusters.

Package: kubelet
Version: 1.28.13-1.1
Priority: optional
Section: net
Maintainer: Kubernetes Authors <dev@kubernetes.io>
Installed-Size: 111 MB
Depends: iptables (>= 1.4.21),kubernetes-cni (>= 1.2.0),iproute2,mount,socat,util-linux,ethtool,ebtables,conntrack,libc6
Homepage: https://kubernetes.io
Download-Size: 19.6 MB
APT-Sources: https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages
Description: Node agent for Kubernetes clusters
 Node agent for Kubernetes clusters.
```

(apt -a show kubelet on a host with 1.29 repos enabled)
```
Package: kubelet
Version: 1.29.9-1.1
Priority: optional
Section: net
Maintainer: Kubernetes Authors <dev@kubernetes.io>
Installed-Size: 113 MB
Depends: iptables (>= 1.4.21),kubernetes-cni (>= 1.2.0),iproute2,mount,conntrack,util-linux,ethtool,libc6
Homepage: https://kubernetes.io
Download-Size: 19.9 MB
APT-Sources: https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages
Description: Node agent for Kubernetes clusters
 Node agent for Kubernetes clusters.

Package: kubelet
Version: 1.29.8-1.1
Priority: optional
Section: net
Maintainer: Kubernetes Authors <dev@kubernetes.io>
Installed-Size: 113 MB
Depends: iptables (>= 1.4.21),kubernetes-cni (>= 1.2.0),iproute2,mount,socat,util-linux,ethtool,ebtables,conntrack,libc6
Homepage: https://kubernetes.io
Download-Size: 19.9 MB
APT-Sources: https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages
Description: Node agent for Kubernetes clusters
 Node agent for Kubernetes clusters.
```

Please note the differeng "Depends:" lines. It is not expected that these would change between bugfix releases. 


#### What did you expect to happen?

We did not expect to receive this warning. We expected socat to be installed as a dependency of kubelet as in previous versions. 

#### How can we reproduce it (as minimally and precisely as possible)?

Install and configure a cluster with either 1.29.9 or 1.28.14 (same behavior) on Ubuntu 22.04

#### Anything else we need to know?

```
# sudo cat /etc/apt/sources.list.d/kubernetes.list 
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.asc] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /
```

```
# sudo cat /etc/apt/sources.list.d/kubernetes.list 
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.asc] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /
```

#### Kubernetes version

<details>

```console
1.29.9
```

</details>
(behavior also exists in 1.28.14)

#### Cloud provider

<details>
installing on bare metal
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

Linux hostname.domain 6.8.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Sep 11 15:25:05 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
在此Issue中，kubelet软件包在升级后，其依赖项'socat'被移除，导致系统在执行'socat'时提示未找到。这会导致某些依赖'socat'的功能无法正常工作。

理论上，如果攻击者能够在系统的PATH环境变量中，以低权限用户在可写目录中放置一个恶意的'socat'可执行文件，当高权限进程尝试调用'socat'时，就可能执行恶意代码。

然而，在标准的系统配置中，PATH中的目录通常不可被非特权用户写入，因此攻击者难以利用该问题实施攻击。

根据风险判断标准：
- 虽然存在一定的安全风险，但由于攻击者需要系统存在不当的权限配置，才能利用该漏洞，风险较低。

因此，该Issue存在安全风险，但风险评级在high以下，判断为低风险。

---

## Issue #127553 watchlist request will be closed abnormally when cacheInterval contains a large amount of watchEvents

- Issue 链接：[#127553](https://github.com/kubernetes/kubernetes/issues/127553)

### Issue 内容

#### What happened?

I'm conducting an investigation into watchList benchmark in 1.30 Kubernetes cluster. I found that using `watchList` request to make the reflector become synced takes longer than using `list` request. 

Then, I found that the apiserver always prints `Forcing %s watcher close due to unresponsiveness: key.... ` log.  I realized that if `cacheInterval ` containes lots of watchEvents, then cacheWatcher.Process will not be invoked until all watchEvents in `cacheInterval` have been sent. As a result, the `input` channel in `cacheWatcher` may become full, leading to the abnormal closure of the watchlist request.

Therefore, are there any solutions to address this issue? Perhaps we need a new watchList request handler to process watchList requests instead of reusing the original watch request handler's logic. 

If there isn't a similar issue in this problem, we can use the current issue to track it. :)

#### What did you expect to happen?

Using watchList request to make the reflector become synced takes approximately the same amount of time as using list request.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create 5K Pods
2. use watchlist request to make reflector become synced. At the same time, start many client to update pods
3. watchlist request will be close and apiserver prints `Forcing %s watcher close due to unresponsiveness: key.... ` log

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes集群中，当`cacheInterval`包含大量的`watchEvents`时，使用`watchList`请求可能导致请求异常关闭，并出现`Forcing %s watcher close due to unresponsiveness`的日志。

从安全角度来看，攻击者可能利用此特性，通过大量创建和更新Pods，导致`apiserver`的`watchList`请求异常关闭，造成服务的拒绝服务（DoS）攻击。但需要注意的是，攻击者需要具备创建和修改Pods的权限，才能实施此攻击。

根据风险判断标准第4条，对于拒绝服务（DoS）攻击，如果攻击者需要一定权限才能实施该攻击，且需要具备创建、修改等非只读权限，则不应判断为高风险，CVSS评级在high以下。

因此，该Issue存在安全风险，但风险评级在high以下，应判断为低风险。

**复现过程：**

```json
[
  {
    "cmd": "for i in $(seq 1 5000); do kubectl run pod-$i --image=nginx --restart=Never & done",
    "explain": "创建5000个Pod，名称为pod-1到pod-5000"
  },
  {
    "cmd": "kubectl get pods --watch",
    "explain": "使用watchList请求，让reflector开始同步"
  },
  {
    "cmd": "for i in $(seq 1 5000); do kubectl patch pod pod-$i -p '{\"metadata\": {\"annotations\": {\"timestamp\": \"$(date +%s)\"}}}' & done",
    "explain": "启动多个客户端，更新每个Pod的annotations，触发大量的watchEvents"
  }
]
```

---

## Issue #127410 Setting externalIPs to the same IP as one node, renders the node unaccessible

- Issue 链接：[#127410](https://github.com/kubernetes/kubernetes/issues/127410)

### Issue 内容

#### What happened?

We switched to use ipvs-mode for kube-proxy and by mistake we had a number of services defining the field "externalIPs" to the IP-address of one of the nodes.
This resulted in that this node became unavailable from a number of other nodes, not even by ping. As far as I could see the nodes affected where nodes running pods for those bad services.
What happened was that kube-proxy created a route entry in the table "local" for the unavailable node on each of the nodes running the affected pods. (A local route will then not forward traffic but just make the traffic go to current node)

#### What did you expect to happen?

kube-proxy should not have allowed this externalIPs address, or at least not created the route entry in the local table

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod and corresponding service where the service externalIPs is set to one of the worker nodes real IP. Make sure to set kube-proxy to run in ipvs-mode.

#### Anything else we need to know?

We are using two IP-addresses on our nodes for more IP-range. I don't think this would affect this situation but just for info.

#### Kubernetes version

<details>

```console
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>
Locally bare-metal cluster.
</details>


#### OS version

<details>

```console
Linux px2csw20 5.15.0-121-generic #131-Ubuntu SMP Fri Aug 9 08:29:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Manual install but maintenance with puppet 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel, rook-ceph
</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes集群中，误将服务的`externalIPs`字段设置为某个节点的IP地址，导致该节点在集群中变得不可访问的情况。具体来说，当kube-proxy以ipvs-mode运行时，如果Service的`externalIPs`被设置为集群中某个节点的IP地址，kube-proxy会在其他运行受影响Pod的节点上为该IP地址创建一条本地路由。这会导致针对该节点IP的流量被本地节点拦截，无法真正到达目标节点，导致拒绝服务。

从安全角度看，如果攻击者具备在集群中创建或修改Service的权限，并且可以将`externalIPs`设置为目标节点的IP地址，则可以导致该节点在网络上不可达，造成拒绝服务攻击。这种情况下，低权限用户可能通过创建恶意Service来影响到其他节点的正常运行。

根据风险判断标准：

1. **该风险能被攻击者利用**（满足）。
2. **该风险有可能成为一个漏洞**，但**攻击者需要具备创建、修改等非只读权限**。
3. 根据标准4，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

因此，该风险存在，但风险评级为**低风险**。

---

## Issue #127401 Missing details in nodeAffinity's API specification

- Issue 链接：[#127401](https://github.com/kubernetes/kubernetes/issues/127401)

### Issue 内容

#### What happened?

> Orinigal issue: https://github.com/kubernetes/kubernetes/issues/126531
> This issue is re-created here instead of in `kubernetes/website` because the API specification in `kubernetes/kubernetes` needs to be fixed.

The `pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields` is introduced in PR [#62202](https://github.com/kubernetes/kubernetes/pull/62002) and used to bind a pod directly to nodes via `metadata.name`. However, we discovered some hidden constraints on this field that are not documented in the API specification.

More concretely, the current API specification only says that
```
> kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields
    KIND:       Pod
    VERSION:    v1

    FIELD: matchFields <[]NodeSelectorRequirement>


    DESCRIPTION:
        A list of node selector requirements by node's fields.
        A node selector requirement is a selector that contains values, a key, and
        an operator that relates the key and values.
    
    FIELDS:
      key   <string> -required-
        The label key that the selector applies to.
    
      operator      <string> -required-
      enum: DoesNotExist, Exists, Gt, In, ....
        Represents a key's relationship to a set of values. Valid operators are In,
        NotIn, Exists, DoesNotExist. Gt, and Lt.
    
        Possible enum values:
         - `"DoesNotExist"`
         - `"Exists"`
         - `"Gt"`
         - `"In"`
         - `"Lt"`
         - `"NotIn"`
    
      values        <[]string>
        An array of string values. If the operator is In or NotIn, the values array
        must be non-empty. If the operator is Exists or DoesNotExist, the values
        array must be empty. If the operator is Gt or Lt, the values array must have
        a single element, which will be interpreted as an integer. This array is
        replaced during a strategic merge patch.
```
**This does not specify that its key must be `metadata.name`, its operator must be `In` or `NotIn`, and the number of its values must be exactly one (though the term itself can be defined multiple times in `nodeSelectorTerms`)**

**And, the value in values must match this regex expression: `[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)`**

>Invalid value: \"f-RohHl\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')]

This issue has also caused confusion in [#115980](https://github.com/kubernetes/kubernetes/issues/115980), [#81725](https://github.com/kubernetes/kubernetes/issues/81725), [#78238](https://github.com/kubernetes/kubernetes/issues/78238) and a reddit [thread](https://stackoverflow.com/questions/67018171/kubernetes-what-are-valid-node-fields)



#### What did you expect to happen?

The hidden constraints should be explicitly documented in the API specification.

#### How can we reproduce it (as minimally and precisely as possible)?

Read the output of `kubectl describe` and the API specification.

#### Anything else we need to know?

/sig docs scheduling

#### Kubernetes version

Since 1.11, exists in 1.30

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出了Kubernetes API文档中关于`nodeAffinity`的`matchFields`字段缺少详细说明，特别是其使用中的一些隐藏限制条件未被记录。这些限制包括：

- `key`必须是`metadata.name`
- `operator`必须是`In`或`NotIn`
- `values`的数量必须正好是一个
- `values`中的值必须符合正则表达式`[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*`，即符合RFC 1123子域名格式

由于这些限制未在API规范中明确说明，可能导致用户在配置Pod的节点亲和性时产生误解或错误配置。这可能导致Pod无法调度成功，或者意外地调度到错误的节点上。

从安全角度考虑，如果攻击者能够利用这些未明确的限制，诱导管理员配置错误的节点亲和性策略，可能会导致Pod被调度到非预期的节点上，可能存在安全隐患。然而，这需要攻击者有能力影响管理员的配置，或者拥有对Pod规格的修改权限。

根据风险判断标准：

1. 该风险需要攻击者具备一定的权限才能利用，且需要管理员的错误配置。
2. 此问题不会直接导致高危漏洞，如命令执行、容器逃逸或提权。
3. 攻击者无法单纯通过该问题提升自身权限或影响其他高权限用户。

因此，该问题的风险评级为低风险。

---

## Issue #127294 Invoke the aggregation service interface. The response headers contain duplicates

- Issue 链接：[#127294](https://github.com/kubernetes/kubernetes/issues/127294)

### Issue 内容

#### What happened?

![image](https://github.com/user-attachments/assets/876aede5-0724-4fdb-ab7a-fb6ee4f4cfbd)
When we access the aggregation service through the apiserver, the response of the request contains duplicate information, which is obviously unreasonable.

#### What did you expect to happen?

There should be no duplicate fields

#### How can we reproduce it (as minimally and precisely as possible)?

Accessing Aggregation Services Through API Server

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出，通过apiserver访问聚合服务时，响应中包含了重复的HTTP头信息。这种不合理的响应可能导致客户端在解析HTTP响应时产生混淆，进而可能引发安全问题。例如，重复的头信息可能导致某些安全策略失效，或者被攻击者利用特定的客户端漏洞进行攻击。然而，根据当前提供的信息，并没有证据表明攻击者可以利用这些重复的头信息进行恶意操作，如命令执行、提权或容器逃逸等高风险行为。根据风险判断标准，此问题可能存在一定的安全风险，但风险评级在High以下，属于低风险。

---

## Issue #127282 [FG:InPlacePodVerticalScaling] api server "INTERNAL_ERROR; received from peer" while executing kubectl  replace 

- Issue 链接：[#127282](https://github.com/kubernetes/kubernetes/issues/127282)

### Issue 内容

#### What happened?

error on kubectl replace cmd : error: error when replacing "pod-updated-example.yaml": Put "https://localhost:6443/api/v1/namespaces/default/pods/high-priority?fieldManager=kubectl-replace&fieldValidation=Strict": stream error: stream ID 5; INTERNAL_ERROR; received from peer 

On api server logs : 
```
E0911 00:12:42.298940  214100 wrap.go:57] "apiserver panic'd" method="PUT" URI="/api/v1/namespaces/default/pods/high-priority?fieldManager=kubectl-replace&fieldValidation=Strict" auditID="d82cb6f9-3244-4002-8575-ffa99805d451"
http2: panic serving 127.0.0.1:44790: runtime error: index out of range [1] with length 1
goroutine 44578 [running]:
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()
        k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0xa5
panic({0x33a7b20?, 0xc00af7b950?})
        runtime/panic.go:770 +0x132
k8s.io/kubernetes/pkg/api/pod.MarkPodProposedForResize(0xc00a9c8488, 0xc00a9c8d88)
        k8s.io/kubernetes/pkg/api/pod/util.go:1238 +0x371
k8s.io/kubernetes/pkg/registry/core/pod.podStrategy.PrepareForUpdate({{0x7f6ce61f0518?, 0x5a45760?}, {0xc0077da860?, 0x145d6e5?}}, {0xa?, 0x35677e0?}, {0x3c62268?, 0xc00a9c8d88}, {0x3c62268, 0xc00a9c8488})
        k8s.io/kubernetes/pkg/registry/core/pod/strategy.go:110 +0xe9
k8s.io/apiserver/pkg/registry/rest.BeforeUpdate({0x3c9e980, 0xc000626520}, {0x3c88f40, 0xc007016b40}, {0x3c62268, 0xc00a9c8d88}, {0x3c62268, 0xc00a9c8488})
        k8s.io/apiserver/pkg/registry/rest/update.go:129 +0x243
k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update.func1({0x3c62268, 0xc00a9c8488}, {0xc00947d3b0?, 0x35f7afb?})
```



#### What did you expect to happen?

api server return this error : The Pod "XXX" is invalid: spec.containers: Forbidden: pod updates may not add or remove containers 

#### How can we reproduce it (as minimally and precisely as possible)?

1-start the cluster with FEATURE_GATE ENABLED (InPlacePodVerticalScaling)
2- create a pod with one container , for example : 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Mi"
        cpu: "10m"
      limits:
        memory: "10Mi"
        cpu: "10m"
```

3- try  to replace (without force) with ( + container) : 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Mi"
        cpu: "10m"
      limits:
        memory: "10Mi"
        cpu: "10m"
  - name: high-priority-1
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Mi"
        cpu: "10m"
      limits:
        memory: "10Mi"
        cpu: "10m"

```

kubectl  replace -f pod-updated-example.yaml  

**ERROR** : Put "https://localhost:6443/api/v1/namespaces/default/pods/high-priority?fieldManager=kubectl-replace&fieldValidation=Strict": stream error: stream ID 5; INTERNAL_ERROR; received from peer 

#### Anything else we need to know?

based on the master branch (ref commit : 139cc3c659dd1624f7a4bbcc3b07fda79539677a )


#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.31.0-beta.0.854+4cfdad0935cfb7-dirty
```

</details>


#### Cloud provider

<details>
locally 
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux 20231797-MARBT 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerD
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
此次Issue涉及到当启用`InPlacePodVerticalScaling`特性时，API Server在处理对Pod的替换操作中，如果尝试添加新的容器，会引发API Server内部的panic错误（索引越界），导致请求失败。

攻击者可能利用这一漏洞，通过发送特制的请求，导致API Server的服务异常，形成拒绝服务（DoS）攻击。但是，由于需要具有修改Pod的权限才能执行该操作，攻击者需要具备一定的权限。

根据风险判断标准第4条，对于需要一定权限才能实施的拒绝服务攻击，应当适当降低风险评级。当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。

因此，该Issue存在安全风险，但风险评级在high以下，判断为低风险。

**复现过程：**

```json
[
  {
    "cmd": "cat > pod-example.yaml <<EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: high-priority\nspec:\n  containers:\n  - name: high-priority\n    image: ubuntu\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"10Mi\"\n        cpu: \"10m\"\n      limits:\n        memory: \"10Mi\"\n        cpu: \"10m\"\nEOF",
    "explain": "创建原始的Pod定义文件 pod-example.yaml"
  },
  {
    "cmd": "kubectl apply -f pod-example.yaml",
    "explain": "创建Pod"
  },
  {
    "cmd": "cat > pod-updated-example.yaml <<EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: high-priority\nspec:\n  containers:\n  - name: high-priority\n    image: ubuntu\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"10Mi\"\n        cpu: \"10m\"\n      limits:\n        memory: \"10Mi\"\n        cpu: \"10m\"\n  - name: high-priority-1\n    image: ubuntu\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"10Mi\"\n        cpu: \"10m\"\n      limits:\n        memory: \"10Mi\"\n        cpu: \"10m\"\nEOF",
    "explain": "创建修改后的Pod定义文件 pod-updated-example.yaml，增加了一个容器"
  },
  {
    "cmd": "kubectl replace -f pod-updated-example.yaml",
    "explain": "尝试替换Pod，触发API Server的panic错误"
  }
]
```

---

## Issue #127262 [FG:InPlacePodVerticalScaling] Static CPU management policy alongside InPlacePodVerticalScaling

- Issue 链接：[#127262](https://github.com/kubernetes/kubernetes/issues/127262)

### Issue 内容

#### What happened?

Container CPUset allocations not updated for Guaranteed QoS Pod ( Integer CPU limits = CPU requests, after Inplace Pod updates with Static CPU Management policy alongside InPlacePodVerticalScaling. 

Static CPU management policy is not supported with this feature, known issue ( ref: https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/#known-issues )

#### What did you expect to happen?

Container CPU set container allocation to be updated if accepted

#### How can we reproduce it (as minimally and precisely as possible)?

From https://github.com/kubernetes/enhancements/issues/2838

Tested the changes being done as a part of KEP --> In Place update of Pod resources (https://github.com/kubernetes/enhancements/issues/1287)

Added the below kubernetes flag before building k8s locally to the hack/local_cluster_up.sh script file. (Note i had created a vm machine with 8 cpus...)

kuberneted flags :-
"--topology-manager-policy=best-effort"
"--cpu-manager-policy=static"
"--reserved-cpus=0,1"

I then created a pod with limit and cpu both assigned as 2.. the pod got successfully created. I then logged in the pod and then checked its cpu set...there it was showing cpu core 3-4 assigned to the pod.. which is fine/correct.

Now i updated the pod limit and cpu to 3 each.. the pod got successfully updated as well...However cpuset assigned to the container was not updated in this case when i logged into the pod and verified..It was same as previous...Not sure if you have checked this thing at your end ??

Thanks & Regards,
ANkit Nigam

#### Anything else we need to know?

As decided in SIG Node meeting, this issue replaces https://github.com/kubernetes/enhancements/issues/2838

#### Kubernetes version

<details>
Kubernetes versions with InPlacePodVerticalScaling > 1.27
</details>


#### Cloud provider

<details>
Independent  of Cloud provider
</details>

#### OS version

<details>
Independent of OS
</details>


#### Install tools

<details>
Independent of install tools
</details>

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes使用Static CPU管理策略与InPlacePodVerticalScaling特性时，更新Pod的CPU资源后，Guaranteed QoS Pod的容器CPUset分配未更新的问题。根据官方文档，这是一个已知问题，Static CPU管理策略目前不支持InPlacePodVerticalScaling特性。

从安全角度分析：

1. **攻击者利用难度**：攻击者需要拥有修改Pod资源的权限，才能触发该问题。
2. **可能的影响**：容器的CPU资源未按照预期更新，可能导致资源分配不均或性能问题。但这不会导致权限提升、容器逃逸等高风险安全问题。
3. **风险评级依据**：
   - 根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。
   - 此问题不涉及命令执行、容器逃逸、提权等高安全风险的问题。

综上所述，该Issue涉及的安全风险评级为低风险。

---

## Issue #127229 volume leak when delete a pod with inline csi during node reboot

- Issue 链接：[#127229](https://github.com/kubernetes/kubernetes/issues/127229)

### Issue 内容

#### What happened?

we find that there are always a lot of unmounted volumes on node, like this:
```bash
[root@XXXX ~]# lsblk
NAME                                                                                          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdc                                                                                             8:32   0  6.6T  0 disk 
├─sdc2                                                                                          8:34   0  4.7T  0 part 
│ ├─volumevg-csi--c74d048f8fbeb2932e2e8d107bf2bfef58d582a9afcd7f0bfb3df6cc5f34906f            253:5    0    1G  0 lvm  
```
these volumes are not used by any pods yet, but are always on the node, resulting in new `lv` object can't being created

We found that this lv belongs to the pod `19022a9f-7074-4caf-8ce8-8f48e412b28f`
```
csi_client.go:271] kubernetes.io/csi: calling NodePublishVolume rpc [volid=csi-c74d048f8fbeb2932e2e8d107bf2bfef58d582a9afcd7f0bfb3df6cc5f34906f,target_path=/var/lib/kubelet/pods/19022a9f-7074-4caf-8ce8-8f48e412b28f/volumes/kubernetes.io~csi/my-csi-volume/mount]
```
and the following logs every 2s:

<img width="1358" alt="image-20240909" src="https://github.com/user-attachments/assets/d4b6f873-ab73-48dd-b544-feffcbcc68e3">

But later the pod was deleted, we found that this pod did not call `NodeUnpublishVolume`, because the csi driver was not registered， logs as follows:
```bash
Operation for "{volumeName:my-csi-volume podName:19022a9f-7074-4caf-8ce8-8f48e412b28f nodeName:}" failed. No retries permitted until 2024-09-05 20:49:02.855613876 +0800 CST m=+7.656880276 (durationBeforeRetry 500ms). Error: "UnmountVolume.TearDown failed for volume \"\" (UniqueName: \"my-csi-volume\") pod \"19022a9f-7074-4caf-8ce8-8f48e412b28f\" (UID: \"19022a9f-7074-4caf-8ce8-8f48e412b28f\") : kubernetes.io/csi: mounter.SetUpAt failed to get CSI client: driver name XXXX not found in the list of registered CSI drivers"
```
This volume has been attached and mounted before, but the `volumeManager` needs to update the cache again when  `kubelet` restarts. 
However, the csi driver has not yet been registered and the volume is marked as unmounted. If you delete the pod at this time,only the information in the `DSW` cache needs to be deleted, there is no need to call csi. 
Therefore, it is possible that this volume does not call NodeUnpublishVolume.

#### What did you expect to happen?

release unused volumes.

#### How can we reproduce it (as minimally and precisely as possible)?

step1. create a inline csi pod, like this:
```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app-inline-volume
spec:
  containers:
  - name: my-frontend
    image: busybox:latest
    command: [ "sleep", "100000" ]
    volumeMounts:
    - mountPath: "/data"
      name: my-csi-volume
  volumes:
  - name: my-csi-volume
    csi:
      driver: XXXX
      fsType: "ext4"
      volumeAttributes:
        size: "1Gi"
```
step2. reboot the node
step3. delete the pod created in step1 when node `notReady`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
version: 1.16.9 with some internal patch features.
```

</details>




#### Cloud provider

<details>
internal cluster, slef-built
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在节点重启过程中，如果CSI驱动尚未注册，当删除使用inline CSI卷的Pod时，可能导致卷未正确卸载和清理，造成节点上存在未使用但未清理的卷。这些残留的卷会导致资源泄漏，随着时间推移，可能导致新卷无法创建，造成资源耗尽。

从安全角度来看，攻击者可能利用该问题，通过反复在节点未就绪状态下创建并删除Pod，导致节点资源耗尽，产生拒绝服务（DoS）攻击。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者可以利用该漏洞造成资源耗尽。
2. **风险评级不在High以上**：根据标准4，涉及拒绝服务（DoS）攻击时，如果攻击者需要具备创建、修改等非只读权限，则不应判断为高风险。

因此，综合判断，该问题存在安全风险，但风险评级在High以下，属于低风险。

---

## Issue #127170 Add support for applying fsgroup with ReadWriteOncePod volume type

- Issue 链接：[#127170](https://github.com/kubernetes/kubernetes/issues/127170)

### Issue 内容

#### What happened?

Hello,

Copied from - https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/1982 

I think I've found a bug with the new ReadWriteOncePod access mode in the latest EBS CSI driver

What happened?

When deploying a statefulset, I realized that the ReadWriteOncePod access mode does not respect fsGroup and fsGroupChange. When using that access modes, the disk is mounted with root:root owner and the process cannot write into the disk.



#### What did you expect to happen?

The volume is mounted into the pod with the right mode, and the process can write to the disks. ,

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod that uses ReadWriteOncePod

#### Anything else we need to know?

_No response_

#### Kubernetes version

Any

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了在使用`ReadWriteOncePod`访问模式时，`fsGroup`和`fsGroupChange`没有被正确应用，导致卷以`root:root`的所有者被挂载，进程无法写入磁盘。这会导致应用程序无法正常工作。

从安全角度来看，由于卷被挂载为`root:root`，非特权进程无法写入，反而限制了权限，并未引入新的安全风险。虽然这影响了服务的可用性，但并不涉及攻击者利用或权限提升。

根据风险判断标准：

1. 该问题未被攻击者利用（标准1）。
2. 不太可能成为一个漏洞，被分配CVE编号，且CVSS评分不会在high以上（标准2）。
4. 这是一个功能性问题，导致拒绝服务，但攻击者需要具备创建或修改权限（标准4）。
7. 因此，风险评级判断为低风险。

---

## Issue #127105 Garbage collector never starts when it fails initial cache sync

- Issue 链接：[#127105](https://github.com/kubernetes/kubernetes/issues/127105)

### Issue 内容

#### What happened?

When running a kind cluster in HA mode, if KCM's garbage collector fails its initial sync, it never starts. As a result, completed job pods are never cleaned up after the `ttlSecondsAfterFinished ` is reached.

#### What did you expect to happen?

I'd expect that one resource failing to sync would not block the garbage collection of other resources.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a Kind cluster in [HA mode](https://kind.sigs.k8s.io/docs/user/quick-start/#control-plane-ha).
2. Create the CRD, a custom resource with two versions and webhook strategy set to conversion ([crd.yaml](https://gist.github.com/rschalo/5a290a87a7c995cdfdd44e1165a2eac2#file-crd-yaml) and [cr.yaml](https://gist.github.com/rschalo/5a290a87a7c995cdfdd44e1165a2eac2#file-cr-yaml))
3. Delete all KCM pods
4. Create the job from the [job.yaml](https://gist.github.com/rschalo/5a290a87a7c995cdfdd44e1165a2eac2#file-job-yaml)

Expect to see similar to the following in KCM logs:
```
E0904 01:50:06.546595       1 shared_informer.go:316] unable to sync caches for garbage collector
E0904 01:50:06.546663       1 garbagecollector.go:268] timed out waiting for dependency graph builder sync during GC sync (attempt 1)
I0904 01:50:06.653410       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
```

And when checking the pod and job:
```
❯ k get pod
NAME        READY   STATUS      RESTARTS   AGE
pi2-8lb5b   0/1     Completed   0          15m

❯ k get job
NAME   STATUS     COMPLETIONS   DURATION   AGE
pi2    Complete   1/1           33s        15m
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
kind v0.23.0 go1.22.3 darwin/arm64
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes集群中，当Kube Controller Manager（KCM）的垃圾回收器在初始同步缓存失败时，垃圾回收器不会启动，导致已经完成的Job Pods在达到`ttlSecondsAfterFinished`后也不会被清理。这可能导致资源泄漏，随着时间推移，占用大量的集群资源。

从安全风险的角度来看，如果攻击者能够触发垃圾回收器的初始同步缓存失败，可能导致拒绝服务（DoS）攻击。然而，触发该问题需要满足以下条件：

- **创建CRD并设置Webhook转换策略**：这需要集群管理员权限，普通用户无法执行。
- **删除所有KCM Pods**：这也是高权限操作，普通用户无法执行。

根据风险判断标准：

1. **风险利用性**：需要高权限才能被利用，普通攻击者无法实施。
2. **风险评级**：由于需要高权限，且漏洞利用需要创建、修改等非只读权限，CVSS评分在High以下。
4. **拒绝服务攻击降级处理**：当攻击者需要一定权限才能实施拒绝服务攻击，应降低风险评级。

综上，该Issue存在安全风险，但风险评级在High以下，属于低风险。

---

## Issue #127056 Panic when comparing two interface.

- Issue 链接：[#127056](https://github.com/kubernetes/kubernetes/issues/127056)

### Issue 内容

#### What happened?

When I used ArgoCD, I found [this line](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/strategicpatch/patch.go#L955) will result in panic if two compared values are `map[string]interface`. The call was initiated from [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/strategicpatch/patch.go#L2105). If the passed in entry does not follow the schema, it may result the two compared values are `map[string]interface` and will raise panic.

#### What did you expect to happen?

From the function `CreateThreeWayMergePatch`'s comments, it should either return the error said the input does not follow the schema or do the comparison. Panic should not be raised. I proposed reflect.DeepEqual could be used instead of "=="

#### How can we reproduce it (as minimally and precisely as possible)?

If the struct does not follow the schema and give one field as map[string]interface instead of string, and calling `CreateThreeWayMergePatch`, the panic will be raised.

#### Anything else we need to know?

I could raise PR if it does make sense.

#### Kubernetes version

1.29.5

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出，当调用`CreateThreeWayMergePatch`函数时，如果输入的结构不符合预期的schema，例如某个字段是`map[string]interface{}`而不是`string`，会导致在比较两个`interface{}`类型的值时，由于未正确处理`map`类型的比较，触发`panic`。在Go语言中，直接比较两个`map`类型的值会引发运行时错误。

从安全角度考虑，如果攻击者能够构造特定的输入数据，导致服务器或应用程序产生`panic`崩溃，可能引发拒绝服务（DoS）攻击。然而，实施这样的攻击通常需要一定的权限，例如能够发送特定的API请求或修改资源，且这些操作需要通过身份验证和授权。因此，攻击者需要具备创建或修改权限，风险评级应为低风险。

根据风险判断标准，拒绝服务风险在攻击者需要一定权限才能实施攻击的情况下，不应判定为高风险，CVSS评级在high以下。

---

# ✅ 不涉及安全风险的 Issues (70 个)

## Issue #127840 `/proxy/stats/summary` returning incorrect PVC capacity bytes

- Issue 链接：[#127840](https://github.com/kubernetes/kubernetes/issues/127840)

### Issue 内容

<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately see https://github.com/kubernetes/kube-state-metrics/blob/main/SECURITY.md
-->

**What happened**:

When querying `kubectl get --raw /api/v1/nodes/<node-ip>/proxy/stats/summary`, the capacity bytes returned for PVCs are not what are shown in tools like k9s which therefore tell me that tooling built around this endpoint will show incorrect values.

**What you expected to happen**:

PVC capacity and other metrics show the correct sizes

**How to reproduce it (as minimally and precisely as possible)**:

```bash
$ kubectl get --raw "/api/v1/nodes/ip-<>.ec2.internal/proxy/stats/summary"
...
        {
          "time": "2024-09-30T18:27:06Z",
          "availableBytes": 467653439488,
          "capacityBytes": 1623168045056,
          "usedBytes": 1155497828352,
          "inodesFree": 100516333,
          "inodes": 100663296,
          "inodesUsed": 146963,
          "name": "pvc-data",
          "pvcRef": {
            "name": "name-pvc",
            "namespace": "namespace-pvc"
          }
        },
...
```

Take capacity bytes, convert to GiB/MiB, compare with k9s or other tooling for `spec.resources.requests.storage`

I used: `kubectl get pvc name-pvc -o json` to get the spec requests to compare against (which was larger by 125 MiB):
```
    },
    "spec": {
        "accessModes": [
            "ReadWriteOnce"
        ],
        "resources": {
            "requests": {
                "storage": "9600Gi"
            }
        },
```

**Anything else we need to know?**:

**Environment**:

* kube-state-metrics version:
* Kubernetes version (use `kubectl version`): v1.27.14
* Cloud provider or hardware configuration: AWS EKS
* Other info:


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`kubectl get --raw /api/v1/nodes/<node-ip>/proxy/stats/summary`命令查询PVC（Persistent Volume Claim）的容量信息时，返回的`capacityBytes`与实际预期值不符的问题。这导致了工具显示的PVC容量信息不正确。

从Issue的描述来看，这是一个关于PVC容量数据不一致的BUG，属于功能性问题，并未涉及任何安全风险。

根据风险判断标准：

- **标准6**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---

## Issue #127748 KMS V2 API Status() message comes every 20s instead of 1m

- Issue 链接：[#127748](https://github.com/kubernetes/kubernetes/issues/127748)

### Issue 内容

#### What happened?

Hello,
during working with KMS v2 API (K8S 1.29), I noticed that the Status() message is sent approx. every 20s while the KEP for V2 (https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#key_id-and-rotation) mentions "about every minute".
In this situation, under a minute, KMS plugin receives 3 calls (20s), while 1m would cause 1 call - it has the potential to "flood" the remote kms server with unnecessary requests.

#### What did you expect to happen?

API server to poll KMS v2 plugin every 1 minute.
On checking the code, I came around the following function: https://github.com/kubernetes/kubernetes/blob/release-1.29/staging/src/k8s.io/apiserver/pkg/server/options/encryptionconfig/config.go#L345

"func (h *kmsv2PluginProbe) check(ctx context.Context) error" is using constants "kmsPluginHealthzPositiveTTL" (20s) and "kmsPluginHealthzNegativeTTL" (3s) which supports the observed behavior.
However, there are constants defined: "kmsv2PluginHealthzPositiveInterval" (1min) and "kmsv2PluginHealthzNegativeInterval" (10) which are the ones mentioned in the KEP 3299.


#### How can we reproduce it (as minimally and precisely as possible)?

Check any KMS plugin with V2 support and verify the timestamp of an arriving Status() call from API server.

#### Anything else we need to know?

Fix proposal: swap the timeouts.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
local kind cluster:
$ kind --version
kind version 0.23.0

Problem affects api server code from 1.29 up, provider seems to be irrelevant
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.4 (Seafoam Ocelot)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.4 (Seafoam Ocelot)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.4"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
SUPPORT_END=2032-06-01

$ uname -a
Linux localhost.localdomain 5.14.0-427.22.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Jun 23 17:57:52 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，KMS V2 API的Status()消息每20秒发送一次，而不是预期的1分钟，这可能导致KMS插件收到更多的请求，可能会对系统性能产生影响。但从安全风险的角度来看，这主要是一个性能和资源优化的问题，并不涉及安全漏洞。

根据风险判断标准：

- **第4条**：当风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能实施攻击，且需要创建、修改等非只读权限，则不应判断为高风险。
- **第6条**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

在此情境下，没有证据表明攻击者可以利用该问题进行攻击，也没有涉及潜在的安全漏洞。因此，此Issue不涉及安全风险。

---

## Issue #127732 Error from server (Forbidden): jobs.batch is forbidden: User "system:node:k8s-master" cannot list resource "jobs" in API group "batch" in the namespace "default"

- Issue 链接：[#127732](https://github.com/kubernetes/kubernetes/issues/127732)

### Issue 内容

#### What happened?

My kubernetes cluster was not available after a power outage and restart. A check revealed that etcd was not started. After deleting the files in /var/lib/etcd, it started normally.

```
[root@k8s-master ~]# kubectl get nodes --show-labels
NAME         STATUS   ROLES    AGE   VERSION   LABELS
k8s-master   Ready    <none>   48m   v1.28.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux
k8s-node01   Ready    <none>   32m   v1.28.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux
```
I noticed that it was all node and no master, and then I checked all the resources in the default namespace and got an error:

```
[root@k8s-master ~]# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   46m
Error from server (Forbidden): replicationcontrollers is forbidden: User "system:node:k8s-master" cannot list resource "replicationcontrollers" in API group "" in the namespace "default"
Error from server (Forbidden): daemonsets. apps is forbidden: User "system:node:k8s-master" cannot list resource "daemonsets" in API group "apps" in the namespace "default"
Error from server (Forbidden): deployments. apps is forbidden: User "system:node:k8s-master" cannot list resource "deployments" in API group "apps" in the namespace "default"
Error from server (Forbidden): replicasets. apps is forbidden: User "system:node:k8s-master" cannot list resource "replicasets" in API group "apps" in the namespace "default"
Error from server (Forbidden): statefulsets. apps is forbidden: User "system:node:k8s-master" cannot list resource "statefulsets" in API group "apps" in the namespace "default"
Error from server (Forbidden): horizontalpodautoscalers. autoscaling is forbidden: User "system:node:k8s-master" cannot list resource "horizontalpodautoscalers" in API group "autoscaling" in the namespace "default"
Error from server (Forbidden): cronjobs. batch is forbidden: User "system:node:k8s-master" cannot list resource "cronjobs" in API group "batch" in the namespace "default"
Error from server (Forbidden): jobs. batch is forbidden: User "system:node:k8s-master" cannot list resource "jobs" in API group "batch" in the namespace "default"
```


#### What did you expect to happen?

Should execute normally

#### How can we reproduce it (as minimally and precisely as possible)?

Delete the files in /var/lib/etcd

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
root@k8s-master ~]# kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.0

```

</details>


#### Cloud provider

<details>
nil
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在电源故障重启后，用户的Kubernetes集群中的etcd未能启动，用户删除了/var/lib/etcd目录下的文件后，虽然etcd能够启动，但在使用`kubectl get all`命令时出现了权限不足的错误。错误信息显示用户“system:node:k8s-master”无法列出默认命名空间中的各种资源。

根据风险判断标准：

1. 该问题是由于用户在操作中删除了etcd的数据文件，导致权限配置出现问题。
2. 这个问题是由错误的操作和配置导致的权限问题，并非Kubernetes本身的安全漏洞。
3. 没有提及任何可被攻击者利用的漏洞或敏感信息泄露。

因此，该Issue不涉及安全风险。

---

## Issue #127729 /sig Network The InternalIP of the master node is abnormal

- Issue 链接：[#127729](https://github.com/kubernetes/kubernetes/issues/127729)

### Issue 内容

#### What happened?

When I "kubectl describe" the master node, the InternalIP field is 192.168.0.133. This is another network interface's IP, **not** the specified IP address 192.168.1.133 during "kubeadm init". By the way, 192.168.1.133 is the IP address of one of network interfaces on the master node. 

#### What did you expect to happen?

The InternalIP becomes the specified IP address.

#### How can we reproduce it (as minimally and precisely as possible)?

1. kubeadm init --pod-network-cidr=133.133.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.1.133 (where 192.168.1.133 is the address of one of the network interfaces on the master node)
2. mkdir -p $HOME/.kube
3. sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
4. sudo chown $(id -u):$(id -g) $HOME/.kube/config
5. kubectl apply -f kube-flannel.yml
6. kubectl describe nodes/ft-d2000-2-1 (where ft-d2000-2-1 is the node name of the master node)

#### Anything else we need to know?

1. Part of the file kube-flannel.yml:
apiVersion: v1
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "133.133.0.0/16",
      "EnableNFTables": false,
      "Backend": {
        "Type": "vxlan"
      }
    }

2. Part of the output of "ifconfig":
enaftgm1i0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.0.133  netmask 255.255.255.0  broadcast 192.168.0.255
        inet6 fe80::8689:88ff:fe10:ffc  prefixlen 64  scopeid 0x20<link>
        ether 84:89:88:10:0f:fc  txqueuelen 1000  (Ethernet)
        RX packets 111  bytes 12995 (12.9 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 60  bytes 6868 (6.8 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        device interrupt 9  base 0x2000  

enp17s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.133  netmask 255.255.255.0  broadcast 192.168.1.255
        inet6 fe80::4e74:a7ff:fe40:a404  prefixlen 64  scopeid 0x20<link>
        ether 4c:74:a7:40:a4:04  txqueuelen 1000  (Ethernet)
        RX packets 4224  bytes 315667 (315.6 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 190  bytes 24877 (24.8 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

3. The content of the file /etc/network/interfaces:
source-directory /etc/network/interfaces.d
auto enaftgm1i0
iface enaftgm1i0 inet dhcp
nameservers 8.8.8.8

iface enp17s0f0 inet static
gateway 192.168.1.1

4. The content of the file /etc/NetworkManager/NetworkManager.conf:
[main]
plugins=ifupdown,keyfile

[ifupdown]
managed=true

[device]
wifi.scan-rand-mac-address=no
5. The output of "ip route show":
default via 192.168.1.133 dev enp17s0f0 scope link 
169.254.0.0/16 dev enp17s0f0 scope link metric 1000 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
192.168.0.0/24 dev enaftgm1i0 proto kernel scope link src 192.168.0.133 
192.168.1.0/24 dev enp17s0f0 proto kernel scope link src 192.168.1.133 metric 100 

6. The output of "route -n":
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.133   0.0.0.0         UG    0      0        0 enp17s0f0
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 enp17s0f0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 enaftgm1i0
192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp17s0f0

7. The content of /etc/rc.local:
#!/bin/bash
insmod /root/test/rnp-0.3.6-rc8-33b3b45/src/rnp.ko
sleep 1
cd /root/test/uart
/root/test/uart/uart_pant &

modprobe iptable_nat
modprobe overlay
modprobe ip_vs
modprobe ip_vs_sh
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe nf_conntrack
modprobe vxlan

sleep 1

sudo ifconfig enaftgm1i0 192.168.0.133 up
sudo ifconfig enp17s0f0 192.168.1.133 up

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.14

```

</details>


#### Cloud provider

<details>
No
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux FT-D2000-2-1 4.19.15 #6 SMP Fri Sep 27 10:12:55 CST 2024 aarch64 aarch64 aarch64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.22 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel:v0.25.1-linuxarm64
flannel-cni-plugin:v1.2.0-linuxarm64
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
通过分析，该Issue描述的是在使用kubeadm初始化Kubernetes集群时，发现master节点的InternalIP与预期不符，显示的是另一块网络接口的IP地址。这可能是由于网络配置或kubeadm参数设置导致的节点IP选择问题。该问题没有涉及到任何安全风险，没有漏洞可被攻击者利用，也没有敏感信息泄露，符合风险判断标准第6条，属于不涉及安全问题的Issue。

---

## Issue #127716 Inconsistent DNS resolution for pod's IP when using headless and headful services

- Issue 链接：[#127716](https://github.com/kubernetes/kubernetes/issues/127716)

### Issue 内容

#### What happened?

I have a statefulset with multiple pods that make up a hashicorp raft. The raft needs stable ip addresses (see https://github.com/hashicorp/raft/issues/236) so I have defined a headful ClusterIP service for each pod. I have also defined a headless service for my statefulset.

The problem is that when I do a reverse DNS lookup of one of the pod's ip addresses (from a different pod) sometimes I get `<ip>.<headful service for that pod>.<namespace>.svc.cluster.local` and sometimes I get `<pod name>.<headless service name>.<namespace>.svc.cluster.local`. 

This is a problem because I have a mysql pod which needs to be able to consistently resolve the pod's ip to `<pod name>.<headless service name>.<namespace>.svc.cluster.local` in order for user authentication to succeed.

#### What did you expect to happen?

I expect the pod's ip to consistently resolve to `<pod name>.<headless service name>.<namespace>.svc.cluster.local`.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the following statefulset and services:
```
apiVersion: v1
kind: Service
metadata:
  name: headless-svc
  namespace: test
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  selector:
    app: dummy
---
apiVersion: v1
kind: Service
metadata:
  name: headful-svc-0
  namespace: test
spec:
  ports:
    - name: dummy
      port: 1111
      targetPort: 1111
  type: ClusterIP
  publishNotReadyAddresses: true
  selector:
    app: dummy
    statefulset.kubernetes.io/pod-name: dummy-0
---
apiVersion: v1
kind: Service
metadata:
  name: headful-svc-1
  namespace: test
spec:
  ports:
    - name: dummy
      port: 1111
      targetPort: 1111
  type: ClusterIP
  publishNotReadyAddresses: true
  selector:
    app: dummy
    statefulset.kubernetes.io/pod-name: dummy-1
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: dummy
  namespace: test
  labels:
    app: dummy
spec:
  replicas: 2
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: dummy
  serviceName: headless-svc
  template:
    metadata:
      labels:
        app: dummy
    spec:
      containers:
        - name: alpine
          image: alpine:latest
          command: ["sleep", "infinity"]
          securityContext:
            runAsUser: 65534
            runAsGroup: 65534
            readOnlyRootFilesystem: true
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 20m
              memory: 1Mi
---
kind: CiliumNetworkPolicy
apiVersion: "cilium.io/v2"
metadata:
  name: network-policy
  namespace: test
spec:
  endpointSelector:
    matchLabels:
      app: dummy
  egress:
    - toEndpoints:
        - matchLabels:
            k8s:io.kubernetes.pod.namespace: kube-system
            k8s-app: kube-dns
      toPorts:
        - ports:
            - port: "53"
              protocol: ANY
          rules:
            dns:
              - matchPattern: "*"
```
Observe inconsistent DNS resolution for pod 0's ip on pod 1:
```
$ k exec -it dummy-1 -- /bin/sh
~ $ getent hosts 100.64.185.189
100.64.185.189    100-64-185-189.headful-svc-0.alex-test.svc.cluster.local  100-64-185-189.headful-svc-0.alex-test.svc.cluster.local
~ $ getent hosts 100.64.185.189
100.64.185.189    dummy-0.headless-svc.alex-test.svc.cluster.local  dummy-0.headless-svc.alex-test.svc.cluster.local
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:14:41Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29+", GitVersion:"v1.29.7-eks-a18cd3a", GitCommit:"713ff29cb54edbe951b4ed70324fb3e7f8c8191b", GitTreeState:"clean", BuildDate:"2024-08-21T06:36:43Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.20.3
PRETTY_NAME="Alpine Linux v3.20"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
$ uname -a
Linux dummy-1 5.15.165-110.161.amzn2.x86_64 #1 SMP Fri Aug 23 18:41:15 UTC 2024 x86_64 Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中使用有头（headful）和无头（headless）服务时，Pod的IP地址的DNS解析不一致的问题。具体来说，进行反向DNS查询时，有时会得到`<ip>.<headful service for that pod>.<namespace>.svc.cluster.local`，有时会得到`<pod name>.<headless service name>.<namespace>.svc.cluster.local`。

这种不一致导致了MySQL Pod在进行用户认证时出现问题，因为MySQL需要IP地址反解析到一致的主机名。

根据给定的风险判断标准：

1. 该问题是由于DNS解析的不一致，导致应用程序功能异常，不涉及攻击者可利用的安全漏洞。

2. 没有证据表明该问题可能被利用成为一个漏洞，也没有达到CVE的高风险评级。

3. 该问题是配置和功能性问题，不属于安全风险。

因此，判断该Issue不涉及安全风险。

---

## Issue #127702 All external domains reolved to localhost

- Issue 链接：[#127702](https://github.com/kubernetes/kubernetes/issues/127702)

### Issue 内容

#### What happened?

Hi, friends, my cluster pods can not able to access all external domains, because all external domains are resolved to localhost. troubleshooting revealed that "localhost" has been added to the first line of /etc/resolv.conf in the pod.
I found that other clusters don't seem to have this "localhost" item. My cluster version is 1.30 .
I noticed that the kubelet clusterDomain config item doesn't have "localhost" either. How do I delete this, and is this a deployment error or something else, I have no idea. Thanks for this trouble!

#### What did you expect to happen?

/etc/resolve.conf
``` sh
kubectl exec -it nginx-deployment-c45d79c8-8fmz2 -- cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local localhost
nameserver 10.96.0.10
options ndots:5
```

ping result
if i use full domain name, thats OK
```
busybox-deploy-85d854b658-k4v49:~# ping rancher.devops.zenlayer.net
PING rancher.devops.zenlayer.net (::1) 56 data bytes
64 bytes from localhost (::1): icmp_seq=1 ttl=64 time=0.096 ms
64 bytes from localhost (::1): icmp_seq=2 ttl=64 time=0.058 ms
64 bytes from localhost (::1): icmp_seq=3 ttl=64 time=0.035 ms
64 bytes from localhost (::1): icmp_seq=4 ttl=64 time=0.077 ms
^C
--- rancher.devops.zenlayer.net ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3000ms
rtt min/avg/max/mdev = 0.035/0.066/0.096/0.022 ms

busybox-deploy-85d854b658-k4v49:~# ping rancher.devops.zenlayer.net.
PING 06b0020a756244a7930f9efcb8244a76.zga.globalconnetct.com (192.169.127.103) 56(84) bytes of data.
64 bytes from 192.169.127.103: icmp_seq=1 ttl=48 time=42.7 ms
64 bytes from 192.169.127.103: icmp_seq=2 ttl=48 time=42.2 ms
64 bytes from 192.169.127.103: icmp_seq=3 ttl=48 time=42.4 ms
64 bytes from 192.169.127.103: icmp_seq=4 ttl=48 time=41.6 ms
64 bytes from 192.169.127.103: icmp_seq=5 ttl=48 time=42.3 ms
```

#### How can we reproduce it (as minimally and precisely as possible)?

Version 1.30, and deploy with kubeadm, maybe.

#### Anything else we need to know?

kubelet clusterDomain configure

```
kubectl get cm -n kube-system       kubelet-config -o yaml
apiVersion: v1
data:
  kubelet: |
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 0s
        cacheUnauthorizedTTL: 0s
    cgroupDriver: systemd
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    ...
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
KVM hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

$ uname -a
Linux k8s-cluster1-master1 3.10.0-1160.45.1.el7.x86_64 #1 SMP Wed Oct 13 17:20:51 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes集群中Pod无法解析外部域名的问题，原因是Pod的/etc/resolv.conf文件中的search域包含了"localhost"，导致DNS解析时会将请求的域名加上"localhost"后缀，进而解析到127.0.0.1或::1（本地环回地址）。

这属于DNS配置的错误或不当配置，引起网络通信故障，但并不涉及安全风险。因为攻击者无法利用此配置错误来获取权限、执行命令或提升权限，也无法利用此错误进行攻击。

根据风险判断标准：

1. **该风险能被攻击者利用**：否，属于内部配置错误，攻击者无法利用。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：否，此问题不涉及安全漏洞。

3. **issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：是，属于配置错误。

因此，风险评级判断为“不涉及”。

---

## Issue #127678 CronJob executed twice: once before and once at the scheduled time

- Issue 链接：[#127678](https://github.com/kubernetes/kubernetes/issues/127678)

### Issue 内容

#### What happened?

While using Kubernetes CronJob for scheduling tasks, we encountered an issue where a specific CronJob was executed earlier than its scheduled time. The CronJob ran approximately 6 hours before the intended schedule and then executed again at the correct scheduled time, resulting in the job running twice. This issue was observed in a Kubernetes 1.25.5 environment.



#### What did you expect to happen?

We expected the CronJob to execute exactly once at the scheduled time. All CronJobs should run according to their defined schedules without any duplication.



#### How can we reproduce it (as minimally and precisely as possible)?

The issue occurred unexpectedly in a CronJob that was previously functioning correctly, and we haven't identified a way to reliably reproduce it. The problem has only happened once so far and has not occurred again since, indicating it may be an intermittent issue. The environment details are as follows:

The affected CronJob is scheduled to run daily.
There are a total of 26 CronJobs running in the cluster, with 12 of them in the affected namespace.
Some of these CronJobs in the same namespace are scheduled to run every minute or hourly.


#### Anything else we need to know?

We attempted to identify the issue by checking the kube-controller-manager logs but did not find any anomalies.

#### Kubernetes version

<details>

```console
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.25.5
WARNING: version difference between client (1.31) and server (1.25) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
on-prem
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.5 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.5 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
```

</details>


#### Install tools

<details>
cluster-api
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.6.8
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.24.1
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在使用Kubernetes CronJob调度任务时，某个CronJob被提前约6小时执行了一次，并在预定时间再次执行，导致任务被执行了两次。这个问题可能是由于Kubernetes调度器的偶发性错误、时间同步问题或CronJob配置异常导致的。从目前提供的信息来看，没有迹象表明此问题可以被攻击者利用，也不存在敏感信息泄露、权限提升、远程代码执行等高安全风险。根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #127676 /sig Network The kube-flannel pod on the worker node stays in CrashLoopBackOff status.

- Issue 链接：[#127676](https://github.com/kubernetes/kubernetes/issues/127676)

### Issue 内容

#### What happened?

I use kubeadm join to add a worker node. However, the flannel pod on the worker node stays in CrashLoopBackOff status.

#### What did you expect to happen?

The flannel pod becomes Running

#### How can we reproduce it (as minimally and precisely as possible)?

1. On the master node
```console
root@NPU-Atlas-2:/home/lincom# kubeadm init --pod-network-cidr=100.100.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.1.122
root@NPU-Atlas-2:/home/lincom# mkdir -p $HOME/.kube
root@NPU-Atlas-2:/home/lincom# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@NPU-Atlas-2:/home/lincom# sudo chown $(id -u):$(id -g) $HOME/.kube/config
root@NPU-Atlas-2:/home/lincom# kubectl apply -f kube-flannel.yml
```
2. On the worker node
```console
kubeadm join 192.168.1.122:6443 --token 2ydxw7.y64x3rl3d2g4fsxh \
> --discovery-token-ca-cert-hash sha256:9e3a2259e1c0d2a3bf0abcd6e344c5f65c7324cb58900d251f2305d4d16e7273
```
3. On the master node
```console
root@NPU-Atlas-2:/home/lincom# kubectl get pods --all-namespaces
NAMESPACE      NAME                                   READY   STATUS              RESTARTS        AGE
default        kubernetes-bootcamp-666cf565fc-97sbb   0/1     ContainerCreating   0               4m39s
kube-flannel   kube-flannel-ds-2tszk                  0/1     CrashLoopBackOff    5 (2m44s ago)   5m46s
kube-flannel   kube-flannel-ds-mkhst                  1/1     Running             0               9m54s
...................
root@NPU-Atlas-2:/home/lincom# kubectl describe pods/kube-flannel-ds-2tszk -n kube-flannel
.............................
Events:
  Type     Reason   Age                    From     Message
  ----     ------   ----                   ----     -------
  Warning  BackOff  32m (x416 over 122m)   kubelet  Back-off restarting failed container kube-flannel in pod kube-flannel-ds-2tszk_kube-flannel(b97155bd-b848-4272-88d4-0e5fa2f89706)
  Normal   Pulled   28m                    kubelet  Container image "swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/flannel/flannel-cni-plugin:v1.4.1-flannel1-linuxarm64" already present on machine
  Normal   Created  28m                    kubelet  Created container install-cni-plugin
  Normal   Started  28m                    kubelet  Started container install-cni-plugin
  Normal   Pulled   28m                    kubelet  Container image "swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/flannel/flannel:v0.25.1-linuxarm64" already present on machine
  Normal   Created  28m                    kubelet  Created container install-cni
  Normal   Started  28m                    kubelet  Started container install-cni
  Normal   Pulled   27m (x4 over 28m)      kubelet  Container image "swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/flannel/flannel:v0.25.1-linuxarm64" already present on machine
  Normal   Created  27m (x4 over 28m)      kubelet  Created container kube-flannel
  Normal   Started  27m (x4 over 28m)      kubelet  Started container kube-flannel
  Warning  BackOff  3m31s (x115 over 28m)  kubelet  Back-off restarting failed container kube-flannel in pod kube-flannel-ds-2tszk_kube-flannel(b97155bd-b848-4272-88d4-0e5fa2f89706)
root@NPU-Atlas-2:/home/lincom# kubectl -n kube-flannel logs kube-flannel-ds-2tszk
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.14
```

</details>


#### Cloud provider

<details>
No
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux FT-D2000-2-1 4.19.15 #5 SMP Tue Sep 24 09:29:10 CST 2024 aarch64 aarch64 aarch64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel:v0.25.1-linuxarm64
flannel-cni-plugin:v1.4.1-flannel1-linuxarm64
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用kubeadm添加worker节点后，flannel pod在worker节点上处于CrashLoopBackOff状态。根据提供的信息，问题可能是由于配置错误或者兼容性问题导致的flannel pod无法正常运行。

从Issue的内容来看，没有体现任何可以被攻击者利用的安全风险，也没有提及可能导致漏洞的情况。此外，Issue中提供的内容并未涉及安全问题。

因此，根据风险判断标准第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127739 Sync.Once in client-go metrics play badly with components that want to provide them by default

- Issue 链接：[#127739](https://github.com/kubernetes/kubernetes/issues/127739)

### Issue 内容

In Controller-Runtime, we register our metrics provider for the client-go [leaderelection](https://github.com/kubernetes-sigs/controller-runtime/blob/4381fa0aeee43e331be14b0d70cd276e1e91ad7a/pkg/metrics/leaderelection.go#L26), [workqueue](https://github.com/kubernetes-sigs/controller-runtime/blob/4381fa0aeee43e331be14b0d70cd276e1e91ad7a/pkg/metrics/workqueue.go#L99) and [clientmetrics](https://github.com/kubernetes-sigs/controller-runtime/blob/4381fa0aeee43e331be14b0d70cd276e1e91ad7a/pkg/metrics/client_go_adapter.go#L43-L54).

This is because controller-runtime provides a metrics endpoint and we want it to by default have all the metrics relevant to controllers.

Unfortunately, all this register funcs are internally a `sync.Once`. This means that if someone wants to register their own metrics, they have to do before someone else does. As of today, controller-runtime does this in an `init`, but even if it did that later, users would have to register this before controller-runtime does and make sure that no other dep registers it first, otherwise it will not work.

IMHO, we should remove the `sync.Once` so that as many metrics provides as wanted can be registered and this doesn't become a "first one wins, rest gets nothing" kind of situation where anyone who wants their custom adapter has to be super careful to be the first to register it to avoid it silently breaking.

There was some prior discussion around this specifically in the context of workqueue metrics, where its now possible to set a per-workqueue metrics provider that takes precedence over the global one as workaround:
* https://github.com/kubernetes/kubernetes/pull/114242

In that context, a PR to allow overriding the global one was rejetected, but i think we should be doing this: https://github.com/kubernetes/kubernetes/pull/116616

There was also some Slack discussion around this: https://kubernetes.slack.com/archives/C0EG7JC6T/p1719851021075269

This originally got reported as an issue in controller-runtime: https://github.com/kubernetes-sigs/controller-runtime/issues/2957

/sig api-machinery
/kind bug

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue讨论了在使用client-go的过程中，由于内部的metrics注册使用了`sync.Once`，导致metrics提供者只能注册一次的问题。这导致了多个组件如果想要注册自己的metrics提供者，就会出现冲突或无法注册的情况。这属于软件的初始化和组件注册的设计问题，可能会影响到metrics的收集和监控功能。

然而，从安全风险的角度来看，该Issue并未涉及任何可以被攻击者利用的漏洞。没有证据表明该问题会导致命令执行、提权、容器逃逸等高风险安全问题。攻击者无法通过该问题进行未授权的操作或攻击。

根据风险判断标准，Issue不涉及安全风险。

---

## Issue #127657 ingress doesn't works well for namespaced scenario

- Issue 链接：[#127657](https://github.com/kubernetes/kubernetes/issues/127657)

### Issue 内容

#### What happened?

with latest ingress/ingressClass design, there is no way to run application without requesting read permission on cluster level resource "IngressClass"
 
This is how issue happens
==As k8s administrator==
1) create the namespace for application "demoapp1" to be installed 
```shell
kubectl create ns demoapp1
```
2) create the role and rolebinding for application "demoapp1" 
```shell
kubectl create role demoapp1-admin --resource="*" --verb="*"
```

3) genenate the kubeconfig file "demoapp1-kubeconfig.yaml" from role "demoapp1-admin"

===As application administrator===
1) get the kubeconfig file of "demoapp1-admin"  from k8s administrator
2) install demoapp1 such as "helm install demoapp1 demoapp1.tgz --kube-config demoapp1-kubeconfig.yaml"
  in demoapp1, there is some ingresses as below
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-class-name-no-perm
  namespace: ingress-nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx

```
in demoapp1, there is one ingress-controller deployed which watch all of ingress whose ingressClassName is "nginx".   this ingress-controller should start and run without any error

as shown above, there is no cluster role binding for "application administrator" to install "demoapp1", it means any attempt to access k8s cluster resource will be denied

unfortunately,  k8s reference doesn't provide clear guidance  on how to valid the <ingress>.spec.ingressClassName, so most of ingress-controller implementation will try to read the IngressClass object from <ingress>.spec.ingressClassName.
obvisously, it will fail

in big shared k8s cluster, k8s administrator don't want to shared cluster level resource permission to applications running the namespace.  so typically, they just provide least privileged kubeconfig for application installation

#### What did you expect to happen?

ingress-controller/ingress can works without read permission on cluster reosurce "IngressClass"

#### How can we reproduce it (as minimally and precisely as possible)?

check details in "what happend" section

#### Anything else we need to know?

it is ingress/ingressclass design issue, had a long discussion with @aojea 
https://kubernetes.slack.com/archives/C09QYUH5W/p1727081948800529

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4


</details>


#### Cloud provider

<details>
on-premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
irrelevant
</details>


#### Install tools

<details>
irrelevant
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
irrelevant
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
irrelevant
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes集群中，由于Ingress和IngressClass的设计原因，部署应用时需要对集群级别的资源"IngressClass"有读取权限。这在多租户的Kubernetes集群中，集群管理员可能不希望为应用程序授予集群级别资源的读取权限，只提供最小权限的kubeconfig。

从安全角度来看，该Issue讨论的是权限设计和资源访问控制的问题，但并没有提到任何可以被攻击者利用的安全漏洞或风险。没有涉及到攻击者可以利用的漏洞、提权、敏感信息泄露等问题。

根据风险判断标准，第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。因此，本Issue风险评级判断为“不涉及”。

---

## Issue #127653 dra_manager_state checkpoint error

- Issue 链接：[#127653](https://github.com/kubernetes/kubernetes/issues/127653)

### Issue 内容

#### What happened?

1. start i create a cluster use kubernetes v1.30.0, cluster running success.
2. and then, i upgrade cluster version to v1.31.1, but kubelet can't start. 

error info is: `failed to get checkpoint dra_manager_state: checkpoint is corrupted`

![image](https://github.com/user-attachments/assets/6e6a1e79-7531-429d-bd49-886645647ee3)


#### What did you expect to happen?

Hope it start success use kubelet v1.31.1 version.

#### How can we reproduce it (as minimally and precisely as possible)?

1. deploy a 1.30.0 cluster
2. upgrader to v.131.1

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```


</details>

root@ubuntu:/etc/kubeasz/bin# kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在将Kubernetes集群从v1.30.0升级到v1.31.1时，kubelet无法启动，出现了`failed to get checkpoint dra_manager_state: checkpoint is corrupted`的错误信息。这表明在升级过程中，dra_manager_state的检查点文件可能出现了损坏或兼容性问题。这是一个在升级过程中遇到的功能性问题，可能需要检查升级步骤或兼容性支持。

根据风险判断标准：

- 第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该问题不涉及安全风险。

---

## Issue #127639 Emulation Version cannot be set in integration test 

- Issue 链接：[#127639](https://github.com/kubernetes/kubernetes/issues/127639)

### Issue 内容

Emulation version needs to be configurable both in unit tests and integration tests. Currently, integration tests always emulate the latest version with no other option to change it.

Ref: https://github.com/kubernetes/kubernetes/pull/127302#discussion_r1776095308

/kind bug
/sig api-machinery
/triage accepted
/cc @jpbetz @aaron-prindle 

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论的是在集成测试（integration tests）中无法设置仿真（emulation）版本的问题。目前，集成测试总是模拟最新的版本，没有其他选项可以更改。这影响了测试的灵活性，特别是在需要针对不同版本进行兼容性测试的情况下。

根据风险判断标准：
1. 该问题没有被攻击者利用的途径，只是测试配置的问题。
2. 不会成为一个漏洞，不会被分配CVE编号，无法进行CVSS评分。
3. Issue提交者未暴露敏感信息或进行不当操作。
6. 该Issue不涉及安全问题，因此风险评级判断为不涉及。

---

## Issue #127618 Completed Jobs Not Fully Removed After EKS v1.29 Update

- Issue 链接：[#127618](https://github.com/kubernetes/kubernetes/issues/127618)

### Issue 内容

#### What happened?

I am experiencing an issue with completed Jobs in my Kubernetes cluster. Despite configuring my CronJobs to retain only 3 completed Jobs, I am seeing many more completed Jobs in `k9s` than in `kubectl`.

This issue started occurring after updating the EKS cluster to v1.29. In another cluster running EKS v1.28, this issue does not occur.

Here we can see the difference in jobs between `k9s` and `kubectl`:

![image](https://github.com/user-attachments/assets/b4dfbb65-f5e7-4090-950c-d49d87df8f99)

Removing the job `api-monitor-services-api-28787625-ssnn9`, we can see the kubelet removing it from the node:

![image](https://github.com/user-attachments/assets/ef478bcc-85b9-46cd-9f54-99d5af47c14c)

#### What did you expect to happen?

I expected `k9s` to show the same number of completed Jobs as `kubectl`, which is 3. Additionally, I expected that the completed Jobs would be completely removed from the node, retaining only the 3 defined by the `successfulJobsHistoryLimit`.

#### How can we reproduce it (as minimally and precisely as possible)?

All CronJobs created in my EKS v1.29 cluster are experiencing the same issue. Here is an example:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-test
  namespace: cronjob-test
  labels:
    app: cronjob-test
spec:
  schedule: "*/15 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      backoffLimit: 3
      activeDeadlineSeconds: 600
      template:
        metadata:
          labels:
            app: cronjob-test
        spec:
          restartPolicy: OnFailure
          containers:
            - name: main
              image: xxxx
              imagePullPolicy: IfNotPresent
```
After 4 or more executions, check the Jobs:
```yaml
kubectl get jobs -n cronjob-test
```
In k9s
```
k9s -n cronjob-test
```
Using k9s, remove the first completed Job and leave the last three. In the kubelet logs, you will see the Job being deleted.

#### Anything else we need to know?

N/A

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8-eks-a737599
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console

# EKS v1.29

# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
5.10.224-212.876.amzn2.x86_64 #1 SMP Thu Aug 22 16:55:24 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在EKS v1.29版本中，CronJob的已完成Job没有完全被删除，导致k9s和kubectl显示的已完成Job数量不一致。此问题是关于CronJob的历史Job清理功能在升级后未按预期工作，属于功能性问题。没有迹象表明该问题会被攻击者利用，也没有涉及到任何安全风险或漏洞。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127610 [Flaky test] GCE Conformance Kubernetes e2e suite.[It] [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]

- Issue 链接：[#127610](https://github.com/kubernetes/kubernetes/issues/127610)

### Issue 内容

#### Which jobs are flaking?
* master-informing:
  * **ci-kubernetes-gce-conformance-latest-kubetest2**

#### Which tests are flaking?
Kubernetes e2e suite.[It] [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]

#### Since when has it been flaking? Failed runs:

**Time:** 09/16/2024 07:40 UTC -5
[Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-kubetest2/1835659785358282752)

Looks like only one failure on our board as seen in Triage:
[ci-kubernetes-gce-conformance-latest-kubetest2](https://storage.googleapis.com/k8s-triage/index.html?date=2024-09-24&job=ci-kubernetes-gce-conformance-latest-kubetest2&test=Pods%20should%20support%20retrieving%20logs%20from%20the%20container%20over%20websockets#462d90d8ac25265be8c3)

But there are some of these same failures across other builds, [see here](https://storage.googleapis.com/k8s-triage/index.html?date=2024-09-24&test=Pods%20should%20support%20retrieving%20logs%20from%20the%20container%20over%20websockets#462d90d8ac25265be8c3).

#### Testgrid link
https://testgrid.k8s.io/sig-release-master-blocking#Conformance%20-%20GCE%20-%20master%20-%20kubetest2

#### Reason for failure (if possible)

Observed:
```
[FAILED] Failed to open websocket to wss://34.41.155.79/api/v1/namespaces/pods-7055/pods/pod-logs-websocket-7573cd2f-65c0-4463-9b77-b90753b2b727/log?container=main: websocket.Dial wss://34.41.155.79/api/v1/namespaces/pods-7055/pods/pod-logs-websocket-7573cd2f-65c0-4463-9b77-b90753b2b727/log?container=main: dial tcp 34.41.155.79:443: connect: connection timed out
```
The websocket connection failed to establish, likely due to transient network issues or connection timeouts in the CI environment. As a result, the logs from the container could not be retrieved, causing the test to fail.

Opening this issue to be safe, but perhaps we close this if it's not continued.

#### Anything else we need to know?
The log retrieval failure could be caused by intermittent network conditions or resource misconfigurations in the CI environment. Additionally, SCP errors showed that logs were not present or inaccessible on the nodes:
```
usr/bin/scp: /var/log/cluster-autoscaler.log*: No such file or directory /usr/bin/scp: /var/log/fluentd.log*: No such file or directory
```

These issues might suggest node failures or problems with the services running on the test nodes, impacting the ability to retrieve logs.

#### Relevant SIG(s)
/sig node  
/kind flake  
cc @kubernetes/release-team-release-signal


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了Kubernetes的一个测试用例在CI环境中不稳定，测试内容是Pods是否支持通过WebSocket从容器中检索日志。失败原因是无法建立WebSocket连接，可能是由于网络问题或CI环境中的连接超时导致的。

错误日志显示连接到IP地址34.41.155.79的443端口时连接超时，此外，使用scp命令时无法找到某些日志文件，如`/var/log/cluster-autoscaler.log*`和`/var/log/fluentd.log*`。这些问题可能表明节点故障或测试节点上的服务配置问题，影响了日志的检索能力。

根据以上信息，该Issue反映的是测试环境中的网络故障或配置问题，不涉及任何潜在的安全风险。没有证据表明存在可被攻击者利用的漏洞，也没有敏感信息的泄露，因此风险评级为“不涉及”。

---

## Issue #127596 duplicate flag "--runtime-config" when calling run_remote.go on hack/make-rules/test-e2e-node.sh 

- Issue 链接：[#127596](https://github.com/kubernetes/kubernetes/issues/127596)

### Issue 内容

#### What happened?

duplicate flag "--runtime-config" when calling run_remote.go on hack/make-rules/test-e2e-node.sh 

#### What did you expect to happen?

remove duplicate flag "--runtime-config" on run_remote.go

#### How can we reproduce it (as minimally and precisely as possible)?

n/a

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue报告了在调用`run_remote.go`时，重复传递了`--runtime-config`参数的问题。这可能是脚本中的一个小错误，导致参数被重复设置，可能会引起运行时的警告或覆盖参数。但是，从Issue的描述来看，没有涉及任何攻击者可以利用的安全风险，也没有提到可能导致安全漏洞的情况。因此，根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #127595 missing release from https://storage.googleapis.com/kubernetes-release

- Issue 链接：[#127595](https://github.com/kubernetes/kubernetes/issues/127595)

### Issue 内容

#### What happened?

In https://storage.googleapis.com/kubernetes-release release 1.29.9 is missing. Latest one for 1.29 line is 1.29.8. 

https://console.cloud.google.com/storage/browser/kubernetes-release/release/v1.29.8?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))

#### What did you expect to happen?

Release 1.29.9 should be available as well.

#### How can we reproduce it (as minimally and precisely as possible)?

Try downloading this file: https://storage.googleapis.com/kubernetes-release/release/v1.29.9/bin/linux/amd64/kubelet

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.29.9
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了Kubernetes发行版1.29.9在指定的存储位置缺失的问题。用户期望能够下载该版本，但发现无法下载。此问题涉及软件发布过程中的版本管理或发布资源缺失，并不涉及潜在的安全风险。

按照风险判断标准第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127593 Removed important check in kubeadm.

- Issue 链接：[#127593](https://github.com/kubernetes/kubernetes/issues/127593)

### Issue 内容

#### What happened?

I initialized kubernetes cluster using kubeadm v1.31 and v1.30 and the network wouldn't work properly.


#### What did you expect to happen?

On kubeadm version 1.29.9 I received an error:

error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist

On v1.30.5 and v1.31.1 this check didn't appear.

#### How can we reproduce it (as minimally and precisely as possible)?

modprobe -r br_netfilter
kubeadm init 


#### Anything else we need to know?

This is fixed by modprobe br_netfilter but it took me a whole day of debugging to get to.

#### Kubernetes version

<details>

```console
$ kubectl version
CHECK DOESNT APPEAR
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5

Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1

CHECK APPEARS
Client Version: v1.29.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.9



```

</details>


#### Cloud provider

<details>
I tested locally, on linode and on OVH
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
Debian 12
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue内容，kubeadm在v1.30和v1.31版本中移除了对br_netfilter模块的预检（preflight check），导致用户在未加载br_netfilter模块的情况下可以成功初始化集群，但随后网络无法正常工作。这是一个功能性问题，可能会给用户带来困扰，但没有直接的安全风险。

从安全角度来看，缺少br_netfilter模块可能导致iptables规则无法正确应用于桥接网络流量。然而，在默认配置下，这不会导致新的攻击面或安全漏洞被攻击者利用。因为攻击者无法通过此问题提升权限、绕过安全控制或执行未授权的操作。

根据风险判断标准，此问题不涉及安全风险。

---

## Issue #127588 During the deployment of Kubernetes 1.31.0 and MultiCIDRServiceAllocator was enabled, which then led to a failure when creating a service.

- Issue 链接：[#127588](https://github.com/kubernetes/kubernetes/issues/127588)

### Issue 内容

#### What happened?

When deploying Kubernetes 1.31.0 with MultiCIDRServiceAllocator enabled, an error occurs when trying to create a service in the cluster with the following error message:

```bash
root@controller-node-1:~# kubectl apply -f test-service.yaml
Error from server (InternalError): error when creating "test-service.yaml": Internal error occurred: failed to allocate a serviceIP: range is full
```

Am I missing some necessary configuration? Can you help me take a look?  Thanks.

#### What did you expect to happen?

Can successfully create a service

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy k8s and enable the featureGate MultiCIDRServiceAllocator function
2. Then create a service and enable IPv6
3. The service yaml information for the test is as follows:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: dual-stack-service
  namespace: default
spec:
  ipFamilyPolicy: RequireDualStack
  ipFamilies:
    - IPv4
    - IPv6
  selector:
    app: my-app
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```

4. Disable MultiCIDRServiceAllocator feature, create with same service yaml, everything works fine.

```bash
kubectl apply -f test.yaml             
service/dual-stack-service created
```

#### Anything else we need to know?

Check kube-apiserver and you will see the following error

```
1ms" userAgent="kube-apiserver/v1.31.0 (linux/amd64) kubernetes/9edcffc" audit-ID="39ca3505-f77b-42fe-b935-2585dc25506b" srcIP="[::1]:50666" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.340321ms" resp=201
I0923 11:07:57.150953       1 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/services?fieldManager=kubectl-client-side-apply&fieldValidation=Strict" latency="15.907975ms" userAgent="kubectl/v1.28.0 (linux/amd64) kubernetes/855e7c4" audit-ID="d27bf18c-7022-4f9f-bb2d-c63dabd64420" srcIP="172.18.0.1:60442" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="15.293168ms" resp=500 statusStack=<

        goroutine 23448 [running]:
        k8s.io/apiserver/pkg/server/httplog.(*respLogger).recordStatus(0xc007415760, 0xc00d878d48?)
                k8s.io/apiserver/pkg/server/httplog/httplog.go:336 +0xfa
        k8s.io/apiserver/pkg/server/httplog.(*respLogger).WriteHeader(0xc007415760, 0x1f4)
                k8s.io/apiserver/pkg/server/httplog/httplog.go:316 +0x1d
        k8s.io/apiserver/pkg/server/filters.(*baseTimeoutWriter).WriteHeader(0xc00d99c330, 0x1f4)
                k8s.io/apiserver/pkg/server/filters/timeout.go:237 +0x1cc
        k8s.io/apiserver/pkg/endpoints/metrics.(*ResponseWriterDelegator).WriteHeader(0x2f5da40?, 0xc00d878c60?)
                k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:810 +0x26
        k8s.io/apiserver/pkg/endpoints/handlers/responsewriters.(*deferredResponseWriter).Write(0xc00d7cf7a0, {0xc00cad0000, 0x10f, 0x8000})
                k8s.io/apiserver/pkg/endpoints/handlers/responsewriters/writers.go:243 +0x67b
        encoding/json.(*Encoder).Encode(0xc00d9cdf28, {0x351bbe0, 0xc00d79eaa0})
                encoding/json/stream.go:230 +0x1f8
        k8s.io/apimachinery/pkg/runtime/serializer/json.(*Serializer).doEncode(0x5a0fd80?, {0x3c4c068?, 0xc00d79eaa0?}, {0x3c42480, 0xc00d7cf7a0})
                k8s.io/apimachinery/pkg/runtime/serializer/json/json.go:246 +0x175
        k8s.io/apimachinery/pkg/runtime/serializer/json.(*Serializer).Encode(0xc0004e4000, {0x3c4c068?, 0xc00d79eaa0}, {0x3c42480, 0xc00d7cf7a0})
                k8s.io/apimachinery/pkg/runtime/serializer/json/json.go:220 +0xd2
        k8s.io/apimachinery/pkg/runtime/serializer/versioning.(*codec).doEncode(0xc00d79eb40, {0x3c4c068, 0xc00d79eaa0}, {0x3c42480, 0xc00d7cf7a0}, {0x0?, 0x0?})
                k8s.io/apimachinery/pkg/runtime/serializer/versioning/versioning.go:268 +0x51e
        k8s.io/apimachinery/pkg/runtime/serializer/versioning.(*codec).encode(0xc00d79eb40, {0x3c4c068?, 0xc00d79eaa0}, {0x3c42480, 0xc00d7cf7a0}, {0x0, 0x0})
                k8s.io/apimachinery/pkg/runtime/serializer/versioning/versioning.go:214 +0x119
        k8s.io/apimachinery/pkg/runtime/serializer/versioning.(*codec).Encode(0x3c76580?, {0x3c4c068?, 0xc00d79eaa0?}, {0x3c42480?, 0xc00d7cf7a0?})
                k8s.io/apimachinery/pkg/runtime/serializer/versioning/versioning.go:207 +0x2d
        k8s.io/apiserver/pkg/endpoints/handlers/responsewriters.SerializeObject({0x35d70a1, 0x10}, {0x7fb72cce3158, 0xc00d79eb40}, {0x3c6cbf0, 0xc00d486880}, 0xc005fa3b00, 0x1f4, {0x3c4c068, 0xc00d79eaa0})
                k8s.io/apiserver/pkg/endpoints/handlers/responsewriters/writers.go:111 +0xa15
        k8s.io/apiserver/pkg/endpoints/handlers/responsewriters.WriteObjectNegotiated.func2()
                k8s.io/apiserver/pkg/endpoints/handlers/responsewriters/writers.go:292 +0x1fd
        k8s.io/apiserver/pkg/endpoints/request.(*durationTracker).Track(0xc00d99c120, 0xc00d407ad0)
                k8s.io/apiserver/pkg/endpoints/request/webhook_duration.go:75 +0x88
        k8s.io/apiserver/pkg/endpoints/request.TrackSerializeResponseObjectLatency({0x3c76580?, 0xc00d99c510?}, 0xc00d407ad0)
                k8s.io/apiserver/pkg/endpoints/request/webhook_duration.go:229 +0x5d
        k8s.io/apiserver/pkg/endpoints/handlers/responsewriters.WriteObjectNegotiated({0x3c60e40, 0xc000af5800}, {0x3c61020, 0x5ac8540}, {{0x0, 0x0}, {0x35c0bb0, 0x2}}, {0x3c6cbf0, 0xc00d486880}, ...)
                k8s.io/apiserver/pkg/endpoints/handlers/responsewriters/writers.go:288 +0x772
        k8s.io/apiserver/pkg/endpoints/handlers/responsewriters.ErrorNegotiated({0x3c3ede0?, 0xc00d79ea00?}, {0x3c60e40, 0xc000af5800}, {{0x0?, 0xc00d9cf0e8?}, {0x35c0bb0?, 0xc00d87e360?}}, {0x3c6cbf0, 0xc00d486880}, ...)
                k8s.io/apiserver/pkg/endpoints/handlers/responsewriters/writers.go:329 +0x271
        k8s.io/apiserver/pkg/endpoints/handlers.(*RequestScope).err(0xc00d9941e0?, {0x3c3ede0?, 0xc00d79ea00?}, {0x3c6cbf0?, 0xc00d486880?}, 0x2?)
                k8s.io/apiserver/pkg/endpoints/handlers/rest.go:113 +0x9f
        k8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1({0x3c6cbf0, 0xc00d486880}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/handlers/create.go:221 +0x1a7c
        k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulCreateResource.func16(0xc00d486860, 0xc003368b60)
                k8s.io/apiserver/pkg/endpoints/installer.go:1282 +0x5c
        k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func17(0xc00d486860, 0xc003368b60)
                k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:600 +0x1ce
        github.com/emicklei/go-restful/v3.(*Container).dispatch(0xc001692d80, {0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                github.com/emicklei/go-restful/v3@v3.11.0/container.go:299 +0x9f0
        github.com/emicklei/go-restful/v3.(*Container).Dispatch(...)
                github.com/emicklei/go-restful/v3@v3.11.0/container.go:204
        k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x35d2672?, 0xc00d9a6098?}, 0xc001692d80?, 0xc00038d730?}, {0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/server/handler.go:146 +0x56c
        k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP(0xc001b7c768?, {0x3c6cbf0?, 0xc00d486460?}, 0x23?)
                k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x25d
        k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc00775f900, {0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x410
        k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc008b669a0?, {0x3c6cbf0?, 0xc00d486460?}, 0xc00d9a63f0?)
                k8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x66
        k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x35d4a43?, 0xc00d9a64e8?}, 0xc002355050?, 0xc0040e1ab0?}, {0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/server/handler.go:154 +0x6f1
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177
        net/http.HandlerFunc.ServeHTTP(0x3c76580?, {0x3c6cbf0?, 0xc00d486460?}, 0x4?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filters/authorization.go:83 +0x626
        net/http.HandlerFunc.ServeHTTP(0xc00d9a68a0?, {0x3c6cbf0?, 0xc00d486460?}, 0xc0005dc000?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
        net/http.HandlerFunc.ServeHTTP(0x1d175ef?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c2c868?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177
        net/http.HandlerFunc.ServeHTTP(0xc00d3756c0?, {0x3c6cbf0?, 0xc00d486460?}, 0x1e13816?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func9()
                k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:292 +0xd9
        k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
                k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x257
        k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1(0xc00d85c2d0?, 0x70?, 0x34460e0?)
                k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x56
        k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish(0xc00d85c2d0, 0xc003368af0)
                k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x39
        k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle(0xc0004cec80, {0x3c76628, 0xc003368770}, {0xc00d6b32b0, {0x3c76428, 0xc00d375640}}, 0x0?, 0x0?, 0xc00d375680?, 0xc00d375680)
                k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x74b
        k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10(0xc0030915e0, {0x3c76580?, 0xc00d99c510?}, {0xc00d6b32b0?, {0x3c76428?, 0xc00d375640?}}, 0xc00d486540, 0xc00d99c540, 0xc00c4501e0, 0xc00d375680)
                k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0xeb
        k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle(0xc0030915e0, {0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:299 +0xb25
        net/http.HandlerFunc.ServeHTTP(0xc00d567068?, {0x3c6cbf0?, 0xc00d486460?}, 0xc0005dc000?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
        net/http.HandlerFunc.ServeHTTP(0x1d175ef?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c2c868?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177
        net/http.HandlerFunc.ServeHTTP(0xc00d99c360?, {0x3c6cbf0?, 0xc00d486460?}, 0xa683a2b45f389d52?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x1b0
        net/http.HandlerFunc.ServeHTTP(0xc00d5677f0?, {0x3c6cbf0?, 0xc00d486460?}, 0xc0005dc000?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
        net/http.HandlerFunc.ServeHTTP(0x1d175ef?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c2c868?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177
        net/http.HandlerFunc.ServeHTTP(0xc00d567a10?, {0x3c6cbf0?, 0xc00d486460?}, 0xc0005dc000?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
        net/http.HandlerFunc.ServeHTTP(0x1d175ef?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c2c868?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x177
        net/http.HandlerFunc.ServeHTTP(0x3c76580?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c26438?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa3b00)
                k8s.io/apiserver/pkg/endpoints/filters/authentication.go:120 +0x79f
        net/http.HandlerFunc.ServeHTTP(0x3c76580?, {0x3c6cbf0?, 0xc00d486460?}, 0x3c2c868?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3c6cbf0, 0xc00d486460}, 0xc005fa37a0)
                k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x354
        net/http.HandlerFunc.ServeHTTP(0xc005fa3560?, {0x3c6cbf0?, 0xc00d486460?}, 0xc00d848a50?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11({0x3c6cbf0, 0xc00d486460}, 0xc005fa3560)
                k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0xb9
        net/http.HandlerFunc.ServeHTTP(0xc00955dfd0?, {0x3c6cbf0?, 0xc00d486460?}, 0xc00955dfd0?)
                net/http/server.go:2171 +0x29
        k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
                k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0x62
        created by k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP in goroutine 23431
                k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x198
 > addedInfo=<
        logging error output: "{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Internal error occurred: failed to allocate a serviceIP: range is full\",\"reason\":\"InternalError\",\"details\":{\"causes\":[{\"message\":\"failed to allocate a serviceIP: range is full\"}]},\"code\":500}\n"
 >
I0923 11:07:57.151260       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR fd00:10:233::/116
I0923 11:07:57.151274       1 cidrallocator.go:243] syncing ServiceCIDR allocators took: 4.363189ms
I0923 11:07:57.152400       1 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/networking.k8s.io/v1beta1/servicecidrs/kubernetes/status?fieldManager=service-cidr-controller&force=true" latency="3.55043ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/system:serviceaccount:kube-system:service-cidrs-controller" audit-ID="8ffd052f-41b7-4402-8e1d-bf7c1f922188" srcIP="172.18.0.3:44714" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.080476ms" resp=200
I0923 11:07:57.158493       1 httplog.go:134] "HTTP" verb="GET" URI="/api" latency="1.345547ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/system:serviceaccount:kube-system:resourcequota-controller" audit-ID="6d69ba6e-d78e-4e0f-a150-123321e861f4" srcIP="172.18.0.3:44714" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="609.308µs" resp=200
I0923 11:07:57.160655       1 httplog.go:134] "HTTP" verb="GET" URI="/apis" latency="1.158272ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/system:serviceaccount:kube-system:resourcequota-controller" audit-ID="333368b2-f5d3-4323-ae03-477ba58da367" srcIP="172.18.0.3:44714" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="767.455µs" resp=200
I0923 11:07:57.642880       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s" latency="5.535226ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="6f6a58be-3d80-4437-abde-a1da9fcd3aa5" srcIP="172.18.0.3:33630" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.081188ms" resp=200
I0923 11:07:57.811806       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="502.755µs" userAgent="kube-probe/1.31" audit-ID="cea8cbae-ed22-49c3-a072-03ab6daaa5bb" srcIP="172.18.0.3:34026" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="265.857µs" resp=200
I0923 11:07:57.820286       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-mhpdv4nvr63tysjgwwxs5rrdle" latency="4.567151ms" userAgent="kube-apiserver/v1.31.0 (linux/amd64) kubernetes/9edcffc" audit-ID="00212d5f-1b2e-4244-8c8b-0d90b11bad2a" srcIP="[::1]:50666" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.239606ms" resp=200
I0923 11:07:57.876446       1 httplog.go:134] "HTTP" verb="GET" URI="/api" latency="1.534883ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="10d2876d-1c4e-4bad-b7f5-b16cb5e35847" srcIP="172.18.0.3:44714" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="646.149µs" resp=200
I0923 11:07:57.879639       1 httplog.go:134] "HTTP" verb="GET" URI="/apis" latency="1.281986ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="b9da6911-6445-4bef-8bbe-03faaca5333f" srcIP="172.18.0.3:44714" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="851.935µs" resp=200
I0923 11:07:58.669695       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=5s" latency="5.4473ms" userAgent="kube-scheduler/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="3a063464-e136-4ed6-a6e8-d35438ab1519" srcIP="172.18.0.3:33668" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.909292ms" resp=200
I0923 11:07:58.813809       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="3.215281ms" userAgent="kube-probe/1.31" audit-ID="e18e3623-98c7-42b1-9bb0-0d3cbc5cf10d" srcIP="172.18.0.3:34042" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.908629ms" resp=200
I0923 11:07:58.843866       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/test-worker?timeout=10s" latency="3.545531ms" userAgent="kubelet/v1.31.0 (linux/amd64) kubernetes/9edcffc" audit-ID="4dc6e3d0-0129-4984-add2-5b90782acdab" srcIP="172.18.0.2:47072" apf_pl="node-high" apf_fs="system-node-high" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.066497ms" resp=200
I0923 11:07:59.649276       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s" latency="4.195773ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="926e2e36-d223-438e-806e-b95e5aade245" srcIP="172.18.0.3:33630" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.599817ms" resp=200
I0923 11:07:59.814701       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="3.535573ms" userAgent="kube-probe/1.31" audit-ID="cf5b3bcc-bb26-4fa7-9d61-a1cd2028b859" srcIP="172.18.0.3:34050" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.275286ms" resp=200
I0923 11:08:00.280649       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/test-control-plane?timeout=10s" latency="5.420941ms" userAgent="kubelet/v1.31.0 (linux/amd64) kubernetes/9edcffc" audit-ID="605f7302-07fe-4d7f-a346-66cae51038eb" srcIP="172.18.0.3:33640" apf_pl="node-high" apf_fs="system-node-high" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.715523ms" resp=200
I0923 11:08:00.682007       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=5s" latency="9.963364ms" userAgent="kube-scheduler/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="a86f1452-075c-4a38-ad61-fe3fe3fd5301" srcIP="172.18.0.3:33668" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="9.517368ms" resp=200
I0923 11:08:00.814357       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="2.975113ms" userAgent="kube-probe/1.31" audit-ID="45f6a1cb-bbc5-439a-9ff2-8ae247d5e52a" srcIP="172.18.0.3:34062" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.408675ms" resp=200
I0923 11:08:01.655910       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s" latency="5.136937ms" userAgent="kube-controller-manager/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="dce89952-3d3d-46df-8d95-612fcd461d4b" srcIP="172.18.0.3:33630" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.497896ms" resp=200
I0923 11:08:01.811610       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="487.758µs" userAgent="kube-probe/1.31" audit-ID="3b7c1ef9-01fb-4087-a5d4-1093bddc9496" srcIP="172.18.0.3:58988" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="244.496µs" resp=200
I0923 11:08:02.689183       1 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=5s" latency="4.023116ms" userAgent="kube-scheduler/v1.31.0 (linux/amd64) kubernetes/9edcffc/leader-election" audit-ID="5502e546-bd18-44c3-b6e9-2e0ee7d2f742" srcIP="172.18.0.3:33668" apf_pl="leader-election" apf_fs="system-leader-election" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.524193ms" resp=200
I0923 11:08:02.812357       1 httplog.go:134] "HTTP" verb="GET" URI="/livez" latency="1.741594ms" userAgent="kube-probe/1.31" audit-ID="f63e072e-8453-4fe3-9476-13edcaac99bc" srcIP="172.18.0.3:59002" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.468739ms" resp=200
I0923 11:08:02.813133       1 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="1.673271ms" userAgent="kube-probe/1.31" audit-ID="b6546ec4-c30b-426f-8691-8e1324fa3426" srcIP="172.18.0.3:59004" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.450684ms" resp=200
E0923 11:08:03.000230       1 repairip.go:500] "Unhandled Error" err="IPAddress fd00:10:233::1cd appears to have leaked: cleaning up" logger="UnhandledError"
E0923 11:08:03.000275       1 repairip.go:500] "Unhandled Error" err="IPAddress 10.233.51.219 appears to have leaked: cleaning up" logger="UnhandledError"
I0923 11:08:03.004206       1 httplog.go:134] "HTTP" verb="POST" URI="/apis/events.k8s.io/v1/namespaces/default/events" latency="1.554619ms" userAgent="kube-apiserver/v1.31.0 (linux/amd64) kubernetes/9edcffc" audit-ID="3fa3d537-3563-44c7-9707-b11662836a95" srcIP="[::1]:50666" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.311628ms" resp=422
E0923 11:08:03.005578       1 event_broadcaster.go:270] "Server rejected event (will not retry!)" err="Event \"fd00:10:233::1cd.17f7daea8a8debae\" is invalid: metadata.name: Invalid value: \"fd00:10:233::1cd.17f7daea8a8debae\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')" event="&Event{ObjectMeta:{fd00:10:233::1cd.17f7daea8a8debae  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},EventTime:2024-09-23 11:08:03.001562851 +0000 UTC m=+119.806647154,Series:nil,ReportingController:ipallocator-repair-controller,ReportingInstance:ipallocator-repair-controller-test-control-plane,Action:IPAddressAllocation,Reason:IPAddressNotAllocated,Regarding:{IPAddress  fd00:10:233::1cd 793dd423-d268-4d5d-afb8-754199a96b05 networking.k8s.io/v1beta1 158844 },Related:nil,Note:IPAddress: fd00:10:233::1cd for Service default/dual-stack-service appears to have leaked: cleaning up,Type:Warning,DeprecatedSource:{ },DeprecatedFirstTimestamp:0001-01-01 00:00:00 +0000 UTC,DeprecatedLastTimestamp:0001-01-01 00:00:00 +0000 UTC,DeprecatedCount:0,}"
```

#### Kubernetes version

<details>

```console
$ kubectl version
root@controller-node-1:~# kubectl version
Client Version: v1.31.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

[kind](https://kind.sigs.k8s.io/)
<details>
root@controller-node-1:~# kind --version

kind version 0.24.0
</details>

K8S Version 1.31.0 deployed through the [kind](https://kind.sigs.k8s.io/) cluster, kind yaml is as follows

```
kind.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: kind
networking:
  serviceSubnet: "10.233.0.0/18,fd00:10:233::/116"
  podSubnet: "10.233.64.0/18,fd00:10:233:64::/64"
  ipFamily: dual
  disableDefaultCNI: true
  kubeProxyMode: iptables
  apiServerAddress: 127.0.0.1
kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    metadata:
      name: config
    apiServer:
        extraArgs:
          enable-admission-plugins: NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
          default-not-ready-toleration-seconds: "10"
          default-unreachable-toleration-seconds: "10"
nodes:
  - role: control-plane
  - role: worker
runtimeConfig: 
   api/beta: "true"
featureGates: 
   "MultiCIDRServiceAllocator": true
```

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在启用MultiCIDRServiceAllocator功能后，创建双栈服务时出现了错误，提示\"failed to allocate a serviceIP: range is full\"。同时，在日志中出现了关于\"Event\"名称无效的错误，原因是IPv6地址包含了不符合RFC 1123子域名规范的字符，例如冒号":"。

这个问题的原因可能是Kubernetes在处理IPv6地址时，没有正确地对其进行格式化，直接将包含冒号的IPv6地址用于事件名称，导致名称验证失败。

然而，这个问题并未涉及安全风险。首先，用户需要具备创建服务的权限才能触发此错误，这已经是较高的权限。其次，错误只是导致服务创建失败和日志记录错误，并未导致拒绝服务攻击、权限提升、命令执行或敏感信息泄露等安全问题。

因此，根据风险判断标准，该Issue不涉及安全风险。

---

## Issue #127554 DRA: Extra unexpected devices allocated when using 'allocationMode: All'

- Issue 链接：[#127554](https://github.com/kubernetes/kubernetes/issues/127554)

### Issue 内容

#### What happened?

I was testing the ability to allocate both node-local resources (GPUs) along with a new network attached resource called an IMEX channel.

My setup is as follows:
* 8 nodes with 1 GPU each
* 2 pools of IMEX channels (with 10 available in each)
* IMEX channels from the first pool are part of "IMEX domain 1" and can be attached to nodes 0-3
* IMEX channels from the second pool are part of "IMEX domain 2" and can be attached to nodes 4-7

With this setup, I create two ResourceClaims, one ResourceClaim template, and 2 deployments. The two resource Claims are for two distinct IMEX channels, the ResourceClaimTemplate is for all GPUs on a given node, and the 2 deployments (of 4 replicas each) consume these claims / claimtemplates across each replica in order to simulate running 2 MPI jobs across two different IMEX domains.

Here are the specs:
```
---
apiVersion: v1
kind: Namespace
metadata:
  name: imex-test1
---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  namespace: imex-test1
  name: shared-imex-channel0
spec:
  devices:
    requests:
    - name: channel
      deviceClassName: imex.nvidia.com
---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  namespace: imex-test1
  name: shared-imex-channel1
spec:
  devices:
    requests:
    - name: channel
      deviceClassName: imex.nvidia.com
---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  namespace: imex-test1
  name: all-node-gpus
spec:
  spec:
    devices:
      requests:
      - name: all-gpus
        deviceClassName: gpu.nvidia.com
        allocationMode: All
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: imex-test1
  name: pod0
  labels:
    app: imex-test1-pod0
spec:
  replicas: 4
  selector:
    matchLabels:
      app: pod0
  template:
    metadata:
      labels:
        app: pod0
    spec:
      containers:
      - name: ctr
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: gpus
          - name: imex-channel
      resourceClaims:
      - name: gpus
        resourceClaimTemplateName: all-node-gpus
      - name: imex-channel
        resourceClaimName: shared-imex-channel0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: imex-test1
  name: pod1
  labels:
    app: imex-test1-pod1
spec:
  replicas: 4
  selector:
    matchLabels:
      app: pod1
  template:
    metadata:
      labels:
        app: pod1
    spec:
      containers:
      - name: ctr
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: gpus
          - name: imex-channel
      resourceClaims:
      - name: gpus
        resourceClaimTemplateName: all-node-gpus
      - name: imex-channel
        resourceClaimName: shared-imex-channel1
```

When I run this, I do not get the resource allocated to each pod in each deployment as expected (instead all pods remain pending forever).

However, If I change to explicitly requesting 1 GPU from a node instead of using `allocationMode: All`, things work as expected.

One thing to note is that the code linked below doesn't consider the CEL expression selector in the `gpu.nvidia.com` and `imex.nvidia.com` device classes when calculating `requestData.numDevices`. That might be factor in this somehow:
https://github.com/kubernetes/kubernetes/blob/52095a8b7b9b75d67a3882a21a6647e4f90ade48/staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go#L176-L210

#### What did you expect to happen?

My expectation was that we would see 4 pods from each deployment with the following set of resources:

**deployment 0, pod0**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 0
IMEX channel 0 from the same IMEX domain as all other pods in deployment 0

**deployment 0, pod1**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 0
IMEX channel 0 from the same IMEX domain as all other pods in deployment 0

**deployment 0, pod2**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 0
IMEX channel 0 from the same IMEX domain as all other pods in deployment 0

**deployment 0, pod3**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 0
IMEX channel 0 from the same IMEX domain as all other pods in deployment 0

**deployment 1, pod0**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 1
IMEX channel 0 from the same IMEX domain as all other pods in deployment 1

**deployment 1, pod1**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 1
IMEX channel 0 from the same IMEX domain as all other pods in deployment 1

**deployment 1, pod2**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 1
IMEX channel 0 from the same IMEX domain as all other pods in deployment 1

**deployment 1, pod3**:
1 GPU from a node in the same IMEX domain as all other pods in deployment 1
IMEX channel 0 from the same IMEX domain as all other pods in deployment 1

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the specs listed above in a cluster with the following deviceClasses and DRA resources available...

Here are my (relevant) device classes:
```
---
- apiVersion: resource.k8s.io/v1alpha3
  kind: DeviceClass
  metadata:
    name: gpu.nvidia.com
  spec:
    selectors:
    - cel:
        expression: device.driver == 'gpu.nvidia.com' && device.attributes['gpu.nvidia.com'].type
          == 'gpu'
---
- apiVersion: resource.k8s.io/v1alpha3
  kind: DeviceClass
    name: imex.nvidia.com
  spec:
    selectors:
    - cel:
        expression: device.driver == 'gpu.nvidia.com' && device.attributes['gpu.nvidia.com'].type
          == 'imex-channel'
```

Here is the definition of one of my GPU nodes (the others look similar except for the node name):
```
- apiVersion: resource.k8s.io/v1alpha3
  kind: ResourceSlice
  metadata:
    name: k8s-dra-driver-cluster-worker-gpu.nvidia.com-dpwj8
  spec:
    devices:
    - basic:
        attributes:
          architecture:
            string: Ampere
          brand:
            string: Nvidia
          cudaComputeCapability:
            version: 8.0.0
          cudaDriverVersion:
            version: 12.6.0
          driverVersion:
            version: 560.35.3
          index:
            int: 0
          minor:
            int: 7
          productName:
            string: NVIDIA A100-SXM4-40GB
          type:
            string: gpu
          uuid:
            string: GPU-b1028956-cfa2-0990-bf4a-5da9abb51763
        capacity:
          memory: 40Gi
      name: gpu-0
    driver: gpu.nvidia.com
    nodeName: k8s-dra-driver-cluster-worker
    pool:
      generation: 0
      name: k8s-dra-driver-cluster-worker
      resourceSliceCount: 1
```

Here is the definition of my 2 IMEX channel resourceSlices:
```
---
apiVersion: v1
items:
- apiVersion: resource.k8s.io/v1alpha3
  kind: ResourceSlice
  metadata:
    name: imex-domain-0f884867-ba2f-4294-9155-b495ff367eea-1
  spec:
    devices:
    - basic:
        attributes:
          channel:
            int: 0
          type:
            string: imex-channel
      name: imex-channel-0
    - basic:
        attributes:
          channel:
            int: 1
          type:
            string: imex-channel
      name: imex-channel-1
    ...
    driver: gpu.nvidia.com
    nodeSelector:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.clusteruuid
          operator: In
          values:
          - 0f884867-ba2f-4294-9155-b495ff367eea
        - key: nvidia.com/gpu.cliqueid
          operator: In
          values:
          - "1"
    pool:
      generation: 0
      name: imex-domain-0f884867-ba2f-4294-9155-b495ff367eea-1
      resourceSliceCount: 1
---
apiVersion: v1
items:
- apiVersion: resource.k8s.io/v1alpha3
  kind: ResourceSlice
  metadata:
    name: imex-domain-0f884867-ba2f-4294-9155-b495ff367eea-2
  spec:
    devices:
    - basic:
        attributes:
          channel:
            int: 0
          type:
            string: imex-channel
      name: imex-channel-0
    - basic:
        attributes:
          channel:
            int: 1
          type:
            string: imex-channel
      name: imex-channel-1
    ...
    driver: gpu.nvidia.com
    nodeSelector:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.clusteruuid
          operator: In
          values:
          - 0f884867-ba2f-4294-9155-b495ff367eea
        - key: nvidia.com/gpu.cliqueid
          operator: In
          values:
          - "2"
    pool:
      generation: 0
      name: imex-domain-0f884867-ba2f-4294-9155-b495ff367eea-2
      resourceSliceCount: 1
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

NONE

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用'ResourceClaimTemplate'和`allocationMode: All`时，资源分配未按预期工作的问题。这是一个关于Kubernetes资源分配的功能性问题，涉及到GPU和IMEX通道的资源请求配置。不存在攻击者可以利用的安全漏洞，也没有涉及到任何敏感信息泄露、权限提升、代码执行或容器逃逸等高风险安全问题。根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #127538 Certificate Mismatch causes repeated reloads and temporary connection disruptions

- Issue 链接：[#127538](https://github.com/kubernetes/kubernetes/issues/127538)

### Issue 内容

#### What happened?

Once we upgraded to 1.23 we started to see intermittent connectivity issues between apiserver and kubelet 

│ kube-apiserver E0920 16:30:36.852849      11 dynamic_serving_content.go:218] key failed with : tls: private key does not match public key                                                                                                                │
│ kube-apiserver I0920 16:30:36.852927      11 dynamic_serving_content.go:192] "Failed to remove file watch, it may have been deleted" file="/srv/kubernetes/kube-apiserver/apiserver-aggregator.crt" err="fsnotify: can't remove non-existent watch: /srv │
│ /kubernetes/kube-apiserver/apiserver-aggregator.crt"                                                                                                                                                                                                     │
│ kube-apiserver E0920 16:30:36.853099      11 dynamic_serving_content.go:218] key failed with : tls: private key does not match public key                                                                                                                │
│ kube-apiserver I0920 16:30:36.853144      11 dynamic_serving_content.go:192] "Failed to remove file watch, it may have been deleted" file="/srv/kubernetes/kube-apiserver/server.key" err="fsnotify: can't remove non-existent watch: /srv/kubernetes/ku │
│ be-apiserver/server.key"                                                                                                                                                                                                                                 │
│ kube-apiserver E0920 16:30:36.853327      11 dynamic_serving_content.go:218] key failed with : tls: private key does not match public key                                                                                                                │
│ kube-apiserver I0920 16:30:36.853520      11 dynamic_serving_content.go:192] "Failed to remove file watch, it may have been deleted" file="/srv/kubernetes/kube-apiserver/apiserver-aggregator.key" err="fsnotify: can't remove non-existent watch: /srv │
│ /kubernetes/kube-apiserver/apiserver-aggregator.key"                                                                                                                                                                                                     │
│ kube-apiserver I0920 16:30:36.854522      11 dynamic_serving_content.go:192] "Failed to remove file watch, it may have been deleted" file="/srv/kubernetes/kube-apiserver/server.crt" err="fsnotify: can't remove non-existent watch: /srv/kubernetes/ku │
│ be-apiserver/server.crt"                                                                                                                                                                                                                                 │
│ kube-apiserver I0920 16:30:36.854747      11 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/srv/kubernetes/kube-apiserver/server.crt::/srv/kubernetes/kube-apiserver/server.key"                                      │
│ kube-apiserver I0920 16:30:36.854747      11 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="aggregator-proxy-cert::/srv/kubernetes/kube-apiserver/apiserver-aggregator.crt::/srv/kubernetes/kube-apiserver/apiserver-aggregator.key" │
│ kube-apiserver I0920 16:30:36.854965      11 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca-bundle::/srv/kubernetes/ca.crt,request-header::/srv/kubernetes/kube-apiserver/apiserver-aggregator-ca.crt" certDetail="\"kubernetes-ca\" [ │
│ ] issuer=\"<self>\" (2023-07-30 14:24:05 +0000 UTC to 2033-07-29 14:24:05 +0000 UTC (now=2024-09-20 16:30:36.85493496 +0000 UTC))"                                                                                                                       │
│ kube-apiserver I0920 16:30:36.855008      11 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca-bundle::/srv/kubernetes/ca.crt,request-header::/srv/kubernetes/kube-apiserver/apiserver-aggregator-ca.crt" certDetail="\"apiserver-aggrega ││ tor-ca\" [] issuer=\"<self>\" (2023-07-30 14:24:04 +0000 UTC to 2033-07-29 14:24:04 +0000 UTC (now=2024-09-20 16:30:36.854979481 +0000 UTC))"                                                                                                            │
│ kube-apiserver I0920 16:30:36.855162      11 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/srv/kubernetes/kube-apiserver/server.crt::/srv/kubernetes/kube-apiserver/server.key" certDetail="\"kubernetes-master\" [serving] validServ │
│ ingFor=[<REDACTED>] issuer=\"kubernetes-ca\" (2024-09-18 16:30:29 +0000 UTC to 2026-01-01 12:30:29 +0000 UTC (now=2024-09-20 16:30:36.855140025 +0000 UTC))"                                              │
│ kube-apiserver I0920 16:30:36.855304      11 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1726736613\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"api │
│ server-loopback-client-ca@1726736613\" (2024-09-19 08:03:33 +0000 UTC to 2025-09-19 08:03:33 +0000 UTC (now=2024-09-20 16:30:36.855284958 +0000 UTC))"        

                                                                                           │
│ kube-apiserver I0920 16:30:39.542320      11 cert_rotation.go:88] certificate rotation detected, shutting down client connections to start using new credentials                                                                                         │
│ kube-apiserver E0920 16:30:39.543643      11 status.go:71] apiserver received an error that is not an metav1.Status: &url.Error{Op:"Get", URL:"https://<WORKER NODE>:10250/containerLogs/default/application/filebeat?sinceSecon │
│ ds=300", Err:(*net.OpError)(0xc071c91090)}: Get "https://11.221.58.11:10250/containerLogs/default/application/filebeat?sinceSeconds=300": write tcp <CONTROL_PLANE NODE>:37554-><WORKER NODE>:10250: use of closed network connection 


At the same time this happens on apiserver, kubelet logs :
Sep 20 16:30:39 ip-11-221-58-11 kubelet[5111]: I0920 16:30:39.542666    5111 log.go:245] http: TLS handshake error from <CONTROL_PLANE NODE>:37554: EOF


I checked certs, they are all valid, 


[v6log.txt](https://github.com/user-attachments/files/17089085/v6log.txt)


#### What did you expect to happen?

I expected, that once certs are rotated, this will not cause any intermittent network issues.


#### How can we reproduce it (as minimally and precisely as possible)?

deploy v1.22 with kops, continually run kubectl logs command and observe 

#### Anything else we need to know?

This issue is not causing outage, we implemented retry mechanism from client side but we would like to remove it as this happened  on 1.23 and currently we are at 1.29 and this issue still persists.

I deployed simple script that is checking in loop certs like : 

openssl x509 -noout -modulus -in /srv/kubernetes/kube-apiserver/server.crt | openssl md5
openssl rsa -noout -modulus -in /srv/kubernetes/kube-apiserver/server.key | openssl md5

and at certain times i can see mismatch.

I checked PRs for 1.23 and only thing that I think might be somehow related is this : https://github.com/kubernetes/kubernetes/pull/104102


#### Kubernetes version

current version 1.29  ( but this issue started to appear since 1.23)

1.22 without any issues 


#### Cloud provider

AWS

Kops 1.25 for test purposes ( works with 1.22, issue started to appear 1.23 and higher)


#### OS version

_No response_

#### Install tools

currently Kops 1.25 for test purposes ( works with k8s 1.22, issue started to appear in version 1.23 and higher)

#### Container runtime (CRI) and version (if applicable)

crictl

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

cilium


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述的是在升级到Kubernetes 1.23后，apiserver与kubelet之间出现了间歇性的连接问题，日志中显示证书和私钥不匹配的错误信息。这种情况似乎是由于证书轮换过程中，证书和私钥文件在替换时不同步，导致短暂的证书不匹配，从而引起连接中断。

从安全风险的角度来看，虽然这种证书不匹配会导致连接中断，但并没有迹象表明攻击者可以利用这一问题进行攻击。没有敏感信息泄露的风险，也没有导致命令执行、容器逃逸、提权等高风险问题的迹象。

根据风险判断标准，第1条，该风险不能被攻击者利用；第2条，该风险不可能成为一个漏洞，被分配CVE编号，使用CVSS 3.1评分标准打分，结果不会在high以上。

因此，综合判断，该Issue不涉及安全风险。

---

## Issue #127526 Pods Not Scaling Up with HPA Despite CPU Utilization Exceeding Target

- Issue 链接：[#127526](https://github.com/kubernetes/kubernetes/issues/127526)

### Issue 内容

#### What happened?

I'm using the HPA to scale my pods based on CPU usage. I've set the target CPU utilization at 50%, with a stabilization window of 0. However, my pods are not scaling up, even though the current CPU usage consistently exceeds 50%. This issue persists for over 30 minutes without any scaling activity.

#### What did you expect to happen?

The HPA will trigger scale up immediately  

#### How can we reproduce it (as minimally and precisely as possible)?

Create an OKE cluster and follow this [guide](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"b6943c3c67cd1e3b8a1269566e755e899ed25ce2", GitTreeState:"clean", BuildDate:"2023-06-23T15:16:54Z", GoVersion:"go1.20.4 4493 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}

Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.12", GitCommit:"19d5e4ee03daf5e8fb55a88b1a52d94332435e7e", GitTreeState:"clean", BuildDate:"2023-07-26T10:00:24Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}

</details>


#### Cloud provider

<details>
Oracle OKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在使用Horizontal Pod Autoscaler（HPA）时，发现当CPU使用率超过目标值时，Pod没有按预期进行扩容。这是一个功能性问题，可能与HPA的配置、Kubernetes集群的设置或Oracle OKE的兼容性有关。

从安全风险的角度来看，此Issue并未涉及任何可被攻击者利用的安全漏洞。没有迹象表明该问题会导致命令执行、容器逃逸、权限提升等高风险安全问题。

根据风险判断标准，第6条指出，如果Issue不涉及安全问题，则风险评级判断为“不涉及”。因此，此Issue的风险评级为“不涉及”。

---

## Issue #127508 PodScheduled status.conditions field does not have an entry in `managedFields` for Pod

- Issue 链接：[#127508](https://github.com/kubernetes/kubernetes/issues/127508)

### Issue 内容

#### What happened?

In the managed fields ownership for a Pod, no owner entry is present for `f.conditions: k:{"type":"PodScheduled"}`

#### What did you expect to happen?

Based on https://github.com/kubernetes/kubernetes/blob/v1.31.1/pkg/kubelet/status/status_manager.go#L639-L640 I expect all these conditions to be owned by the `kubelet` manager.

#### How can we reproduce it (as minimally and precisely as possible)?

Schedule a pod
`kubectl get pod -o yaml --show-managed-fields`
Look for `k:{"type":"PodScheduled"}` and it will not be present

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
running on my local machine
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
# paste output here
Linux kind-control-plane 6.6.26-linuxkit #1 SMP Sat Apr 27 04:13:19 UTC 2024 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind create cluster
</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在获取Pod的managedFields时，`f.conditions: k:{"type":"PodScheduled"}`字段没有对应的owner entry。根据提供的信息，这是一个关于Kubernetes中managedFields所有者追踪的问题。该问题可能影响到Kubernetes资源的管理和状态追踪，但并未涉及任何安全风险。没有证据表明攻击者可以利用此问题进行攻击，也不会导致权限提升、命令执行等高危后果。

根据风险判断标准：

1. 该风险不能被攻击者利用。

2. 此问题不可能成为一个漏洞，不会被分配CVE编号，使用CVSS 3.1评分也不会达到High以上。

6. Issue不涉及安全问题，风险评级判断为不涉及。

---

## Issue #127505 Upgrade failed when using patches directory

- Issue 链接：[#127505](https://github.com/kubernetes/kubernetes/issues/127505)

### Issue 内容

#### What happened?

We have some args already added to our kube-system StaticPods like etcd, kube-apiserver that needs to be persist during upgrade. until now we were passing `--config` flag with a path to a file include ClusterConfiguration and all configs that must be persist. But some of these args like `encryption-provider-config` in apiserver or  `listen-metrics-urls` in etcd gets removed when we run upgrade, so we need to manually add/update those values during upgrade process so the upgrade can continue.
And also using `--config` flags is not recommended for upgrade and soon it'll be deprecated.

so we decided to use `--patches` flags, but it seems none of the patchStrategy (merge, strategic, json) are capable of adding args to the current args already exist in those StaticPods.

here's an example with merge patchStrategy:
```
file name: etcd0+merge.yaml / etcd0+strategic.yam

apiVersion: v1
kind: Pod
metadata:
  name: etcd
spec:
  containers:
  - name: etcd
    command:
      - /usr/local/bin/etcd
    args:
      - --quota-backend-bytes=8589934592
      - --listen-metrics-urls=http://127.0.0.1:2381,https://192.68.210.21:2381
 ```
 
 here's an example with json patchStrategy :
 
 ```
 file name: etcd0+json.json
 
 [
    {
        "op": "add",
        "path": "/spec/containers/0/command/-",
        "value":  [ "--quota-backend-bytes=8589934592" ]
    },
    {
        "op": "add",
        "path": "/spec/containers/0/command/-",
        "value":  [ "--listen-metrics-urls=http://127.0.0.1:2381,https://192.68.210.21:2381" ]
    }
]
```

Upgrade command:
```
kubeadm upgrade apply v1.29.8  --patches /etc/kubernetes/patches/ --dry-run
```

No args added to to the StaticPod yaml file.


#### What did you expect to happen?

We're expecting to add additional args to kube-system StaticPods during upgrade process while keeping what is already there in their specs. ( not overriding )

#### How can we reproduce it (as minimally and precisely as possible)?

here's an example with merge patchStrategy:
```
file name: /etc/kubernetes/patches/etcd0+merge.yaml

apiVersion: v1
kind: Pod
metadata:
  name: etcd
spec:
  containers:
  - name: etcd
    command:
      - /usr/local/bin/etcd
    args:
      - --quota-backend-bytes=8589934592
      - --listen-metrics-urls=http://127.0.0.1:2381,https://192.68.210.21:2381
 ```
 
 ```
 kubeadm upgrade apply v1.29.8  --patches /etc/kubernetes/patches/ --dry-run
 ```
 

#### Anything else we need to know?

Here in this document an example provided to check available patchStrategy for PodSpec:

https://v1-29.docs.kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch

```
"io.k8s.api.core.v1.PodSpec": {
    ...,
    "containers": {
        "description": "List of containers belonging to the pod.  ...."
    },
    "x-kubernetes-patch-merge-key": "name",
    "x-kubernetes-patch-strategy": "merge"
}
```

But hew in OpemApi spec for args there's no patchStrategy provided:
https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json

```
    "io.k8s.api.core.v1.Container": {
      "description": "A single application container that you want to run within a pod.",
      "properties": {
        "args": {
          "description": "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell",
          "items": {
            "type": "string"
          },
          "type": "array",
          "x-kubernetes-list-type": "atomic"
        },
````


#### Kubernetes version

Current: 1.28.4
Upgrade to : 1.29.8

#### Cloud provider

<details>

</details>


#### OS version

```
NAME="AlmaLinux"
VERSION="8.10 (Cerulean Leopard)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="AlmaLinux 8.10 (Cerulean Leopard)"
```

```
uname -a
Linux kubm01 4.18.0-553.el8_10.x86_64 x86_64 x86_64 x86_64 GNU/Linux
```


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

containerd://1.6.31

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，Issue所描述的问题是关于在使用kubeadm升级Kubernetes集群时，尝试使用`--patches`参数添加补丁以在升级过程中保留或添加StaticPods（如etcd、kube-apiserver）的参数。由于`args`字段在OpenAPI规范中被标记为`x-kubernetes-list-type: atomic`，导致无法通过补丁策略（merge、strategic、json）来添加或合并参数。这是一个关于升级工具功能和配置的问题，不涉及任何可被攻击者利用的安全风险。

根据风险判断标准：
1. 该问题不涉及被攻击者利用的安全漏洞。
2. 不会成为一个漏洞，也不会被分配CVE编号，按照CVSS 3.1评分标准，得分不会在高危以上。
6. Issue不涉及安全问题，风险评级判断为不涉及。

---

## Issue #127502 Getting Unknown Field error for "podLogsDir"

- Issue 链接：[#127502](https://github.com/kubernetes/kubernetes/issues/127502)

### Issue 内容

#### What happened?

I want to change the default pod logs directory path from  "/var/log/pods" to ""/storage/kubelet/pods""

Below is the kubelet configuration in /etc/kubernetes/kube-cluster.conf
**apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
podLogsDir: "/storage/kubelet/pods"**
staticPodPath: "/etc/kubernetes/manifests"
tlsCertFile: "/etc/kubernetes/pki/kubelet.crt"



[root@kubemaster device-plugins]# /usr/local/bin/kubeadm init phase kubelet-start --config /etc/kubernetes/kube-cluster.conf
W0920 11:02:21.615655 1875303 initconfiguration.go:307] **error unmarshaling configuration schema.GroupVersionKind{Group:"kubelet.config.k8s.io", Version:"v1beta1", Kind:"KubeletConfiguration"}: strict decoding error: unknown field "podLogsDir"**
W0920 11:02:21.616537 1875303 configset.go:177] error unmarshaling configuration schema.GroupVersionKind{Group:"kubelet.config.k8s.io", Version:"v1beta1", Kind:"KubeletConfiguration"}: strict decoding error: unknown field "podLogsDir"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[root@kubemaster device-plugins]#



#### What did you expect to happen?

POD should start sending logs to "/storage/kubelet/pods" directory

#### How can we reproduce it (as minimally and precisely as possible)?

Add below line in KubeletConfiuration 
podLogsDir: "/storage/kubelet/pods"

Run below command 
kubeadm init phase kubeconfig kubelet --config <config-file-path>

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9

```

</details>


#### Cloud provider

NA. Using self managed K8s cluster in Vcenter


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Oracle Linux Server"
VERSION="8.10"
ID="ol"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Oracle Linux Server 8.10"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:oracle:linux:8:10:server"
HOME_URL="https://linux.oracle.com/"
BUG_REPORT_URL="https://github.com/oracle/oracle-linux"

ORACLE_BUGZILLA_PRODUCT="Oracle Linux 8"
ORACLE_BUGZILLA_PRODUCT_VERSION=8.10
ORACLE_SUPPORT_PRODUCT="Oracle Linux"
ORACLE_SUPPORT_PRODUCT_VERSION=8.10

$ uname -a
Linux kubemaster 5.4.17-2136.333.5.1.el8uek.x86_64 #3 SMP Fri Jul 12 12:38:51 PDT 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
[ root@kubemaster kubernetes]# crio version
INFO[2024-09-20 09:49:14.931951423Z] Starting CRI-O, version: 1.28.4, git: unknown(clean)
Version:        1.28.4
GitCommit:      unknown
GitCommitDate:  unknown
GitTreeState:   clean
BuildDate:      2024-04-10T21:13:08Z
GoVersion:      go1.19
Compiler:       gc
Platform:       linux/amd64
Linkmode:       dynamic
BuildTags:
  rpm_crashtraceback
  exclude_graphdriver_btrfs
  btrfs_noversion
  exclude_graphdriver_devicemapper
  libdm_no_deferred_remove
  seccomp
  containers_image_openpgp
LDFlags:          -X github.com/cri-o/cri-o/internal/pkg/criocli.DefaultsPath= -X  github.com/cri-o/cri-o/internal/version.buildDate=2024-04-10T21:13:08Z -X  github.com/cri-o/cri-o/internal/version.gitCommit=c5fc2a463053cf988db2aebe9b762700484922e5 -X  github.com/cri-o/cri-o/internal/version.version=1.28.4 -X  github.com/cri-o/cri-o/internal/version.gitTreeState=clean  -B 0x604967098e3ed4e008efb0700b88f0add5131ca3 -extldflags '-Wl,-z,relro  -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld ' -compressdwarf=false
SeccompEnabled:   true
AppArmorEnabled:  false

[ root@kubemaster kubernetes]#

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了用户尝试在`KubeletConfiguration`中添加`podLogsDir`字段以更改Pod日志目录，但在初始化kubelet时遇到了错误：`unknown field "podLogsDir"`。这是因为在所使用的`kubelet.config.k8s.io/v1beta1`版本的配置中，并不存在`podLogsDir`字段，导致解析配置文件时出现未知字段错误。

这个问题属于配置错误或版本兼容性问题，并未涉及任何安全风险。没有证据表明该问题可被攻击者利用，也没有造成任何潜在的漏洞。

依据风险判断标准第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。因此，该Issue不涉及安全风险。

---

## Issue #127474 API Server responds 200 OK, but not properly handling the request

- Issue 链接：[#127474](https://github.com/kubernetes/kubernetes/issues/127474)

### Issue 内容

#### What happened?

Tried to perform a `DELETE` operation using a k8s client obtained from a `RestConfig`. This config is constructed with a `token` that is obtained using a Service Principal which has `admin` permission to the k8s cluster.

There are 2 scenarios:
1. When using `https://` in the `APISERVER_ENDPOINT`, the object (an `ExternalSecret` in our case) is deleted, the API Server returns 200 OK and everything is alright.
2. When not setting `https://` in the `APISERVER_ENDPOINT`, the API server still returns 200 OK, **but the object is not deleted from the cluster.**

#### What did you expect to happen?

When not setting the `https://` prefix, the API Server should respond with something different from 200 OK (timeout or unauthorized, let's say). 

#### How can we reproduce it (as minimally and precisely as possible)?

I exemplified the 2 scenarios described above in this snippet.

```go
package main

import (
	"context"
	"fmt"
	"log"
	"os"

	es "github.com/external-secrets/external-secrets/apis/externalsecrets/v1beta1"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

// K8sClient is a wrapper around the Kubernetes clientset and dynamic client
type K8sClient struct {
	*kubernetes.Clientset
	config *rest.Config
	dynamic.Interface
	metaAccessor meta.MetadataAccessor
}

// getRestConfig creates a new REST config using a token and an API Server endpoint
func getRestConfig(token, endpoint string) (*rest.Config, error) {
	var tlsConfig rest.TLSClientConfig
	restconfig := &rest.Config{
		Host:            endpoint,
		BearerToken:     token,
		TLSClientConfig: tlsConfig,
	}

	log.Printf("Restconfig obtained successfully")
	return restconfig, nil
}

// NewClientForRESTConfig creates a new K8sClient using a REST config
func NewClientForRESTConfig(config *rest.Config) (*K8sClient, error) {
	dynamicClient, err := dynamic.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("couldn't create dynamic client")
	}

	clientSet, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, fmt.Errorf("couldn't create kubernetes clientset")
	}
	accessor := meta.NewAccessor()
	return &K8sClient{clientSet, config, dynamicClient, accessor}, nil
}

// getClient creates a new K8sClient using a token and an API Server endpoint
func getClient(token, endpoint string) (*K8sClient, error) {
	// Get restConfig using a token (obtained using a Service Principal) and the API Server endpoint
	restConfig, err := getRestConfig(token, endpoint)
	if err != nil {
		log.Printf("Error getting restconfig: %v", err)
		return nil, err
	}

	// Create a new K8sClient using the restConfig
	client, err := NewClientForRESTConfig(restConfig)
	if err != nil {
		log.Printf("Error creating kubernetes client: %v", err)
		return nil, err
	}
	log.Printf("Kubernetes client created successfully")
	return client, nil
}

func main() {
	// Get API Server endpoint from environment variable
	apiServerEndpoint := os.Getenv("API_SERVER_ENDPOINT")
	if apiServerEndpoint == "" {
		fmt.Println("API Server endpoint not provided")
		os.Exit(1)
	}

	// Get namespace from environment variable
	namespace := os.Getenv("NAMESPACE")
	if namespace == "" {
		fmt.Println("Namespace not provided")
		os.Exit(1)
	}

	// Get ExternalSecret's name from environment variable
	esName := os.Getenv("ES_NAME")
	if esName == "" {
		fmt.Println("ExternalSecret name not provided")
		os.Exit(1)
	}

	// Get token from environment variable (obtained using a Service Principal)
	token := os.Getenv("TOKEN")
	if token == "" {
		fmt.Println("Token not provided")
		os.Exit(1)
	}

	// Define the GVR for ExternalSecret
	esGVR := schema.GroupVersionResource{
		Group:    es.Group,
		Version:  es.Version,
		Resource: "externalsecrets",
	}

	/** IMPORTANT NOTE:
			It correctly deletes the externalsecret when using `https://<API_SERVER_ENDPOINT>` (and returns 200 response code
			It doesn't work when using `<API_SERVER_ENDPOINT>` (with `http://`)
			The externalsecret is not deleted, but the weird thing is that the API Server is still returning 200 response code
	**/

	// Create a new kubernetes client using the token and the API Server endpoint
	// With `https://<API_SERVER_ENDPOINT>` it works correctly
	// httpsAPIEndpoint := fmt.Sprintf("https://%s", apiServerEndpoint)
	// client, err := getClient(token, httpsAPIEndpoint)

	// With `<API_SERVER_ENDPOINT>` it returns 200, but the ExternalSecret is not deleted
	client, err := getClient(token, apiServerEndpoint)

	// Check if there was an error creating the kubernetes client
	if err != nil {
		fmt.Println("Error creating kubernetes client")
		return
	}

	// Delete ExternalSecret
	err = client.Resource(esGVR).Namespace(namespace).Delete(context.Background(), esName, metav1.DeleteOptions{})
	if err != nil {
		fmt.Println("Error deleting ExternalSecret: %v", err)
		os.Exit(1)
	}

	// Print success message
	fmt.Println("ExternalSecret deleted successfully")
}
```

And also, this is the testing script
```bash
#!/bin/bash

# Set environment variables
export CLUSTER_NAME="<cluster_name>"
export API_SERVER_ENDPOINT="<api_server_endpoint>"
export NAMESPACE="default"
export ES_NAME="example-es"
export TOKEN="<token>"

# Create the ExternalSecret
echo "Create the ExternalSecret '$ES_NAME' in the '$NAMESPACE' namespace"
kubectl --context=$CLUSTER_NAME apply -f ./external-secret.yaml
echo "---------------------------------------------------------"

# Check if the ExternalSecret was created
echo "Check if the ExternalSecret was created"
kubectl --context=$CLUSTER_NAME get externalsecret -n $NAMESPACE $ES_NAME
echo "---------------------------------------------------------"

# Run the test
echo "Test deleting when it exists"
go run main.go
echo "---------------------------------------------------------"

# Check if the ExternalSecret was deleted
echo "Check if the ExternalSecret was deleted (it should not exist anymore)"
kubectl --context=$CLUSTER_NAME get externalsecret -n $NAMESPACE $ES_NAME
echo "---------------------------------------------------------"

# Run the test
echo "Test deleting when it does not exist"
go run main.go
echo "---------------------------------------------------------"
```

And that's the `ExternalSecret` dummy object
```yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: example-es
  namespace: default
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: secretstore-sample
    kind: SecretStore
  target:
    name: secret-to-be-created
    creationPolicy: Owner
  data:
  - secretKey: secret-key-to-be-managed
    remoteRef:
      key: provider-key
      version: provider-key-version
      property: provider-key-property
  dataFrom:
  - extract:
      key: remote-key-in-the-provider
```

Below are the 2 runs
1. Logs when using the `https://` prefix (returns 200 OK and deletes the object)
```
Create the ExternalSecret 'example-es' in the 'default' namespace
externalsecret.external-secrets.io/example-es configured
---------------------------------------------------------
Check if the ExternalSecret was created
NAME         STORE                REFRESH INTERVAL   STATUS              READY
example-es   secretstore-sample   1h                 SecretSyncedError   False
---------------------------------------------------------
Test deleting when it exists
2024/09/19 16:48:20 Restconfig obtained successfully
2024/09/19 16:48:20 Kubernetes client created successfully
ExternalSecret deleted successfully
---------------------------------------------------------
Check if the ExternalSecret was deleted (it should not exist anymore)
Error from server (NotFound): externalsecrets.external-secrets.io "example-es" not found
---------------------------------------------------------
Test deleting when it does not exist
2024/09/19 16:48:23 Restconfig obtained successfully
2024/09/19 16:48:23 Kubernetes client created successfully
Error deleting ExternalSecret: %v externalsecrets.external-secrets.io "example-es" not found
exit status 1
---------------------------------------------------------
```

2. Logs when **not** using the `https://` prefix (returns 200 OK, **but doesn't delete the object**)
```
Create the ExternalSecret 'example-es' in the 'default' namespace
externalsecret.external-secrets.io/example-es created
---------------------------------------------------------
Check if the ExternalSecret was created
NAME         STORE                REFRESH INTERVAL   STATUS              READY
example-es   secretstore-sample   1h                 SecretSyncedError   False
---------------------------------------------------------
Test deleting when it exists
2024/09/19 16:49:15 Restconfig obtained successfully
2024/09/19 16:49:15 Kubernetes client created successfully
ExternalSecret deleted successfully
---------------------------------------------------------
Check if the ExternalSecret was deleted (it should not exist anymore)
NAME         STORE                REFRESH INTERVAL   STATUS              READY
example-es   secretstore-sample   1h                 SecretSyncedError   False
---------------------------------------------------------
Test deleting when it does not exist
2024/09/19 16:49:19 Restconfig obtained successfully
2024/09/19 16:49:19 Kubernetes client created successfully
ExternalSecret deleted successfully
---------------------------------------------------------
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.7-eks-2f46c53

```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，在未指定`https://`前缀的情况下，API服务器返回200 OK，但是对象并未被删除。这可能是由于客户端配置不正确导致的请求未正确发送，但服务器错误地返回了200 OK。

根据风险判断标准：

1. **该风险能被攻击者利用**：此问题主要是由于客户端在构建请求时未加上`https://`前缀，导致请求未正确发送。攻击者无法利用该问题对系统或其他用户造成影响。

3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险，因为它是issue提交者的问题，而不是项目的问题**：该问题属于客户端配置不当，属于使用者的问题，不是项目本身的安全漏洞。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue不涉及安全风险。

综上所述，此Issue不涉及安全风险。

---

## Issue #127465 topologySpreadConstraints for availability zone in aws is not working as expected 

- Issue 链接：[#127465](https://github.com/kubernetes/kubernetes/issues/127465)

### Issue 内容

#### What happened?

Stateful set has 3az spread 
```
│       topologySpreadConstraints:                                                                                                                               │
│       - labelSelector:                                                                                                                                         │
│           matchLabels:                                                                                                                                         │
│             app: myApp                                                                                                                                           │
│             component: myComponent                                                                                                                                      │
│             id:  app-65                                                                                                                                         │
│             app-id: app-65                                                                                                                                     │
│         maxSkew: 1                                                                                                                                             │
│         topologyKey: topology.kubernetes.io/zone                                                                                                               │
│         whenUnsatisfiable: DoNotSchedule
```

#### What did you expect to happen?

It supposed to have 3 replica with 1 in each az but it ended up all 3 replica in one az

#### How can we reproduce it (as minimally and precisely as possible)?

Able to reproduce 2 times but not always 

#### Anything else we need to know?

there were capacity issues in other azs but it should result it not scheduling pods there

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here

Server Version: v1.28.12-eks-2f46c53
WARNING: version difference between client (1.31) and server (1.28) exceeds the supported minor version skew of +/-1

```

</details>


#### Cloud provider

<details>
AWS

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中使用`topologySpreadConstraints`时，期望StatefulSet在3个可用区（AZ）中各部署一个副本，但实际所有3个副本都部署在了一个可用区内。这可能是由于其他可用区的容量问题导致Pod无法调度，但这属于集群资源配置或调度策略的问题。

根据风险判断标准：

1. 该问题无法被攻击者利用，不存在被恶意利用的风险。
2. 不会导致任何安全漏洞产生，不会被分配CVE编号，CVSS评分不适用。
6. 因此，该Issue不涉及安全问题，风险评级判断为不涉及。

---

## Issue #127463 A DaemonSet pod environment variable did not inject service information

- Issue 链接：[#127463](https://github.com/kubernetes/kubernetes/issues/127463)

### Issue 内容

#### What happened?

I have a k8s cluster and created a DaemonSet in the cluster that is associated with three pods. I found that one of the pods did not inject service information into its environment variables

services:
```shell
[root@controller-0-2:/k8s]$ kubectl get svc -n admin
NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
assem-apiserver           ClusterIP   fd00::5be3   <none>        2509/TCP   23h
assem-apiserver-cluster   ClusterIP   fd00::e0b3   <none>        2509/TCP   23h
```
The problematic pod environment variable information is as follows:
```shell
[root@controller-0-2:/k8s]$ kubectl exec -it -n admin         assem-cic-kbbb8 -- env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=controller-0-2
ENV=/etc/profile
POD_NAMESPACE=admin
POD_NAME=assem-cic-kbbb8
KUBERNETES_PORT=tcp://[fd00::1]:443
KUBERNETES_PORT_443_TCP=tcp://[fd00::1]:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=fd00::1
KUBERNETES_SERVICE_HOST=fd00::1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
TERM=xterm
```

The normal pod environment variables are as follows:
```shell
[root@controller-0-0:/k8s]$ kubectl exec -it -n admin         assem-cic-jbzbh -- env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=-controller-0-0
ENV=/etc/profile
POD_NAME=op-assem-cic-jbzbh 
POD_NAMESPACE=admin
ASSEM_APISERVER_CLUSTER_PORT_2509_TCP=tcp://[fd00::e0b3]:2509
ASSEM_APISERVER_PORT_2509_TCP_PORT=2509
KUBERNETES_SERVICE_HOST=fd00::1
ASSEMAPISERVER_CLUSTER_SERVICE_PORT_HTTPS=2509
ASSEM_APISERVER_CLUSTER_PORT=tcp://[fd00::e0b3]:2509
ASSEM_APISERVER_CLUSTER_PORT_2509_TCP_PROTO=tcp
ASSEM_APISERVER_CLUSTER_PORT_2509_TCP_PORT=2509
ASSEM_APISERVER_SERVICE_PORT_HTTPS=2509
ASSEM_APISERVER_PORT_2509_TCP_PROTO=tcp
ASSEM_APISERVER_PORT_2509_TCP_ADDR=fd00::5be3
ASSEM_APISERVER_CLUSTER_SERVICE_HOST=fd00::e0b3
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
ASSEM_APISERVER_CLUSTER_SERVICE_PORT=2509
ASSEM_APISERVER_PORT=tcp://[fd00::5be3]:2509
ASSEM_APISERVER_CLUSTER_PORT_2509_TCP_ADDR=fd00::e0b3
ASSEM_APISERVER_SERVICE_PORT=2509
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP=tcp://[fd00::1]:443
ASSEM_APISERVER_PDM_SERVICE_PORT=2509
ASSEM_APISERVER_SERVICE_HOST=fd00::5be3
ASSEM_APISERVER_PORT_2509_TCP=tcp://[fd00::5be3]:2509
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT=tcp://[fd00::1]:443
KUBERNETES_PORT_443_TCP_ADDR=fd00::1
TERM=xterm
```

`assem-apiserver` service creation time:  
```shell
creationTimestamp: "2024-09-18T03:37:28Z"
```

`assem-apiserver-cluster` service creation time:  
```shell
creationTimestamp: "2024-09-18T03:37:28Z"
```

`assem-cic-kbbb8 ` pod creation time:
```yaml
[root@controller-0-2:/k8s]$ kubectl get po -n admin        assem-cic-kbbb8 -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-09-18T03:37:28Z"
 ...
  namespace: admin
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: assem-cic
    uid: 0c5601ca-d6ab-4db7-8d8b-e18321e3e741
  resourceVersion: "2092"
  uid: 4b96702a-c0ff-4a6e-9510-2ec0f467ceee
spec:
  ...
  dnsPolicy: ClusterFirstWithHostNet
  enableServiceLinks: true
  hostNetwork: true
 ...
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:37:31Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:38:46Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:38:46Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:37:28Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://9529e9564e71706ada80b6280e16537418e35fe61aa5e013d4fbc90c29f113c5
    ...
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-09-18T03:37:32Z"
  hostIP: 193:116:66::19
  initContainerStatuses:
  - containerID: containerd://69507fe03ce358b168be50336dc6361b2aa1581ad0bae1bec4d3b3da62d7b90a
   ...
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: containerd://69507fe03ce358b168be50336dc6361b2aa1581ad0bae1bec4d3b3da62d7b90a
        exitCode: 0
        finishedAt: "2024-09-18T03:37:30Z"
        reason: Completed
        startedAt: "2024-09-18T03:37:30Z"
  phase: Running
  podIP: 193:116:66::19
  podIPs:
  - ip: 193:116:66::19
  - ip: 192.0.0.40
  qosClass: Burstable
  startTime: "2024-09-18T03:37:28Z"

```  

` assem-cic-jbzbh ` pod creation time:
```yaml
[root@controller-0-2:/k8s]$ kubectl get po -n admin         assem-cic-jbzbh -oyaml
apiVersion: v1
kind: Pod
metadata:
  ...
  creationTimestamp: "2024-09-18T03:37:28Z"
  generateName: assem-cic-
  ...
  namespace: admin
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: assem-cic
    uid: 0c5601ca-d6ab-4db7-8d8b-e18321e3e741
  resourceVersion: "2138"
  uid: f29089eb-f11e-4994-9b37-d87d50da41f9
spec:
  ...
  dnsPolicy: ClusterFirstWithHostNet
  enableServiceLinks: true
  hostNetwork: true
  ...
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:37:31Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:38:50Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:38:50Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-09-18T03:37:28Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://146868dac3e14064117e31f4497ae6d7f19d3f0b876b0710945d039ff2baf404
   ...
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-09-18T03:37:31Z"
  hostIP: 193:116:66::2c
  initContainerStatuses:
  - containerID: containerd://d6829037467552e02fb4688872fdd5d6b19087a8320b2c03db75a9d2703231f0
    ...
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: containerd://d6829037467552e02fb4688872fdd5d6b19087a8320b2c03db75a9d2703231f0
        exitCode: 0
        finishedAt: "2024-09-18T03:37:31Z"
        reason: Completed
        startedAt: "2024-09-18T03:37:31Z"
  phase: Running
  podIP: 193:116:66::2c
  podIPs:
  - ip: 193:116:66::2c
  - ip: 192.0.0.45
  qosClass: Burstable
  startTime: "2024-09-18T03:37:28Z"
```



#### What did you expect to happen?

Service information can also be injected into the pod `assem-cic-kbbb8` environment variable.

#### How can we reproduce it (as minimally and precisely as possible)?

deploy svc and pod

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.28.3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了一个DaemonSet中的一个Pod没有将服务信息注入到其环境变量中，而其他Pod则正常。这可能是由于配置问题或特定Pod的环境导致的，并不涉及安全风险。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127457 Kubelet plugin registration reliability

- Issue 链接：[#127457](https://github.com/kubernetes/kubernetes/issues/127457)

### Issue 内容

I am trying to assess the reliability of the kubelet plugins registration.

I am trying it out with the Device Plugin, but the same is likely can be applied to the DRA. The issue describes some findings and concerns, I didn't perform the full review.

I started the e2e to try out things: https://github.com/kubernetes/kubernetes/pull/127304

Plugins documentation: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/pluginmanager/pluginwatcher/README.md

So the flow is:

- Kubelet looks up sockets in `/var/lib/kubelet/plugins_registry/`
- Kubelet tries to get plugin details by calling `GetInfo`
- One information received, kubelet tries to connect to the plugin

- Kubelet proceeds with getting the list of pods to run

The flow is much better in terms of race conditions comparing to the Device Plugin registration when Device Plugin need to detect when kubelet was restarted and reconnect to it.

Two reliability issues I notices immediately in tests:

1. When `GetInfo` fails, there seems to be no retries for while to get plugin details. The timeout for GetInfo is `1sec`. 1 second is a big enough number for an endpoint that simply returns the structure. But flakes are easy to imagine here.
2. When `GetDevicePluginOptions` fails, there is no retry to create a plugin for at least 30 seconds. There may be many reason for a flake when the API didn't return sucessfully for the first time. So retry is essential here.

I need to look deeper into other reliability things:

1. How fast kubelet will detect that the plugin was restarted and will attempt to reconnect? What is the mechanism for it?
2. Will there be retries on `ListAndWatch` failures?
3. Should interface implementing `GetInfo` be checking the health of the plugin? Should the best practice implementation be recreating the socket?

Similar questions can be explored for the DRA.


I think the best way to explore this is to continue working on e2e tests: https://github.com/kubernetes/kubernetes/pull/127304 demonstrating the behavior and a best practice registering the device plugin.

If we can confirm that the kubelet plugins system is almost as reliable as the today's most used device plugin registration mechanism, while eliminating the race condition on kubelet restart (https://github.com/kubernetes/kubernetes/issues/120146#issuecomment-2302666130), we should consider deprecating the `RegisterDevicePluginServer` API.


/sig node
/kind bug

CC: @ffromani @johnbelamaric 

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论的是Kubelet插件注册的可靠性问题，特别是关于在`GetInfo`和`GetDevicePluginOptions`调用失败时缺乏重试机制，可能导致插件注册失败。这可能影响系统的稳定性和可用性，但并未提及任何可能被攻击者利用的安全漏洞，也未涉及攻击者可执行的恶意操作。根据风险判断标准，第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。因此，该Issue不涉及安全风险。

---

## Issue #127436 Kubernetes 1.28.14 coredns not working, [ERROR] plugin/errors: 2 7550147287576560633.7996095784120876925. HINFO: read udp 10.0.1.5:55591->8.8.8.8:53: read: no route to host

- Issue 链接：[#127436](https://github.com/kubernetes/kubernetes/issues/127436)

### Issue 内容

#### What happened?

I am using kubernetes 1.28.14 version on RHEL 9.4

cat /etc/*release*
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"

Coredns is throwing following error

linux/amd64, go1.20, 055b2c3
[ERROR] plugin/errors: 2 7550147287576560633.7996095784120876925. HINFO: read udp 10.0.1.5:55591->8.8.8.8:53: read: no route to host
[ERROR] plugin/errors: 2 7550147287576560633.7996095784120876925. HINFO: read udp 10.0.1.5:34104->8.8.8.8:53: read: no route to host

Before on RHEL 7 we resolved this by implementing following

iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT
iptables -F

But seems its not working in the current version





#### What did you expect to happen?

Core dns should work fine

#### How can we reproduce it (as minimally and precisely as possible)?

Install kubernetes version 1.28.14 on RHEL 9.4

#### Anything else we need to know?

_No response_

#### Kubernetes version

 kubectl version
Client Version: v1.28.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.14


#### Cloud provider

Bare metal


#### OS version

cat /etc/*release*
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
Red Hat Enterprise Linux release 9.4 (Plow)
Red Hat Enterprise Linux release 9.4 (Plow)
cpe:/o:redhat:enterprise_linux:9::baseos


#### Install tools

kubeadm, calico and containerd

#### Container runtime (CRI) and version (if applicable)

Containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了用户在RHEL 9.4上使用Kubernetes 1.28.14时，CoreDNS无法正常工作，出现了错误信息：`[ERROR] plugin/errors: 2 7550147287576560633.7996095784120876925. HINFO: read udp 10.0.1.5:55591->8.8.8.8:53: read: no route to host`。用户提到之前在RHEL 7上，通过将iptables的INPUT、FORWARD、OUTPUT策略设置为ACCEPT，并清空所有规则（`iptables -F`），可以解决这个问题，但在当前版本中不起作用。

从安全角度来看，将iptables的默认策略设置为ACCEPT并清空所有规则，会导致防火墙失效，系统将暴露在外部网络中，存在安全风险。然而，根据风险判断标准第3条："issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险，因为它是issue提交者的问题，而不是项目的问题"。

因此，该Issue反映的是用户在配置和使用过程中的问题，并未发现Kubernetes项目本身存在被攻击者利用的安全漏洞，也不符合分配CVE编号的条件。故该Issue不涉及安全风险。

---

## Issue #127429 Endpoints controller uses stale endpoints in reconciling, the endpoint Subsets will be wrong and never restores correctly

- Issue 链接：[#127429](https://github.com/kubernetes/kubernetes/issues/127429)

### Issue 内容

#### What happened?

I have a service with two pods . These pods are ready, the endpoints subset contains these pods.

1. these pod became not ready at 12:35:39.149052Z
2. the endpoints controller update the endpoint, and move these pods to `notReadyAddresses`
3. one pod became Ready at 12:35:39.763051Z
4. the endpoints controller try to update the endpoint, but it failed with error: the object has been modified; please apply your changes to the latest version and try again
5. the other pod became Ready at 12:35:39.786936Z
6. the endpoints controller don't reconcile this endpoint any more, these pod are all in `notReadyAddresses`


The endpoint controller compare the endpoint subset (from cache) to pod status. At step 6, the endpoint informer watch is delayed,   the controller use the stale endpoint for comparison. From then on, the  endpoint subset is wrong. 


#### What did you expect to happen?

The endpoint subset should be reconciled to correctly status

#### How can we reproduce it (as minimally and precisely as possible)?

It is a little hard to reproduce, we need mock the endpoint watch delay.

#### Anything else we need to know?

The controller pod informer resync not work in this case. Pod sync event will be ignore in `podEndpointsChanged`

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.0-aliyun.1
Kustomize Version: v5.4.2
Server Version: v1.31.0-aliyun.1
```

</details>


#### Cloud provider

<details>
Alibaba cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes中endpoints controller在进行同步时使用了过期的endpoints信息，导致endpoint的Subsets错误且无法正确恢复。这会导致服务的endpoints状态不正确，可能影响服务的正常运行。但从Issue描述来看，问题的产生需要延迟endpoint informer的watch，这种情况下重现问题需要模拟watch的延迟，比较困难。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者需要能够控制或影响endpoint informer的watch延迟，这在正常情况下是不可行的，除非攻击者对集群的控制平面有高权限访问。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题并不涉及代码漏洞或安全漏洞，更多的是由于系统同步延迟导致的数据不一致，属于可靠性问题，而非安全问题。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

综合以上分析，该Issue不涉及安全风险。

---

## Issue #127408 [Flaky Test] capz-windows-master ci-kubernetes-e2e-capz-master-windows.Overall

- Issue 链接：[#127408](https://github.com/kubernetes/kubernetes/issues/127408)

### Issue 内容

#### Which jobs are flaking
- master-informing:
  - capz-windows-master

#### Which tests are flaking?
ci-kubernetes-e2e-capz-master-windows.Overall

#### Since when has it been flaking?
- Often once or twice daily since 09-03 05:10 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1830911221885308928)

Failed runs:
- 09-16 17:04 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1835801973924827136)
- 09-16 11:04 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1835711376044068864)
- 09-15 23:03 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1835529928867581952)
- 09-15 14:02 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1835711376044068864)
- 09-15 02:02 CDT [Prow link](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-capz-master-windows/1835212584630882304)


#### Testgrid link
[Testgrid link](https://testgrid.k8s.io/sig-release-master-informing#capz-windows-master)

#### Reason for failure (if possible)
```
Sun, 15 Sep 2024 19:12:24 +0000: cluster creation complete
Sun, 15 Sep 2024 19:12:25 +0000: bastion info: capi@null:22
Sun, 15 Sep 2024 19:12:25 +0000: wait for cluster to stabilize
Sun, 15 Sep 2024 19:17:25 +0000: cleaning up
./capz/run-capz-e2e.sh: line 103: capz::ci-build-azure-ccm::cleanup: command not found
E0915 19:17:55.212078    2635 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://capz-conf-l4mu8v-a076bee8.westus2.cloudapp.azure.com:6443/api?timeout=32s\": dial tcp 20.120.140.205:6443: i/o timeout"
E0915 19:18:25.214460    2635 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://capz-conf-l4mu8v-a076bee8.westus2.cloudapp.azure.com:6443/api?timeout=32s\": dial tcp 20.120.140.205:6443: i/o timeout"
E0915 19:18:55.216389    2635 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://capz-conf-l4mu8v-a076bee8.westus2.cloudapp.azure.com:6443/api?timeout=32s\": dial tcp 20.120.140.205:6443: i/o timeout"
E0915 19:19:25.218324    2635 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://capz-conf-l4mu8v-a076bee8.westus2.cloudapp.azure.com:6443/api?timeout=32s\": dial tcp 20.120.140.205:6443: i/o timeout"
E0915 19:19:55.220740    2635 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://capz-conf-l4mu8v-a076bee8.westus2.cloudapp.azure.com:6443/api?timeout=32s\": dial tcp 20.120.140.205:6443: i/o timeout"
Unable to connect to the server: dial tcp 20.120.140.205:6443: i/o timeout
+ EXIT_VALUE=1
+ set +o xtrace
Cleaning up after docker in docker.
```

#### Anything else we need to know?


#### Relevant SIG(s)
/sig windows
/kind flake

cc: @kubernetes/release-team-release-signal

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue报告的是Kubernetes的CAPZ（Cluster API Provider for Azure）Windows版本的e2e测试不稳定（Flaky Test）的问题。日志中显示了集群创建完成后，在等待集群稳定的过程中发生了错误，最终导致测试失败。

错误日志中包含了一些调试信息，例如脚本执行错误、无法连接API服务器等。但这些信息主要涉及测试环境的配置和网络连接问题，没有涉及到可能被攻击者利用的安全风险。

此外，虽然日志中暴露了API服务器的域名和IP地址，但是这些信息属于临时测试环境，且是在Issue提交者的日志中暴露的，根据风险判断标准第3条，这不属于安全风险，因为它是Issue提交者的问题，而不是项目的问题。

因此，该Issue不涉及安全问题。

---

## Issue #127390 /proxy/metics/cadvisor metrics data not updating metrics frequently

- Issue 链接：[#127390](https://github.com/kubernetes/kubernetes/issues/127390)

### Issue 内容

#### What happened?

HI,
we have a single server cluster and the container metrics are not updating frequently in /proxy/metrics/cadvisor, because of this prometheus is showing same values over period of time and range give 0 output.
`kubectl get --raw /api/v1/nodes/mynode/proxy/metrics/cadvisor | grep container_cpu_usage_seconds_total | grep mypod`

Kubernetes: **v1.30.1**

kubelet ps output:
`ps -ef | grep kubelet
root       11396       1 76 Sep13 ?        2-08:19:14 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.myzone.local:5000/pause:3.9 --cert-dir=/var/lib/kubelet/pki --cpu-manager-policy=static --housekeeping-interval=3600s --node-ip=172.16.0.18 --node-labels=node.uuid=39343550-3036-5a43-3233-343530425a33,node.uuid_source=mycluster,mycluster/version=2.29.1,node-pool=ucc,my.cluster.com/node-images-version=v3,infra.my.cluster.com/cluster=aaaa,infra.my.cluster.com/pool=ucc,infra.my.cluster.com/singleserver=,nic-type-sriov=true,isolation-interrupts=true,type=high-throughput --register-with-taints=infra.my.cluster.com/taskset=:NoSchedule --reserved-cpus=0,1,2,3,4,5,64,65,66,67,68,69 --runtime-cgroups=/myzone.slice/containerd.service --system-reserved=cpu=100m
`

```
>>> kubectl get --raw /api/v1/nodes/mynode/proxy/metrics/cadvisor | grep container_cpu_usage_seconds_total | grep mypod
container_cpu_usage_seconds_total{container="",cpu="total",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod711e104c_a951_4a1a_b123_a3169121bed7.slice",image="",name="",namespace="kube-system",pod="mypod"} 160.314258 1726485335746
container_cpu_usage_seconds_total{container="",cpu="total",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod711e104c_a951_4a1a_b123_a3169121bed7.slice/cri-containerd-60cb609ce98a2c5ce7c55892150af456850ffe11b59085e5e3a21f85362e6ea1.scope",image="registry.myzone.local:5000/pause:3.9",name="60cb609ce98a2c5ce7c55892150af456850ffe11b59085e5e3a21f85362e6ea1",namespace="kube-system",pod="mypod"} 0.208044 1726486650276
container_cpu_usage_seconds_total{container="mypod-consumer",cpu="total",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod711e104c_a951_4a1a_b123_a3169121bed7.slice/cri-containerd-47af50324ccf418da19327b140f5e9d4329ea893d62f9ba6290151949401a22b.scope",image="registry.myzone.local:5000/mypod:1.5-86-3ef3cd5d",name="47af50324ccf418da19327b140f5e9d4329ea893d62f9ba6290151949401a22b",namespace="kube-system",pod="mypod"} 158.987157 1726483476487
```

`curl -s http://$VM_SELECT:8481/select/0/prometheus/api/v1/query --data-urlencode 'query=(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor",pod="mypod",container="mypod-consumer"}[5m])' | jq
{
  "status": "success",
  "data": {
    "resultType": "matrix",
    "result": [
      {
        "metric": {
          "__name__": "container_cpu_usage_seconds_total",
          "namespace": "kube-system",
          "job": "kubernetes-nodes-cadvisor",
          "type": "high-throughput",
          "pod": "mypod",
          "name": "47af50324ccf418da19327b140f5e9d4329ea893d62f9ba6290151949401a22b",
          "beta_kubernetes_io_arch": "amd64",
          "beta_kubernetes_io_os": "linux",
          "container": "my-consumer",
          "cpu": "total",
          "id": "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod711e104c_a951_4a1a_b123_a3169121bed7.slice/cri-containerd-47af50324ccf418da19327b140f5e9d4329ea893d62f9ba6290151949401a22b.scope",
          "image": "registry.myzone.local:5000/mypod:1.5-86-3ef3cd5d",
          "isolation_interrupts": "true",
          "kubernetes_io_arch": "amd64",
          "kubernetes_io_os": "linux",
          "nic_type_sriov": "true",
          "node_pool": "ucc",
          "node_uuid": "39343550-3036-5a43-3233-343530425a33",
          "topology_topolvm_cybozu_com_node": "ucc-mycluster"
        },
        "values": [
          [
            1726488133.816,
            "158.987157"
          ],
          [
            1726488163.816,
            "158.987157"
          ],
          [
            1726488193.816,
            "161.795579"
          ],
          [
            1726488223.816,
            "161.795579"
          ],
          [
            1726488253.816,
            "161.795579"
          ],
          [
            1726488283.816,
            "161.795579"
          ],
          [
            1726488313.816,
            "161.795579"
          ],
          [
            1726488343.816,
            "161.795579"
          ],
          [
            1726488373.816,
            "161.795579"
          ],
          [
            1726488403.816,
            "161.795579"
          ]
        ]
      }
    ]
  }
}
`
When we enable verbosity for kubelet, we see these logs being printed but not sure if its related.
Aug 13 09:39:46 ucc-kubelet[10909]: I0813 09:39:46.185047   10909 cri_stats_provider.go:220] "
Unable to find cadvisor stats for container"
containerID="8cf4faf5f7a800b0430b7e081d362ca29fa24903df4f9253aa30b4d1163836c8"

#### What did you expect to happen?

cadvisor metrics should be updated frequently, one thing observered is /proxy/metrics/resources is keep updating the data.

#### How can we reproduce it (as minimally and precisely as possible)?

Observed in our local cluster

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
 kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="SLES"
VERSION="15-SP5"
VERSION_ID="15.5"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP5"
ID="sles"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:15:sp5"
DOCUMENTATION_URL="https://documentation.suse.com/"

$ uname -a
Linux ucc-myzone 5.14.21-150500.55.65.1.28147.1.PTF.1222893-default #1 SMP PREEMPT_DYNAMIC Tue May 28 12:11:24 UTC 2024 (e6fa633) x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在使用Kubernetes集群时，cAdvisor的metrics数据未能频繁更新，导致Prometheus显示相同的值。Issue中提供了相关的日志、命令输出以及配置信息，但这些内容主要用于排查性能和功能问题。根据风险判断标准：

1. 在Issue内容中，没有发现能够被攻击者利用的安全风险。
2. 没有发现可能成为漏洞的问题，且不存在可能被分配CVE编号的情况。
3. Issue提交者在提交内容中未暴露敏感信息或配置错误。

综上所述，该Issue不涉及安全问题。

---

## Issue #127384 Scheduler perf incorrectly shows percentiles for fast metrics

- Issue 链接：[#127384](https://github.com/kubernetes/kubernetes/issues/127384)

### Issue 内容

#### What happened?

When all metric values are in the first bucket, percentiles are incorrectly computed:
```json
{
  "data": {
    "Average": 0.0009773531999999999,
    "Perc50": 0.05,
    "Perc90": 0.09000000000000001,
    "Perc95": 0.095,
    "Perc99": 0.099
  },
  "unit": "ms",
  "labels": {
    "Metric": "scheduler_framework_extension_point_duration_seconds",
  }
}
```

#### What did you expect to happen?

Correctly computed percentiles. Perhaps bucketing should be changed for some metrics. 

#### How can we reproduce it (as minimally and precisely as possible)?

Run scheduler_perf or see some [perf-dash](https://perf-dash.k8s.io/#/?jobname=scheduler-perf-benchmark&metriccategoryname=Scheduler&metricname=BenchmarkPerfResults&Metric=scheduler_framework_extension_point_duration_seconds&Name=SchedulingBasic%2F5000Nodes_10000Pods%2Fnamespace-2&extension_point=Permit&plugin=not%20applicable&result=not%20applicable&event=not%20applicable) charts.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了当所有指标值都位于第一个桶时，百分位数计算不正确的问题。这是关于调度器性能指标计算的一个错误，可能导致性能统计数据不准确。此问题不会导致权限提升、命令执行、信息泄露等安全风险。根据风险判断标准，此问题无法被攻击者利用，也不会成为安全漏洞。

---

## Issue #127370 Endpoints do not reconcile with EndpointSlices for Services with selector

- Issue 链接：[#127370](https://github.com/kubernetes/kubernetes/issues/127370)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/pull/125675 was merged to 1.31, and backported and regressed the following releases:
* 1.31.0+
* 1.30.3+
* 1.29.7+
* 1.28.12+

After an upgrade to 1.28 (1.28.13), we have had significant problems with Services that use a `selector` to target Pods in a Deployment. This is likely happening only with very large services with 1000-2000 backing Pods. The Endpoints eventually appear to be "stuck" with LOTS of IPs as targets (these are large services) and they are not removing old Pod IPs from the Endpoints object, even long after the Pod is gone.

These services are Knative Services, and are operating under frequent and wide scaling. It does seem like Knative is still reading from the Endpoints API.

Deleting the Endpoints object causes it to be recreated and instantly resolves problems from things downstream which are relying on the Endpoints API.

I can show the behavior, but it is difficult to reproduce this without mimicking the scale and churn (scale out and scale in) of a. real service.

Here, I have a service `my-app` - it is in the failed state where Endpoints are not being updated. Note, the source `Service` we are concerned about here is `my-app-00112-private`.
```
% kubectl -n app get endpointslices | grep my-app-00               
my-app-00112-4vk84                   IPv4          8012,8112,8012               10.32.2.22,10.32.5.21,10.32.31.20 + 997 more...         40m
my-app-00112-private-5662t           IPv4          8012,8022,8112 + 3 more...   10.32.86.67,10.32.86.68                                 34m
my-app-00112-private-9mdgr           IPv4          8012,8022,8112 + 3 more...   10.32.210.18,10.32.210.12,10.32.210.32 + 3 more...      36m
my-app-00112-private-kzkxb           IPv4          8012,8022,8112 + 3 more...   10.32.222.22,10.32.222.21,10.32.222.29                  36m
my-app-00112-private-mrpt4           IPv4          8012,8022,8112 + 3 more...   10.32.217.26,10.32.86.63,10.32.86.64 + 12 more...       36m
my-app-00112-private-qnd6m           IPv4          8012,8022,8112 + 3 more...   10.32.139.22,10.32.139.16,10.32.139.20                  37m
my-app-00112-private-swrhv           IPv4          8012,8022,8112 + 3 more...   10.32.85.54,10.32.85.57,10.32.85.55                     34m
my-app-00112-private-xm2w7           IPv4          8012,8022,8112 + 3 more...   10.32.10.180,10.32.96.167,10.32.200.143 + 4 more...     40m
my-app-00112-private-zlp44           IPv4          8012,8022,8112 + 3 more...   10.32.217.21,10.32.217.29,10.32.217.16 + 6 more...      36m
```

And if we look at the scale of that Deployment:
```
% kubectl -n app get deploy my-app-00112-deployment                
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
my-app-00112-deployment   28/28   28           28          40m
```

This seems correct. But when we look at the Endpoints, it is a much different story:
```
% kubectl -n app get endpoints my-app-00112-private
NAME                      ENDPOINTS                                                            AGE
my-app-00112-private   10.32.0.70:9091,10.32.10.180:9091,10.32.101.10:9091 + 5907 more...   40m
```

Nearly 6000 endpoints - at about 6 ports per pod (5 are from Knative, basically), that is 1000 pods, the over capacity limit for Endpoints API. In fact, we DO see this annotation:
```
Annotations:  endpoints.kubernetes.io/over-capacity: truncated
```

But that is supposed to come down when pods fall below 1k. Since this service is using a selector, and I think is managed by the Endpoints / EndpointSlices controller, the Endpoints object comes right back if it is deleted, effectively forcing reconciliation from EndpointSlices to Endpoints. And that is exactly what happens.
```
% kubectl -n app delete endpoints my-app-00112-private
endpoints "my-app-00112-private" deleted
% kubectl -n app get endpoints my-app-00112-private     
NAME                      ENDPOINTS                                                           AGE
my-app-00112-private   10.32.0.70:9091,10.32.10.180:9091,10.32.160.111:9091 + 81 more...   4s
```

So it does seem like EndpointSlices -> Endpoints reconciliation is broken in some fashion, under these conditions.


#### What did you expect to happen?

Endpoints should be updated when EndpointSlices are changed, even on scale-in operations where Pods are removed.

#### How can we reproduce it (as minimally and precisely as possible)?

It is very difficult for me to provide clear details. But it happens with large Knative services, which are just Deployments that are autoscaled and feed a service.

#### Anything else we need to know?

I see audit logs where `endpoint-controller/kube-system` is making frequent use of the permission `io.k8s.core.v1.endpoints.update` on that endpoints resource. But then the audit logs abruptly stop. Which seems to indicate that the controller is no longer even attempting to update the Endpoints resource. This appears to happend after. particularly rapid set of calls to update the endpoints - anywhere from 500ms to 5s apart and about a total of 15-20 times in a minute.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.13-gke.1119000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

Google COS

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提交的 issue 内容，在升级到 Kubernetes 1.28 版本后，使用 selector 的 Service 在大规模（1000-2000 个 Pod）场景下，Endpoints 对象没有正确更新，旧的 Pod IP 没有被移除，导致 Endpoints 对象中包含了过多的、已经不存在的 Pod IP。

从安全风险的角度来看，此问题属于功能性缺陷，没有迹象表明攻击者可以利用该问题进行攻击。没有涉及到信息泄露、权限提升、命令执行等安全风险。

根据风险判断标准，该问题不涉及安全风险。

---

## Issue #127362 test-e2e-node prerequisites check are incorrectly calculated

- Issue 链接：[#127362](https://github.com/kubernetes/kubernetes/issues/127362)

### Issue 内容

#### What happened?

The compute project and zone in the prerequisites check are incorrectly calculated: https://github.com/kubernetes/kubernetes/blob/master/hack/make-rules/test-e2e-node.sh#L134-L145 

`!!! Error in hack/make-rules/test-e2e-node.sh:214
  Error in hack/make-rules/test-e2e-node.sh:214. 'tee -i "${artifacts}/build-log.txt"' exited with status 1 0
Call stack:
  1: hack/make-rules/test-e2e-node.sh:214 main(...)
Exiting with status 1`

#### What did you expect to happen?

blocked by the following errors in each case:
1- Could not find gcloud compute/zone when running: .... 
2- Could not find gcloud project when running: .....


#### How can we reproduce it (as minimally and precisely as possible)?

1- gcloud config unset compute/zone
2- gcloud config unset project
3- make test-e2e-node REMOTE=true LIST_IMAGES=true

#### Anything else we need to know?

gcloud --version

Google Cloud SDK 453.0.0
alpha 2023.10.27
beta 2023.10.27
bq 2.0.98
core 2023.10.27
gcloud-crc32c 1.0.0

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在运行`make test-e2e-node REMOTE=true LIST_IMAGES=true`时，由于未设置`gcloud`的`compute/zone`和`project`配置，导致脚本`test-e2e-node.sh`计算前置条件时出现错误，无法正常执行测试。这属于配置缺失引起的功能性问题，并不涉及任何安全风险。根据提供的信息，没有涉及敏感信息泄露、权限提升、远程代码执行等安全漏洞。也不存在攻击者可以利用的安全风险。

---

## Issue #127356 [InPlacePodVerticalScaling]Got RunContainerError when patch pod image and resources

- Issue 链接：[#127356](https://github.com/kubernetes/kubernetes/issues/127356)

### Issue 内容

#### What happened?

In cluster which enable InPlacePodVerticalScaling, if update image and resources(scale up), the pod will be CrashLoopBackOff with reason "StartError" and message `failed to create containerd task: failed to create shim task: OCI
          runtime create failed: runc create failed: unable to start container process:
          error during container init: error setting cgroup config for procHooks process:
          failed to write "300000": write /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-xxx.slice/cri-containerd-xxx.scope/cpu.cfs_quota_us:
          invalid argument: unknown`

#### What did you expect to happen?

The pod can successfully updated with new image and new resources.

#### How can we reproduce it (as minimally and precisely as possible)?

pod yaml: pod-simple.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: clone-test-pod
  namespace: default
spec:
  containers:
  - env:
    - name: test
      value: foo
    image: registry.k8s.io/e2e-test-images/nginx:1.14-4
    imagePullPolicy: IfNotPresent
    name: nginx
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: NotRequired
    resources:
      limits:
        cpu: "2"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 100Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
```


Create pod
```bash
kubectl apply -f pod-simple.yaml
```

Wait pod ready

Update image and resources

```bash
kubectl patch pod clone-test-pod --type='json' -p='[{"op": "replace", "path": "/spec/containers/0/image", "value":"registry.k8s.io/e2e-test-images/nginx:1.15-4"}, {"op": "replace", "path": "/spec/containers/0/resources/requests/cpu", "value": "200m"},{"op": "replace", "path": "/spec/containers/0/resources/requests/memory", "value": "200Mi"},{"op": "replace", "path": "/spec/containers/0/resources/limits/cpu", "value": "3"},{"op": "replace", "path": "/spec/containers/0/resources/limits/memory", "value": "3Gi"}]'
```

#### Anything else we need to know?

Enable InPlacePodVerticalScaling

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here

Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
I test in kind in github action and ack.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在启用InPlacePodVerticalScaling的集群中，更新Pod的镜像和资源（例如CPU和内存）时，Pod会出现`CrashLoopBackOff`状态，错误信息与cgroup的配置有关。这看起来是一个在特定配置和操作下产生的运行错误。

根据提供的信息，此问题是由于在同时更新镜像和资源时，系统处理不当导致的错误，并没有涉及到潜在的安全风险。没有迹象表明攻击者可以利用此问题进行恶意操作，例如命令执行、权限提升或影响其他用户的资源等。

按照风险判断标准，本Issue不涉及安全问题。

---

## Issue #127343 https://kubernetes.io/releases/download/  is not showing v1.31.1

- Issue 链接：[#127343](https://github.com/kubernetes/kubernetes/issues/127343)

### Issue 内容

#### What happened?

https://kubernetes.io/releases/download/  is not showing any  v1.31.1 binaries. 
also 

```
curl --fail --location --remote-name-all https://storage.googleapis.com/kubernetes-release/release/v1.31.1/bin/linux/amd64/{kubeadm,kubelet,kubectl}
```  

fails with 404 not found

Or

```
curl --fail --location --remote-name-all https://dl.k8s.io/v1.31.1/bin/linux/amd64/{kubeadm,kubelet,kubectl}
```

#### What did you expect to happen?

there is a release  v1.31.1  
https://github.com/kubernetes/kubernetes/releases/tag/v1.31.1

so i expect binaries

#### How can we reproduce it (as minimally and precisely as possible)?

execute 

```
curl --fail --location --remote-name-all https://storage.googleapis.com/kubernetes-release/release/v1.31.1/bin/linux/amd64/{kubeadm,kubelet,kubectl}
```  

Or

```
curl --fail --location --remote-name-all https://dl.k8s.io/v1.31.1/bin/linux/amd64/{kubeadm,kubelet,kubectl}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

-

#### Cloud provider

-

#### OS version

-

#### Install tools

-

#### Container runtime (CRI) and version (if applicable)

-


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

-


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在尝试下载Kubernetes v1.31.1版本的二进制文件时，无法获取到相应的文件，出现404错误。这可能是由于发布流程未完成、下载链接错误或资源尚未同步导致的。这属于软件发布或部署过程中的问题，并不涉及安全风险。

根据风险判断标准：

1. 没有发现攻击者可以利用该问题进行攻击的可能性。
2. 该问题不会成为一个漏洞，不会被分配CVE编号，按照CVSS 3.1评分标准评分，结果不会达到High以上。
3. 该问题不是由于Issue提交者的操作导致的，而是项目发布过程中的正常问题。
4. 不涉及拒绝服务（DoS）攻击。
5. 未涉及凭据泄露等风险。
6. 因此，风险评级判断为不涉及。

---

## Issue #127403 Is event.InvolvedObject fields is required in kubernetes?

- Issue 链接：[#127403](https://github.com/kubernetes/kubernetes/issues/127403)

### Issue 内容

#### What happened?

When I try to create an `event` resource with an empty `InvolvedObject` in k8s with client-go, I get a validation error `“InvolvedObject.namespace does not match event.namespace”` and the creation fails.

#### What did you expect to happen?

This is probably because the validation step of event does not check the status of the `InvolvedObject` itself, but refers to event.InvolvedObject.Namespace.
https://github.com/kubernetes/kubernetes/blob/7ad1eaa66bc5cf270a0b44889aac555529feff83/pkg/apis/core/validation/events.go#L159-L161

#### How can we reproduce it (as minimally and precisely as possible)?

This function will fail due to a validating error.

```go
func createEvent(clientset *kubernetes.Clientset, pod *corev1.Pod) {
	event := &corev1.Event{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "pod-created-",
			Namespace:    pod.Namespace,
		},
		// InvolvedObject: corev1.ObjectReference{
		// 	Kind:      "Pod",
		// 	Namespace: pod.Namespace,
		// 	Name:      pod.Name,
		// 	UID:       pod.UID,
		// },
		Reason:  "PodCreated",
		Message: fmt.Sprintf("Pod %s was created", pod.Name),
		Type:    "Normal",
		Source: corev1.EventSource{
			Component: "pod-watcher",
		},
	}

	_, err := clientset.CoreV1().Events(pod.Namespace).Create(context.TODO(), event, metav1.CreateOptions{})
	if err != nil {
		fmt.Printf("Error creating event: %v\n", err)
	}
}
```

The below function is working.

```go
func createEvent(clientset *kubernetes.Clientset, pod *corev1.Pod) {
	event := &corev1.Event{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "pod-created-",
			Namespace:    pod.Namespace,
		},
		InvolvedObject: corev1.ObjectReference{
		// 	Kind:      "Pod",
			Namespace: pod.Namespace,
		// 	Name:      pod.Name,
		// 	UID:       pod.UID,
		// },
		Reason:  "PodCreated",
		Message: fmt.Sprintf("Pod %s was created", pod.Name),
		Type:    "Normal",
		Source: corev1.EventSource{
			Component: "pod-watcher",
		},
	}

	_, err := clientset.CoreV1().Events(pod.Namespace).Create(context.TODO(), event, metav1.CreateOptions{})
	if err != nil {
		fmt.Printf("Error creating event: %v\n", err)
	}
}
```

#### Anything else we need to know?

It looks like validating only checks the `Namespace` field in `InvolvedObject`.
If kubernetes decide to require `InvolvedObject` field for event resource, I feel we need to add more validation.

#### Kubernetes version

<details>

```console
 Kubernetes v1.29.2
```

</details>


#### Cloud provider

<details>
```
on-prem
```
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue主要讨论在使用client-go创建Event资源时，如果InvolvedObject字段为空，导致验证错误并无法创建Event。这是关于Event资源的验证逻辑和使用方式的问题，与安全风险无关。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127344 Cronjob controller doesn't honor defined schedules when hour is defined

- Issue 链接：[#127344](https://github.com/kubernetes/kubernetes/issues/127344)

### Issue 内容

#### What happened?

Cronjob scheduled as follows `"*/5 14 * * *"` wont run. First i thought, if the schedule was started at in example 14:15, it will be run next day at 14:00, but no success. There is no problems with cronjobs, when scheduled like this: `"*/5 * * * *"`

#### What did you expect to happen?

run job at 14:00, 14:05 14:10... etc.

#### How can we reproduce it (as minimally and precisely as possible)?

define cronjob with schedule at speciffic hour

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
kubectl-1.30.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
$ cat /etc/os-release
NAME="Oracle Linux Server"
VERSION="9.4"
ID="ol"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Oracle Linux Server 9.4"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:oracle:linux:9:4:server"
HOME_URL="https://linux.oracle.com/"
BUG_REPORT_URL="https://github.com/oracle/oracle-linux"

ORACLE_BUGZILLA_PRODUCT="Oracle Linux 9"
ORACLE_BUGZILLA_PRODUCT_VERSION=9.4
ORACLE_SUPPORT_PRODUCT="Oracle Linux"
ORACLE_SUPPORT_PRODUCT_VERSION=9.4
$ uname -a
Linux l000r00k8ma01.luxmed.pl 5.15.0-209.161.7.2.el9uek.x86_64 #2 SMP Tue Aug 20 10:44:41 PDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.21
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，当Cronjob定义了特定小时的调度（例如"*/5 14 * * *"）时，Cronjob无法按预期运行的问题。这是一个功能性缺陷或bug，导致Cronjob没有按照设定的时间执行。但这个问题并不会被攻击者利用来实施攻击，不涉及权限提升、命令执行、数据泄露等安全风险。根据风险判断标准，第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127342 nodeAffinityPolicy not effective

- Issue 链接：[#127342](https://github.com/kubernetes/kubernetes/issues/127342)

### Issue 内容

#### What happened?

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 6
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: nginx
```

````
cat <<EOF | kubectl apply -n k8s -f -
apiVersion: v1
kind: Pod
metadata:
  name: web-server
  labels:
    app: myapp
spec:
  containers:
  - name: web-server
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - web
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: myapp
    nodeAffinityPolicy: Ignore
EOF
```

```
root@k8s-master01:~# kubectl get pod -n k8s -owide
NAME                                READY   STATUS              RESTARTS   AGE     IP              NODE           NOMINATED NODE   READINESS GATES
myapp-deployment-79bf885b89-8js64   1/1     Running             0          12m     10.244.79.145   k8s-worker01   <none>           <none>
myapp-deployment-79bf885b89-cs2w9   1/1     Running             0          12m     10.244.79.129   k8s-worker01   <none>           <none>
myapp-deployment-79bf885b89-g8lf7   1/1     Running             0          12m     10.244.0.89     k8s-worker02   <none>           <none>
myapp-deployment-79bf885b89-hcblx   1/1     Running             0          12m     10.244.0.90     k8s-worker02   <none>           <none>
myapp-deployment-79bf885b89-lfgbx   1/1     Running             0          12m     10.244.79.142   k8s-worker01   <none>           <none>
myapp-deployment-79bf885b89-zk6bj   0/1     ContainerCreating   0          12m     <none>          k8s-master01   <none>           <none>
web-server                          0/1     Pending             0          4m57s   <none>          <none>         <none>           <none>
```

pod should schedule to k8s-master01,but not
master01 do not have node-type label but     nodeAffinityPolicy: Ignore

#### What did you expect to happen?

schedule to k8s-master01

#### How can we reproduce it (as minimally and precisely as possible)?

do the experiment as me

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31.0

</details>


#### Cloud provider

<details>
vmware workstation
</details>


#### OS version

<details>
unbunt 2404

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中使用`nodeAffinityPolicy`时，发现策略未生效的问题。这是一个Kubernetes调度策略配置或功能实现方面的问题，不涉及安全风险。根据风险判断标准，此问题无法被攻击者利用，不会导致安全漏洞，也不涉及敏感信息泄露或高风险操作，因此风险评级判断为不涉及。

---

## Issue #127339 Terminated pod is stuck in preStop hook

- Issue 链接：[#127339](https://github.com/kubernetes/kubernetes/issues/127339)

### Issue 内容

#### What happened?

Pod cannot be fully removed even if it's graceDeletionPeriod has been 0. It's stuck in preStop hook.

#### What did you expect to happen?

Remove terminated pod

#### How can we reproduce it (as minimally and precisely as possible)?

1. deploy a nginx

```
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
  creationTimestamp: "2024-09-13T01:42:11Z"
  generation: 2
  labels:
    app: test
  name: test
  namespace: default
  resourceVersion: "14464340"
  uid: 4eac36d2-dda1-40b1-9a1a-557e73a8b6f4
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: test
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: test
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 10000000
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 300
```

2. delete the pod

```
kubectl delete pod test-54f748667b-26cnm
```

3. delete the pod with a short grace period

```
kubectl delete pod test-54f748667b-26cnm --grace-period=1
```

4. we have to wait 5min and then the pod will be fully removed.  

#### Anything else we need to know?

If we restart the kubelet, the pod will immediately removed.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，包含一个执行长时间sleep的preStop hook的Pod在删除时无法立即删除的问题，即使指定了最小的grace period。这可能导致Pod长时间无法终止，占用系统资源。但需要注意的是，利用此行为需要具备创建或修改Pod的权限，且影响的仅是攻击者自身的Pod，不会影响其他用户或系统。因此，根据风险判断标准，此问题不涉及安全风险。

---

## Issue #127334 is:issue is:open embed:rejected connection from #106.0.18.66:35592"(error"remoteerror :t1s:certificateembed:servername""

- Issue 链接：[#127334](https://github.com/kubernetes/kubernetes/issues/127334)

### Issue 内容

#### What happened?

Due to an accidental operation, the etcd certificates were replaced, causing the Kubernetes cluster to become corrupted and the kubectl command to become unusable. I attempted to regenerate the cluster certificates using kubeadm init phase certs all --config /etc/kubernetes/kubeadm-config.yaml and copied the entire /etc/kubernetes/pki directory to other nodes. After restarting all cluster components, the following error occurs:

#### What did you expect to happen?

my cluster recover

#### How can we reproduce it (as minimally and precisely as possible)?

Severely affects production use.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
[zhuiyi@k8s-master-node1 ~]$  kubectl version
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.15", GitCommit:"8f1e5bf0b9729a899b8df86249b56e2c74aebc55", GitTreeState:"clean", BuildDate:"2022-01-19T17:27:39Z", GoVersion:"go1.15.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.15", GitCommit:"8f1e5bf0b9729a899b8df86249b56e2c74aebc55", GitTreeState:"clean", BuildDate:"2022-01-19T17:23:01Z", GoVersion:"go1.15.15", Compiler:"gc", Platform:"linux/amd64"}
[zhuiyi@k8s-master-node1 ~]$ 

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
[zhuiyi@k8s-master-node1 ~]$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

[zhuiyi@k8s-master-node1 ~]$ 
$ uname -a
[zhuiyi@k8s-master-node1 ~]$ uname -a
Linux k8s-master-node1 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
[zhuiyi@k8s-master-node1 ~]$ 


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
docker
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的信息，用户因误操作替换了etcd证书，导致Kubernetes集群损坏，kubectl命令无法使用。用户尝试重新生成集群证书并复制到其他节点，但问题仍然存在。这属于用户自身的不当操作导致的配置问题，并不涉及项目本身的安全风险。根据风险判断标准，第3条指出：issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险，因为它是issue提交者的问题，而不是项目的问题。

综上所述，该问题不涉及安全风险。

---

## Issue #127316 kubelet fails to start pods after upgrade to 1.31.X

- Issue 链接：[#127316](https://github.com/kubernetes/kubernetes/issues/127316)

### Issue 内容

#### What happened?

After the first controller upgrade from 1.30.4 to 1.31.0 and later 1.31.1. The kubelet service fails to start the majority of pods running on the controller.

#### What did you expect to happen?

All pods should start as before the upgrade.

```
$ kubectl get pod -o wide --all-namespaces -w | grep controller2
calico-system     calico-node-p4fzm                                          0/1     CreateContainerConfigError   0              153m    192.168.16.112   kubecontroller2   <none>           <none>
calico-system     calico-typha-685957f77b-k2xf8                              0/1     CreateContainerConfigError   4 (122m ago)   40d     192.168.16.112   kubecontroller2   <none>           <none>
calico-system     csi-node-driver-b6jkc                                      0/2     CreateContainerConfigError   8 (122m ago)   40d     10.245.98.170    kubecontroller2   <none>           <none>
default           netdata-child-pctf4                                        0/2     CreateContainerConfigError   2 (122m ago)   11d     192.168.16.112   kubecontroller2   <none>           <none>
kube-system       etcd-kubecontroller2                                       1/1     Running                      1 (122m ago)   122m    192.168.16.112   kubecontroller2   <none>           <none>
kube-system       kube-apiserver-kubecontroller2                             1/1     Running                      1 (122m ago)   122m    192.168.16.112   kubecontroller2   <none>           <none>
kube-system       kube-controller-manager-kubecontroller2                    1/1     Running                      1 (122m ago)   122m    192.168.16.112   kubecontroller2   <none>           <none>
kube-system       kube-proxy-q2drw                                           0/1     CreateContainerConfigError   0              17h     192.168.16.112   kubecontroller2   <none>           <none>
kube-system       kube-scheduler-kubecontroller2                             1/1     Running                      1 (122m ago)   122m    192.168.16.112   kubecontroller2   <none>           <none>
kube-system       kube-vip-kubecontroller2                                   1/1     Running                      1 (122m ago)   122m    192.168.16.112   kubecontroller2   <none>           <none>
monitoring        kube-prometheus-stack-prometheus-node-exporter-gnwsj       0/1     CreateContainerConfigError   1 (122m ago)   5d1h    192.168.16.112   kubecontroller2   <none>           <none>
```

#### How can we reproduce it (as minimally and precisely as possible)?

Basic Debian 12 VM used as a controller node. First controller upgraded from 1.30.4 to 1.31.0 and later to 1.31.1.

#### Anything else we need to know?

Reinstalling kubelet 1.30.4 allows pods to start correctly.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4
```

</details>


#### Cloud provider

<details>
Self hosted / metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux kubecontroller2 6.1.0-25-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.106-3 (2024-08-26) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.22 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.28.1
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经分析，该Issue描述了在将kubelet从1.30.4升级到1.31.0及1.31.1之后，kubelet无法启动大多数pod的问题。这是一个功能性故障，可能与版本兼容性、配置变化或bug有关。但在Issue内容中，没有涉及任何可能被攻击者利用的安全风险，也没有提及任何敏感信息泄露或高危漏洞。因此，按照风险判断标准，此Issue不涉及安全风险。

---

## Issue #127311 windows node join to linux master kubelet failed

- Issue 链接：[#127311](https://github.com/kubernetes/kubernetes/issues/127311)

### Issue 内容

#### What happened?

![image](https://github.com/user-attachments/assets/68a7e2d1-5670-41c8-862e-8526374c529e)


#### What did you expect to happen?

join success

#### How can we reproduce it (as minimally and precisely as possible)?

**安装git**

https://git-scm.com/download/win

**安装containerd**

```
.\Install-Containerd.ps1
```

**加入集群**

```
# 自动初始化Windows 下 kubernetes 的脚本
https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/Install-Containerd.ps1
https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/PrepareNode.ps1

https://dl.k8s.io/v1.31.0/kubernetes-node-windows-amd64.tar.gz

unzip kubernetes-node-windows-amd64.tar.gz
mv bin files to k directory

https://k8stestinfrabinaries.blob.core.windows.net/nssm-mirror/nssm-2.24.zip

https://github.com/Microsoft/SDN/raw/master/Kubernetes/windows/hns.psm1

https://github.com/rancher/wins/releases/download/v0.0.4/wins.exe

PrepareNode.ps1注释下面内容，自己下载
<#
DownloadFile $kubeletBinPath https://dl.k8s.io/$KubernetesVersion/bin/windows/amd64/kubelet.exe
DownloadFile "$global:KubernetesPath\kubeadm.exe" https://dl.k8s.io/$KubernetesVersion/bin/windows/amd64/kubeadm.exe
DownloadFile "$global:KubernetesPath\wins.exe" https://github.com/rancher/wins/releases/download/v0.0.4/wins.exe
#>

mkdir c:\k

upload file to c:\k

.\PrepareNode.ps1 -KubernetesVersion v1.31.0 -ContainerRuntime  containerD

kubeadm token create --print-join-command

.\kubeadm token create --print-join-command
kubeadm join 192.168.229.180:6443 --token mcpuze.9xgaztfwhbwoz62p --discovery-token-ca-cert-hash sha256:ce6b446b6fca5e82fdf104d82cc8a786789d1a4fc1168afb2373f049ecc77bf5 
```



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.31.0

</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

`windows server 2022 data center

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在尝试将Windows节点加入到Linux主节点的Kubernetes集群时遇到了失败。他们详细描述了自己的安装步骤，包括下载和安装所需的软件、修改脚本以及执行加入集群的命令。

根据风险判断标准：

- **第3条**：Issue提交者在提交内容中暴露的敏感信息（如token等），属于提交者自身的问题，不属于项目的安全风险。
- **第6条**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

在该Issue中，没有发现可能被攻击者利用的安全漏洞，也没有涉及命令执行、容器逃逸、提权等高安全风险的问题。

因此，综合判断，该Issue不涉及安全风险。

---

## Issue #127330 removing a pod affinity doesn't take effect until pods are rolled

- Issue 链接：[#127330](https://github.com/kubernetes/kubernetes/issues/127330)

### Issue 内容

#### What happened?

after creating a deployment with a pod affinity, and then updating it (thus recreating all the pods) to have no affinity, the recreated pods remain under the influence of the removed affinity until they are recreated a second time.

#### What did you expect to happen?

I would expect the pods to immediately obey the default topology spread constraint and be distributed across hosts and zones upon removal of the pod affinity.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:latest
```

```console
$ k apply -f with-affinity.yaml
deployment.apps/nginx created

$ k get po -owide
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES
nginx-6866547996-245rz   1/1     Running   0          10s   10.116.0.40   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-6866547996-b4wck   1/1     Running   0          10s   10.116.0.38   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-6866547996-ch9vd   1/1     Running   0          10s   10.116.0.41   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-6866547996-j74gm   1/1     Running   0          10s   10.116.0.37   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-6866547996-kpmpg   1/1     Running   0          10s   10.116.0.42   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-6866547996-sz68c   1/1     Running   0          10s   10.116.0.39   gke-lab-default-pool-f642a75e-w308   <none>           <none>
```


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
```

```console
$ k apply -f without-affinity.yaml
deployment.apps/nginx configured

$ k get po -owide
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES
nginx-7584b6f84c-4j74c   1/1     Running   0          8s    10.116.0.45   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-7584b6f84c-d66cz   1/1     Running   0          5s    10.116.0.48   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-7584b6f84c-f8fch   1/1     Running   0          8s    10.116.0.43   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-7584b6f84c-tqtdf   1/1     Running   0          7s    10.116.0.46   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-7584b6f84c-vk6br   1/1     Running   0          8s    10.116.0.44   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-7584b6f84c-wg675   1/1     Running   0          6s    10.116.0.47   gke-lab-default-pool-f642a75e-w308   <none>           <none>

$ k rollout restart deploy/nginx
deployment.apps/nginx restarted

$ k get po -owide
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES
nginx-59f5fd959d-4mgzk   1/1     Running   0          7s    10.116.2.8    gke-lab-default-pool-c44628b2-1b70   <none>           <none>
nginx-59f5fd959d-525l9   1/1     Running   0          7s    10.116.0.49   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-59f5fd959d-92b28   1/1     Running   0          4s    10.116.0.50   gke-lab-default-pool-f642a75e-w308   <none>           <none>
nginx-59f5fd959d-ng8h4   1/1     Running   0          5s    10.116.1.17   gke-lab-default-pool-0ef416fe-thsr   <none>           <none>
nginx-59f5fd959d-pr9fn   1/1     Running   0          7s    10.116.1.16   gke-lab-default-pool-0ef416fe-thsr   <none>           <none>
nginx-59f5fd959d-vzv4d   1/1     Running   0          5s    10.116.2.9    gke-lab-default-pool-c44628b2-1b70   <none>           <none>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ k version
Client Version: v1.29.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4-gke.1213000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux 0 5.15.0-1066-gcp #74~20.04.1-Ubuntu SMP Fri Jul 26 09:28:41 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，当创建了带有Pod亲和性的Deployment后，更新Deployment以移除亲和性，并重新创建Pods时，重新创建的Pods仍然受到已移除的亲和性的影响，直到再次重建Pods才会生效。这可能导致Pods没有按照预期分布在不同的节点或区域。

根据风险判断标准：

1. **该风险能被攻击者利用**：该问题需要有权限修改Deployment配置并重建Pods，这是管理员的权限，普通攻击者无法利用此问题进行攻击。

2. **该风险有可能成为一个漏洞，并被分配CVE编号**：该问题不涉及安全漏洞，属于功能性缺陷，不会被分配CVE编号。

4. **当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险**：此问题需要修改Deployment的配置，属于非只读权限。

因此，该Issue不涉及安全风险。

---

## Issue #127301 orphaned pod <uid> found, but failed to rmdir() volume at path ...

- Issue 链接：[#127301](https://github.com/kubernetes/kubernetes/issues/127301)

### Issue 内容

#### What happened?

After a k8s 1.28.9 node was rebooted the following errors started to be repeated in jounalctl:

`kubelet[2135]: E0911 16:27:10.436330 2135 kubelet_volumes.go:263] "There were many similar errors. Turn up verbosity to see them." err="orphaned pod \"0a002fc3-b8f5-4a36-a8d2-e7a9c7bbe8ac\" found, but failed to rmdir() volume at path /var/lib/kubelet/pods/0a002fc3-b8f5-4a36-a8d2-e7a9c7bbe8ac/volumes/kubernetes.io~local-volume/local-pv-c20f968b: device or resource busy" numErrs=2`

The pod was running under a new uid and so both the old and new mounts were listed when I ran `mount | grep local-pv-c20f968b`:

```
/dev/mapper/apicSecureDisk on /var/lib/kubelet/pods/0a002fc3-b8f5-4a36-a8d2-e7a9c7bbe8ac/volumes/kubernetes.io~local-volume/local-pv-c20f968b type ext4 (rw,relatime)
/dev/mapper/apicSecureDisk on /data/secure/var/lib/kubelet/pods/0a002fc3-b8f5-4a36-a8d2-e7a9c7bbe8ac/volumes/kubernetes.io~local-volume/local-pv-c20f968b type ext4 (rw,relatime)
/dev/mapper/apicSecureDisk on /var/lib/kubelet/pods/3e64e4d8-f0b8-4031-a2c0-fa9cdcc86dd8/volumes/kubernetes.io~local-volume/local-pv-c20f968b type ext4 (rw,relatime)
/dev/mapper/apicSecureDisk on /data/secure/var/lib/kubelet/pods/3e64e4d8-f0b8-4031-a2c0-fa9cdcc86dd8/volumes/kubernetes.io~local-volume/local-pv-c20f968b type ext4 (rw,relatime)
```

In new pod any new directories that were created inside the double mounted volume got deleted within a few seconds of creation, so the new pod was throwing errors related to those directories vanishing. To fix the error and the pod I ran: `umount /var/lib/kubelet/pods/0a002fc3-b8f5-4a36-a8d2-e7a9c7bbe8ac/volumes/kubernetes.io~local-volume/local-pv-c20f968b` and then the issue was resolved.

#### What did you expect to happen?

kubelet to unmount the orphaned pod directory without manual intervention

#### How can we reproduce it (as minimally and precisely as possible)?

It does not happen everytime, but it does seem to only happen on node reboot. Perhaps it only happens with local storage as well.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux apicdev4010 5.4.0-195-generic #215-Ubuntu SMP Fri Aug 2 18:28:05 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1. 从Issue内容来看，问题在于kubelet在节点重启后未能正确卸载孤立的Pod目录，导致出现了错误消息，需要手动干预才能解决。

2. 这个问题导致了新旧Pod的挂载点共存，可能引起文件系统的错误和应用程序的问题。

3. 但是，从安全角度来看，没有迹象表明攻击者可以利用这个问题进行攻击。

4. 根据风险判断标准，没有发现可能导致命令执行、容器逃逸、提权等高安全风险的漏洞。

5. 因此，该Issue不涉及安全风险。

---

## Issue #127290 scheduler: Pods aren't retried at all with cluster events to unschedulable Pod

- Issue 链接：[#127290](https://github.com/kubernetes/kubernetes/issues/127290)

### Issue 内容

It's initially raised at https://github.com/kubernetes/kubernetes/issues/110175#issuecomment-1140397251. Just create a separate issue so that we don't forget.

So, currently, we don't trigger requeueing with cluster events to unschedulable Pods. It's OK for in-tree plugins, but not OK for out-of-tree plugins that could change the scheduling results with unsched Pods state.

/sig scheduling
/kind bug
/cc @macsko 
/assign

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes调度器在处理不可调度的Pod时存在的问题：当集群发生事件时（例如节点状态改变、资源释放等），调度器没有重新尝试调度这些不可调度的Pod。这对内置插件没有影响，但是对于可能改变不可调度Pod的调度结果的第三方插件（out-of-tree plugins）而言，这是个问题。

从安全角度分析，该问题属于调度器的功能性错误，可能导致Pod长时间处于Pending状态，影响工作负载的部署和运行效率。但这种影响仅限于可用性方面，不涉及安全风险。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者无法通过该问题获得任何权限提升或执行任意代码的能力。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题不会被分配CVE编号，CVSS评分也不会达到high以上。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue不涉及安全问题。

综上所述，该Issue不涉及安全风险。

---

## Issue #127288 k8s Error nested exception is java.lang.NumberFormatException: For input string: "tcp://10.109.145.47:9522"，but There is no problem running on docker

- Issue 链接：[#127288](https://github.com/kubernetes/kubernetes/issues/127288)

### Issue 内容

#### What happened?

 / ____|    /\    |  \/  ||  ____|| |         |  _ \  / __ \  / __ \|__   __|
 | |        /  \   | \  / || |__   | |  ______ | |_) || |  | || |  | |  | |
 | |       / /\ \  | |\/| ||  __|  | | |______||  _ < | |  | || |  | |  | |
 | |____  / ____ \ | |  | || |____ | |____     | |_) || |__| || |__| |  | |
  \_____|/_/    \_\|_|  |_||______||______|    |____/  \____/  \____/   |_|



      :: Spring Boot ::             (v2.3.0.RELEASE)
      :: camel-boot ::        (0.1)

2024-09-11 05:22:01.021 [main] INFO  com.camel.modules.CamelApplication:55 - Starting CamelApplication v1.0.0-RELEASE on server-8664c79698-z2mjq with PID 1 (/zk_server/app.jar started by root in /zk_server)
2024-09-11 05:22:01.024 [main] INFO  com.camel.modules.CamelApplication:655 - The following profiles are active: version,dev
2024-09-11 05:22:03.057 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext:558 - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanDefinitionStoreException: Failed to process import candidates for configuration class [com.camel.modules.CamelApplication]; nested exception is java.lang.IllegalStateException: Error processing condition on org.springframework.boot.actuate.autoconfigure.web.server.ManagementContextAutoConfiguration$DifferentManagementContextConfiguration
2024-09-11 05:22:03.069 [main] INFO  o.s.b.a.l.ConditionEvaluationReportLoggingListener:136 - 

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2024-09-11 05:22:03.071 [main] ERROR org.springframework.boot.SpringApplication:837 - Application run failed
org.springframework.beans.factory.BeanDefinitionStoreException: Failed to process import candidates for configuration class [com.camel.modules.CamelApplication]; nested exception is java.lang.IllegalStateException: Error processing condition on org.springframework.boot.actuate.autoconfigure.web.server.ManagementContextAutoConfiguration$DifferentManagementContextConfiguration
	at org.springframework.context.annotation.ConfigurationClassParser.processImports(ConfigurationClassParser.java:609)
	at org.springframework.context.annotation.ConfigurationClassParser.access$800(ConfigurationClassParser.java:110)
	at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorGroupingHandler.lambda$processGroupImports$1(ConfigurationClassParser.java:811)
	at java.util.ArrayList.forEach(ArrayList.java:1257)
	at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorGroupingHandler.processGroupImports(ConfigurationClassParser.java:808)
	at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorHandler.process(ConfigurationClassParser.java:779)
	at org.springframework.context.annotation.ConfigurationClassParser.parse(ConfigurationClassParser.java:192)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:319)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:236)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:280)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:96)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:706)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:532)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226)
	at com.camel.modules.CamelApplication.main(CamelApplication.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49)
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:109)
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58)
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:88)
Caused by: java.lang.IllegalStateException: Error processing condition on org.springframework.boot.actuate.autoconfigure.web.server.ManagementContextAutoConfiguration$DifferentManagementContextConfiguration
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60)
	at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:108)
	at org.springframework.context.annotation.ConfigurationClassParser.processConfigurationClass(ConfigurationClassParser.java:225)
	at org.springframework.context.annotation.ConfigurationClassParser.processMemberClasses(ConfigurationClassParser.java:371)
	at org.springframework.context.annotation.ConfigurationClassParser.doProcessConfigurationClass(ConfigurationClassParser.java:271)
	at org.springframework.context.annotation.ConfigurationClassParser.processConfigurationClass(ConfigurationClassParser.java:249)
	at org.springframework.context.annotation.ConfigurationClassParser.processImports(ConfigurationClassParser.java:599)
	... 28 common frames omitted
Caused by: org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [java.lang.Integer] for value 'tcp://10.109.145.47:9522'; nested exception is java.lang.NumberFormatException: For input string: "tcp://10.109.145.47:9522"
	at org.springframework.core.convert.support.ConversionUtils.invokeConverter(ConversionUtils.java:47)
	at org.springframework.core.convert.support.GenericConversionService.convert(GenericConversionService.java:191)
	at org.springframework.core.convert.support.GenericConversionService.convert(GenericConversionService.java:174)
	at org.springframework.core.env.AbstractPropertyResolver.convertValueIfNecessary(AbstractPropertyResolver.java:262)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:91)
	at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:68)
	at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:546)
	at org.springframework.boot.actuate.autoconfigure.web.server.ManagementPortType.getPortProperty(ManagementPortType.java:64)
	at org.springframework.boot.actuate.autoconfigure.web.server.ManagementPortType.get(ManagementPortType.java:58)
	at org.springframework.boot.actuate.autoconfigure.web.server.OnManagementPortCondition.getMatchOutcome(OnManagementPortCondition.java:49)
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:47)
	... 34 common frames omitted
Caused by: java.lang.NumberFormatException: For input string: "tcp://10.109.145.47:9522"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.valueOf(Integer.java:766)
	at org.springframework.util.NumberUtils.parseNumber(NumberUtils.java:211)
	at org.springframework.core.convert.support.StringToNumberConverterFactory$StringToNumber.convert(StringToNumberConverterFactory.java:62)
	at org.springframework.core.convert.support.StringToNumberConverterFactory$StringToNumber.convert(StringToNumberConverterFactory.java:49)
	at org.springframework.core.convert.support.GenericConversionService$ConverterFactoryAdapter.convert(GenericConversionService.java:436)
	at org.springframework.core.convert.support.ConversionUtils.invokeConverter(ConversionUtils.java:41)
	... 44 common frames omitted


#### What did you expect to happen?

How it should be resolved ？？

#### How can we reproduce it (as minimally and precisely as possible)?

docker 上可以正常运行


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.23.0
# paste output here
```

</details>


#### Cloud provider

<details>
local</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
CentOS9
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes上运行Spring Boot应用程序时出现`java.lang.NumberFormatException`错误，而在Docker上运行没有问题。这是由于配置错误导致的异常，与安全风险无关。因此，风险评级为'不涉及'。

---

## Issue #127287 Improve message `prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers`

- Issue 链接：[#127287](https://github.com/kubernetes/kubernetes/issues/127287)

### Issue 内容

#### What happened?

k8s component developers are using FQDNs as finalizers (e.g. `cluster.cluster.x-k8s.io` or `finalizer.acme.cert-manager.io` which triggers an error message that isn't particularly easy for an end user to understand:

`prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers`

To a naive end user, a fully qualified domain name is a pretty safe thing that is unlikely to conflict with anything else  (as long as it's only used by someone from within the domain that owns it, which is a reasonable assumption).

#### What did you expect to happen?

If the finalizer has `.`s inside it but no `/`, the message should suggest that finalizers need a `/`.

Something like:
`prefer a domain-qualified finalizer name including a path (/) to avoid accidental conflicts with other finalizer writers`

#### How can we reproduce it (as minimally and precisely as possible)?

Do whatever it is that triggered the warning in https://github.com/kubernetes-sigs/cluster-api/issues/10914 (this is probably simpler than installing cert-manager and asking for a certificate)

#### Anything else we need to know?

#119508 introduced: https://github.com/kubernetes/kubernetes/blob/f836773540cc83488250a12637e7d924078ef284/staging/src/k8s.io/apiextensions-apiserver/pkg/registry/customresource/validator.go#L89

#### Kubernetes version

<details>

```console
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.7-gke.1274000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue主要讨论了k8s在使用某些finalizer名称时，提示的错误信息不够友好，建议改进错误提示信息以便用户理解。此问题涉及的是用户体验和错误提示的改进，并未涉及任何安全风险。根据风险判断标准，第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #127286 Handling WebSocket requests through the API server, the server received two connections

- Issue 链接：[#127286](https://github.com/kubernetes/kubernetes/issues/127286)

### Issue 内容

#### What happened?

![微信截图_20240911111842](https://github.com/user-attachments/assets/6e95beb0-3a43-4bed-a797-379951313605)
第一次连接h.UpgradeTransport.WrapRequest(req)，
第二次连接dial(updatedReq, h.UpgradeTransport)

#### What did you expect to happen?

only one conn

#### How can we reproduce it (as minimally and precisely as possible)?

...

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在通过API服务器处理WebSocket请求时，服务器收到了两个连接而不是预期的一个连接。这可能是由于代码实现问题，导致在处理WebSocket请求时意外地建立了两次连接。从安全角度来看，除非攻击者能够利用这种行为导致资源消耗过度，进而形成拒绝服务（DoS）攻击，否则这种情况一般不被视为高风险安全问题。根据目前提供的信息，无法确定攻击者是否能够利用该问题实施有效的攻击，因此该Issue不涉及安全风险。

---

## Issue #127264 kube api audit log to a file

- Issue 链接：[#127264](https://github.com/kubernetes/kubernetes/issues/127264)

### Issue 内容

#### What happened?

Hello,

Im configuring kube-apiserver to send the logs to the file so that I can see the audit events. But after adding the required details the kube-apiserver got restarted but the log was not created.  Below are the rule I have added for logging

```yaml
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
    - --audit-log-path=/var/log/kubernetes-apiserver.log
    - --audit-log-maxsize=100
    - --audit-log-maxbackup=3
    - --audit-log-format=json
```

I have added the policy as below
```yaml 
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
```

After that pod git restarted  but there is no log was writing into the file

kubernetes version 1.29.8

#### What did you expect to happen?

It was supposed to write audit events to the file specified in the kube-apiserver.yaml file but it is not happened

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce configure audit loggin with the kubernetes version 1.29.8

#### Anything else we need to know?

When I tested the same configuration on 1.27.x version kubernetes  it worked.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
RHEL8.8
$ uname -a
4.18.0-477.27.1.el8_8.x86_64 #1 SMP Thu Aug 31 10:29:22 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
containerd
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue涉及用户在配置kube-apiserver的审计日志时，审计日志文件未生成的问题。这是一个功能性配置错误或版本兼容性的问题，并不涉及潜在的安全风险。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为“不涉及”。

---

## Issue #127263 mismatchLabelKeys matchLabelKeys not effective

- Issue 链接：[#127263](https://github.com/kubernetes/kubernetes/issues/127263)

### Issue 内容

#### What happened?

```
cat <<EOF | kubectl apply -n k8s -f -
apiVersion: v1
kind: Pod
metadata:
  name: app-mysql
  labels:
    app: mysql
    app2: mysql2
    test: test
spec:
  nodeName: k8s-worker02
  containers:
  - name: mysqldb
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/examples-bookinfo-mysqldb:1.17.0
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 3306
    env:
      - name: MYSQL_ROOT_PASSWORD
        value: "123456"
    args: ["--default-authentication-plugin","mysql_native_password"]
    volumeMounts:
    - name: var-lib-mysql
      mountPath: /var/lib/mysql
  volumes:
  - name: var-lib-mysql
    emptyDir: {}
---
apiVersion: v1
kind: Pod
metadata:
  name: rating
  labels:
    app: rating
spec:
  containers:
  - name: ratings
    image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/examples-bookinfo-ratings-v2:1.17.0
    imagePullPolicy: IfNotPresent
    env:
     - name: DB_TYPE
       value: "mysql"
     - name: MYSQL_DB_HOST
       value: localhost
     - name: MYSQL_DB_PORT
       value: "3306"
     - name: MYSQL_DB_USER
       value: root
     - name: MYSQL_DB_PASSWORD
       value: "123456"
    ports:
    - containerPort: 9080
    securityContext:
      runAsUser: 1000
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 10
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: analytics
            matchExpressions:
              - key: department
                operator: In
                values:
                - sales
              - key: department
                operator: NotIn
                values:
                - finance
          topologyKey: "kubernetes.io/hostname"
          namespaceSelector:
            matchLabels:
              project: myproject 
      - weight: 90
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: mysql
          topologyKey: "kubernetes.io/hostname"
          mismatchLabelKeys:
          - test
          matchLabelKeys:
          - app3
          namespaces:
          - k8s
EOF
```

root@k8s-master01:~# kubectl get pod -owide -n k8s
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
app-mysql   1/1     Running   0          10s   10.244.0.88   k8s-worker02   <none>           <none>
rating      1/1     Running   0          10s   10.244.0.87   k8s-worker02   <none>           <none>


![image](https://github.com/user-attachments/assets/b13a5955-e033-4778-96a4-2e357e589641)

![image](https://github.com/user-attachments/assets/98a2842f-ad89-4ad0-b122-b7bf65e6203b)
![image](https://github.com/user-attachments/assets/f0ee43d9-ac52-40b1-86df-6ef5bf6da53f)



#### What did you expect to happen?

scheduler to difference node

#### How can we reproduce it (as minimally and precisely as possible)?

apply the config,modify nodeName

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.31.0

#### Cloud provider

<details>

</details>
wmware workstation

#### OS version

<details>
ubuntu 2404
```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从提供的Issue内容来看，问题涉及Kubernetes中Pod的调度策略，其中使用了`matchLabelKeys`和`mismatchLabelKeys`属性，但调度结果没有达到预期。该Issue的主要内容是关于Pod的亲和性（Affinity）配置未生效，导致Pod未按照期望被调度到不同的节点上。

按照风险判断标准：

1. 该Issue未涉及可被攻击者利用的安全风险。
2. 未发现该问题有可能成为漏洞，不会被分配CVE编号。
3. Issue提交者在配置中的错误或不当配置，不属于项目的安全风险。
4. 该问题不涉及拒绝服务攻击。
5. 未涉及凭据泄露或敏感信息泄露。
6. 因此，风险评级判断为“不涉及”。

---

## Issue #127249 Fix issue where alpha APIs that have `k8s:prerelease-lifecycle-gen:introduced` have an auto generated `APILifecycleRemoved` (should only be for beta/GA APIs)

- Issue 链接：[#127249](https://github.com/kubernetes/kubernetes/issues/127249)

### Issue 内容

#### What happened?

**EDIT: currently all the info in the issue here uses APIs generally but it should state alpha APIs as the current `APILifecycleRemoved` policy is being added to alpha APIs when it is unclear it should be and that is the root of the issue**

Currently when using the `// +k8s:prerelease-lifecycle-gen:introduced=<version>` tag on API types.go files, the associated generated code (`zz_generated.prerelease-lifecycle.go`) creates an APILifecycleRemoved method for that API automatically set for introduced + 6 minor versions.  Specifically, there are a number of APIs that have // +k8s:prerelease-lifecycle-gen:introduced=1.26 and this means that these APIs have APILifecycleRemoved methods targeting v1.32.  This means when attempting to bump `DefaultKubeBinaryVersion` from v1.31 -> v1.32 there are integration test failures where the test expects APIs to exist that were removed via the above flow (more details in other sections).  Below is an example of such an entry from `master` @ HEAD:
File: ./staging/src/k8s.io/api/authentication/v1alpha1/zz_generated.prerelease-lifecycle.go
```
// APILifecycleRemoved is an autogenerated function, returning the release in which the API is no longer served as int versions of major and minor for comparison.
// It is controlled by "k8s:prerelease-lifecycle-gen:removed" tags in types.go or  "k8s:prerelease-lifecycle-gen:deprecated" plus three minor.
func (in *SelfSubjectReview) APILifecycleRemoved() (major, minor int) {
	return 1, 32
```

For the integration tests, I have validated it is what [folks mentioned in the PR comments](https://github.com/kubernetes/kubernetes/pull/126977#issuecomment-2332596429) - the usage of `// +k8s:prerelease-lifecycle-gen:introduced=1.26"`

It seems for all APIs where there is used there is a APILifecycleRemoved method added in all of the related generated code with 1.32 as the version to remove that API.   This makes it so that the associated tests fail.  If you manually change these values to 1.33, the tests all pass.  
```
diff --git a/staging/src/k8s.io/api/authentication/v1alpha1/zz_generated.prerelease-lifecycle.go b/staging/src/k8s.io/api/authentication/v1alpha1/zz_generated.prerelease-lifecycle.go
index 62a70a781d1..af598cb4db1 100644
--- 
 // APILifecycleRemoved is an autogenerated function, returning the release in which the API is no longer served as int versions of major and minor for comparison.
 // It is controlled by "k8s:prerelease-lifecycle-gen:removed" tags in types.go or  "k8s:prerelease-lifecycle-gen:deprecated" plus three minor.
 func (in *SelfSubjectReview) APILifecycleRemoved() (major, minor int) {
-    return 1, 32
+    return 1, 33
 }
```

For further evidence this the root cause of the failing integration tests at https://github.com/kubernetes/kubernetes/pull/126977, below is the list of failing integration tests:
- TestGetsSelfAttributes
- TestCTBAttestPlugin
- TestCTBSignerNameFieldSelector
- TestCTBSignerNameChangeForbidden
- TestEncryptAll
- TestEtcdStoragePath
- TestAPIServerMetrics

and here are k8s types that APILifeCycleRemoved and APILifeCycleDeprecated set for 1.32:

APILifecycleDeprecated
- APIGroupDiscovery (from apidiscovery/v2beta1)
- APIGroupDiscoveryList (from apidiscovery/v2beta1)
- VolumeAttributesClass (from storage/v1alpha1)
- VolumeAttributesClassList (from storage/v1alpha1)

APILifecycleRemoved
- SelfSubjectReview (from authentication/v1alpha1)
- FlowSchema (from flowcontrol/v1beta3)
- FlowSchemaList (from flowcontrol/v1beta3)
- PriorityLevelConfiguration (from flowcontrol/v1beta3)
- PriorityLevelConfigurationList (from flowcontrol/v1beta3)
- ClusterTrustBundle (from certificates/v1alpha1)
- ClusterTrustBundleList (from certificates/v1alpha1)

See the gist here for the full code snippets showing the above methods:
https://gist.github.com/aaron-prindle/3e8a5c3cef3b8ff10763be0d4858254d

You can see that the k8s types above align 1:1 with the integration test failues stating that k8s objects don't exist.

In speaking offline w/ @jpbetz it seems a viable short-term solution to unblock PR https://github.com/kubernetes/kubernetes/pull/126977 is to make it so that `APILifecycleRemoved` & `APILifecycleDeprecated` is non-blocking and only warns.  This would mean modifying logic in the two methods APILifecycleRemoved is currently called in:
- staging/src/k8s.io/apiserver/pkg/endpoints/deprecation/deprecation.go
- staging/src/k8s.io/apiserver/pkg/server/deleted_kinds.go



#### What did you expect to happen?

I expected to be able to bump DefaultKubeBinaryVersion s/v1.31/v1.32 without any integration test failures.

#### How can we reproduce it (as minimally and precisely as possible)?

Modify the code [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/component-base/version/base.go#L69) to bump DefaultKubeBinaryVersion and then run the kubernetes integration tests.  For a quick validation, you can run: test/integration/auth/selfsubjectreview_test.go `TestGetsSelfAttributes`

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在为alpha版本的API生成代码时，使用了`// +k8s:prerelease-lifecycle-gen:introduced`标签，导致自动生成了`APILifecycleRemoved`方法，将API的移除版本设置为引入版本加6个小版本。这对alpha版本的API来说可能不合适，导致在升级`DefaultKubeBinaryVersion`时出现集成测试失败。

这个问题主要是关于代码生成和API生命周期管理的逻辑错误，导致了测试失败和潜在的API不可用。但从安全角度来看，没有涉及到可被攻击者利用的安全风险。攻击者无法通过此问题进行攻击，也不涉及敏感信息泄露、权限提升、命令执行等高风险安全漏洞。

因此，根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #127240 PVC goes into Lost state

- Issue 链接：[#127240](https://github.com/kubernetes/kubernetes/issues/127240)

### Issue 内容

#### What happened?

Deploying a host path PVC results in Lost state.

1. User deployes below Persistent Volume.

```
kubectl get pv host-only-pv -oyaml

apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"name":"host-only-pv"},"spec":{"accessModes":["ReadWriteOnce"],"capacity":{"storage":"1Gi"},"hostPath":{"path":"/mnt/tmp-host","type":"DirectoryOrCreate"},"persistentVolumeReclaimPolicy":"Retain"}}
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: "2024-09-10T07:33:09Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: host-only-pv
  resourceVersion: "353506815"
  uid: 47ac2af2-ecd2-4ccd-8ee0-93b872980640
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: host-only-pvc
    namespace: bkp-pvc
    resourceVersion: "353506793"
    uid: be600292-2d9e-46db-b0f3-df709741930b
  hostPath:
    path: /mnt/tmp-host
    type: DirectoryOrCreate
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
status:
  phase: Bound
```
2. User creates Persistent volume claim reffering to Persiste volume created in step 1 but this PVC immidately moves into `Lost` Status.

```
kubectl get pvc host-only-pvc -n bkp-pvc -oyaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"host-only-pvc","namespace":"bkp-pvc"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}}}}
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: "2024-09-10T07:33:09Z"
  finalizers:
  - kubernetes.io/pvc-protection
  name: host-only-pvc
  namespace: bkp-pvc
  resourceVersion: "353508452"
  uid: be600292-2d9e-46db-b0f3-df709741930b
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
  volumeName: host-only-pv
status:
  phase: Lost
```

#### What did you expect to happen?

PVC should remain in bound state.

#### How can we reproduce it (as minimally and precisely as possible)?

-

#### Anything else we need to know?

I have attached kube-controller-managerlog here, where bunch of `Bound claim has lost its PersistentVolume. Data on the volume is lost!` error messages can be seen happening for other volumes as well.

[kube-controller-managerlogs.txt](https://github.com/user-attachments/files/16944022/kube-controller-managerlogs.txt)


#### Kubernetes version

<details>

```console
$ kubectl version
# 1.27.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ uname -a
# Linux ISVBK8CL3T06 4.18.0-553.5.1.el8_10.x86_64 #1 SMP Tue May 21 03:13:04 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
CRI-O
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经分析，此Issue描述了用户在创建使用hostPath类型的PersistentVolume（PV）和PersistentVolumeClaim（PVC）时，PVC立即进入了Lost状态的问题。用户还提供了相关的kube-controller-manager日志，其中显示了“Bound claim has lost its PersistentVolume. Data on the volume is lost!”的错误信息。

根据风险判断标准：

1. 该问题是关于PV和PVC绑定过程中出现的状态异常，没有迹象表明该问题可以被攻击者利用（标准1）。

2. 该问题不会成为一个漏洞，不会被分配CVE编号，按照CVSS 3.1评分标准也不会达到high以上的风险级别（标准2）。

6. 该Issue不涉及安全问题，而是功能性或配置方面的问题（标准6）。

因此，判断该Issue不涉及安全风险。

---

## Issue #127259 Need better conntrack management for UDP services (especially DNS)

- Issue 链接：[#127259](https://github.com/kubernetes/kubernetes/issues/127259)

### Issue 内容

#### What happened?

I created deployments with hostname, subdomain and headless service. I had the pods query their DNS records and log the results.

It typically took ~30 seconds for name resolution to be correct, though in some cases it could be much faster. Name resolution seems to fail occasionally returning NXDOMAIN. When the pod is deleted and recreated results are similar though the previous A record may also be returned.

I initially ran very short duration tests that stopped after observing a single intermittent failure. I later ran some longer duration tests and observed that these intermittent failures continue to occur and vary in duration.

#### First are some results of short duration tests after creating resources with reproducer-1.yml

Fairly typical, took ~30 seconds for initial successful response.
```
+ kubectl -n name-space logs --timestamps pod/example-9-85b9cf77c4-74dpb
2024-09-09T23:05:14.821639151Z ip address: 10.244.0.165
2024-09-09T23:05:46.613337669Z 10.244.0.165      example-9.headless.name-space.svc.cluster.local  example-9.headless.name-space.svc.cluster.local example-9.headless
2024-09-09T23:05:46.613549795Z success
2024-09-09T23:06:45.000865071Z 10.244.0.165      example-9.headless.name-space.svc.cluster.local  example-9.headless.name-space.svc.cluster.local example-9.headless
2024-09-09T23:06:45.001438197Z done
```

Note the intermittent failure nearly ~30 seconds after the pod started:
```
+ kubectl -n name-space logs --timestamps pod/example-46-5f88d64f8f-fkkmf
2024-09-09T23:05:26.142870309Z ip address: 10.244.0.205
2024-09-09T23:05:56.156259513Z 10.244.0.205      example-46.headless.name-space.svc.cluster.local  example-46.headless.name-space.svc.cluster.local example-46.headless
2024-09-09T23:05:56.156283138Z success
2024-09-09T23:05:56.159916727Z failure
2024-09-09T23:05:56.166636988Z 10.244.0.205      example-46.headless.name-space.svc.cluster.local  example-46.headless.name-space.svc.cluster.local example-46.headless
2024-09-09T23:05:56.166718155Z success
2024-09-09T23:05:56.166743405Z done
```

Note the intermittent failure nearly ~60 seconds after the pod started:
```
+ kubectl -n name-space logs --timestamps pod/example-8-7d65cf6595-5kgm8
2024-09-09T23:05:15.292906336Z ip address: 10.244.0.164
2024-09-09T23:05:45.299029486Z 10.244.0.164      example-8.headless.name-space.svc.cluster.local  example-8.headless.name-space.svc.cluster.local example-8.headless
2024-09-09T23:05:45.299171153Z success
2024-09-09T23:06:12.934370064Z 10.244.0.164      example-8.headless.name-space.svc.cluster.local  example-8.headless.name-space.svc.cluster.local example-8.headless
2024-09-09T23:06:12.934805231Z failure
2024-09-09T23:06:12.935585357Z 10.244.0.164      example-8.headless.name-space.svc.cluster.local  example-8.headless.name-space.svc.cluster.local example-8.headless
2024-09-09T23:06:12.935622357Z success
2024-09-09T23:06:12.935626232Z done
```

Correct name resolution nearly ten times faster then the typical ~30 seconds.
```
+ kubectl -n name-space logs --timestamps pod/example-50-5d7bd9f7dd-ldm9z
2024-09-09T23:05:25.994666234Z ip address: 10.244.0.203
2024-09-09T23:05:28.507822913Z 10.244.0.203      example-50.headless.name-space.svc.cluster.local  example-50.headless.name-space.svc.cluster.local example-50.headless
2024-09-09T23:05:28.507850872Z success
2024-09-09T23:05:28.512090337Z failure
2024-09-09T23:05:28.517020845Z 10.244.0.203      example-50.headless.name-space.svc.cluster.local  example-50.headless.name-space.svc.cluster.local example-50.headless
2024-09-09T23:05:28.517073470Z success
2024-09-09T23:05:28.517076387Z done
```

#### Next are some short duration examples after deleting the pod created with reproducer-1.yml

Resolved to previous IP address until ~30 seconds when name resolution become correct.
```
+ kubectl -n name-space logs --timestamps pod/example-32-94f578796-ggr5r
2024-09-09T23:28:58.365987901Z ip address: 10.244.0.189
2024-09-09T23:28:58.366879902Z 10.244.0.135      example-32.headless.name-space.svc.cluster.local  example-32.headless.name-space.svc.cluster.local example-32.headless
2024-09-09T23:28:58.366941985Z success
2024-09-09T23:29:28.372333094Z 10.244.0.135      example-32.headless.name-space.svc.cluster.local  example-32.headless.name-space.svc.cluster.local example-32.headless
2024-09-09T23:30:29.538217372Z 10.244.0.189      example-32.headless.name-space.svc.cluster.local  example-32.headless.name-space.svc.cluster.local example-32.headless
2024-09-09T23:30:29.540434333Z done
```

In this case name resolution failed briefly ~90 seconds after pod started.
```
+ kubectl -n name-space logs --timestamps pod/example-33-5c94568945-zwhk8
2024-09-09T23:28:56.224260600Z ip address: 10.244.0.177
2024-09-09T23:28:56.225344685Z 10.244.0.136      example-33.headless.name-space.svc.cluster.local  example-33.headless.name-space.svc.cluster.local example-33.headless
2024-09-09T23:28:56.225403852Z success
2024-09-09T23:29:26.229834542Z 10.244.0.136      example-33.headless.name-space.svc.cluster.local  example-33.headless.name-space.svc.cluster.local example-33.headless
2024-09-09T23:30:22.876064342Z 10.244.0.177      example-33.headless.name-space.svc.cluster.local  example-33.headless.name-space.svc.cluster.local example-33.headless
2024-09-09T23:30:22.878206470Z failure
2024-09-09T23:30:22.881135099Z 10.244.0.177      example-33.headless.name-space.svc.cluster.local  example-33.headless.name-space.svc.cluster.local example-33.headless
2024-09-09T23:30:22.881363766Z success
2024-09-09T23:30:22.881381516Z done
```

Here name resolution alternated between the old & new address, and failed briefly before correctly providing new address.
```
+ kubectl -n name-space logs --timestamps pod/example-34-5f8cdcc9bb-zp2xj
2024-09-09T23:28:56.311666142Z ip address: 10.244.0.178
2024-09-09T23:28:58.814520539Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.814552873Z success
2024-09-09T23:28:58.816665626Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845242167Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845250417Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845252042Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845253708Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845255333Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845256750Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:28:58.845258125Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:03.874183954Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:03.874214370Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:03.874216745Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:03.874218495Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
...
2024-09-09T23:29:28.801997330Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:28.801998872Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:28.802000372Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:29:28.802001830Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.697987578Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.698009453Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.698011870Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.698013661Z 10.244.0.146      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.698017411Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.699209288Z failure
2024-09-09T23:30:04.701096499Z 10.244.0.178      example-34.headless.name-space.svc.cluster.local  example-34.headless.name-space.svc.cluster.local example-34.headless
2024-09-09T23:30:04.701108082Z success
2024-09-09T23:30:04.701110207Z done
```

#### Finally an example of a longer duration test with reproducer-2.yml

Failures continue to occur and there appears to be a failure lasting ~5 seconds from 00:58:06-00:58:11.
```
+ kubectl -n name-space logs --timestamps pod/example-44-7b4c6db795-jnpx4
2024-09-10T00:53:45.606690224Z ip address: 10.244.0.148
2024-09-10T00:54:15.645760871Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:54:15.645830788Z success
2024-09-10T00:54:21.547652384Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:54:21.549761139Z failure
2024-09-10T00:54:21.551739643Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:54:21.551747935Z success
2024-09-10T00:55:08.732400872Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:08.733033040Z failure
2024-09-10T00:55:08.733892585Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:08.733955002Z success
2024-09-10T00:55:13.974409562Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:13.974909230Z failure
2024-09-10T00:55:13.975787899Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:13.975875275Z success
2024-09-10T00:55:19.037338608Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:19.037842693Z failure
2024-09-10T00:55:19.038832488Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:19.038843155Z success
2024-09-10T00:55:29.117418683Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:29.117966018Z failure
2024-09-10T00:55:29.118763229Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:29.118802687Z success
2024-09-10T00:55:44.373593325Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:44.374101410Z failure
2024-09-10T00:55:44.374768245Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:55:44.374816787Z success
2024-09-10T00:57:10.127070908Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:57:10.128437322Z failure
2024-09-10T00:57:10.133007897Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:57:10.134041104Z success
2024-09-10T00:57:51.941566804Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:57:51.941980888Z failure
2024-09-10T00:57:51.942865181Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:57:51.942923889Z success
2024-09-10T00:58:06.189370517Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:58:06.189968810Z failure
2024-09-10T00:58:11.194641064Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:58:11.194660897Z success
2024-09-10T00:58:31.853444367Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:58:31.854188701Z failure
2024-09-10T00:58:31.855070078Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T00:58:31.855520870Z success
2024-09-10T01:00:14.012500903Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:00:14.013042733Z failure
2024-09-10T01:00:14.014231435Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:00:14.014433351Z success
2024-09-10T01:00:25.099501939Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:00:25.100104894Z failure
2024-09-10T01:00:25.101076931Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:00:25.101219472Z success
2024-09-10T01:01:28.106594377Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:01:28.108120337Z failure
2024-09-10T01:01:28.109668172Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:01:28.109720922Z success
2024-09-10T01:01:47.173618917Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:01:47.176251003Z failure
2024-09-10T01:01:47.178596298Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:01:47.178728923Z success
2024-09-10T01:02:34.829412651Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:02:34.829895652Z failure
2024-09-10T01:02:34.831115611Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:02:34.831143778Z success
2024-09-10T01:02:45.651782651Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:02:45.652329402Z failure
2024-09-10T01:02:45.653055861Z 10.244.0.148      example-44.headless.name-space.svc.cluster.local  example-44.headless.name-space.svc.cluster.local example-44.headless
2024-09-10T01:02:45.653182319Z success
```

#### What did you expect to happen?

I expected that once name resolution provided the correct address it would not fail intermittently or subsequently return the wrong (old) address.

It would also be nice if it were possible that name resolution could start sooner more often, and ideally never return the wrong (old) address.


#### How can we reproduce it (as minimally and precisely as possible)?

I initially used this configuration and script:
[reproducer-1.yml.txt](https://github.com/user-attachments/files/16938125/reproducer-1.yml.txt)
[script-1.sh.txt](https://github.com/user-attachments/files/16938127/script-1.sh.txt)

The script above stopped after the first failure, to see more failures I used these:
[reproducer-2.yml.txt](https://github.com/user-attachments/files/16938137/reproducer-2.yml.txt)
[script-2.sh.txt](https://github.com/user-attachments/files/16938138/script-2.sh.txt)


#### Anything else we need to know?

This may be related to some existing issues.

The mostly ~30 second delay: https://github.com/kubernetes/kubernetes/issues/92559

The intermittent failures: https://github.com/coredns/coredns/issues/6518

Assuming I need to workaround the intermittent failure, are there any downsides to modifying the hosts file directly (not through HostAliases) other than losing changes when the container exits?

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
kind version 0.22.0
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="40.20240529.0 (Silverblue)"
ID=fedora
VERSION_ID=40
VERSION_CODENAME=""
PLATFORM_ID="platform:f40"
PRETTY_NAME="Fedora Linux 40.20240529.0 (Silverblue)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:40"
DEFAULT_HOSTNAME="fedora"
HOME_URL="https://silverblue.fedoraproject.org"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora-silverblue/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://github.com/fedora-silverblue/issue-tracker/issues"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=40
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=40
SUPPORT_END=2025-05-13
VARIANT="Silverblue"
VARIANT_ID=silverblue
OSTREE_VERSION='40.20240529.0'
$ uname -a
Linux fedora 6.8.10-300.fc40.aarch64 #1 SMP PREEMPT_DYNAMIC Fri May 17 21:52:12 UTC 2024 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes环境中，Pods在DNS解析时出现了间歇性的失败或延迟的问题，尤其是在使用UDP服务（如DNS）时出现。通过提供的日志可以看出，DNS解析有时返回错误的结果，或者解析时间过长。

根据风险判断标准：

1. **攻击利用性**：该问题是由于DNS解析的不稳定导致的，但没有提及任何攻击者可以利用的漏洞或方法。

2. **漏洞严重性**：没有证据表明该问题会导致严重的安全漏洞，不符合分配CVE编号或根据CVSS 3.1评级为高风险的条件。

3. **信息泄露**：Issue内容没有涉及敏感信息的泄露或配置错误。

4. **权限影响**：问题没有涉及权限提升、命令执行、容器逃逸等高风险操作。

综上所述，该Issue主要反映了DNS解析在Kubernetes中的性能和稳定性问题，属于功能性缺陷或配置优化范畴，不涉及安全风险。

---

## Issue #127234 Headless service end point update problem

- Issue 链接：[#127234](https://github.com/kubernetes/kubernetes/issues/127234)

### Issue 内容

#### What happened?

I have a problem about headless service.

I have a statefulset redis app on my 3 node cluster, as you see I shutdown node1, now redis-node-1 is in terminating state
```
kubectl get pods -A -o wide | grep redis
mynamespace   redis-node-0                                    3/3     Running            0                    18m     10.244.248.4     ha3-node2
mynamespace   redis-node-1                                    3/3     Terminating        0                    68m     10.244.230.119  ha3-node1
mynamespace   redis-node-2                                    3/3     Running            0                    67m     10.244.192.208   ha3-node3
```
I have two services, redis and redis-headless
In redis-headless 10.244.230.119 is still in endpoints
```
kubectl describe endpoints  -n mynamespace redis-headless
Name:         redis-headless
Namespace:    mynamespace
Subsets:
  Addresses:          10.244.192.208,10.244.230.119,10.244.248.4
```
For redis service (clusterIP) endpoints are OK. Is this behaviour normal, how to solve for headless?
k8s version: v1.27.15 (edited) 

#### What did you expect to happen?

headless service end point should be updated as the node goes down.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce it you can install redis cluster on kubernetes. 

#### Anything else we need to know?

I've tested same scenario with nginx but headless ep updated immediately as expected.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T20:04:38Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T19:56:02Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="18.04.6 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.6 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
$ uname -a
Linux ha2-node1 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes集群中，当节点关闭后，headless service的endpoints没有及时更新，仍然包含了处于Terminating状态的Pod的IP地址。这可能导致服务发现不准确，但从安全角度来看，这并不构成安全风险。

根据风险判断标准：

1. **该风险能被攻击者利用**：此问题不涉及被攻击者利用来进行未授权的操作或攻击。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题不符合CVE漏洞的定义，不会被分配CVE编号，且不满足CVSS高风险漏洞的条件。
3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险**：此Issue未暴露任何敏感信息。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

综上所述，该Issue不涉及安全风险。

---

## Issue #127232 API emulation versioning seems break Cohabitating Resources overwriting

- Issue 链接：[#127232](https://github.com/kubernetes/kubernetes/issues/127232)

### Issue 内容

#### What happened?

I have an extension apiserver, and I was trying to upgrading the dependency `k8s.io/apiserver`.  In our code, we have a serval resources has its `Cohabitating Resources`, and we expect the storage version is their `Cohabitating Resources`.

for example, we have a resource `machine` in both `raw` and `core` group, and they are wired up by `AddCohabitatingResources` method, and expected encoded as `raw` to storage.
```
storageFactory.AddCohabitatingResources(raw.Resource("machines"), core.Resource("machines"))
```

However, this seems broken after upgrade to v1.31, so I did some invertgation by myself, and I find the storage version is changed by `BackwardCompatibileStorageEncodingFor`  here:
https://github.com/kubernetes/kubernetes/blob/403301bfdf2c7312591077827abd2e72f445a53a/staging/src/k8s.io/apiserver/pkg/server/storage/storage_factory.go#L244-L257

In old behavior `StorageEncodingFor` honor `chosenStorageResource`, but now `BackwardCompatibileStorageEncodingFor` don't.

#### What did you expect to happen?

storage version semantic of `Cohabitation Resources` would be honored.

#### How can we reproduce it (as minimally and precisely as possible)?

there is not easy approach, need to create an extension api server and find out. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-beta.0.25+a70cb76847ada1-dirty

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

Kind Cluster run on Darwin

Darwin ReficuldeMacBook-Pro.local 23.0.0 Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:34 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T8103 arm64

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题描述了在升级`k8s.io/apiserver`到v1.31后，`Cohabitating Resources`的存储版本行为发生了变化，不再按照预期存储为指定的版本。这可能导致了资源的存储版本不一致或不符合预期的情况。但是，从安全角度来看，这个问题主要是功能性缺陷，涉及资源版本管理和存储策略的变化。

根据风险判断标准：

1. **该风险能被攻击者利用**：该问题需要创建扩展的API服务器，并特意配置`Cohabitating Resources`，普通攻击者难以利用，且需要较高的权限。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题未涉及权限提升、命令执行、数据泄露等高危安全问题，CVSS评分不会达到High以上。
3. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue未涉及安全风险。

综上所述，该Issue不涉及安全风险。

---

## Issue #127194 DRA: draplugin fails silently if vital parameter is missing

- Issue 链接：[#127194](https://github.com/kubernetes/kubernetes/issues/127194)

### Issue 内容

#### What happened?

When calling draplugin.Start without supplying kubeclient no error is reported, while the kubeclient is vital for further announcement of resourceSlice.

#### What did you expect to happen?

An error needs to be returned when nodeName or kubeclient parameters are not set.

#### How can we reproduce it (as minimally and precisely as possible)?

Do not supply kubeclient parameter when creating draplugin, for isntance here: https://github.com/kubernetes-sigs/dra-example-driver/blob/abc52cfb8adfe5b160c168fd45f40a2e8f08c439/cmd/dra-example-kubeletplugin/driver.go#L50

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在调用`draplugin.Start`时，如果未提供`kubeclient`参数，插件会静默失败，没有报错。然而，`kubeclient`对于后续的`resourceSlice`公告是至关重要的。

从安全角度来看，这可能导致插件未能正常工作，但并未体现出可被攻击者利用的安全风险。缺少参数导致的错误一般属于功能性问题，而非安全漏洞。

根据风险判断标准：

1. **该风险不能被攻击者利用**，因为它需要开发者在调用插件时遗漏必要参数，攻击者无法控制此情况。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

---

## Issue #127181 Status of APIServingWithRoutine gate

- Issue 链接：[#127181](https://github.com/kubernetes/kubernetes/issues/127181)

### Issue 内容

#### What happened?

The code says this (staging/src/k8s.io/apiserver/pkg/features/kube_features.go):

```go
	APIServingWithRoutine: {Default: false, PreRelease: featuregate.Alpha},
```

While the comment says this (staging/src/k8s.io/apiserver/pkg/features/kube_features.go):

```go
	// owner: @linxiulei
	// beta: v1.30
	//
	// Enables serving watch requests in separate goroutines.
	APIServingWithRoutine featuregate.Feature = "APIServingWithRoutine"
```

So..., what happened? The feature is still Alpha or it is a Beta?

#### What did you expect to happen?

Be consistent about the features we add or change.

#### How can we reproduce it (as minimally and precisely as possible)?

Read the code.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30.0

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了代码中`APIServingWithRoutine`特性门的状态与注释不一致的问题。具体来说，代码中将该特性标记为Alpha阶段 (`PreRelease: featuregate.Alpha`)，默认关闭 (`Default: false`)，而注释中则标记为Beta阶段 (`// beta: v1.30`)。这种不一致可能导致开发人员或用户对特性的稳定性和可靠性产生误解。在某些情况下，可能因为误认为是Beta特性而在生产环境中启用，但该特性实际上仍处于Alpha阶段，可能不够稳定。然而，根据提供的信息，此问题并不涉及潜在的安全风险。没有迹象表明攻击者可以利用此不一致性进行攻击，也没有涉及任何可能被利用的漏洞。因此，按照风险判断标准，该Issue不涉及安全风险。

---

## Issue #127156 Unable to set new pre-alpha versioned featuregate to enabled during apiserver initialisation in integration tests

- Issue 链接：[#127156](https://github.com/kubernetes/kubernetes/issues/127156)

### Issue 内容

While developing a new in-tree feature, I need to add integration tests which conditionally enable/disable my feature. I've added the gate to the 'versioned feature gates' file, which seems to work ok. However, my feature gate's configuration is read upon initialisation of the apiserver (it's read by an admission plugin). This means I *must* set this feature gate to be enabled *prior* to calling `StartTestServerOrDie` (or at least, prior to this function actually running/starting the apiserver startup functions).

This feature is considered 'pre-alpha' in 1.31, as it'd be only become alpha in 1.32 (assuming it's merged etc.). This means if I attempt to set this flag to 'true' without also changing the 'emulation version', I receive an error that the feature cannot be enabled at this version (1.31) as it is PreAlpha.

So, I have updated my call to StartTestServer to set the BinaryVersion in the instance options to '1.32':

```
	// Force to run in 1.32 mode as we are testing a feature that only exists (in alpha) from 1.32 onwards.
	opts := kubeapiservertesting.NewDefaultTestServerOptions()
	opts.BinaryVersion = "1.32"
	server := kubeapiservertesting.StartTestServerOrDie(t, opts, framework.DefaultTestServerFlags(), framework.SharedEtcd())
```

However, I still need to set the feature gate itself to enabled, as it's disabled by default (it's an alpha feature). This presents a chicken-egg problem for me:

If I add `featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.SetPodTopologyLabels, true)` before `StartTestServerOrDie`, the call to set the emulation version to 1.32 has not happened yet (it is within StartTestServerOrDie).

If I set the flag *after*, my feature flag value will have already been read by the admission plugin and I won't get the behaviour I need.

The only alternative I can think of here is to set feature flags using the `--feature-flags` argument passed in InstanceOptions, but this is a departure from how we'd have done this prior to the introduction of versioned feature gates, so I wanted to gather some feedback on how we're _supposed_ to enable pre-alpha feature flags that are required during apiserver initialisation.

/area testing
/sig architecture


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
通过对Issue的分析，该问题涉及在集成测试中如何启用一个新的预先版本（pre-alpha）的feature gate。开发者遇到了在测试环境下启用该feature gate的困难，主要是由于版本兼容性和初始化顺序的问题。

整个讨论集中在测试环境的配置和代码初始化顺序的调整上，没有涉及任何安全方面的内容。没有证据表明此问题会引入安全风险，或者可以被攻击者利用。

根据提供的风险判断标准，尤其是标准6“如果Issue不涉及安全问题，则风险评级判断为不涉及”，因此可以判断该Issue不涉及安全风险。

---

## Issue #127199 topologySpreadConstraints not working as expected

- Issue 链接：[#127199](https://github.com/kubernetes/kubernetes/issues/127199)

### Issue 内容

#### What happened?

After an upgrade where the deployment restarts, we have identified that the ingress-nginx pods didn't spread across all three AWS AZ's although we have topologyspeadcontraint defined, which resulted in a failure.
While doing a NSlookup of NLB, we observed that only 2 public IP's were returned, even though NLB has 3 public IP's. The third one is supposed to be on availability zone 1b of K8S NLB, but we don't have any ingress pods running in 1b AZ. 
As a fix we added more pods to ingress-nginx controller, which added new pods to 1b AZ, then we did the nslookup again and found three IP's.

Attached the topologyspeadcontraint as defined in our nginx pod deployment-

      nodeSelector:
        dedicated: system
      restartPolicy: Always
      schedulerName: default-scheduler
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: ingress-nginx
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule






#### What did you expect to happen?

The pods are supposed to be evenly spread across all 3 AZs

#### How can we reproduce it (as minimally and precisely as possible)?

Pod deployment with topologySpreadConstraints has this issue

#### Anything else we need to know?

Seems to be an issue when node Selector is defined

#### Kubernetes version

EKS version v1.28.3

#### Cloud provider

AWS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

```[tasklist]
#### Tasks
```


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题涉及在Kubernetes中`topologySpreadConstraints`未按照预期工作，导致Ingress Nginx Pods没有均匀分布在所有的可用区。这是一个部署配置或调度策略的问题，可能影响系统的高可用性或负载均衡效果，但并未涉及任何可被攻击者利用的安全漏洞或安全风险。按照风险判断标准，此Issue不涉及安全问题。

---

## Issue #127132 [FG:InPlacePodVerticalScaling] ResourceQuota unresponsive to scale-down

- Issue 链接：[#127132](https://github.com/kubernetes/kubernetes/issues/127132)

### Issue 内容

/kind bug

**TL;DR:**

If a pod's requests are scaled down with InPlacePodVerticalScaling, it can take a long time (up to 5 minutes by default) for resource quota to free the resources. If the pod is scaled back up in that window, the difference will be double-counted.

**Explanation:**

If a pod is resized, the resource quota admission controller relies on the [resource helper `resource.PodRequests`](https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/resource/helpers.go#L50) to determine the total request size. That function uses the maximum of the resource request (`.spec.containers[*].resources.requests`) and the allocated resources (`.status.containerStatuses[*].allocatedResources`).

In the event of scaling down, the allocated resource value will be larger, and therefore the value that is used to set the resource quota. This is working as intended, but the ResourceQuota controller should pick up the scale down once the Kubelet updates the allocated resources.

The resource quota's default filter function needs to be updated to consider updates to allocated resources:
https://github.com/kubernetes/kubernetes/blob/48d6d55a47ca8202692098639f7b3c76f107f46a/pkg/quota/v1/install/update_filter.go#L26-L33

Currently, the status update is ignored, so resource quota never sees the actual scale down, and the quota is only updated by the periodic resync.

/sig node
/milestone v1.32
/priority important-longterm

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用InPlacePodVerticalScaling缩减Pod的资源请求时，资源配额（ResourceQuota）在5分钟内未能及时释放资源，可能导致在此窗口期内再次扩容时出现资源双重计算的问题。这可能会导致资源配额统计不准确，但不会引发安全风险。

根据风险判断标准，第6条：“如果Issue不涉及安全问题，则风险评级判断为不涉及”。此问题仅涉及系统性能和资源管理方面的Bug，没有涉及攻击者可利用的漏洞，不会导致未授权的访问、权限提升、数据泄露等安全问题。

因此，综合分析，该Issue不涉及安全风险。

---

## Issue #127125 Optional secret mounts taint pod directories on host

- Issue 链接：[#127125](https://github.com/kubernetes/kubernetes/issues/127125)

### Issue 内容

#### What happened?

1. Create a pod with a volume mount of an optional secret
2. Create the secret
3. Trigger kubelet trying to recreate the container _but not the pod_ 
    - For the repro case I rebooted the VM, but there's probably an easier way to do this
4. Pod now has `CreateContainerConfigError` status and doesn't come up

It looks like what happens is that kubelet creates a directory where it later expects a file:
```
Sep 04 18:05:24 node kubelet[1300]: E0904 18:05:24.448350    1300 kubelet_pods.go:349] "Failed to prepare subPath for volumeMount of the container" err="error creating file /var/lib/kubelet/pods/e32797bd-b956-482c-af31-bffa78ba3ded/volume-subpaths/vol/test/0: open /var/lib/kubelet/pods/e32797bd-b956-482c-af31-bffa78ba3ded/volume-subpaths/vol/test/0: is a directory" containerName="test" volumeMountName="vol"
```
The pod's description says:
```
Error: failed to prepare subPath for volumeMount "vol" of container "test"
```

Restarting the pod (e.g. recreating it, or deleting it if it's in a deployment) solves the problem, because it's a whole new kubelet directory on the host and there is no conflict.

#### What did you expect to happen?

I'm not entirely sure if it's reasonable, but I would expect kubelet to detect this provisional directory and delete it first before recreating it now that the secret exists. 

If not, I would at least expect better documentation around this.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply this yaml:
```
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Equal
      effect: NoSchedule

  containers:
  - name: test
    image: busybox:latest
    command: [ "sh", "-c", "sleep infinity" ]
    volumeMounts:
    - mountPath: /run/map/foo
      name: vol
      subPath: foo
  volumes:
    - name: vol
      secret:
        secretName: secret
        optional: true
```
Observe the pod comes up without issue. You can verify the subpath directory is a directory:
```
$ ls -l /var/lib/kubelet/pods/e32797bd-b956-482c-af31-bffa78ba3ded/volume-subpaths/vol/test/
total 4
drwxr-x--- 2 root root 4096 Sep  4 17:59 0
```
Create the secret:
```
$ kubectl create secret generic secret --from-literal=foo=hello
```
Reboot the node/VM.
The pod is now in the error state.
You can also delete and recreate the pod and observe the same directory above is now a file with the expected contents.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
```

</details>

#### Install tools

kubeadm

#### Container runtime (CRI) and version (if applicable)

containerd github.com/containerd/containerd 1.7.12

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，问题描述是在使用可选的Secret进行卷挂载时，kubelet在某些情况下会创建目录，而后期期望的位置应该是文件，导致容器无法启动并报错。这是由于kubelet在处理可选Secret挂载时的逻辑问题。当Secret不存在时，kubelet创建了一个目录占位，之后Secret被创建，但kubelet没有正确处理之前创建的目录，导致文件和目录冲突。

从安全角度来看，这个问题并未涉及任何潜在的安全风险。攻击者无法利用此问题进行权限提升、命令执行、信息泄露等攻击。它只是一个导致服务不可用的功能性错误，且需要管理权限才能配置和重启pod。

根据风险判断标准：
1. 攻击者无法利用该问题。
2. 该问题不可能成为一个漏洞，也不会被分配CVE编号，按照CVSS 3.1评分标准，得分不会在高危以上。
6. 该Issue不涉及安全问题，风险评级判断为不涉及。

综上所述，该Issue不涉及安全风险。

---

## Issue #127114 upgrading kernel version of master node causes apiserver keeps restarting

- Issue 链接：[#127114](https://github.com/kubernetes/kubernetes/issues/127114)

### Issue 内容

#### What happened?

My environment：
**kubernetes：1.24.17 (3 masters and 1 worker)**
etcd and kube-apiserver run as pod in my cluster.

**It runs well when all of 3 masters are 3.10.0-1160.99.1.el7.x86_64 kernel version;**

**I upgraded one master's kernel version to 5.4.277-1.el7.elrepo.x86_64(delete this node—>upgrade kernel version—>join the cluster)** 

this master's kube-apiserver runs well within the first hour, then it begins restarting and make etcd restarting, Besides, kube-apiserver makes the node stuck, after mv kube-apiserver.yaml from /etc/kubernets/manifests to somewhere else ,my node becomes normal.

I can't figure out what happened to my apiserver, any help would be appreciate.

The following is the related logs of kube-apiserver.
```
E0904 07:37:58.681591       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.1.19.38:443/apis/metrics.k8s.io/v1beta1: Get "https://10.1.19.38:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

E0904 07:38:01.593119       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0904 07:38:01.593177       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0904 07:38:01.913053       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.1.19.38:443: connect: connection timed out

E0904 07:38:36.200130       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again

E0904 08:43:43.765735       1 status.go:71] apiserver received an error that is not an metav1.Status: context.deadlineExceededError{}: context deadline exceeded
E0904 08:43:43.765855       1 writers.go:118] apiserver was unable to write a JSON response: http: Handler timeout
E0904 08:43:43.767076       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0904 08:43:43.768363       1 writers.go:131] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0904 08:43:43.769740       1 trace.go:205] Trace[1304417108]: "Get" url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager,user-agent:kube-controller-manager/v1.24.17 (linux/amd64) kubernetes/22a9682/leader-election,audit-id:d139ac48-6f60-4895-b634-5260be768ba8,client:10.1.69.88,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (04-Sep-2024 08:43:38.780) (total time: 4989ms):
Trace[1304417108]: [4.989288367s] [4.989288367s] END
E0904 08:43:43.779268       1 timeout.go:141] post-timeout activity - time-elapsed: 13.468579ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager" result: <nil>
{"level":"warn","ts":"2024-09-04T08:43:43.965Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000a15340/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2024-09-04T08:43:45.338Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002013880/127.0.0.1:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E0904 08:43:45.338314       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0904 08:43:45.338398       1 writers.go:118] apiserver was unable to write a JSON response: http: Handler timeout
E0904 08:43:45.339620       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0904 08:43:45.340857       1 writers.go:131] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0904 08:43:45.342200       1 trace.go:205] Trace[1213158620]: "Get" url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler,user-agent:kube-scheduler/v1.24.17 (linux/amd64) kubernetes/22a9682/leader-election,audit-id:e034f986-8083-4c20-b66c-029720afa2ba,client:10.1.69.88,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (04-Sep-2024 08:43:40.347) (total time: 4994ms):
Trace[1213158620]: [4.994932702s] [4.994932702s] END
E0904 08:43:45.347125       1 timeout.go:141] post-timeout activity - time-elapsed: 8.904939ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler" result: <nil>

E0904 08:45:01.625216       1 controller.go:220] unable to create required kubernetes system namespace kube-public: Internal error occurred: resource quota evaluation timed out
E0904 08:45:01.626394       1 controller.go:220] unable to create required kubernetes system namespace kube-node-lease: Post "https://[::1]:6443/api/v1/namespaces": dial tcp [::1]:6443: connect: connection refused
{"level":"warn","ts":"2024-09-04T08:45:01.679Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000a15340/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\""}
```

#### What did you expect to happen?

I think the new kube-apiserver will work fine

#### How can we reproduce it (as minimally and precisely as possible)?

**upgraded one master's kernel version to 5.4.277-1.el7.elrepo.x86_64(delete this node—>upgrade kernel version—>join the cluster)** 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.17", GitCommit:"22a9682c8fe855c321be75c5faacde343f909b04", GitTreeState:"clean", BuildDate:"2023-08-23T23:44:35Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.17", GitCommit:"22a9682c8fe855c321be75c5faacde343f909b04", GitTreeState:"clean", BuildDate:"2023-08-23T23:37:25Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}

```

</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

$ uname -a
Linux master-6988 5.4.277-1.el7.elrepo.x86_64 #1 SMP Sun May 26 13:12:21 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd-1.6
</details>

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico-3.25.2
</details>

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在升级一台master节点的内核版本后，kube-apiserver和etcd出现了重启问题。根据提供的日志，出现了连接超时、请求被取消以及连接被拒绝等错误。这些问题可能是由内核升级导致的系统兼容性或配置问题引起的，并未涉及任何安全风险。根据风险判断标准，此Issue不涉及安全问题。

---

## Issue #127108 csidriver register failed and kubelet will not retry

- Issue 链接：[#127108](https://github.com/kubernetes/kubernetes/issues/127108)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/95956671d8da7783a726133709b8085f56dda052/pkg/kubelet/pluginmanager/operationexecutor/operation_generator.go#L124-L126
When Kubelet registers the CSI, if the registration of the CSI plug-in fails due to some reasons, Kubelet notifies the CSI of the registration failure but does not retry. Is this reasonable?
Whether the retry operation is performed by the csi plug-in or by the kubelet

#### What did you expect to happen?

The csi plug-in registration mechanism must have a retry mechanism to ensure reliable operation.

#### How can we reproduce it (as minimally and precisely as possible)?

Create some network errors when the csi plug-in is registered.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
在该Issue中，描述了当Kubelet注册CSI插件失败时，不会进行重试的问题。这主要涉及到CSI插件的可靠性和Kubelet的重试机制。从安全角度来看，没有明显的安全风险。攻击者无法利用该行为进行攻击，也不存在信息泄露、权限提升、远程代码执行等安全问题。

根据风险判断标准：
1. 该风险不能被攻击者利用。
6. 如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---

## Issue #127106 error: unable to upgrade connection: error dialing backend: dial tcp 127.0.0.1:25241: connect: connection refusedssh

- Issue 链接：[#127106](https://github.com/kubernetes/kubernetes/issues/127106)

### Issue 内容

#### What happened?

# CurrentBehavior 
connect: connection refusedssh 

#### What did you expect to happen?

# ExpectedBehavior 
kubectl works fine 

#### How can we reproduce it (as minimally and precisely as possible)?

ALL VERSION

#### Anything else we need to know?

_No response_

#### Kubernetes version

ALL

#### Cloud provider

ALL

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在使用kubectl时出现连接被拒绝的错误，提示无法升级连接：错误拨号后端：拨打tcp 127.0.0.1:25241时连接被拒绝。这显示kubectl无法正常连接到Kubernetes集群的后端服务，可能是由于配置错误、网络问题或服务未启动造成的。这是一个功能性问题或环境配置问题，并不涉及安全风险。

---

## Issue #127087 PreFilterResult test in TestCoreResourceEnqueue isn't run

- Issue 链接：[#127087](https://github.com/kubernetes/kubernetes/issues/127087)

### Issue 内容

> The test case you added in test/integration/scheduler/queue_test.go will not be executed when enableSchedulingQueueHint is empty. And this test case will always fail regardless of whether we set it to true or false. 
https://github.com/kubernetes/kubernetes/blob/c86a2d6925a61ac181468b74f573518db1d645d2/test/integration/scheduler/queue_test.go#L325-L359
https://github.com/kubernetes/kubernetes/pull/122251#issuecomment-2325937664



### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在测试文件test/integration/scheduler/queue_test.go中，新增的测试用例由于enableSchedulingQueueHint为空而未被执行，并且无论设置为true还是false，该测试用例都会失败。这可能导致相关功能未被正确测试，潜在地可能引入代码缺陷。然而，根据风险判断标准，除非这些缺陷能够被攻击者利用，成为可被分配CVE编号的漏洞，并且使用CVSS 3.1评级达到High级别以上，否则不属于安全风险。由于该Issue仅涉及测试用例的问题，没有直接指出任何可被利用的安全漏洞，因此不涉及安全风险。

---

## Issue #127085 When kubelet is restarted, the endpoints of the service are lost temporarily

- Issue 链接：[#127085](https://github.com/kubernetes/kubernetes/issues/127085)

### Issue 内容

#### What happened?

My k8s version is 1.22.1,
service type is Cluster.
When i restart kubelet on business node and use this cmd on k8s node:kubectl get endpoints fkft7-nslb-north-svc -nns000000000000000000001，i find the endpoints of the service are lost temporarily. and log of kube-controller-manager show me like this
![image](https://github.com/user-attachments/assets/62659cbf-5aae-4b3d-ba65-08c00fa9781f)


Are other conditions triggering this bug as well? I wonder if this problem has been fixed.

#### What did you expect to happen?

endpoints won't lost

#### How can we reproduce it (as minimally and precisely as possible)?

restart kubelet and watch endpoints of service

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.22.1

#### Cloud provider

<details>

</details>
None

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
suse 12.5

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提交的Issue内容，问题描述为当重启kubelet时，服务的endpoints会暂时丢失。这可能导致服务短暂不可用，但这是因为管理员手动重启kubelet引起的。

根据风险判断标准：

1. **该风险不能被攻击者利用**，因为重启kubelet需要高权限，普通攻击者无法执行。

4. **当拒绝服务攻击需要攻击者具备高权限时，不应判断为高风险**。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

综上所述，该Issue不涉及安全风险。

---

## Issue #127081 ReplicaSet do not update latest failure condition into status

- Issue 链接：[#127081](https://github.com/kubernetes/kubernetes/issues/127081)

### Issue 内容

#### What happened?

If I have 2 admissions to check before a POD can be create, the 2 admissions checks run one after one (not in parallel), with 2 different error messages.
I'll use the below admission check for POD creation as an example to reproduce the bug:
```
if currentTime.Minute() < 40 {
		admissionResponse.Allowed = false
		admissionResponse.Result = &metav1.Status{
			Reason:  metav1.StatusReasonForbidden,
			Message: "Pod creation is not allowed during the first 40 minutes of the hour.",
		}
	} else {
		admissionResponse.Allowed = false
		admissionResponse.Result = &metav1.Status{
			Reason:  metav1.StatusReasonForbidden,
			Message: "Pod creation is not allowed during the second 20 minutes of the hour.",
		}
	}
```
Which means, in the first 40 minutes of every hour, the error message will be "Pod creation is not allowed during the first 40 minutes of the hour.", while in the left 20 minutes of every hour, the error message will be "Pod creation is not allowed during the second 20 minutes of the hour".

If I create a deployment now, in the left 20 minutes (xx:40 - xx:59), the ReplicaSet will be:
Describe result:
```
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age                 From                   Message
  ----     ------        ----                ----                   -------
  Warning  FailedCreate  40m (x19 over 95m)  replicaset-controller  Error creating: admission webhook "pod-blocker-service.pod-blocker.com" denied the request: Pod creation is not allowed during the first 40 minutes of the hour.
  Warning  FailedCreate  7m2s (x4 over 84m)  replicaset-controller  Error creating: admission webhook "pod-blocker-service.pod-blocker.com" denied the request: Pod creation is not allowed during the second 20 minutes of the hour.
```
Status:
```
status:
  conditions:
  - lastTransitionTime: "2024-09-03T02:31:25Z"
    message: 'admission webhook "pod-blocker-service.pod-blocker.com" denied the request:
      Pod creation is not allowed during the first 40 minutes of the hour.'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 1
  replicas: 0
```
Apparently, the second error message "Pod creation is not allowed during the second 20 minutes of the hour." is not updated into the status condition.



#### What did you expect to happen?

The status shows the up to date error condition:
```
status:
  conditions:
  - lastTransitionTime: "2024-09-03T02:31:25Z"
    message: 'admission webhook "pod-blocker-service.pod-blocker.com" denied the request:
      Pod creation is not allowed during the second 20 minutes of the hour.'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 1
  replicas: 0
```

#### How can we reproduce it (as minimally and precisely as possible)?

Just create 2 admissions to block POD creation one by one, with different error message.
After a period of time, let the first admission pass, and the second admission continue to block.

Which should show the error message from the second admission in the ReplicaSet status condition.

#### Anything else we need to know?

It appears this block of code introduced the above behavior: https://github.com/kubernetes/kubernetes/blob/e5bafe2bed13fe72e88a3597930bdfbab5267e9d/pkg/controller/replicaset/replica_set_utils.go#L110

It only set condition when failed condition is `nil`; if the failed condition is still there, it just won't set the up to date condition.

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.3", GitCommit:"aef86a93758dc3cb2c658dd9657ab4ad4afc21cb", GitTreeState:"clean", BuildDate:"2022-07-13T14:21:56Z", GoVersion:"go1.18.4", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-22T05:28:27Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"linux/arm64"}
```

But from the code, it seems still in the latest version

</details>


#### Cloud provider

<details>
Reproduced on KinD
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在多个admission检查阻止Pod创建且返回不同错误信息时，ReplicaSet的status condition未能更新最新的错误信息。这是一个功能性问题，导致ReplicaSet的状态信息未能反映最新的错误。

根据风险判断标准，此问题并不涉及安全风险。这一行为不会被攻击者利用，不会导致漏洞，也不涉及敏感信息泄露或高风险的安全问题。故风险评级为不涉及。

---

## Issue #127073 kubectl get nodes with JSONPath doesn't report node.kubernetes.io/unschedulable taint

- Issue 链接：[#127073](https://github.com/kubernetes/kubernetes/issues/127073)

### Issue 内容

#### What happened?

I cordoned a node, then ran the following command to list all nodes that have the `node.kubernetes.io/unschedulable` taint:

```bash
kubectl get nodes -o jsonpath="{.items[?(@.spec.taints[].key=='node.kubernetes.io/unschedulable')].metadata.name}"
```

the cordoned node was not included in the list, even though describing the node I can see that it does have the taint and the field `.spec.unschedulable` is set to `true`.

#### What did you expect to happen?

I expected to get `metadata.name` of the node that has been marked as unschedulable instead.

#### How can we reproduce it (as minimally and precisely as possible)?

#### Common

1. Cordon a node.

#### Test 1

2. Run the command:

```bash
kubectl get nodes -o jsonpath="{.items[?(@.spec.taints[].key=='node.kubernetes.io/unschedulable')].metadata.name}"
```

3. The node is **not** listed.

#### Test 2

4. Run the command:

```bash
kubectl get nodes -o jsonpath="{.items[?(@.spec.unschedulable)].metadata.name}"
```

5. The node's `metadata.name` is printed.

#### Test 3

6. Run the command:

```bash
kubectl get nodes -ojsonpath='{}' |  jq  -r '.items[] | select(try .spec.taints[].key == "node.kubernetes.io/unschedulable") .metadata.name'
```

7. The node's `metadata.name` is printed.

#### Anything else we need to know?

THe following also works:

```bash
kubectl get nodes -ojson | jq -r '.items[] | select(try .spec.taints[].key == ("node.kubernetes.io/unschedulable")) .metadata.name'
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
Kubeadm cluster
</details>


#### OS version

<details>

```console
Client running on macos, Kubernetes API server running on:

$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux master1 6.8.0-41-generic #41-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug  2 23:26:06 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用kubectl命令通过JSONPath查询具有特定taint的节点时，未能得到预期的结果。具体来说，用户尝试查询被cordon的节点（被设置为不可调度），但查询命令未能正确返回该节点的信息。这属于对kubectl命令和JSONPath查询用法的问题，涉及到工具使用和查询语法的理解。

根据风险判断标准：

1. 该Issue没有提及任何可以被攻击者利用的漏洞。
2. 没有潜在的漏洞可被分配CVE编号，亦不存在高风险的CVSS评分。
3. Issue中没有涉及敏感信息泄露或不当配置的问题。
4. 不涉及拒绝服务攻击或权限提升等安全风险。

因此，该Issue不涉及安全风险。

---

