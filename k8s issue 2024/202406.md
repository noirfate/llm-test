# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (7 个)

## Issue #125619  endpoints cannot be changed from notReadyAddresses to addresses

- Issue 链接：[#125619](https://github.com/kubernetes/kubernetes/issues/125619)

### Issue 内容

#### What happened?

I have a k8s clusters that have encountered the following situation: 
1、All master nodes are full of memory burst
2、After the memory of all master nodes is restored, the Ready type status of pod is true,  but some endpoints in the subsets still remain in notReadyAddresses and do not recover. 
service:
```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-05-29T07:42:43Z"
  name: nginx-service
  namespace: default
  resourceVersion: "412201"
  uid: 17f43ff9-16fe-4507-a593-ede1a29a1255
spec:
  clusterIP: 123.123.95.164
  clusterIPs:
  - 123.123.95.164
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
```
enpoints:
```yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: "2024-06-12T02:41:41Z"
 creationTimestamp: "2024-05-29T07:42:43Z"
  name: nginx-service
  namespace: default
  resourceVersion: "468584"
  uid: 85b1134f-7f5d-4087-b76f-e7aa5371c016
subsets:
- notReadyAddresses:
  - ip: 172.22.161.17
    nodeName: 192.168.3.89
    targetRef:
      kind: Pod
      name: nginx
      namespace: default
      uid: 6022f2d6-ead8-4882-9a76-8e51901731ee
  ports:
  - port: 80
    protocol: TCP
```

pod
```yaml
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:43Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:49Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:49Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:43Z"
    status: "True"
    type: PodScheduled
```

3、The issue is resolved only by restarting kube-controller-manager.


/area controller-manager 

#### What did you expect to happen?

When all master node memory is restored, endpoints change from `notReadyAddresses` to `addresses`

#### How can we reproduce it (as minimally and precisely as possible)?

Refer to the description of "What happened?"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.28.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

涉及安全风险。

**分析：**

该Issue描述了一个问题，当所有master节点内存耗尽（“All master nodes are full of memory burst”）并恢复后，虽然Pod的Ready状态为True，但Endpoints中的地址仍然停留在`notReadyAddresses`，无法转换为`addresses`，导致服务不可用。只有在重启`kube-controller-manager`后，问题才得以解决。

**潜在安全风险：**

攻击者可能利用这一行为，诱使`kube-controller-manager`进入异常状态，导致服务持续不可用。这可能通过以下方式实现：

1. **内存耗尽攻击**：攻击者在集群中创建大量消耗内存的资源，诱发master节点内存耗尽。
2. **持久化的服务中断**：即使内存恢复，`kube-controller-manager`仍未正常更新Endpoints，攻击者无需持续耗尽内存，即可保持服务不可用状态。
3. **需要重启服务**：管理员需要手动重启`kube-controller-manager`才能恢复服务，增加了恢复难度和时间。

**满足风险判断标准：**

1. **可被攻击者利用**：攻击者可通过在集群中部署恶意工作负载导致master节点内存耗尽，触发该问题。
2. **可能成为漏洞并被分配CVE编号**：按照CVSS 3.1评分标准，此漏洞的利用无需高权限（假设攻击者有部署权限），影响范围广，攻击复杂度低，可能获得高于High的评分。
3. **不属于Issue提交者的问题**：该问题是`kube-controller-manager`在内存恢复后未能正确更新状态，引起的服务不可用。
4. **无需高权限即可实施攻击**：如果攻击者有权在集群中创建资源，就可能实施此攻击。

**可能的影响：**

- **服务不可用**：集群内的服务因Endpoints未更新，无法被访问。
- **增加运维难度**：需要人工干预重启`kube-controller-manager`，耗费时间和人力。
- **影响范围广泛**：在多租户环境下，可能影响其他租户的服务。

**Proof of Concept：**

1. **步骤一**：攻击者在集群中部署大量高内存消耗的Pod，目标是耗尽master节点的内存。
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: memory-hog
     namespace: default
   spec:
     replicas: 50
     selector:
       matchLabels:
         app: memory-hog
     template:
       metadata:
         labels:
           app: memory-hog
       spec:
         containers:
         - name: memory-hog
           image: alpine
           command: ["dd"]
           args: ["if=/dev/zero", "of=/dev/null"]
           resources:
             requests:
               memory: "1Gi"
             limits:
               memory: "1Gi"
   ```
2. **步骤二**：观察master节点内存被耗尽，`kube-controller-manager`开始异常工作。
3. **步骤三**：停止内存消耗，master节点内存恢复。
4. **步骤四**：检测发现Endpoints的`notReadyAddresses`未转换为`addresses`，服务仍不可用。
5. **步骤五**：重启`kube-controller-manager`，服务恢复正常。

**总结：**

该问题存在被利用的风险，攻击者可以通过耗尽master节点内存并利用`kube-controller-manager`在内存恢复后未能正确更新Endpoints的漏洞，导致服务持续性不可用，造成严重的拒绝服务攻击。

---

## Issue #125580 kube-apiserver oom, list resource consume too much memory cause json decode

- Issue 链接：[#125580](https://github.com/kubernetes/kubernetes/issues/125580)

### Issue 内容

#### What happened?

The APIServer concurrency capability is too weak. In the test, the memory usage of 20 concurrent requests increases to 12 GB. The data size of "kubectl get crd -A -o yaml" is 20 MB.

1. Why does serialization consume so much memory? Is there any optimization mechanism?
2. Another point to note: when I call 100 watches concurrently（and we know that watch will initially treat all items as add events, which are equivalent to lists.）, kube-apiserver only goes up to 3GB, while 10 LIST concurrent operations go up to 8GB. Why is the memory usage difference between watches and lists so huge?

![image](https://github.com/kubernetes/kubernetes/assets/17514799/5c86cad6-0963-4745-bcc2-f969db55ba7f)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/5f8e8143-059a-4a0a-892c-e0407ad2818a)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/4b7858cd-2d47-4876-8ac1-1144f22ed207)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/fc70cd4c-3dec-466a-a04d-2b3c4d52b035)


after i specfic  resourceVersion=0, memory usage does not improve: 


![image](https://github.com/kubernetes/kubernetes/assets/17514799/4b7adf44-9be3-4595-8276-8578d113b429)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/d5ee683d-09b5-4243-9422-179158b0eca9)
#### What did you expect to happen?

Why does serialization consume so much memory? Is there any optimization mechanism?

#### How can we reproduce it (as minimally and precisely as possible)?

just kubectl get crd -A -o yaml & concurrently 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，Issue内容涉及到在kube-apiserver中，通过并发的LIST请求会导致内存消耗过高，甚至导致OOM（内存溢出）。这可能被攻击者利用，构成潜在的安全风险，符合以下风险判断标准：

1. **该风险能被攻击者利用**：攻击者可以通过发送大量并发的LIST请求（如`kubectl get crd -A -o yaml`），导致kube-apiserver消耗大量内存，最终可能导致服务崩溃。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在high以上**：

   - **攻击向量（AV）**：网络（Network，N）——攻击者可以通过网络远程发送请求。
   - **攻击复杂度（AC）**：低（Low，L）——执行攻击不需要复杂的条件。
   - **所需权限（PR）**：低（Low，L）——攻击者需要具备低权限（读取权限）即可。
   - **用户交互（UI）**：无（None，N）——不需要额外的用户交互。
   - **作用范围（S）**：未改变（Unchanged，U）——攻击影响的组件与被攻击的组件相同。
   - **机密性（C）**：无（None，N）
   - **完整性（I）**：无（None，N）
   - **可用性（A）**：高（High，H）——服务被拒绝，严重影响可用性。

   根据以上指标，使用CVSS 3.1计算得分为7.5，评级为**高风险（High）**。

5. **对于日志中泄露凭据的风险……（此项不适用本情况）**

**可能的影响**：

- 攻击者可以利用低权限用户，通过发送大量并发的LIST请求，导致kube-apiserver内存消耗殆尽，最终导致服务拒绝响应（DoS攻击）。
- 这会影响整个Kubernetes集群的稳定性和可用性，阻碍正常的服务运行。

**Proof of Concept（概念验证）**：

攻击者可以执行以下操作来重现问题：

1. **编写并发请求脚本**：

   使用如下脚本，模拟多个并发的LIST请求：

   ```bash
   for i in {1..20}; do
     kubectl get crd -A -o yaml &
   done
   wait
   ```

   或者使用压力测试工具，如`ab`或`siege`，对kube-apiserver的相应API端点发送并发请求。

2. **监控kube-apiserver内存使用情况**：

   在攻击期间，观察kube-apiserver的内存使用情况，可以使用如下命令：

   ```bash
   kubectl top pod kube-apiserver -n kube-system
   ```

   或者使用监控工具，如Prometheus和Grafana，绘制内存使用曲线。

3. **观察服务状态**：

   在内存消耗过高时，kube-apiserver可能会崩溃或重启，导致集群不可用。

**建议**：

- **优化kube-apiserver的内存管理**：检查序列化和反序列化过程，优化内存占用，避免单个请求消耗过多资源。
- **限制并发请求数**：在API网关或Ingress层面，限制单个用户或IP的最大并发请求数，防止恶意滥用。
- **增加认证和授权控制**：确保只有经过授权的用户才能访问高消耗资源的API接口。
- **监控和报警**：设置对kube-apiserver内存使用的监控，一旦超过阈值，立即报警并采取措施。

综上所述，该Issue涉及的内容存在潜在的安全风险，需要引起重视并及时修复。

---

## Issue #125484 Missed k8s.io/kube-openapi/cmd/openapi-gen dependency on code-generator go.mod

- Issue 链接：[#125484](https://github.com/kubernetes/kubernetes/issues/125484)

### Issue 内容

#### What happened?

We're trying to upgrade `k8s.io/code-generator` to `v1.30.1`  (https://github.com/kubernetes-sigs/kueue/pull/2402) and found the issue on executing `kube::codegen::gen_openapi` (https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_kueue/2402/pull-kueue-verify-main/1800788997593632768). 

```
../kube-openapi@v0.0.0-20240228011516-70dd3763d340/pkg/common/common.go:23:2: missing go.sum entry for module providing package github.com/emicklei/go-restful/v3 (imported by k8s.io/kube-openapi/pkg/common); to add:
	go get k8s.io/kube-openapi/pkg/common@v0.0.0-20240228011516-70dd3763d340
```

The problem is that on go.mod (https://github.com/kubernetes/code-generator/blob/master/go.mod) missed `github.com/emicklei/go-restful/v3` dependency that not allow to install `k8s.io/kube-openapi` (https://github.com/kubernetes/code-generator/blob/master/kube_codegen.sh#L330C45-L330C64)

#### What did you expect to happen?

No error

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the same what happening on https://github.com/kubernetes/code-generator/blob/master/kube_codegen.sh#L325-L330, just execute this command on code-generator folder:
```bash
GO111MODULE=on go install k8s.io/kube-openapi/cmd/openapi-gen
```

#### Anything else we need to know?

I think it's happens after this commit https://github.com/kubernetes/kubernetes/commit/1a4450f6ffdad44bfc12fae5911c93e3bd238f0f.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该 Issue 涉及潜在的安全风险。

**原因分析：**

在升级 `k8s.io/code-generator` 至 `v1.30.1` 时，执行 `kube::codegen::gen_openapi` 遇到错误，提示缺少 `go.sum` 条目，需要手动运行 `go get` 命令来获取缺失的依赖包 `github.com/emicklei/go-restful/v3`。

缺少 `go.sum` 中的校验和条目，意味着 Go 在拉取依赖时无法验证模块的完整性和真实性。攻击者可能利用这一点，通过中间人攻击（MITM）、DNS 劫持或供应链攻击，向受害者提供一个伪造的、包含恶意代码的模块。

由于受害者在构建过程中需要拉取该依赖，如果未开启 Go 的校验和验证（例如设置了环境变量 `GOSUMDB=off`），或者使用了不安全的代理，攻击者的恶意代码可能被编译进最终的二进制文件中。

**可能的影响：**

- **远程代码执行（RCE）**：恶意代码被编译并执行，攻击者能够在受害者的系统或最终用户的环境中执行任意代码。
- **供应链攻击**：影响使用该代码生成器的下游项目，导致大范围的安全事件。
- **数据泄露或破坏**：攻击者可能访问、篡改或破坏敏感数据。

根据 CVSS 3.1 评分标准，此漏洞的严重性可能为高危（High）或严重（Critical），因为攻击者可以未经授权执行任意代码，全面危害系统的机密性、完整性和可用性。

**Proof of Concept（概念验证）：**

1. **攻击者准备恶意模块：**

   攻击者在自己的 GitHub 账户下创建一个名为 `github.com/emicklei/go-restful/v3` 的仓库，加入恶意代码。

2. **诱使受害者拉取恶意依赖：**

   - 如果受害者在执行 `go get` 时，未使用官方的 Go 模块代理和校验和数据库（例如设置了 `GOPROXY=direct` 和 `GOSUMDB=off`），Go 将直接从攻击者的仓库拉取模块。
   - 或者，攻击者通过劫持 DNS，将 `github.com` 指向恶意服务器。

3. **受害者构建并运行含有恶意代码的二进制文件：**

   受害者按照提示执行 `go get`，拉取并编译了包含恶意代码的模块，最终生成的工具或应用程序被植入了后门。

**防范措施：**

- **完善依赖管理：** 确保在 `go.mod` 和 `go.sum` 中明确声明所有依赖，并包含正确的校验和。
- **使用可信的模块代理和校验和数据库：** 不要禁用 Go 的模块校验和验证机制，避免使用不安全的代理。
- **定期审核依赖：** 定期检查和更新依赖项，关注官方发布的安全公告。

**结论：**

该 Issue 暴露了一个可能被攻击者利用的安全风险，符合风险判断标准 1 和 2。建议尽快修复缺失的依赖声明，确保构建过程的安全性。

---

## Issue #125426 ExtendedResourceToleration adds tolerations even when the quantity of requested resources is "0"

- Issue 链接：[#125426](https://github.com/kubernetes/kubernetes/issues/125426)

### Issue 内容

#### What happened?

ExtendedResourceToleration is useful for setting tolerations automatically to pods requesting extended resources like GPUs, but the tolerations are still given even if the quantity of extended resources is set to “0”.

#### What did you expect to happen?

ExtendedResourceToleration admission should skip adding toleration if the quantity of extended resources is set to “0”.

#### How can we reproduce it (as minimally and precisely as possible)?

You can reproduce it by creating the following pod:

```
❯ cat pod-with-extended-resource.yaml    
apiVersion: v1
kind: Pod
metadata:
  name: debug
spec:
  containers:
  - command:
    - /bin/sh
    - -c
    - sleep infinity
    image: busybox
    name: debug
    resources:
      limits:
        nvidia.com/gpu: "0"

❯ kubectl apply -f pod-with-extended-resource.yaml 
pod/debug created

❯ kubectl get po debug -o jsonpath='{.spec.tolerations[?(@.key == "nvidia.com/gpu")]}' | jq
{
  "effect": "NoSchedule",
  "key": "nvidia.com/gpu",
  "operator": "Exists"
}
```


#### Anything else we need to know?

It seems that this issue can be resolved by adding an additional conditional check whether the quantity of the extended resources is over 0 here: 
https://github.com/kubernetes/kubernetes/blob/a7ca13ea29ba5b3c91fd293cdbaec8fb5b30cee2/plugin/pkg/admission/extendedresourcetoleration/admission.go#L72-L85

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5
```

</details>


#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

该问题涉及潜在的安全风险，原因如下：

**问题描述：**

在使用 `ExtendedResourceToleration` 准入插件时，即使 Pod 请求的扩展资源数量为 "0"，插件仍会为该 Pod 添加相应的容忍（toleration）。这意味着 Pod 会被自动添加对特定扩展资源（如 GPU）的容忍，即使实际上并不需要该资源。

**潜在风险分析：**

1. **绕过节点隔离机制：** Kubernetes 中，节点可以通过污点（taints）来阻止非预期的 Pods 被调度到特定节点上，而 Pod 需要相应的容忍（toleration）才能被调度到这些节点。攻击者可以利用这一问题，通过创建请求 0 个扩展资源的 Pod，获得对应的容忍，从而将 Pod 调度到受污点保护的节点上。

2. **未授权访问：** 如果节点上存在敏感资源、数据或服务，攻击者的 Pod 被调度到这些节点上可能会导致未授权的访问，造成机密性和完整性的威胁。

3. **资源滥用和拒绝服务：** 攻击者可以在受保护的节点上运行高负载的 Pod，占用节点资源，影响其他关键工作负载的正常运行，造成可用性影响。

**符合风险判断标准：**

1. **该风险能被攻击者利用：** 是的，攻击者可以通过请求 0 个扩展资源，获取相应的容忍，进而将 Pod 调度到特定节点。

2. **可能成为漏洞并被分配 CVE 编号，CVSS 评分在 High 以上：**

   - **攻击向量（AV）：** 网络（N）— 攻击者通过 Kubernetes API 提交恶意 Pod。
   - **攻击复杂度（AC）：** 低（L）— 攻击不需要特殊条件。
   - **所需权限（PR）：** 低（L）— 需要具备创建 Pod 的权限，这是许多用户正常具有的权限。
   - **用户交互（UI）：** 无（N）— 攻击不需要额外的用户交互。
   - **作用范围（S）：** 未改变（U）— 攻击影响在 Kubernetes 集群范围内。
   - **机密性影响（C）：** 高（H）— 可能访问到敏感数据。
   - **完整性影响（I）：** 高（H）— 可能篡改数据或配置。
   - **可用性影响（A）：** 高（H）— 可能导致服务中断或拒绝服务。

   根据 CVSS 3.1 计算，总分为 8.8，高危（High）。

3. **Issue 提交者的行为不影响风险评估：** 不适用。

4. **攻击者利用需要的权限：** 虽然需要创建 Pod 的权限，但在许多环境中这是常见权限，不应降低风险评级。

5. **日志泄露凭据的风险：** 不适用。

**可能的影响：**

- **安全隔离被破坏：** 攻击者可以访问到原本通过污点保护的节点，绕过了集群的安全策略。
- **数据泄露：** 如果受影响节点上有敏感数据，可能被攻击者的 Pod 读取。
- **服务中断：** 攻击者的 Pod 可能影响节点上其他关键服务的稳定性。

**Proof of Concept：**

1. **创建请求 0 个扩展资源的 Pod：**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: malicious-pod
   spec:
     containers:
     - name: malicious-container
       image: busybox
       command: ["sleep", "infinity"]
       resources:
         limits:
           nvidia.com/gpu: "0"
   ```

2. **应用 Pod：**

   ```bash
   kubectl apply -f malicious-pod.yaml
   ```

3. **查看 Pod 的容忍：**

   ```bash
   kubectl get pod malicious-pod -o jsonpath='{.spec.tolerations}'
   ```

   输出显示 Pod 具有对 `nvidia.com/gpu` 的容忍：

   ```json
   [
     {
       "key": "nvidia.com/gpu",
       "operator": "Exists",
       "effect": "NoSchedule"
     }
   ]
   ```

4. **验证 Pod 被调度到受保护的节点：**

   如果集群中存在带有 `nvidia.com/gpu` 污点的节点，Pod 会被调度到这些节点上。

**总结：**

该问题允许攻击者通过请求数量为 0 的扩展资源，获取对应的容忍，进而将 Pod 调度到受污点保护的节点上，可能导致未授权的访问和安全策略的绕过，符合高风险漏洞的评估标准。

---

## Issue #125403 kubectl --server-side apply replaces the live manifest instead of merging when migrating from clinet side apply to server side apply 

- Issue 链接：[#125403](https://github.com/kubernetes/kubernetes/issues/125403)

### Issue 内容

#### What happened?

If a resource is created initially with kubectl client-side apply and tries to update it with server-side apply, the live manifest got replaced by the applied manifest and the missing fields got removed,  instead of merging with kubectl version  v1.29.2. This was working with a lower version(v1.22.0) of kubectl 

#### What did you expect to happen?

I expected the live manifest and the applied manifest to be merged instead of replacing.

#### How can we reproduce it (as minimally and precisely as possible)?

1. ```kubectl create namespace kiali-test``` to create a testing namespace.
2. ```kubectl apply -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/1-manual/1-kiali.io_kialis.yaml``` to install the CRD.
3. ```kubectl apply -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/1-manual/2-kiali.io_v1alpha1_kiali.yaml``` to install a specific Kiali instance manifest.
4. ```kubectl apply --server-side=true -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/2-argocd-application/kiali.yaml```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
kind cluster on local machines
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue描述了在使用`kubectl`从客户端应用（client-side apply）迁移到服务端应用（server-side apply）时，资源的现场清单（live manifest）被应用的清单所替换，缺失的字段被移除，而不是进行合并。

**潜在的安全风险分析：**

1. **风险描述：**
   - 在从客户端应用迁移到服务端应用的过程中，如果资源的某些关键配置（例如安全策略、访问控制、网络配置等）被意外删除，可能导致资源的安全配置被破坏。
   - 攻击者可能利用这一行为，诱导管理员或自动化流程进行服务端应用，从而移除关键的安全配置，导致权限提升或未授权的访问。

2. **风险利用可能性：**
   - **可被攻击者利用**：如果攻击者能够控制或影响应用的清单，使其缺少关键的安全配置，而由于`kubectl`的行为会移除现场资源中的这些配置，那么攻击者可以利用这一点破坏资源的安全性。
   - **可能成为漏洞并被分配CVE编号**：由于这一行为可能导致安全配置被意外删除，符合漏洞的定义，可以被分配CVE编号。

3. **影响分析：**
   - **安全配置被移除**：关键的安全设置（如RBAC规则、网络策略、Pod安全策略等）被意外删除，可能导致未授权的访问、数据泄露或权限提升。
   - **服务中断**：重要的配置被删除，可能导致服务不可用，影响业务连续性。

4. **CVSS评分（示例）：**
   - **攻击向量（AV）**：网络（N）
   - **攻击复杂度（AC）**：低（L）
   - **特权要求（PR）**：低（L）
   - **用户交互（UI）**：需要（R）
   - **影响范围（S）**：改变（C）
   - **保密性（C）**：高（H）
   - **完整性（I）**：高（H）
   - **可用性（A）**：高（H）
   - **综合评分**：9.0（Critical）

**Proof of Concept（概念验证）：**

1. **创建具备安全配置的资源：**

   ```bash
   kubectl apply -f secure-resource.yaml
   ```

   `secure-resource.yaml`包含了关键的安全配置，例如RBAC规则或网络策略。

2. **使用客户端应用进行更新：**

   ```bash
   kubectl apply -f updated-resource.yaml
   ```

   `updated-resource.yaml`更新了一些非安全相关的字段。

3. **迁移到服务端应用：**

   ```bash
   kubectl apply --server-side=true -f updated-resource.yaml
   ```

   由于迁移到服务端应用，且`updated-resource.yaml`缺少之前的安全配置，`kubectl`会将现场资源的缺失字段移除。

4. **结果验证：**

   ```bash
   kubectl get resource -o yaml
   ```

   发现关键的安全配置已被删除，资源处于不安全的状态，可能被攻击者利用。

**总结：**

该Issue揭示了从客户端应用迁移到服务端应用时，`kubectl`的行为可能导致资源的关键配置被意外删除，存在潜在的安全风险。建议在迁移过程中，仔细检查资源清单，确保所有关键配置均被包含，或者等待官方修复该行为。

---

## Issue #125357 kubeadm leaves backup files after a successful upgrade

- Issue 链接：[#125357](https://github.com/kubernetes/kubernetes/issues/125357)

### Issue 内容

#### What happened?

When I successfully upgraded the cluster using kubeadm, I found the /etc/kubernetes/tmp, which contains the backup files, was left.
```
~]# du -h /etc/kubernetes/tmp/
20K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-02-44
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44
16K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-13-10
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10
4.0K	/etc/kubernetes/tmp/kubeadm-kubelet-config3316271885
16K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-22-34
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34
4.0K	/etc/kubernetes/tmp/kubeadm-kubelet-config3278283034
463M	/etc/kubernetes/tmp/
```

#### What did you expect to happen?

I would like to know if this is intended or if it's a bug.
Maybe the document https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ should make it clear that the user needs to remove the backup files manually.


#### How can we reproduce it (as minimally and precisely as possible)?

Upgrade the cluster with kubeadm.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8
```
upgraded from v1.25.16 to v1.26.15 to 1.27.12 to 1.27.8

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因及可能的影响：**

在使用kubeadm成功升级集群后，/etc/kubernetes/tmp/目录中遗留了备份文件，这些备份文件包含了etcd的数据备份。etcd是Kubernetes集群的关键组件，负责存储集群的状态和配置信息，其中可能包含敏感信息，例如：

- 集群的配置信息
- 认证凭据
- 密钥和证书
- ServiceAccount令牌
- 其他敏感数据

如果这些备份文件的权限设置不当，非授权用户（如具有低权限的本地用户）可能访问到这些备份文件，从而提取其中的敏感信息。一旦攻击者获取了这些信息，可能导致以下严重后果：

- **未授权访问：** 攻击者可以使用获取的凭据访问集群资源。
- **权限提升：** 利用敏感信息，攻击者可能提升自身权限，控制整个集群。
- **数据篡改：** 攻击者可能修改集群的配置或数据，导致服务中断或数据泄露。
- **横向移动：** 利用集群中的信息，攻击者可能进一步攻击其他系统或服务。

根据**CVSS 3.1评分标准**，由于该漏洞可能导致高机密性影响（Confidentiality Impact），且攻击复杂度低（低权限用户即可访问），综合评分可能达到**High**或更高。

**概念验证（Proof of Concept）：**

1. **查看备份文件的权限设置：**

   ```bash
   ls -l /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   如果输出显示文件对其他用户可读，例如：

   ```
   -rw-r--r-- 1 root root 123456 Jun 6 18:02 /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   表明其他用户具有读取权限。

2. **低权限用户读取敏感文件：**

   切换到非特权用户：

   ```bash
   sudo -u someuser bash
   ```

   尝试读取etcd备份文件：

   ```bash
   cat /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   如果成功读取，则证明存在权限问题。

3. **提取敏感信息：**

   攻击者可以解析etcd的WAL（Write-Ahead Log）文件，使用工具或脚本提取其中的键值数据，获取敏感信息。

**建议：**

- **权限控制：** 确保备份文件的权限设置正确，仅限root用户访问。
- **清理备份：** 在升级完成后，自动删除临时备份文件，防止长期存在导致风险。
- **文档说明：** 在官方文档中明确指出备份文件的安全注意事项，提醒用户手动清理或检查权限。
- **改进工具：** 修改kubeadm的行为，在升级成功后自动清理临时备份，或提示用户进行处理。

---

## Issue #125343 ephermal containers stdout written to container logs

- Issue 链接：[#125343](https://github.com/kubernetes/kubernetes/issues/125343)

### Issue 内容

#### What happened?

The stdout of commands running in an ephemeral container started with `kubectl debug --interactive --tty`, including the echo of commands entered by users, is written to the container's log file, where it can be displayed with `kubectl logs` and picked up by log file ingestion.

#### What did you expect to happen?

It is expected that by passing `--tty` the standard file descriptors would be attached to kubectl, and not the log files. This difference in behavior to `kubectl exec --interactive --tty` surprises users who are debugging containers and may inspect the values of files or the environment.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy a standard pod. I used `mccutchen/go-httpbin` as my container image.
2. Create an ephemeral container for the deployed pod. `kubectl debug httpbin --interactive --tty --image=busybox`
3. Inside the tty the terminal is attached to, do something to mimic exposing a password. I chose `echo "topsecretpassword2"`
4. Observe that the shell echo and output of the command is visible in `kubectl logs httpbin -c debugger` and in the kubelet pod logs at `/var/logs/containers`.

#### Anything else we need to know?

<details>

From **/var/log/containers/httpbin-56d7db8d6d-55ql7_default_debugger-f5lwr-4ad6566be3590520b0135c3e621ebd3cd8a2088149174fc5c72553d9861f6234.log**:
```log
2024-06-05T11:23:20.051079932+00:00 stdout F M/ # [[JM
2024-06-05T11:23:20.051079932+00:00 stdout P / #
2024-06-05T11:23:39.975294284+00:00 stdout P e
2024-06-05T11:23:40.057105746+00:00 stdout P c
2024-06-05T11:23:40.139436734+00:00 stdout P h
2024-06-05T11:23:40.319770664+00:00 stdout P o
2024-06-05T11:23:40.523725228+00:00 stdout P
2024-06-05T11:23:40.973457013+00:00 stdout P "
2024-06-05T11:23:41.257984775+00:00 stdout P t
2024-06-05T11:23:41.349361250+00:00 stdout P o
2024-06-05T11:23:41.411602281+00:00 stdout P p
2024-06-05T11:23:41.623549256+00:00 stdout P s
2024-06-05T11:23:41.752892792+00:00 stdout P e
2024-06-05T11:23:41.867923955+00:00 stdout P c
2024-06-05T11:23:42.098814184+00:00 stdout P r
2024-06-05T11:23:42.204348673+00:00 stdout P e
2024-06-05T11:23:42.345322167+00:00 stdout P t
2024-06-05T11:23:42.770278261+00:00 stdout P p
2024-06-05T11:23:42.871145319+00:00 stdout P a
2024-06-05T11:23:43.016143517+00:00 stdout P s
2024-06-05T11:23:43.172425724+00:00 stdout P s
2024-06-05T11:23:43.336211796+00:00 stdout P w
2024-06-05T11:23:43.467535508+00:00 stdout P o
2024-06-05T11:23:43.602068604+00:00 stdout P r
2024-06-05T11:23:43.771406396+00:00 stdout P d
2024-06-05T11:23:43.953834957+00:00 stdout P 2
2024-06-05T11:23:44.269025862+00:00 stdout P "
2024-06-05T11:23:44.470971214+00:00 stdout F M
2024-06-05T11:23:44.471392405+00:00 stdout P topsecretpassword2
2024-06-05T11:23:44.471506289+00:00 stdout F M
2024-06-05T11:23:44.471766497+00:00 stdout P / #
```

</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>

- minikube with cri-o and containerd for versions 1.28-1.30
- GKE autocluster 1.28
- EKS 1.29 with Amazon Linux 2 AMI

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

该问题涉及潜在的安全风险。

**原因：**

当使用 `kubectl debug --interactive --tty` 启动临时容器（ephemeral container）进行调试时，用户在终端中输入的命令和命令的输出都会被写入容器的日志文件。这些日志可以通过 `kubectl logs` 命令查看，或者被日志收集系统收集到集中式日志平台。由于这些日志中可能包含敏感信息（例如密码、秘钥、机密配置等），如果攻击者能够访问这些日志，就可能获取到这些敏感信息。

**可能的影响：**

1. **敏感信息泄露：** 攻击者可以从日志中获取用户在调试过程中输入的命令和查看的敏感数据，例如数据库密码、API密钥等。

2. **扩大攻击面：** 如果日志被集中收集到日志管理系统，而该系统的访问权限控制不严格，更多的人员可能有机会接触到这些敏感信息。

3. **权限提升：** 攻击者利用获取的敏感信息，可能对系统进行进一步的攻击，获取更高的权限或访问更多的资源。

**符合风险判断标准：**

1. **可被攻击者利用：** 攻击者只需要具备访问容器日志的权限，即可获取敏感信息，而访问日志的权限通常比直接访问容器的权限要求更低。

2. **可能成为漏洞并被分配 CVE 编号：** 根据 CVSS 3.1 评级，这个漏洞的得分可能在高危（High）范围内。

   **CVSS 3.1 评分计算：**

   - **攻击向量 (AV)：** 网络 (N)
   - **攻击复杂度 (AC)：** 低 (L)
   - **权限要求 (PR)：** 低 (L)
   - **用户交互 (UI)：** 无 (N)
   - **机密性影响 (C)：** 高 (H)
   - **完整性影响 (I)：** 低 (L) / 无 (N) （视具体情况）
   - **可用性影响 (A)：** 无 (N)

   **综合得分：** 7.5（高危）

3. **日志中泄露凭据的风险：** 攻击者可能以比泄露凭据更低的权限访问日志，从而获取更高级别的凭据，符合提高风险评级的条件。

**PoC（概念验证）示例：**

1. **部署一个标准的 Pod：**

   ```bash
   kubectl run sensitive-app --image=yourappimage
   ```

2. **使用 kubectl debug 启动一个临时容器进行调试：**

   ```bash
   kubectl debug sensitive-app --interactive --tty --image=busybox
   ```

3. **在调试容器中，执行查看敏感信息的命令：**

   ```bash
   # 假设存在包含敏感信息的文件 /etc/secret-config
   cat /etc/secret-config
   ```

   或者输入敏感信息：

   ```bash
   echo "MySecretPassword123"
   ```

4. **退出调试会话。**

5. **查看调试容器的日志：**

   ```bash
   kubectl logs sensitive-app -c debugger
   ```

6. **在日志中找到刚才输入的命令和输出，包括敏感信息：**

   ```
   / # cat /etc/secret-config
   sensitive_key=ABC123XYZ
   / # echo "MySecretPassword123"
   MySecretPassword123
   / #
   ```

**总结：**

由于调试过程中用户输入的命令和输出被记录在日志中，且这些日志可能被具有较低权限的用户或系统访问，导致敏感信息泄露，存在高风险的安全漏洞，建议尽快修复。

---

# 📌 不涉及安全风险的 Issues (66 个)

## Issue #125810 NodePort service with endpoints has "has no local endpoints" in iptables

- Issue 链接：[#125810](https://github.com/kubernetes/kubernetes/issues/125810)

### Issue 内容

#### What happened?

I set up a NodePort service pointing to an envoy gateway. Initially this works, and the port is connected to the envoy gateway correctly. However when something changes about the gateway (does not matter what), the service does not connect anymore. In iptables-save, I found the following rule which blocks traffic:
```
-A KUBE-EXTERNAL-SERVICES -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 has no local endpoints" -m addrtype --dst-type LOCAL -m tcp --dport 30100 -j DROP
```

Even though this rule also still exists:
```
-A KUBE-NODEPORTS -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80" -m tcp --dport 30100 -j KUBE-EXT-GGWL76EB3A5WOB4Q
-A KUBE-EXT-GGWL76EB3A5WOB4Q -s 10.1.0.0/16 -m comment --comment "pod traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -j KUBE-SVC-GGWL76EB3A5WOB4Q
-A KUBE-EXT-GGWL76EB3A5WOB4Q -m comment --comment "masquerade LOCAL traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -m addrtype --src-type LOCAL -j KUBE-MARK-MASQ
-A KUBE-EXT-GGWL76EB3A5WOB4Q -m comment --comment "route LOCAL traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -m addrtype --src-type LOCAL -j KUBE-SVC-GGWL76EB3A5WOB4Q
-A KUBE-SVC-GGWL76EB3A5WOB4Q ! -s 10.1.0.0/16 -d 10.0.0.251/32 -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 cluster IP" -m tcp --dport 80 -j KUBE-MARK-MASQ
-A KUBE-SVC-GGWL76EB3A5WOB4Q -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 -> 10.1.1.19:10080" -j KUBE-SEP-TW3CXZOYNBZZ6B2U
```

#### What did you expect to happen?

When the envoy gateway changes, the nodeport should still work.

#### How can we reproduce it (as minimally and precisely as possible)?

The nodeport service:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: envoy-default-eg-e41e7b31
  namespace: envoy-gateway-system
spec:
  ports:
    - name: http-80
      protocol: TCP
      port: 80
      targetPort: 10080
      nodePort: 30100
    - name: https-443
      protocol: TCP
      port: 443
      targetPort: 10443
      nodePort: 30101
  selector:
    app.kubernetes.io/component: proxy
    app.kubernetes.io/managed-by: envoy-gateway
    app.kubernetes.io/name: envoy
    gateway.envoyproxy.io/owning-gateway-name: eg
    gateway.envoyproxy.io/owning-gateway-namespace: default
  type: NodePort
```

The gateway:

```yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: GatewayClass
metadata:
  name: eg
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: eg
spec:
  gatewayClassName: eg
  listeners:
  - name: http
    protocol: HTTP
    port: 80
  - name: https
    protocol: HTTPS
    hostname: "bot.yele.dev"
    port: 443
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        name: eg-https
```

#### Anything else we need to know?

```
; kubectl -n envoy-gateway-system get endpointslices
NAME                              ADDRESSTYPE   PORTS                           ENDPOINTS   AGE
envoy-default-eg-e41e7b31-cx6mm   IPv4          10443,10080                     10.1.1.19   66m
envoy-gateway-mxd5w               IPv4          19001,18000,18002 + 1 more...   10.1.2.80   4h4m
; kubectl -n envoy-gateway-system get endpoints envoy-default-eg-e41e7b31
NAME                        ENDPOINTS                         AGE
envoy-default-eg-e41e7b31   10.1.1.19:10443,10.1.1.19:10080   57m
```

When I replace the service, it starts working again.

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-21T14:33:49Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.4", GitCommit:"bae2c62678db2b5053817bc97181fcc2e8388103", GitTreeState:"archive", BuildDate:"1980-01-01T00:00:00Z", GoVersion:"go1.21.5", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.25) and server (1.28) exceeds the supported minor version skew of +/-1```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
BUG_REPORT_URL="https://github.com/NixOS/nixpkgs/issues"
BUILD_ID="23.11.20240213.01885a0"
DOCUMENTATION_URL="https://nixos.org/learn.html"
HOME_URL="https://nixos.org/"
ID=nixos
LOGO="nix-snowflake"
NAME=NixOS
PRETTY_NAME="NixOS 23.11 (Tapir)"
SUPPORT_END="2024-06-30"
SUPPORT_URL="https://nixos.org/community.html"
VERSION="23.11 (Tapir)"
VERSION_CODENAME=tapir
VERSION_ID="23.11"
$ uname -a
Linux vmbox-1 6.1.77 #1-NixOS SMP PREEMPT_DYNAMIC Mon Feb  5 20:13:03 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125807 pull-kubernetes-typecheck doesn't notice compile errors in staging test files

- Issue 链接：[#125807](https://github.com/kubernetes/kubernetes/issues/125807)

### Issue 内容

_Originally posted by @liggitt in https://github.com/kubernetes/kubernetes/issues/125571#issuecomment-2198419708_

/sig testing
/area test
/kind bug

`pull-kubernetes-typecheck — Job succeeded.`
yet:
```
ERROR: staging/src/k8s.io/dynamic-resource-allocation/structured/namedresources/cel/compile.go:1: : # k8s.io/dynamic-resource-allocation/structured/namedresources/cel [k8s.io/dynamic-resource-allocation/structured/namedresources/cel.test]
ERROR: staging/src/k8s.io/dynamic-resource-allocation/structured/namedresources/cel/compile_test.go:127:14: undefined: Compiler (typecheck)
```

(I fixed this in that PR, but it should be reproducible by reintroducing a compile error in a test file like that)

### 分析结果

不涉及

---

## Issue #125792 Invalid kube-reserved configuration in kubelet causes frequent node status patch updates ignoring node-status-report-frequency

- Issue 链接：[#125792](https://github.com/kubernetes/kubernetes/issues/125792)

### Issue 内容

#### What happened?

#### Summary

If the CPU values set in kube-reserved/system-reserved configurations are in terms of decimal values of millicores, Kubelet should throw an error and not start, Else, it should throw a warning to aide the triaging. 

Withiout this check, the issue manifests in unexpected places, like frequent status updates, ignoring the node-status-report-frequency value.

#### Detailed description

1. We have set the Kubelet's node-status-report-frequency property to 5 minutes to reduce the frequency of the status updates to kube-apiserver
2. But we noticed that this is not honoured and the updates were sent every few seconds
3. On further triaging, we found that, since we  set the kube-reserved values as a function of number of cores, for some worker node compute shapes, we are setting the kube-reserved cpu values as decimal values (in millicores).

E.g. 
"kubeReserved": {
    "cpu": "237.5m",
    "memory": "10Gi"
  }

Kubelet accepted the values provided and kubelet was running without any errors or warnings.

The kubelet module that posts the statuses to kube-apiserver  calculates the current status (to compare  with the status posted already to API server) so as to post the status even if the node-status-report-frequency value is not exhausted.

This module calculates the current capacity in terms of millicores and this results in mismatch with the status stored in kube-apiserver.  

#### What did you expect to happen?

Kubelet should fail to start or throw warnings when the kube-reserved/system-reserved values are incorrect

#### How can we reproduce it (as minimally and precisely as possible)?

Set the kubeReserved value to a decimal value as below and increase the node-status-report-frequency to 5 minutes

` "kubeReserved": {
    "cpu": "237.5m",
    "memory": "10Gi"
  },``

You would notice that the status is posted every few seconds 


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Server Version: v1.27.2
```

</details>


#### Cloud provider

<details>
Oracle Container Engine for Kubernetes
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125775 CEL: CallCost the function args mix receiver with argument in way that is prone to misuse

- Issue 链接：[#125775](https://github.com/kubernetes/kubernetes/issues/125775)

### Issue 内容

#### What happened?

Reported here: https://github.com/kubernetes/kubernetes/pull/125571/files/b0bcc0b20d5d97efdd30215ea410c3bc56d8916b..e2cee4d48f596d6ac4032e22bb08cc98252cb3f5#r1657864062

#### What did you expect to happen?

CallCost is improved to provide receiver argument separate from positional arguments (nil if absent).

#### How can we reproduce it (as minimally and precisely as possible)?

See report

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125770 unexpected grpc error (use of closed network connection) during apiserver lifecycle

- Issue 链接：[#125770](https://github.com/kubernetes/kubernetes/issues/125770)

### Issue 内容

#### What happened?

We have recently noticed an increase in gRPC errors (specifically, `use of closed network connection`) in apiserver logs.

log sample
`
I0619 00:22:[31.927056 11](tel:3192705611) http2_client.go:959] "[transport] [client-transport 0xc004144900] Closing: connection error: desc = \"error reading from server: read tcp 10.0.33.210:43716→10.0.32.16:2379: use of closed network connection\"\n"
`

These errors occurs intermittently every few hours during the lifecycle of apiserver.

Although there have been no reported performance regression yet, the recurring error message is concerning.



#### What did you expect to happen?

Assume the error may occur when the apiserver is restarted, rather than during its lifecycle.

#### How can we reproduce it (as minimally and precisely as possible)?

launch apiserver against k8s version `v1.28.9`







#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Kubernetes v1.28.9


</details>


#### Cloud provider

<details>

AWS EKS
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

N/A

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125763 Unable to set enforceNodeAllocatable for system-reserved on cgroup v1 systems

- Issue 链接：[#125763](https://github.com/kubernetes/kubernetes/issues/125763)

### Issue 内容

#### What happened?

While trying to set enforceNodeAllocatable (kubelet setting) for system-reserved on cgroup v1 systems using, 

```yaml
    enforceNodeAllocatable:
      - "pods"
      - "system-reserved"
```
ends up kubelet not being able to start with following error, 

```
Jun 27 14:03:55 ip-10-0-11-74 kubenswrapper[2669]: I0627 14:03:55.845147    2669 node_container_manager_linux.go:116] "Enforcing system reserved on cgroup" cgroupName="/system.slice" limits={"cpu":"1","ephemeral-storage":"1Gi","memory":"1Gi"}
Jun 27 14:03:55 ip-10-0-11-74 kubenswrapper[2669]: E0627 14:03:55.845233    2669 kubelet.go:1559] "Failed to start ContainerManager" err="Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": cgroup [\"system\"] has some missing paths: /sys/fs/cgroup/hugetlb/system.slice, /sys/fs/cgroup/cpuset/system.slice"
Jun 27 14:03:55 ip-10-0-11-74 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jun 27 14:03:55 ip-10-0-11-74 systemd[1]: kubelet.service: Failed with result 'exit-code'.
```

It works fine on cgroup v2 systems though. 

#### What did you expect to happen?

Users should be able to set enforceNodeAllocatable for system-reserved on cgroup v1 systems

#### How can we reproduce it (as minimally and precisely as possible)?

Make sure the node is using cgroup v1 and apply the kubelet's enforceNodeAllocatable for system-reserved. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125762 pods with PVs stuck Pending even though PVCs bound to PVs correctly

- Issue 链接：[#125762](https://github.com/kubernetes/kubernetes/issues/125762)

### Issue 内容

#### What happened?

I'm running a kubeadm cluster on RHEL9 VMs. I was encountering the panic "integer divide by zero" [issue ](https://github.com/kubernetes/kubernetes/issues/124930) so I just upgraded all of my VMs to the latest kubernetes 1.30.2 when it released.
I used helm to install my application. Most of the pods are running fine, but all pods that have a Persistent Volume (PV) are stuck in Pending. This usually means that the PV is not binding to the PVC correctly, but in this case, all are bound as they should be.

I described the nodes, and checked for resource shortage issues, but there are none. When I describe the Pending pods, the only event is
 
```  
Warning  FailedScheduling  4m26s (x663 over 114m)  default-scheduler  0/4 nodes are available: preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.
```

I checked the kube-scheduler-mgr pod logs and it was full of these errors:

```
W0626 11:26:14.020041       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
E0626 11:26:14.020095       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
W0626 11:26:14.020195       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0626 11:26:14.020232       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0626 11:26:14.020282       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0626 11:26:14.020351       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0626 11:26:14.020448       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0626 11:26:14.020514       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0626 11:26:14.020604       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0626 11:26:14.020667       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0626 11:26:14.020753       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0626 11:26:14.020825       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0626 11:26:14.020915       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0626 11:26:14.020984       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0626 11:26:14.021065       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0626 11:26:14.021157       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0626 11:26:14.030767       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
E0626 11:26:14.030890       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
I0626 11:26:15.109531       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0626 11:26:15.609850       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I0626 11:26:33.327740       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
```

I checked and all of the clusterroles that it said are missing were indeed present, so I just updated them as follows:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:basic-user
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:discovery
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-scheduler
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:volume-scheduler
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
```

After applying these, the kube-scheduler-mgr pod no longer generates any new logs, even when I delete the pending pods, or even when i delete the kube-scheduler-mgr pod, then follow it's logs while I delete a Pending pod. The kube-scheduler-mgr pod is not crashing or reporting any errors.

I am at a loss to figure out why all of the pods that have PVs are stuck pending, while all of their PVs are properly bound to the PVCs. It is also weird that the kube-scheduler-mgr is reporting the clusterroles as missing, although when I list clusterroles, I see them there. If the kube-scheduler-mgr pod were still generating logs, that would be helpful, but ever since I applied the file to grant the full permissions to the clusterroles it said were "missing" it seems that no action prompts it to generate logs. I'm wondering if this is a bug that was introduced in this release. I've often had pods stuck Pending when the PVCs are not binding correctly to the PVs, but I've never seen them stuck Pending when the binds are successful and there is no resource contention like this.

Oh, and all nodes are in a Ready state, of course.
```

#### What did you expect to happen?

pods to run

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a kubeadm cluster on kubernetes v1.30.2 and try to run a pod that has a PV.

#### Anything else we need to know?

@mikkeloscar  @AxeZhan tagging you here in case you can think of anything in the latest fix that may be causing this.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2

```

</details>


#### Cloud provider

AWS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="9.0 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.0"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.0 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/red_hat_enterprise_linux/9/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.0
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.0"
Red Hat Enterprise Linux release 9.0 (Plow)
Red Hat Enterprise Linux release 9.0 (Plow)

$ uname -a
Linux mgr 5.14.0-70.75.1.el9_0.x86_64 #1 SMP PREEMPT Thu Sep 14 13:42:21 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
helm version
version.BuildInfo{Version:"v3.14.0", GitCommit:"3fc9f4b2638e76f26739cd77c7017139be81d0ea", GitTreeState:"clean", GoVersion:"go1.21.5"}

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
ctr -v
ctr containerd.io 1.7.18

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125760 flaky test: TestCustomResourceDefaultingWithoutWatchCache

- Issue 链接：[#125760](https://github.com/kubernetes/kubernetes/issues/125760)

### Issue 内容

#### Which jobs are flaking?

https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/125758/pull-kubernetes-integration/1806280169099366400

#### Which tests are flaking?

--- FAIL: TestCustomResourceDefaultingWithoutWatchCache (4.58s)

#### Since when has it been flaking?

unknown

#### Testgrid link

https://testgrid.k8s.io/presubmits-kubernetes-blocking#pull-kubernetes-integration

#### Reason for failure (if possible)

```
{Failed;  === RUN   TestCustomResourceDefaultingWithoutWatchCache
    testserver.go:280: Resolved testserver package path to: "/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/cmd/server/testing"
    testserver.go:164: runtime-config=map[api/all:true]
    testserver.go:165: Starting apiextensions-apiserver on port 33019...
I0627 11:11:35.766913  103190 serving.go:380] Generated self-signed cert (/tmp/apiextensions-apiserver4170115585/apiserver.crt, /tmp/apiextensions-apiserver4170115585/apiserver.key)
W0627 11:11:36.322777  103190 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.
I0627 11:11:36.328509  103190 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0627 11:11:36.328547  103190 genericapiserver.go:761] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
W0627 11:11:36.328727  103190 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.
    testserver.go:191: Waiting for /healthz to be ok...
I0627 11:11:36.341029  103190 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/tmp/apiextensions-apiserver4170115585/apiserver.crt::/tmp/apiextensions-apiserver4170115585/apiserver.key"
I0627 11:11:36.341623  103190 secure_serving.go:213] Serving securely on 127.0.0.1:33019
I0627 11:11:36.341819  103190 customresource_discovery_controller.go:292] Starting DiscoveryController
I0627 11:11:36.341859  103190 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0627 11:11:36.341959  103190 naming_controller.go:294] Starting NamingConditionController
I0627 11:11:36.342008  103190 controller.go:142] Starting OpenAPI controller
I0627 11:11:36.342030  103190 controller.go:90] Starting OpenAPI V3 controller
I0627 11:11:36.342053  103190 crd_finalizer.go:269] Starting CRDFinalizer
I0627 11:11:36.342086  103190 establishing_controller.go:79] Starting EstablishingController
I0627 11:11:36.342104  103190 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0627 11:11:36.342117  103190 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
W0627 11:11:36.342225  103190 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.1.2.3:12345: connect: connection refused
E0627 11:11:36.342333  103190 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused" logger="UnhandledError"
I0627 11:11:36.546629  103190 handler.go:286] Adding GroupVersion tests.example.com v1beta1 to ResourceManager
W0627 11:11:37.822574  103190 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.1.2.3:12345: connect: connection refused
E0627 11:11:37.822650  103190 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused" logger="UnhandledError"
W0627 11:11:39.048433  103190 warnings.go:70] unknown field "alpha"
W0627 11:11:39.048463  103190 warnings.go:70] unknown field "beta"
W0627 11:11:39.048468  103190 warnings.go:70] unknown field "delta"
W0627 11:11:39.048472  103190 warnings.go:70] unknown field "epsilon"
W0627 11:11:39.048476  103190 warnings.go:70] unknown field "gamma"
    defaulting_test.go:288: Creating CR and expecting defaulted fields in spec, but status does not exist at all
    defaulting_test.go:301: CR created: map[string]interface {}{"apiVersion":"tests.example.com/v1beta1", "kind":"Foo", "metadata":map[string]interface {}{"creationTimestamp":"2024-06-27T11:11:39Z", "generation":1, "managedFields":[]interface {}{map[string]interface {}{"apiVersion":"tests.example.com/v1beta1", "fieldsType":"FieldsV1", "fieldsV1":map[string]interface {}{"f:spec":map[string]interface {}{".":map[string]interface {}{}, "f:a":map[string]interface {}{}, "f:b":map[string]interface {}{}, "f:replicas":map[string]interface {}{}, "f:v1beta1":map[string]interface {}{}}}, "manager":"integration.test", "operation":"Update", "time":"2024-06-27T11:11:39Z"}}, "name":"foo", "resourceVersion":"3714", "uid":"1fd9d30a-5657-4f26-bff8-e7b2f99cd18c"}, "spec":map[string]interface {}{"a":"a", "b":"B", "replicas":1, "v1beta1":"v1beta1", "v1beta2":"v1beta2"}}
    defaulting_test.go:308: Updating status and expecting 'a' and 'b' to show up.
    defaulting_test.go:315: Add 'c' default to the storage version and wait until GET sees it in both status and spec
    defaulting_test.go:318: wait until GET sees 'c' in both status and spec
    defaulting_test.go:325: will retry, did not find spec.c in the object
    defaulting_test.go:335: wait until GET sees 'c' in both status and spec of cached get
    defaulting_test.go:352: verify LIST sees 'c' in both status and spec
    defaulting_test.go:361: verify LIST from cache sees 'c' in both status and spec
    defaulting_test.go:374: verify WATCH sees 'c' in both status and spec
W0627 11:11:39.282080  103190 watcher.go:338] watch chan error: etcdserver: mvcc: required revision has been compacted
    defaulting_test.go:382: unexpected watch event: ERROR, &v1.Status{TypeMeta:v1.TypeMeta{Kind:"Status", APIVersion:"v1"}, ListMeta:v1.ListMeta{SelfLink:"", ResourceVersion:"", Continue:"", RemainingItemCount:(*int64)(nil)}, Status:"Failure", Message:"The resourceVersion for the provided watch is too old.", Reason:"Expired", Details:(*v1.StatusDetails)(nil), Code:410}
I0627 11:11:39.282619  103190 customresource_discovery_controller.go:328] Shutting down DiscoveryController
I0627 11:11:39.282624  103190 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0627 11:11:39.282651  103190 controller.go:170] Shutting down OpenAPI controller
I0627 11:11:39.282657  103190 naming_controller.go:305] Shutting down NamingConditionController
I0627 11:11:39.282675  103190 apiapproval_controller.go:201] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0627 11:11:39.282674  103190 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/tmp/apiextensions-apiserver4170115585/apiserver.crt::/tmp/apiextensions-apiserver4170115585/apiserver.key"
I0627 11:11:39.282681  103190 crd_finalizer.go:281] Shutting down CRDFinalizer
I0627 11:11:39.282721  103190 nonstructuralschema_controller.go:207] Shutting down NonStructuralSchemaConditionController
I0627 11:11:39.282742  103190 secure_serving.go:258] Stopped listening on 127.0.0.1:33019
I0627 11:11:39.282761  103190 establishing_controller.go:90] Shutting down EstablishingController
I0627 11:11:39.282778  103190 controller.go:120] Shutting down OpenAPI V3 controller
I0627 11:11:39.282762  103190 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
--- FAIL: TestCustomResourceDefaultingWithoutWatchCache (4.58s)
;}
```

#### Anything else we need to know?

_No response_

#### Relevant SIG(s)

/sig api-machinery


### 分析结果

不涉及

---

## Issue #125734 Volume with storage medium "memory" must be title case with unhelpful error message

- Issue 链接：[#125734](https://github.com/kubernetes/kubernetes/issues/125734)

### Issue 内容

#### What happened?

When you create a deployment with volume that has a medium of `memory` (note here that it is not title case), it fails with an error message of `unknown storage medium "memory"` and the pod gets stuck in the `ContainerCreating` state. As this is a `beta` feature (enabled with the `SizeMemoryBackedVolumes` feature gate), we originally thought that this feature gate was disabled and was the cause of this error message. It turns out that is was simply because it was `memory` and not `Memory`.

#### What did you expect to happen?

Either accept `memory` as a value or there would be a more meaningful error message with something along the lines of `unknown storage medium 'memory' - only the following are accepted: 'Memory', [...]`.

In addition, it was misleading that this value was accepted when applying. I would have expected the apply to fail when attempting to apply an invalid value for this field. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create a deployment with the following volume definition:
```yaml
volumes:
- emptyDir:
     medium: memory # note the lower case.
     sizeLimit: 32Mi
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
This originally occurred on EKS, but we were also able to replicate this on a `kind` cluster.
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125720 Pod Memory Request=Limit Triggers Pagecache Reclamation and Service Jitter on Node

- Issue 链接：[#125720](https://github.com/kubernetes/kubernetes/issues/125720)

### Issue 内容

#### What happened?

##### Description:
When deploying pods on a Kubernetes node with memory request equal to memory limit, despite no memory overcommitment on the host, scheduling a new pod triggers memory reclamation, leading to service jitter for some pods.
<img width="1285" alt="image" src="https://github.com/kubernetes/kubernetes/assets/58540165/dfdfa8ed-3d09-4060-b929-250688c8d786">


##### Observations:
1. Monitoring indicates that pagecache reclamation is being triggered.
2. Before the issue occurs, the node has sufficient memory requests available for scheduling the pod, and the host memory is not overcommitted.
3. Comparison of RSS (via `ps/top` on the host or `docker stats`) and `container_memory_usage_bytes` (from cAdvisor metrics, which are derived from `/sys/fs/cgroup/<pod_uid>/<container_uid>/memory/memory.usage_in_bytes`) shows a significant difference:
   - RSS: 14.2GB
   - `container_memory_usage_bytes`: 32GB (equal to the limit)

   This discrepancy can be attributed to RSS not including pagecache, while `memory.usage_in_bytes` does.

##### Questions and Analysis:
- If the host memory is not overcommitted and there are sufficient memory requests available for scheduling, why does scheduling a new pod trigger pagecache reclamation?
- Given that the limit already constrains the total memory usage (including pagecache), why does this issue arise?

##### Detailed Scenario:
- Our pods (Kafka, Elasticsearch) make extensive use of pagecache.
- Based on documentation, RSS metrics are from the `/proc` filesystem, while `memory.usage_in_bytes` is from the cgroup filesystem. The former excludes pagecache, and the latter includes it.
- This makes sense because the pods are designed to use significant pagecache. However, in a non-overcommitted memory scenario with sufficient remaining requests, new pod scheduling shouldn't cause pagecache reclamation issues.

##### Additional Information:
- Logs and monitoring metrics can be provided upon request.
- Detailed setup and configuration files can be shared if needed.

Thank you for investigating this issue.

#### What did you expect to happen?

The node should handle new pod scheduling without triggering pagecache reclamation if the total memory usage is within limits and memory requests are available.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy pods on a Kubernetes node with memory requests equal to memory limits.
2. Ensure the node has available memory requests and is not overcommitted.
3. Schedule a new pod on the node.
4. Observe the triggered pagecache reclamation and service jitter in existing pods.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.18.8-aliyun.1
WARNING: version difference between client (1.29) and server (1.18) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Alibaba Cloud Linux"
VERSION="3 (Soaring Falcon)"
ID="alinux"
ID_LIKE="rhel fedora centos anolis"
VERSION_ID="3"
PLATFORM_ID="platform:al8"
PRETTY_NAME="Alibaba Cloud Linux 3 (Soaring Falcon)"
ANSI_COLOR="0;31"
HOME_URL="https://www.aliyun.com/"
$ uname -a
Linux xxx 5.10.134-12.2.al8.x86_64 #1 SMP Thu Oct 27 10:07:15 CST 2022 x86_64 x86_64 x86_64 GNU/Linux

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125715 K8S not running POD only in k8s but working in K3S and Minikube

- Issue 链接：[#125715](https://github.com/kubernetes/kubernetes/issues/125715)

### Issue 内容

#### What happened?

Been getting this error from many days initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘processorMetrics’ defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/SystemMetricsAutoConfiguration.class]: Failed to instantiate [io.micrometer.core.instrument.binder.system.ProcessorMetrics]: Factory method ‘processorMetrics’ threw exception with message: java.lang.reflect.InvocationTargetException
this only happens in k8s and not in k3s and minikube why??

#### What did you expect to happen?

The pod should have ran successfully as it did in minikube and k3s

#### How can we reproduce it (as minimally and precisely as possible)?

Taking a good spring boot application image and running it 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server 10.251.0.188:6443 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"

$ uname -a
# paste output here
Linux demo 4.898.0-553.5.1.el8_10.x86_64 #1 SMP Tue May 21 03:13:04 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125702 CEL stored messageExpressions of CRDs are not validated with the correct CEL environment

- Issue 链接：[#125702](https://github.com/kubernetes/kubernetes/issues/125702)

### Issue 内容

#### What happened?

Noticed a copy & paste issue when reading through the CEL code.

Basically here: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/validation/validation.go#L182

This should be 
> expressions.messageExpressions.Insert(v.MessageExpression)
instead of:
> expressions.messageExpressions.Insert(v.Rule)

The impact of this issue should be that messageExpressions are always evaluated with the CEL environment of the compatibility version. But stored (i.e. pre-existing) messageExpressions should be evaluated with a CEL environment with the "max" version instead.

For users of Kubernetes this means that messageExpressions deployed with one minor version of Kubernetes might not work anymore if they rollback e.g. by one minor version.




#### What did you expect to happen?

messageExpressions should be treated the same way as rules in that regard.



#### How can we reproduce it (as minimally and precisely as possible)?

* Deploy Kubernetes 1.30
* Deploy CRD that uses messageExpression that is using Sets (which became available with Kubernetes 1.30 that is using compatibilty version 1.29)
* Rollback to Kubernetes 1.29
* Update something in the CRD
  * => CRD update will fail because messageExpression is evaluated with the wrong CEL environment (it should still work)



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

At least on Kubernetes main, but I would assume this isuse is there since CEL was implemented

</details>


#### Cloud provider

-

#### OS version

-

#### Install tools

-

#### Container runtime (CRI) and version (if applicable)

-

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125695 Kubernetes Job failed status after a graceful termination

- Issue 链接：[#125695](https://github.com/kubernetes/kubernetes/issues/125695)

### Issue 内容

#### What happened?

Let's take the following Kubernetes Cronjob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sleep-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 0 # Avoid two executions
      template:
        spec:
          containers:
          - name: sleep
            image: busybox
            command: ["sh", "-c", "sleep 120"] # 2 minutes
          restartPolicy: Never
          terminationGracePeriodSeconds: 300 # 5 minutes
```

It runs a job every 5 minutes, and the pod sleeps for 2 minutes.

If during the sleep I execute the kubectl delete pod <POD_NAME> for a graceful termination, it will work, it will wait for the specified terminationGracePeriodSeconds and in this case, it will allow the pod to finish its job successfully.

But, the Kubernetes Job will be marked as 1 Failed with the reason BackoffLimitExceeded.

I have tried the Job configuration [activeDeadlineSeconds](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup) and I'm still getting the error.

Could this be a bug or am I missing some configuration?

#### What did you expect to happen?

The job is mark as completed.

#### How can we reproduce it (as minimally and precisely as possible)?

Using the following, run it, and delete the pod before it finish its operation and then check for associated job status.

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sleep-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 0 # Avoid two executions
      template:
        spec:
          containers:
          - name: sleep
            image: busybox
            command: ["sh", "-c", "sleep 120"] # 2 minutes
          restartPolicy: Never
          terminationGracePeriodSeconds: 300 # 5 minutes
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5-gke.1091000
```

</details>


#### Cloud provider

<details>
GCP with GKE v1.29.5-gke.1091000
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及。

---

## Issue #125684 Performance Optimization for UpdateSnapshot when there is only one zone

- Issue 链接：[#125684](https://github.com/kubernetes/kubernetes/issues/125684)

### Issue 内容

#### What happened?

When there is a node add or delete event, the `UpdateSnapshot` method triggers the updateAll logic, which reallocates memory for the entire nodeInfoList.

#### What did you expect to happen?

When there is only a single zone, we can operate on the nodeInfoList directly to optimize it and avoid reallocating memory. This approach is both CPU and memory-friendly for large clusters where nodes are frequently added or deleted. We don't need additional memory in this scenario, as there is only one zone by default.

#### How can we reproduce it (as minimally and precisely as possible)?

1. add or delete node
2. call `UpdateSnapshot` function

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

master

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125680 invalid memory address or nil pointer dereference" in wait.JitterUntil

- Issue 链接：[#125680](https://github.com/kubernetes/kubernetes/issues/125680)

### Issue 内容

#### What happened?

![event-panic](https://github.com/kubernetes/kubernetes/assets/76192004/3ed97110-e8ad-4b8b-9a64-0ce1cd0ff4fa)


#### What did you expect to happen?

no panic 

#### How can we reproduce it (as minimally and precisely as possible)?

func (e *EventWatcher) OnAdd(obj interface{}) {
	defer recoverPanic("addEvent")

	event := obj.(*corev1.Event)
	e.watchEvent(event)
}

func (e *EventWatcher) OnUpdate(oldObj, newObj interface{}) {
	defer recoverPanic("updateEvent")
	event := newObj.(*corev1.Event)
	e.watchEvent(event)
}

func (e *EventWatcher) OnDelete(obj interface{}) {
	// nil
}

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
k8s version 1.19.5
k8s.io/client-go v0.22.6
```

</details>


### 分析结果

不涉及

---

## Issue #125671 TypeMeta is empty in Type client Apply and Patch responses

- Issue 链接：[#125671](https://github.com/kubernetes/kubernetes/issues/125671)

### Issue 内容

#### What happened?

Called:

```go
applied, err := client.AppsV1().Deployments("default").Apply(context.TODO(), deployment, metav1.ApplyOptions{FieldManager: "test-fieldmanager"})
```
I found:

```go
applied.TypeMeta.Kind == ""
applied.TypeMeta.APIVersion == ""
```



#### What did you expect to happen?

```go
applied.TypeMeta.Kind == "Deployment"
applied.TypeMeta.APIVersion == "apps/v1"
```

#### How can we reproduce it (as minimally and precisely as possible)?

Make the above call.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30, but this probably goes back to introduction of patch support in typed clients.

#### Cloud provider

N/A

#### OS version

Linux

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125639 Pod with exitCode 137， The reason has nothing to do with resources。

- Issue 链接：[#125639](https://github.com/kubernetes/kubernetes/issues/125639)

### Issue 内容

#### What happened?

After the pod runs for a period of time, it will be killed. Through auditctl tracking, it is found that the container is killed by runc.

Through monitoring, it is found that all resource utilization is very low, requests and limits are within the range, and there is no OOM situation.

#### What did you expect to happen?

Reason for being killed

#### How can we reproduce it (as minimally and precisely as possible)?

Because it is sporadic, it is difficult to reproduce accurately

#### Anything else we need to know?

The probe used is execProbe

#### Kubernetes version

> kubectl version
Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCommit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:"2021-01-13T13:23:52Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.20", GitCommit:"1f3e19b7beb1cc0110255668c4238ed63dadb7ad", GitTreeState:"clean", BuildDate:"2021-06-16T12:51:17Z", GoVersion:"go1.13.15", Compiler:"gc", Platform:"linux/amd64"}

> docker info
.....
 Server Version: 19.03.15
 Storage Driver: overlay2
......
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce
 runc version: v1.1.6-0-g0f48801a
 init version: fec3683
 ......
 Kernel Version: 5.4.242-1.el7.elrepo.x86_64
 Operating System: CentOS Linux 7 (Core)
 ......

#### Cloud provider

Private cloud, self-built VM

#### OS version

# cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
# uname -a
......5.4.242-1.el7.elrepo.x86_64 #1 SMP Tue Apr 25 09:46:07 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux



#### Install tools

Nothing


#### Container runtime (CRI) and version (if applicable)

# docker version
Client: Docker Engine - Community
 Version:           19.03.15
 API version:       1.40
 Go version:        go1.13.15
 Git commit:        99e3ed8919
 Built:             Sat Jan 30 03:17:57 2021
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.15
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       99e3ed8919
  Built:            Sat Jan 30 03:16:33 2021
  OS/Arch:          linux/amd64
 containerd:
  Version:          1.2.2
  GitCommit:        9754871865f7fe2f4e74d43e2fc7ccd237edcbce
 runc:
  Version:          1.1.6
  GitCommit:        v1.1.6-0-g0f48801a
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Nothing

### 分析结果

不涉及

---

## Issue #125638 The endpoint status does not update when the pod state changes rapidly.

- Issue 链接：[#125638](https://github.com/kubernetes/kubernetes/issues/125638)

### Issue 内容

#### What happened?


![image](https://github.com/kubernetes/kubernetes/assets/19217340/7686ae10-e598-4d8f-8c8c-b28b16338912)

----
t2
1. Pod update event: ready to notReady
2. syncService
3. currentEndpoints state(from lister: Ready) do not equal to  target state(notReady)
4. clientSet.Endpoints.Update: update etcd to notReady

----
t3
1. Pod update event : notReady to ready(<xxMS)
2. syncService
3. **currentEndpoints state(from lister: Ready) equal to  target state(ready)，do not update Endpoints**

----
----
t4
1. Endpoint update event:  update endpoint lister cache to notReady
----


#### What did you expect to happen?

The status of the endpoint should match the status of the pod.

#### How can we reproduce it (as minimally and precisely as possible)?

Make the Pod status change rapidly. 
Lowering the etcd push interval should make it easier to reproduce.









#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.25.3


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125624 [FG:InPlacePodVerticalScaling] resources in pod status are never updated if EventedPLEG is enabled

- Issue 链接：[#125624](https://github.com/kubernetes/kubernetes/issues/125624)

### Issue 内容

#### What happened?

If eventedPLEG is enabled, resources in a pod status are never updated after the pod is resized.


#### What did you expect to happen?

Resources in a pod status are updated after the pod is resized.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable `EventedPLEG` and `InPlacePodVerticalScaling`.
2. Create a pod and update resources in the pod following the [documentation](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#updating-the-pod-s-resources).

```
$ kubectl -n qos-example patch pod qos-demo-5 --patch '{"spec":{"containers":[{"name":"qos-demo-ctr-5", "resources":{"requests":{"cpu":"800m"}, "limits":{"cpu":"800m"}}}]}}'
$ # Wait for more than 10 minutes...
$ kubectl -n qos-example get pod qos-demo-5 -o json | jq '.status.containerStatuses[0].resources'
{
  "limits": {
    "cpu": "700m",
    "memory": "200Mi"
  },
  "requests": {
    "cpu": "700m",
    "memory": "200Mi"
  }
}
```


#### Anything else we need to know?

The root cause of this issue is the same problem mentioned in #124297. Evented PLEG passes a timestamp wrongly to a cache:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/pleg/evented.go#L269

EventedPLEG does not emit any event or update a cache because a runtime raises no event at resizing a container without restart. Though GenericPLEG is expected to back up in such a case, this does not work. Generic PLEG relists pods every 5 minutes. It also updates the cache when pod resizing is in progress:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/kubelet.go#L2008-L2011

However, an update from GenericPLEG is rejected because the new timestamp looks older than the existing timestamp that EventedPLEG sets at starting the pod because of the bug mentioned above:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/container/cache.go#L106-L112


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.29.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125623 'kubectl delete istag/$ISTAG --dry-run=server' is unexpectedly deleting the object from the server

- Issue 链接：[#125623](https://github.com/kubernetes/kubernetes/issues/125623)

### Issue 内容

#### What happened?

'kubectl delete istag/$ISTAG --dry-run=server' is unexpectedly deleting the object from the server this is not expected behavior.

- Attempted to delete the `istag` resource object by using the `dry-run=server` option. However, the object(image) was actually deleted from the server.
- Conversely, when applied the same `dry-run=server` parameter to other resources, they remained intact and were not deleted.

#### What did you expect to happen?

After deleting the `istag` using the `--dry-run=server` option with the server strategy, the `istag` object should still remain on the server.


#### How can we reproduce it (as minimally and precisely as possible)?

Try deleting the `istag` resource using the `--dry-run=server` option with the server strategy.

**Steps to Reproduce:**

1. Example with resource ImageStreamTag istag

- Try to delete the `istage` object using the `--dry-run` option with `client`  strategy 
~~~ 
[quickcluster@upi-0 ~]$ kubectl delete -n openshift istag/ubi8-openjdk-8:1.11 --dry-run=client
imagestreamtag.image.openshift.io "ubi8-openjdk-8:1.11" deleted (dry run)
~~~
- Verify whether the deleted object with `client` strategy is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get istag -n openshift | grep -i ubi8-openjdk-8:1.11
ubi8-openjdk-8:1.11                                         image-registry.openshift-image-registry.svc:5000/openshift/ubi8-openjdk-8@sha256:022488b1bf697b7dd8c393171a3247bef4ea545a9ab828501e72168f2aac9415                                       4 weeks ago
~~~
- Now, delete the `istage`  object using the `--dry-run` option with `server` strategy 
~~~
[quickcluster@upi-0 ~]$ kubectl delete -n openshift istag/ubi8-openjdk-8:1.11 --dry-run=server
imagestreamtag.image.openshift.io "ubi8-openjdk-8:1.11" deleted (server dry run)
~~~
- Verify whether the deleted object with `server` strategy  is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get -n openshift istag | grep -i ubi8-openjdk-8:1.11 
~~~

2. Example with resource secret
- Try to delete the `secret` using the `--dry-run` option with `client`  strategy
~~~ 
[quickcluster@upi-0 ~]$ kubectl delete secrets/deployer-token-jz46t --dry-run=client
secret "deployer-token-jz46t" deleted (dry run)
~~~
- Verify whether the deleted object with `client` strategy is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get secret | grep -i deployer-token-jz46t
deployer-token-jz46t       kubernetes.io/service-account-token   4      28d
~~~
- Now, delete the `secret` using the `--dry-run` option with `server` strategy 
~~~
[quickcluster@upi-0 ~]$ kubectl delete secrets/deployer-token-jz46t --dry-run=server
secret "deployer-token-jz46t" deleted (server dry run)
~~~
-  Verify whether the deleted object with `server` strategy  is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get secret | grep -i deployer-token-jz46t
deployer-token-jz46t       kubernetes.io/service-account-token   4      28d
~~~ 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7+c1f5b34
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux Server"
VERSION="7.9 (Maipo)"
ID="rhel"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="7.9"
PRETTY_NAME="Red Hat Enterprise Linux Server 7.9 (Maipo)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:7.9:GA:server"
HOME_URL="https://www.redhat.com/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 7"
REDHAT_BUGZILLA_PRODUCT_VERSION=7.9
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="7.9"

$ uname -a
Linux upi-0.harshaltest24.lab.psi.pnq2.redhat.com 3.10.0-1160.el7.x86_64 #1 SMP Tue Aug 18 14:50:17 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
```
- oc 4.15
- kubectl  v1.28.2
```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经分析，您提供的Issue描述了在使用`kubectl delete istag/$ISTAG --dry-run=server`命令时，期望只进行服务器端的模拟删除操作（即不实际删除资源），但实际上`istag`资源被真正删除了。这种行为与预期不符，属于功能性错误。

根据您提供的风险判断标准，具体分析如下：

1. **该风险能被攻击者利用**：要执行`kubectl delete`命令，无论是否使用`--dry-run`参数，用户都需要对目标资源具有删除权限。也就是说，只有具备删除权限的用户才能执行该命令。攻击者如果没有适当的权限，无法利用此问题删除资源。因此，该问题不能被未授权的攻击者利用。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：由于该问题不涉及权限提升、未授权访问或其他安全漏洞，且不满足CVSS高危漏洞的评分标准，所以不太可能被分配CVE编号。

3. **Issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险，因为它是Issue提交者的问题，而不是项目的问题**：该问题是由于命令的意外行为引起的功能性错误，不涉及提交者的不当操作或配置。

4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：本问题中，删除资源的操作需要具备删除权限，因此即使存在资源被意外删除的情况，也不属于高风险的DoS攻击。

5. **对于日志中泄露凭据的风险**：本问题与凭据泄露无关。

综上所述，该Issue反映的是在特定命令下功能未按预期执行的问题，并不涉及安全风险。

---

## Issue #125618 Node Lifecycle Controller does not mark pods not ready when node becomes Ready=False

- Issue 链接：[#125618](https://github.com/kubernetes/kubernetes/issues/125618)

### Issue 内容

#### What happened?

The node lifecycle controller is responsible for marking the ready condition on pods with Ready=False when the node becomes unhealthy. See https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L757

It's not document clearly today, but the behavior appears to be that the only time when the node lifecycle controller will mark pods not ready is if the **kubelet failed to renewed it's lease for more than `nodeMonitorGracePeriod`**.  The node lifecycle controller will **not** mark pods as Ready=False, if the kubelet continues to updates it's lease but reports Ready=False.

This is because for `MarkPodsNotReady` to be called [this expression](https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L752) must be evaluated where `currentReadyCondition != observedReadyCondition`. The only situation where this will be true is if the lease was [not renewed](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L936-L987). 

This has also been raised in this discussion thread -- https://github.com/kubernetes/kubernetes/pull/112928#discussion_r992874302

#### What did you expect to happen?

I think it's worth to consider if this behavior should be changed -- if nodes report `Ready=False`, should the node-lifecycle-controller mark the pods as not ready?

One of the motivations of the change is for the Graceful Node Shutdown feature. As was discussed in https://github.com/kubernetes/kubernetes/issues/116965, in some cases, Graceful Shutdown is not able to fully complete and shutdown all the pods on the node (for example on a short emergency shutdown or if kubelet was unable to make the status update). Graceful node shutdown does set the node ready condition as false prior to the shutdown. If the node lifecycle controller would mark pods as not ready in this case, it would help https://github.com/kubernetes/kubernetes/issues/116965 and remove pods from endpoints (by marking them as non ready).

#### How can we reproduce it (as minimally and precisely as possible)?

n/a

#### Anything else we need to know?

_No response_

#### Kubernetes version

n/a

#### Cloud provider

n/a

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125614 kube-apiserver logs watch requests before they end in 1.30

- Issue 链接：[#125614](https://github.com/kubernetes/kubernetes/issues/125614)

### Issue 内容

#### What happened?


In 1.30, WATCH request is logged immediately when a watch is opened, with very low latency. There appears to be no trace of when the watch ends. This significantly impacts debugging capability for issues with establishing watches. 

Example for consecutive watches on pods opened by kube-scheduler - each appears to be lasting 8-9 minutes, everything works in the cluster, but the latency doesn't match watch length, the request is logged immediately, and with `resp=0`:

```
I0620 12:33:17.389016      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=97186&timeout=9m12s&timeoutSeconds=552&watch=true" latency="470.891µs" userAgent="kube-scheduler/v1.30.2 (linux/amd64) kubernetes/85fad5b/scheduler" audit-ID="2cf3b6d8-8c04-4065-8bb8-ccd209f24063" srcIP="[::1]:60566" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="252.569µs" apf_execution_time="253.277µs" resp=0
...
I0620 12:42:29.390797      11 get.go:261] "Starting watch" path="/api/v1/pods" resourceVersion="135790" labels="" fields="status.phase!=Failed,status.phase!=Succeeded" timeout="6m10s"
I0620 12:42:29.391006      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=135790&timeout=6m10s&timeoutSeconds=370&watch=true" latency="629.842µs" userAgent="kube-scheduler/v1.30.2 (linux/amd64) kubernetes/85fad5b/scheduler" audit-ID="619843d2-3f2a-4c2e-960a-17eecc71695b" srcIP="[::1]:60566" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="299.938µs" apf_execution_time="300.55µs" resp=0
```

#### What did you expect to happen?

Watches are usually long (~minutes). In 1.29 and earlier log looks like this:

```
I0620 12:30:26.940451      11 get.go:260] "Starting watch" path="/api/v1/pods" resourceVersion="93431" labels="" fields="status.phase!=Failed,status.phase!=Succeeded" timeout="8m18s"
...
I0620 12:38:44.940817      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=93431&timeout=8m18s&timeoutSeconds=498&watch=true" latency="8m18.00060226s" userAgent="kube-scheduler/v1.29.6 (linux/amd64) kubernetes/43a444e/scheduler" audit-ID="c39f0303-10ad-4b51-baf4-87c592fcb85e" srcIP="[::1]:39438" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="280.462µs" apf_execution_time="281.612µs" resp=200
```

#### How can we reproduce it (as minimally and precisely as possible)?

Compare kube-apiserver log in 1.29 and 1.30.

#### Anything else we need to know?

Log line examples are taken from passing runs of gce-scalabiltiy-100 tests: 
- https://testgrid.k8s.io/sig-scalability-gce#gce-cos-1.29-scalability-100 : https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-scalability-1-29/1803759918289784832
- https://testgrid.k8s.io/sig-scalability-gce#gce-cos-1.30-scalability-100 : https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-scalability-1-30/1803759918403031040

#### Kubernetes version

Observed in 1.30.2 and 1.30.1. To be verified if present in 1.30.0..

#### Cloud provider

N/A


#### OS version

N/A

#### Install tools

N/A


#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

不涉及

---

## Issue #125611 NetPol block self pod trafic using an svc and not direct call

- Issue 链接：[#125611](https://github.com/kubernetes/kubernetes/issues/125611)

### Issue 内容

#### What happened?

Hello,

In case of a pod exposed by an internal service (ClusterIP) and a presence of a network-policy allowing ingress traffic from some namespaces and th pod namespace it self (my-namespace) like :

```yaml
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: my-namespace
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: nginx
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: prometheus
  podSelector: {}
  policyTypes:
  - Ingress
```


When the pod call it self using the service IP or service name (DNS), got timeout when the NetPol is presente.



#### What did you expect to happen?

From the pod, i can : 

1. Pod can call it self by his IP
2. Pod can call it self by the service IP
3. Pod can call it self by the service FQDN (DNS)

#### How can we reproduce it (as minimally and precisely as possible)?


Deploy pod with svc and netpol with ingress filtering. Run from inside the pod, a curl call to the service like :  
`wget http://hello-world.my-namespace.svc.cluster.local`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.25.7
```

</details>


#### Cloud provider

<details>
On promise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNS : Antre v1.9.0
</details>


### 分析结果

不涉及

---

## Issue #125610 cronjob schedule with multiple conditions not working - conflict between day (week) and day (month)

- Issue 链接：[#125610](https://github.com/kubernetes/kubernetes/issues/125610)

### Issue 内容

#### What happened?

Hello,
I have a kubernetes cronjob (apiVersion: batch/v1 type: CronJob) that I would like to run only on Tuesdays. Then I'd like to run it this first week of the month and the third week of the month. Here is my definition of cronjob: `0 18 1-7,15-21 * TUE`
The problem is that the day (week) condition does not work and my cron runs every day between 1 - 7 and 15 - 21 days (month).

#### What did you expect to happen?

I would like the two conditions to work together: AND on Tuesday AND between the 1st and 7th of the month and the 15th and 21st of the month.

#### How can we reproduce it (as minimally and precisely as possible)?

It is possible to reproduce it on each cronjob, here is the model from the Kubernetes documentation
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "0 18 1-7,15-21 * TUE"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1+rke2r1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125609 [FG:InPlacePodVerticalScaling] e2e test does not verify resource update in pod status

- Issue 链接：[#125609](https://github.com/kubernetes/kubernetes/issues/125609)

### Issue 内容

#### What happened?

e2e tests for pod resizing does not verify if resources in a pod status is updated to the same values in a pod spec after resizing is actuated. This is caused because the result of the runtime support check is reversed:

https://github.com/kubernetes/kubernetes/blob/1519f802816f6a6b9bd4cfb259c93644fe950e0e/test/e2e/node/pod_resize.go#L501-L513


#### What did you expect to happen?

Resources in pod status are verified.


#### How can we reproduce it (as minimally and precisely as possible)?

Run e2e test with enabling InPlacePodVerticalScaling.


#### Anything else we need to know?

I'm not sure if we should fix it immediately. If this issue is fixed, it will take much longer to finish the tests because there are some issues that it takes much time (60-90s) till resources in a pod status is updated after resizing is actuated such as #123940 and #125559. In addition, pod resize e2e tests are being migrated to e2e node tests in PR #124296.


#### Kubernetes version

master branch

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125608 Node Labeling node.kubernetes.io/out-of-service Taint Label Delay

- Issue 链接：[#125608](https://github.com/kubernetes/kubernetes/issues/125608)

### Issue 内容

#### What happened?

After the node is powered off, the node.kubernetes.io/out-of-service label is not added in time. Therefore, the pod created by the statefulset is not deleted in time.
 ```
 taints:
  - effect: NoSchedule
    key: node.kubernetes.io/out-of-service
    value: nodeshutdown
  - effect: NoExecute
    key: node.kubernetes.io/out-of-service
    value: nodeshutdown
  - effect: NoSchedule
    key: node.kubernetes.io/unreachable
    timeAdded: "2024-06-20T08:56:43Z"
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    timeAdded: "2024-06-20T08:56:55Z"
```
A node is powered off. At least one pod created by statefulset exists on the node. The pod tolerance is set to 30s. After the node is powered off, the node status changes to notready. The pod is in the terminating state 30s later but is deleted after a long time. The reason is that the node.kubernetes.io/out-of-service taint label is updated late.
So I want to know when the node.kubernetes.io/out-of-service taint tag was added.

#### What did you expect to happen?

The pod should be deleted in a timely manner.

#### How can we reproduce it (as minimally and precisely as possible)?

Powering Off a Node

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125607 kubectl --server-side --dry-run=server - wrong output for converting client side applied manifest

- Issue 链接：[#125607](https://github.com/kubernetes/kubernetes/issues/125607)

### Issue 内容

#### What happened?

When converting a client-side-applied manifest to a server side applied manifest `--dry-run=server` doesn't show the correct output.
It still shows client-side-applied fields, which will be removed, when running without `--dry-run=server`.

#### What did you expect to happen?

running `--server-side` with `--dry-run=server` will generate the same manifest as running without `--dry-run=server`

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply an example configmap with 3 keys with client-side-apply:

`kubectl apply -o yaml --show-managed-fields -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-original.yaml`

Output:

<details>

<pre>
apiVersion: v1
data:
  key1: key-original
  key2: key-original
  key3: key-original
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"key1":"key-original","key2":"key-original","key3":"key-original"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"configmap","namespace":"default"}}
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:key1: {}
        f:key2: {}
        f:key3: {}
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2024-06-20T10:55:40Z"
  name: configmap
  namespace: default
  resourceVersion: "27130"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244
</pre>

</details>

2. Do a dry run for a server-side-apply for a diff manifest changing a single key of the configmap:
`kubectl apply -o yaml --server-side --dry-run=server --show-managed-fields --force-conflicts --field-manager=manager -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-patch.yaml`

Look at the resulting manifest, which is merged by both the manifests:
<details>

<pre>
apiVersion: v1
data:
  key1: key-original
  key2: key-patch
  key3: key-original
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"key1":"key-original","key2":"key-original","key3":"key-original"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"configmap","namespace":"default"}}
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        f:key2: {}
    manager: manager
    operation: Apply
    time: "2024-06-20T10:59:45Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:key1: {}
        f:key3: {}
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2024-06-20T10:55:40Z"
  name: configmap
  namespace: default
  resourceVersion: "27130"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244

</pre>

</details>

3. Run the server-side-apply:
`kubectl apply -o yaml --server-side --show-managed-fields --force-conflicts --field-manager=manager -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-patch.yaml`

Look at the applied manifest, where the fields from client-side-apply are removed:
<details>

<pre>
apiVersion: v1
data:
  key2: key-patch
kind: ConfigMap
metadata:
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        f:key2: {}
    manager: manager
    operation: Apply
    time: "2024-06-20T11:03:56Z"
  name: configmap
  namespace: default
  resourceVersion: "28025"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244
</pre>

</details>


#### Anything else we need to know?

Regarding #125403 kubectl is removing existing client-side-fields which are not part of a manifest for the same resource applied by server-side-apply.
This behavior was introduced since #112905 is fixed.

This should also be reflected, if -dry-run=server is used 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
local Podman Desktop v1.10.3 - kind-cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                   OSArchitecture  Version
22631        Microsoft Windows 11 Pro  64-bit          10.0.22631
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125604 Can't get secrets when adding imagePullSecrets 

- Issue 链接：[#125604](https://github.com/kubernetes/kubernetes/issues/125604)

### Issue 内容

#### What happened?

Image can't be pulled after edit container image and imagePullSecrets, and I get this error in kubelets log:

```
 User "system:node:xxxx cannot get resource "secrets" in API group "" in the namespace "yyyy": no relationship found between node "xxxx" and this object。
```

#### What did you expect to happen?

The pod should be ready with the new image.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a normal pod and it get ready on node **without any workload**.
2. edit the image with a **different registry** and **add** the correct imagePullSecret has been created under the namespace

#### Anything else we need to know?

From the code

https://github.com/kubernetes/kubernetes/blob/1519f802816f6a6b9bd4cfb259c93644fe950e0e/plugin/pkg/auth/authorizer/node/graph_populator.go#L95-L102

I think the graph for this pod in the node authorizer will only populated once when it is scheduled to a node, so when I add a new imagePullSecrets to the pod, the secret will not be added to the graph.


#### Kubernetes version

<details>

```console
$ kubectl version
v1.18
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125602 The startup time of the init container is later than that of the application container.

- Issue 链接：[#125602](https://github.com/kubernetes/kubernetes/issues/125602)

### Issue 内容

#### What happened?

When the argo task is executed, The startup time of the init container is later than that of the application container.

In the output of the kubectl describe pod xxx command, the value of `startedAt` for the init container is later than that for the application container.

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/default-container: main
    kubernetes.io/psp: psp-global
    workflows.argoproj.io/node-id: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    workflows.argoproj.io/node-name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99.coverage(1:cov-type:coverage_report,version:0.2.5)
  creationTimestamp: "2024-06-19T19:44:39Z"
  labels:
    fuzz-worker: "true"
    workflows.argoproj.io/completed: "true"
    workflows.argoproj.io/workflow: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
  name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
  namespace: argo
  ownerReferences:
  - apiVersion: argoproj.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: Workflow
    name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    uid: 2e182e9c-f249-44cf-911a-b3c10310e451
  resourceVersion: "544998173"
  selfLink: /api/v1/namespaces/argo/pods/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
  uid: 9b7ba2d3-65f7-4e3f-8621-538d1cfb256f
spec:
  activeDeadlineSeconds: 259200
  containers:
  - command:
    - argoexec
    - wait
    - --loglevel
    - info
    env:
    - name: ARGO_POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: ARGO_POD_UID
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.uid
    - name: ARGO_CONTAINER_RUNTIME_EXECUTOR
      value: emissary
    - name: GODEBUG
      value: x509ignoreCN=0
    - name: ARGO_WORKFLOW_NAME
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    - name: ARGO_CONTAINER_NAME
      value: wait
    - name: ARGO_TEMPLATE
      value: '....'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ../argoexec:v3.3.8
    imagePullPolicy: IfNotPresent
    name: wait
    resources:
      requests:
        cpu: 10m
        memory: 64Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /argo/secret/my-minio-cred
      name: my-minio-cred
      readOnly: true
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
  - args:
    - 'python3 entry_point.py; sh run.sh '
    command:
    - /var/run/argo/argoexec
    - emissary
    - --
    - sh
    - -c
    env:
    - name: TENANT_ID
      value: "1009"
    - name: DEBUG_COV
      value: "false"
    - name: GIT_SSL_NO_VERIFY
      value: "1"
    - name: GIT_BRANCH
      value: master
    - name: FUZZ_CONFIG
      value: '...'
    - name: COVERAGE_ARGS
      value: -C 10000000 -T 3600 -r reportPath -c corpusPath
    - name: PROJECT_NAME
      value: VOS_CortexM
    - name: JOB_NAME
      value: VOS_CortexM
    - name: S3_DIR
      value: SECTRACY/261035810/VOS_CortexM/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99/
    - name: S3_REPORT_DIR
      value: SECTRACY-REPORT/261035810/VOS_CortexM/0.2.5/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99/
    - name: VERSION
      value: 0.2.5
    - name: COV_TYPE
      value: coverage_report
    - name: FUZZ_DB_HOST
      value: 7.191.2.211
    - name: FUZZ_DB
      value: fuzz-admin
    - name: FUZZ_DB_PASS
      value: VGRhdGFfb3NzQDIwMjI=
    - name: WORKFLOW_ID
      value: 3763805e-581a-4070-96e9-8c8ff7b74058
    - name: BRANCH
      value: master
    - name: COVERAGE_URL
      value: ..
    - name: FUZZTYPE
      value: DTFUZZ
    - name: SERVICE_AREA
      value: yellow
    - name: SERVICE_ENVIR
      value: pro
    - name: PROJECT_PBI
      value: "261035810"
    - name: ARGO_CONTAINER_NAME
      value: main
    - name: ARGO_TEMPLATE
      value: '...'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ..
    imagePullPolicy: IfNotPresent
    name: main
    resources: {}
    securityContext:
      privileged: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
    workingDir: /Sectracy
  dnsConfig:
    options:
    - name: single-request-reopen
      value: ""
    - name: timeout
      value: "2"
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: regcred
  - name: default-secret
  initContainers:
  - command:
    - argoexec
    - init
    - --loglevel
    - info
    env:
    - name: ARGO_POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: ARGO_POD_UID
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.uid
    - name: ARGO_CONTAINER_RUNTIME_EXECUTOR
      value: emissary
    - name: GODEBUG
      value: x509ignoreCN=0
    - name: ARGO_WORKFLOW_NAME
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    - name: ARGO_CONTAINER_NAME
      value: init
    - name: ARGO_TEMPLATE
      value: '...'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ../argoexec:v3.3.8
    imagePullPolicy: IfNotPresent
    name: init
    resources:
      requests:
        cpu: 10m
        memory: 64Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /argo/secret/my-minio-cred
      name: my-minio-cred
      readOnly: true
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
  nodeName: 7.218.69.27
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - emptyDir: {}
    name: var-run-argo
  - name: my-minio-cred
    secret:
      defaultMode: 420
      items:
      - key: accesskey
        path: accesskey
      - key: secretkey
        path: secretkey
      secretName: my-minio-cred
  - name: default-token-6htk6
    secret:
      defaultMode: 420
      secretName: default-token-6htk6
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:38Z"
    message: 'containers with incomplete status: [init]'
    reason: ContainersNotInitialized
    status: "False"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:39Z"
    message: 'containers with unready status: [wait main]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:39Z"
    message: 'containers with unready status: [wait main]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T19:44:39Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://9208031ef8307e4d1561020ae84cfb867ea435bac4fffde09aa2efd33c05531c
    image: ..
    imageID: ..
    lastState: {}
    name: main
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: docker://9208031ef8307e4d1561020ae84cfb867ea435bac4fffde09aa2efd33c05531c
        exitCode: 137
        finishedAt: "2024-06-19T21:43:38Z"
        reason: Error
        startedAt: "2024-06-19T21:43:36Z"
  - containerID: docker://5d1d650c0225e09a0077768cd0bba2b24458de5495e88e4db5793aa488fbd20e
    image: ../argoexec:v3.3.8
    imageID: ..
    lastState: {}
    name: wait
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: docker://5d1d650c0225e09a0077768cd0bba2b24458de5495e88e4db5793aa488fbd20e
        exitCode: 0
        finishedAt: "2024-06-19T21:43:38Z"
        message: Step terminated
        reason: Completed
        startedAt: "2024-06-19T19:44:41Z"
  hostIP: 7.218.69.27
  initContainerStatuses:
  - containerID: docker://d9ab7d5ec93b06c04f2970ce2b30f672298069ed3098176683a654f487f28072
    image: ../argoexec:v3.3.8
    imageID: ..
    lastState: {}
    name: init
    ready: false
    restartCount: 0
    state:
      terminated:
        containerID: docker://d9ab7d5ec93b06c04f2970ce2b30f672298069ed3098176683a654f487f28072
        exitCode: 1
        finishedAt: "2024-06-19T21:43:37Z"
        message: 'open /var/run/argo/argoexec: text file busy'
        reason: Error
        startedAt: "2024-06-19T21:43:37Z"
  phase: Failed
  podIP: 12.11.1.191
  podIPs:
  - ip: 12.11.1.191
  qosClass: Burstable
  startTime: "2024-06-19T19:44:39Z"

```

#### What did you expect to happen?

The init container starts to run the application container after the running of the init container is complete. However, the application container runs first. The init container runs during the running of the application container.

When the two containers run at the same time, the argoexec file is operated. As a result, the open` /var/run/argo/argoexec: text file busy` exception occurs.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how to reproduce it, Same here. Sometimes it shows up, sometimes it doesn't.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.0", GitCommit:"ab69524f795c42094a6630298ff53f3c3ebab7f4", GitTreeState:"clean", BuildDate:"2021-12-07T18:16:20Z", GoVersion:"go1.17.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.10-r1.0.0-source-124-g05dc99ca321c86", GitCommit:"05dc99ca321c86aa539928891fcd3622eba7e965", GitTreeState:"clean", BuildDate:"2021-09-16T17:07:56Z", GoVersion:"go1.15.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME="Ubuntu"
VERSION="20.04.1 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.1 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=foca

$ uname -a
# paste output here
Linux pekphis94681 5.4.0-162-generic #179-Ubuntu SMP Mon Aug 14 08:51:31 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker 

Client:
 Version:           20.10.25
 API version:       1.41
 Go version:        go1.20.3
 Git commit:        20.10.25-0ubuntu1~20.04.2
 Built:             Thu Aug 10 20:14:50 2023
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.25
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.20.3
  Git commit:       20.10.25-0ubuntu1~20.04.2
  Built:            Thu Aug  3 18:03:37 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.7.2
  GitCommit:        
 runc:
  Version:          1.1.7-0ubuntu1~20.04.2
  GitCommit:        
 docker-init:
  Version:          0.19.0
  GitCommit:
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125579 Node reboot leaving existing pod using resources stuck with error UnexpectedAdmissionError 

- Issue 链接：[#125579](https://github.com/kubernetes/kubernetes/issues/125579)

### Issue 内容

#### What happened?

When a node is rebooted, pod using resources allocated by device plugin will encounter UnexpectedAdmissionError error as below:

```
  Warning  UnexpectedAdmissionError  84s                kubelet            Allocate failed due to no healthy devices present; cannot allocate unhealthy devices xxx, which is unexpected
```

What makes it really bad is if it's a raw pod, it stucks in such state and never recover.

#### What did you expect to happen?

The pod should be retried until device plugin is ready

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod requesting any resources allocated via a device plugin
2. reboot the node
3. observe pod failure


#### Anything else we need to know?

The behavior is introduced with #116376

And there are various issues opened around kubelet restart #118559 #124345

But this issue is about node restart. When node is restarted, kubelet started to rerun existing pods in random order. So a pod can run into this issue before device plugin pod is healthy on the node

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125568 When Deployment is editing replicas and strategy simultaneously, it may get stuck and not continue to execute

- Issue 链接：[#125568](https://github.com/kubernetes/kubernetes/issues/125568)

### Issue 内容

#### What happened?

When Deployment is editing replicas and strategy simultaneously, it may get stuck and not continue to execute

#### What did you expect to happen?

deployment continue upgrade or scaled

#### How can we reproduce it (as minimally and precisely as possible)?

1. created a deployment with a rolling upgrade and set a large number of replicas, such as 100. 
2. After creating 100 pods, use a slow rolling upgrade like RollingUpdateStrategy:  1 max unavailable, 1 max surge
 to make two active replicasets appear at the same time. 
3. While in the process of rolling upgrade (2 replicaSet), simultaneously edit the upgrade strategy of the deployment to recreate, and modify the number of replicas to 101. (kubectl apply -f xxx.yaml)
4. At this time, the deployment will be in a stuck state, and it will not be upgraded or scaled no matter how it is modified.

#### Anything else we need to know?

Deployment will get stuck because of these few lines of code.

https://github.com/kubernetes/kubernetes/blob/64355780d9d945a1135a004829ab9dcc38c3911d/pkg/controller/deployment/deployment_controller.go#L665C2-L671C3

#### Kubernetes version

<details>
v1.23.3
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125561 RuntimeHandlerResolver: interface invalid nil checking

- Issue 链接：[#125561](https://github.com/kubernetes/kubernetes/issues/125561)

### Issue 内容

#### What happened?

the nil checking for interface RuntimeHandlerResolver in #L109 will never check whether the underlying struct is nil. 
A reflect nil checking is needed to avoid panic in running 'rcManager.LookupRuntimeHandler'(#L111)
https://github.com/kubernetes/kubernetes/blob/a3a49887ee73fa1108adac97a797dec02ccb00d4/pkg/kubelet/kuberuntime/util/util.go#L107-L115

#### What did you expect to happen?

Add reflect nil checking for rcManager


#### How can we reproduce it (as minimally and precisely as possible)?

1. kubelet enable standaloneMode
2. pod.Spec.SecurityContext not nil

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125559 [FG:InPlacePodVerticalScaling] Pod Resize - resize stuck "InProgress" when only resizing memory requests

- Issue 链接：[#125559](https://github.com/kubernetes/kubernetes/issues/125559)

### Issue 内容

#### What happened?

InPlacePodVerticalScaling causes pods to get stuck in resizing "InProgress" when resizing solely memory requests for a QoS Burstable pod. The feature works for memory limits etc.

Example is variation from [here](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/) (see below)

#### What did you expect to happen?

InPlacePodVerticalScaling resizes pod inplace when only memory request is patched.

#### How can we reproduce it (as minimally and precisely as possible)?

Pod Spec: 
```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-5
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr-5
    image: nginx
    resizePolicy:
    - resourceName: cpu
      restartPolicy: RestartContainer
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        memory: "800Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
```

`kubectl apply -f podspec.yaml`
`kubectl -n qos-example patch pod qos-demo-5 --patch '{"spec":{"containers":[{"name":"qos-demo-ctr-5", "resources":{"requests":{"memory":"400Mi"}}}]}}'`

Output:

Initial PodInfo:

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"NotRequired"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:11:33Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "818"
  uid: f19d015b-6e38-42ae-b245-6936383e4102
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xpksb
      readOnly: true
  ...
status:
  conditions:
    ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 200Mi
    containerID: containerd://c4193e1640c9dc7b32c910cecb46b622d472c122a454f2fedbba15ef3932c1d5
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:11:34Z"
  ...
  qosClass: Burstable
  startTime: "2024-06-17T21:11:33Z"
```

After resize memory request (no restart):

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"NotRequired"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:11:33Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "941"
  uid: f19d015b-6e38-42ae-b245-6936383e4102
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 400Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xpksb
      readOnly: true
  ...
status:
  conditions:
    ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 400Mi
    containerID: containerd://c4193e1640c9dc7b32c910cecb46b622d472c122a454f2fedbba15ef3932c1d5
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:11:34Z"
  ...
  qosClass: Burstable
  resize: InProgress
  startTime: "2024-06-17T21:11:33Z"
```

After resize memory request (with restart):

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"RestartContainer"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:16:14Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "1230"
  uid: 80927d45-019e-4fe6-b7d4-7334a653e4b7
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: RestartContainer
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 400Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-pqrld
      readOnly: true
  ...
status:
  conditions:
  ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 400Mi
    containerID: containerd://98d8f353ff23df61c9a31a96f18f70d0566406125304908e92c256931a3f9707
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:16:15Z"
  ...
  qosClass: Burstable
  resize: InProgress
  startTime: "2024-06-17T21:16:14Z"
```

Strangely the Pod allocations are updated, but the containerStatus.resources are not, and the pod is not restarted.

#### Anything else we need to know?

Reported in #124712 

This differs from #124786 in that the memory request after resizing is large. Differs from the samples previously run by @esotsal as only the memory requests are changed and not the limits.

#### Kubernetes version

<details>
Tested on Kubernetes v1.30.1
</details>


#### Cloud provider

Using KinD to run on local WSL2 (ew) machine. Definitely need somebody to check this on proper Linux or otherwise.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125541 The PV may be in a Terminating state and cannot be deleted when the pod are created and then the pod and pvc are quickly force deleted .

- Issue 链接：[#125541](https://github.com/kubernetes/kubernetes/issues/125541)

### Issue 内容

#### What happened?

In my testing environment, I discovered a strange phenomenon: when I create a pod that uses a PVC and then forcefully delete the pod and PVC before the pod creation is complete, the PV bound to the PVC **may** enter a Terminating state and cannot be deleted.

The log is as follows
```
I0614 10:04:56.153700   77153 kubelet.go:2389] "SyncLoop ADD" source="api" pods=[default/hp-volume-dbwfm]

I0614 10:04:58.233778   77153 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/XXXX^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") " pod="default/hp-volume-dbwfm"
I0614 10:04:58.234344   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/eac1a6c0-bdc4-4b75-804f-7f89df07694b\"" pod="default/hp-volume-dbwfm"
I0614 10:04:58.308685   77153 operation_generator.go:1111] "MapVolume.WaitForAttach succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"csi-75dac338e0aff53508bd6995ba2aa58d65d6eb42310c3d9d7111f2720a1d2823\"" pod="default/hp-volume-dbwfm"

I0614 10:05:02.444088   77153 kubelet.go:2405] "SyncLoop DELETE" source="api" pods=[default/hp-volume-dbwfm]
I0614 10:05:02.520860   77153 kubelet.go:2399] "SyncLoop REMOVE" source="api" pods=[default/hp-volume-dbwfm]

I0614 10:05:17.729528   77153 operation_generator.go:1225] "MapVolume.MapPodDevice succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") volumeMapPath \"/var/lib/kubelet/pods/42077f53-c1df-4b6f-8647-f06100647095/volumeDevices/kubernetes.io~csi\"" pod="default/hp-volume-dbwfm"
I0614 10:05:17.734612   77153 operation_generator.go:1233] MapVolume.NodeExpandVolume failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : mountVolume.NodeExpandVolume get PVC failed : persistentvolumeclaims "pvc-volume-251e86b0-cfe7-475b-88f4-cced2416d3ff" is forbidden: User "system:node:worker3" cannot get resource "persistentvolumeclaims" in API group "" in the namespace "default": no relationship found between node 'worker3' and this object
I0614 10:05:17.734835   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:05:18.734806235 +0800 CST m=+62756.584763139 (durationBeforeRetry 1s). Error: MapVolume.MarkVolumeAsMounted failed while expanding volume for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : mountVolume.NodeExpandVolume get PVC failed : persistentvolumeclaims "pvc-volume-251e86b0-cfe7-475b-88f4-cced2416d3ff" is forbidden: User "system:node:worker3" cannot get resource "persistentvolumeclaims" in API group "" in the namespace "default": no relationship found between node 'worker3' and this object
I0614 10:05:18.795992   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-r5sp7\" (UID: \"eac1a6c0-bdc4-4b75-804f-7f89df07694b\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/42077f53-c1df-4b6f-8647-f06100647095\"" pod="default/hp-volume-r5sp7"
I0614 10:06:03.681110   77153 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") " pod="default/hp-volume-dbwfm"
I0614 10:06:03.681366   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/7fb78f1b-06f4-4b95-8514-a80de1f0294d\"" pod="default/hp-volume-dbwfm"
I0614 10:06:03.726906   77153 operation_generator.go:1111] "MapVolume.WaitForAttach succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"csi-75dac338e0aff53508bd6995ba2aa58d65d6eb42310c3d9d7111f2720a1d2823\"" pod="default/hp-volume-dbwfm"
I0614 10:06:15.315317   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:15.815290561 +0800 CST m=+62813.665247389 (durationBeforeRetry 500ms). Error: MapVolume.MapPodDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : rpc error: code = Internal desc = failed to check client lun mapping status for volume pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820
I0614 10:06:16.176459   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:16.676425647 +0800 CST m=+62814.526382436 (durationBeforeRetry 500ms). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]
I0614 10:06:16.696898   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:17.696865394 +0800 CST m=+62815.546822186 (durationBeforeRetry 1s). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]
I0614 10:06:17.742271   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:19.742240671 +0800 CST m=+62817.592197458 (durationBeforeRetry 2s). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]

```

kubectl get pv pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820
```
pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820   2Gi       RWX            Delete           Terminating   default/pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820  csi-XXX            1h
```

 I drawn up a time-series chart for the log contents .

![image](https://github.com/kubernetes/kubernetes/assets/8870947/7f97ebcb-b963-4c02-9cce-bcc5b040a558)


#### What did you expect to happen?

the PV can be delete successfully

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pvc , the bound pv's persistentVolumeReclaimPolicy is **Delete**;
2. create a pod use the pvc;
3. force delete the pod and the pvc（kubectl delete po xx --force && kubectl delete pvc yy --force ）
4. It is possible that the PV may be in a Terminating state and cannot be deleted

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.28

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125530 kube-controller-manager Master Election Time Exceeds the Lease Time

- Issue 链接：[#125530](https://github.com/kubernetes/kubernetes/issues/125530)

### Issue 内容

#### What happened?

First, the configuration of controller-manager election is as follows:
`--leader-elect-lease-duration=25s
 --leader-elect-renew-deadline=20s`
The active controller node is master2. After the master2 node is powered off:
The controller-manager master election request of the master1 node fails because the persistent connection of http2 is not disconnected for about 30s.
After the connection is restored, it takes about 30 seconds to select the active node.
![image](https://github.com/kubernetes/kubernetes/assets/54977497/dfa6b745-6b4b-4ffb-8d43-de57c52f68ac)
![image](https://github.com/kubernetes/kubernetes/assets/54977497/649db3ef-aa87-475b-8e32-236eb864ba6a)
![image](https://github.com/kubernetes/kubernetes/assets/54977497/fefd0dd0-f8d6-449e-808c-8b475ab80782)


#### What did you expect to happen?

The owner can complete the selection according to the lease time.

#### How can we reproduce it (as minimally and precisely as possible)?

Power on and off the active kube-controller-manager node. There is a low probability that this problem recurs.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及安全风险。

---

## Issue #125526 AppArmor Profile not activated. #2310

- Issue 链接：[#125526](https://github.com/kubernetes/kubernetes/issues/125526)

### Issue 内容

#### What happened?

I installed SPO and followed the documentation regarding an example installation of an AppArmor Profile. I am running Kubernetes 1.30.0. If I use the securityContext clause, it has no effect. Even more, after Pod creation, its content is deleted. If I use the deprecated annotation, I get an error telling me The Pod "testpod2" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: Invalid value: test-profile: invalid AppArmor profile name: test-profile

#### What did you expect to happen?

I'd expect, after preparing everything by the book to have a Pod running with an AppArmor Profile.



#### How can we reproduce it (as minimally and precisely as possible)?

I've installed SPO via OLM:
```
---
apiVersion: v1
kind: Namespace
metadata:
  name: security-profiles-operator
  labels:
    openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: security-profiles-operator
  namespace: security-profiles-operator
spec:
  targetNamespaces:
  - security-profiles-operator
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: security-profiles-operator-sub
  namespace: security-profiles-operator
spec:
  channel: stable
  name: security-profiles-operator
  source: operatorhubio-catalog
  sourceNamespace: olm
```

I then applied the patch and created an example Profile, as documented in: https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/installation-usage.md#create-an-apparmor-profile

I can verify that up to this point, all is fine:

```
$ k -n security-profiles-operator get spod spod -o yaml

apiVersion: security-profiles-operator.x-k8s.io/v1alpha1
kind: SecurityProfilesOperatorDaemon
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"security-profiles-operator.x-k8s.io/v1alpha1","kind":"SecurityProfilesOperatorDaemon","metadata":{"annotations":{},"name":"spod","namespace":"security-profiles-operator"},"spec":{"enableAppArmor":true,"enableLogEnricher":false,"enableSelinux":false}}
  creationTimestamp: "2024-06-13T18:34:41Z"
  generation: 4
  labels:
    app: security-profiles-operator
  name: spod
  namespace: security-profiles-operator
  resourceVersion: "5738208"
  uid: b0427364-278a-43a9-bfeb-1b5cfa1ead63
spec:
  disableOciArtifactSignatureVerification: false
  enableAppArmor: true
  enableLogEnricher: false
  enableSelinux: false
  hostProcVolumePath: /proc
  priorityClassName: system-node-critical
  selinuxOptions:
    allowedSystemProfiles:
    - container
  selinuxTypeTag: spc_t
  staticWebhookConfig: false
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
    operator: Exists
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
status:
  conditions:
  - lastTransitionTime: "2024-06-13T23:08:28Z"
    reason: Available
    status: "True"
    type: Ready
  state: RUNNING


$ k get securityprofilenodestatuses.security-profiles-operator.x-k8s.io

NAME                           STATUS      AGE
test-profile-master0-k8s.lan   Installed   45h
test-profile-master1-k8s.lan   Installed   45h
test-profile-master2-k8s.lan   Installed   45h
test-profile-worker1-k8s.lan   Installed   45h
test-profile-worker2-k8s.lan   Installed   45h
test-profile-worker3-k8s.lan   Installed   45h
test-profile-worker4-k8s.lan   Installed   45h


$ k get apparmorprofiles.security-profiles-operator.x-k8s.io test-profile -o yaml

apiVersion: security-profiles-operator.x-k8s.io/v1alpha1
kind: AppArmorProfile
metadata:
  annotations:
    description: Block writing to any files in the disk.
  creationTimestamp: "2024-06-13T20:35:30Z"
  finalizers:
  - worker4-k8s.lan-deleted
  - master1-k8s.lan-deleted
  - master0-k8s.lan-deleted
  - worker2-k8s.lan-deleted
  - worker3-k8s.lan-deleted
  - master2-k8s.lan-deleted
  - worker1-k8s.lan-deleted
  generation: 1
  labels:
    spo.x-k8s.io/profile-id: AppArmorProfile-test-profile
  name: test-profile
  namespace: default
  resourceVersion: "5720939"
  uid: ea8c3705-1ba2-4b14-afc5-d0a05aa958fc
spec:
  policy: |
    #include <tunables/global>

    profile test-profile flags=(attach_disconnected) {
      #include <abstractions/base>

      file,

      # Deny all file writes.
      deny /** w,
    }
```

Here is my simple Pod yaml used in a first test:

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: testpod2
  name: testpod2
  annotations:
    container.apparmor.security.beta.kubernetes.io/testpod2: test-profile
spec:
# securityContext:
#   appArmorProfile:
#     type: Localhost
#     localhostProfile: test-profile
  containers:
  - image: nginx
    name: testpod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
```

When I would like to create this Pod, it looks like this:

```
Warning: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: deprecated since v1.30; use the "appArmorProfile" field instead
The Pod "testpod2" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: Invalid value: "test-profile": invalid AppArmor profile name: "test-profile"
```

If I remove the annotation and uncomment the securityContext, the Pod will be created, but no AppArmor Profile is active. And if I check the deployed Pod, it looks like this then:

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-06-15T19:41:11Z"
  labels:
    run: testpod2
  name: testpod2
  namespace: default
  resourceVersion: "5759727"
  uid: 67ec29a1-c224-4832-9c1a-0c79f71d8aa4
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: testpod2
    resources: {}
    securityContext: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xqpz2
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: worker4-k8s.lan
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-xqpz2
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:11Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:11Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://c48853001023f76e93ece437368807343209a3711706e3a16a52c352cbac2f73
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:0f04e4f646a3f14bf31d8bc8d885b6c951fdcf42589d06845f64d18aec6a3c4d
    lastState: {}
    name: testpod2
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-15T19:41:13Z"
  hostIP: 192.168.121.153
  hostIPs:
  - ip: 192.168.121.153
  phase: Running
  podIP: 10.0.2.208
  podIPs:
  - ip: 10.0.2.208
  qosClass: BestEffort
  startTime: "2024-06-15T19:41:11Z"
```


#### Anything else we need to know?



Here's what I get, when I check for AppArmor from withing the Pod:

```
$ kubectl exec testpod2 -- cat /proc/1/attr/current

crio-default (enforce)
```

And here's what I see regarding loaded Profiles on each Node:

```
$ sudo cat /sys/kernel/security/apparmor/profiles

test-profile (enforce)
crio-default (enforce)
/{,usr/}sbin/dhclient (enforce)
/usr/lib/connman/scripts/dhclient-script (enforce)
/usr/lib/NetworkManager/nm-dhcp-helper (enforce)
/usr/lib/NetworkManager/nm-dhcp-client.action (enforce)
/usr/sbin/chronyd (enforce)
nvidia_modprobe (enforce)
nvidia_modprobe//kmod (enforce)
man_groff (enforce)
man_filter (enforce)
/usr/bin/man (enforce)
lsb_release (enforce)
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

```

</details>


#### Cloud provider

<details>
Bare Metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux worker3-k8s.lan 6.1.0-15-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.66-1 (2023-12-09) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
Version:  0.1.0
RuntimeName:  cri-o
RuntimeVersion:  1.30.0
RuntimeApiVersion:  v1
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125522 ValidatingAdmissionPolicy: auditAnnotations are included in the audit event always

- Issue 链接：[#125522](https://github.com/kubernetes/kubernetes/issues/125522)

### Issue 内容

#### What happened?

When an audit annotation is defined in a `Validating Admission Policy` (VAP), this annotation is added to the api-server audit event always.

#### What did you expect to happen?

The audit annotation is only included to the audit event in case any of the VAP validations expressions evaluates to false, in oder words, in case the VAP is violated.

> The documentation is kind of confusing in that regard, because [here](https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/#audit-annotations) it says: _In this example the annotation will only be included if the `spec.replicas` of the Deployment is more than 50, otherwise the CEL expression evaluates to null and the annotation will not be included_. But if in fact `spec.replicas` is more than 50, it means that the VAP is met, because the CEL expression evaluates to true.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a VAP and binding that prevents creating pods in `default` namespace, like this:

> **Edited:** I had a typo in the  `spec.auditAnnotations[0].key`, correct value is validation_failed, and not validation_failure as I initially posted.

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "prevent-default-namespace"
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   ["*"]
      apiVersions: ["*"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["pods"]
  validations:
    - expression: "object.metadata.namespace != 'default'"
      message: "Pods should not be created in the default namespace"
  auditAnnotations:
    - key: "validation_failed"
      valueExpression: "'true'"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "prevent-default-namespace"
spec:
  policyName: "prevent-default-namespace"
  validationActions: [Warn, Audit]
```

This policy expression will validate to `false` for all pods created in `default` namespace and to `true` for all pods created in any other pod.

Now create two pods, one of them in `default` namespace:
```bash
❯ kubectl run my-pod-10 --image=curlimages/curl:8.6.0 --command -- sleep 1000
Warning: Validation failed for ValidatingAdmissionPolicy 'prevent-default-namespace' with binding 'prevent-default-namespace': Pods should not be created in the default namespace. 
pod/my-pod-10 created

❯ kubectl run my-pod-20 -n testing --image=curlimages/curl:8.6.0 --command -- sleep 1000
pod/my-pod-20 created
```

If we check the api-server audit event corresponding to the creation of each pod, we can see that the audit annotation `"prevent-default-namespace/validation_failed":"true"` has been set in both cases:

```bash
❯ talosctl read /var/log/audit/kube/kube-apiserver.log | grep "my-pod-10" | grep "validation_failed"
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"7703d71a-00e8-4b81-9fb2-d08fbd136fc5","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/default/pods?fieldManager=kubectl-run","verb":"create","user":{"username":"admin","groups":["system:masters","system:authenticated"]},"sourceIPs":["192.168.65.1"],"userAgent":"kubectl/v1.28.0 (darwin/arm64) kubernetes/855e7c4","objectRef":{"resource":"pods","namespace":"default","name":"my-pod-10","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":201},"requestReceivedTimestamp":"2024-06-15T15:14:40.823489Z","stageTimestamp":"2024-06-15T15:14:40.830217Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"","mutation.webhook.admission.k8s.io/round_0_index_1":"{\"configuration\":\"gatekeeper-mutating-webhook-configuration\",\"webhook\":\"mutation.gatekeeper.sh\",\"mutated\":false}","mutation.webhook.admission.k8s.io/round_0_index_4":"{\"configuration\":\"opentelemetry-operator-mutation\",\"webhook\":\"mpod.kb.io\",\"mutated\":false}","pod-security.kubernetes.io/enforce-policy":"privileged:latest","prevent-default-namespace/validation_failed":"true","validation.policy.admission.k8s.io/validation_failure":"[{\"message\":\"Pods should not be created in the default namespace\",\"policy\":\"prevent-default-namespace\",\"binding\":\"prevent-default-namespace\",\"expressionIndex\":0,\"validationActions\":[\"Warn\",\"Audit\"]}]"}}

❯ talosctl read /var/log/audit/kube/kube-apiserver.log | grep "my-pod-20" | grep "validation_failed"
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"7ba57b1d-6cbd-489d-bef1-709f337b9eda","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/testing/pods?fieldManager=kubectl-run","verb":"create","user":{"username":"admin","groups":["system:masters","system:authenticated"]},"sourceIPs":["192.168.65.1"],"userAgent":"kubectl/v1.28.0 (darwin/arm64) kubernetes/855e7c4","objectRef":{"resource":"pods","namespace":"testing","name":"my-pod-20","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":201},"requestReceivedTimestamp":"2024-06-15T15:15:49.607569Z","stageTimestamp":"2024-06-15T15:15:49.614769Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"","mutation.webhook.admission.k8s.io/round_0_index_1":"{\"configuration\":\"gatekeeper-mutating-webhook-configuration\",\"webhook\":\"mutation.gatekeeper.sh\",\"mutated\":false}","mutation.webhook.admission.k8s.io/round_0_index_4":"{\"configuration\":\"opentelemetry-operator-mutation\",\"webhook\":\"mpod.kb.io\",\"mutated\":false}","pod-security.kubernetes.io/enforce-policy":"privileged:latest","prevent-default-namespace/validation_failed":"true"}}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

I'm using in this case Talos in Docker, but it should not be relevant in this case:

```console
❯ talosctl read /etc/os-release
NAME="Talos"
ID=talos
VERSION_ID=v1.7.2
PRETTY_NAME="Talos (v1.7.2)"
HOME_URL="https://www.talos.dev/"
BUG_REPORT_URL="https://github.com/siderolabs/talos/issues"
VENDOR_NAME="Sidero Labs"
VENDOR_URL="https://www.siderolabs.com/"

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125506 /var/lib/kubelet/device-plugins/kubelet.sock Connection refused

- Issue 链接：[#125506](https://github.com/kubernetes/kubernetes/issues/125506)

### Issue 内容

#### What happened?

The service pod needs to register the device plug-in. Invoking the kubelet registration interface times out. The kubelet log does not contain error information. The OS environment is normal.When we look at the code, we find that the s.grpc.Serve (ln) method does not handle the returned err information. Is it possible that this is the cause?
https://github.com/kubernetes/kubernetes/blob/eb6840928df59bf8203b1eda839ccd3da68fb37d/pkg/kubelet/cm/devicemanager/plugin/v1beta1/server.go#L110-L113

#### What did you expect to happen?

The connection to kubelet through kubelet.sock is normal.

#### How can we reproduce it (as minimally and precisely as possible)?

The problem occurs when the OS is restarted.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125505 Suppress all logs and only see errors. Verbosity=0 does not help

- Issue 链接：[#125505](https://github.com/kubernetes/kubernetes/issues/125505)

### Issue 内容

#### What happened?

Setting verbosity to 0 does not disable all logs. 

#### What did you expect to happen?

Setting verbosity to the lowest level should only show fatal error messages or there should be some other setting that the user can set to suppress all logs and see only error logs.

#### How can we reproduce it (as minimally and precisely as possible)?

Create kind cluster with verbosity of k8s components set to 0.
example config:
```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        v: "0"
    extraArgs:
      kube-apiserver:
        v: "0"
      kube-controller-manager:
        v: "0"
      kube-scheduler:
        v: "0"
- role: worker
  kubeadmConfigPatches:
  - |
    kind: JoinConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        v: "0"
```
Get logs from any of k8s components and you will see info logs.

#### Anything else we need to know?

this is needed for optimizing the CPU usage in resource intensive environments.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647", GitTreeState:"clean", BuildDate:"2023-05-17T14:20:07Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-15T00:38:14Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
Kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125503 The cluster kube-scheduler scheduling is unbalanced, causing the pod to hang and fail to run, even though there are currently idle nodes

- Issue 链接：[#125503](https://github.com/kubernetes/kubernetes/issues/125503)

### Issue 内容

#### What happened?

I have 10 nodes in my cluster, and I keep the default cluster scheduling mechanism without configuring the manual scheduling mechanism for tasks (including node taints, label scheduling affinity, etc., and the configuration of pod limit request), but I found that one of my pods will use up 50% of the node's memory after running for a period of time, and the total memory occupied by other nodes of this node is more than 90%. But when I restarted this pod, I found the first problem, it was still scheduled to this node, although there were other nodes with only 30% memory usage, and the second problem was that I found that the pod seemed to like this node very much, and other newly started pods would also be assigned to this 90% load machine, resulting in these pod tasks can only be suspended, can not run, resulting in business stagnation. Why is this?

Off topic: I understand that you can use artificial scheduling mechanism to solve this problem, but the default scheduling I found from the official website's documentation explanation is to score nodes, and there is no additional mechanism to affect other I am very curious about this problem, and I found that many people in the k8s community have encountered this problem.

#### What did you expect to happen?

I want to know two questions about the above:
1. Regarding the node score, even though the load of this node is 90%, why is its score still higher than other nodes or is it scheduled to this machine for other reasons?
2. Why do subsequent pods like to run on this node so much, when there are other nodes and their models and configurations are exactly the same? Is there any data affinity (pod will look for the node where the data was stored last time to reduce the cluster's response time and performance consumption)?

#### How can we reproduce it (as minimally and precisely as possible)?

This is a long-term practical problem

#### Anything else we need to know?

![187d5c530b74963a020687139c0cc24](https://github.com/kubernetes/kubernetes/assets/66284983/7d3996d3-3523-47d2-b58f-a04529838206)
![8612d5413d9fd36f5052b6b1ea96936](https://github.com/kubernetes/kubernetes/assets/66284983/cbd2348e-aaac-41ab-8f34-f0dfec06f59f)


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
v1.26.1

#### Cloud provider

<details>

</details>
aws and tencent k8s cluster

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
centos7

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>
kubeadm

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125491 Scheduler pre-binding can cause race conditions with automated empty node removal

- Issue 链接：[#125491](https://github.com/kubernetes/kubernetes/issues/125491)

### Issue 内容

#### What happened?

In a Google Kubernetes Engine (GKE) environment, a pod was requesting a large Persistent Volume Claim (PVC). After the appropriate node was identified for the pod, the pod became stuck in the prebinding stage for several minutes while the volume provisioning process completed.  Since the node name was not assigned to the pod during this time, the Cluster Autoscaler perceived the node as unoccupied. Consequently, the Cluster Autoscaler initiated a scale-down of the node, unaware that the pending pod was scheduled to run there.

#### What did you expect to happen?

I would expect that the Scheduler would communicate the intended binding of the pod to the identified node. This would enable the Cluster Autoscaler to recognize that the node is not actually empty and prevent it from being scaled down prematurely.

#### How can we reproduce it (as minimally and precisely as possible)?

The issue arose in a large GKE cluster with pods requesting substantial PVCs, making replication potentially challenging. However, the race condition within the Scheduler is evident.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.27
</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及。

---

## Issue #125476 Can k8s restrict kubelet from using kmem through configuration

- Issue 链接：[#125476](https://github.com/kubernetes/kubernetes/issues/125476)

### Issue 内容

#### What happened?

Can k8s restrict kubelet from using kmem through configuration

#### What did you expect to happen?

Which specific version is supported

#### How can we reproduce it (as minimally and precisely as possible)?

Which specific version is supported

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125471 Race between seeing a CRD added event and being able to select the kind

- Issue 链接：[#125471](https://github.com/kubernetes/kubernetes/issues/125471)

### Issue 内容

#### What happened?

We have observed a situation that looks to be a race condition whereby we have a watch on `CustomResourceDefinitions` and when we see an `added` event for a new one, we perform a list on the `apiVersion` + `kind`. When this runs on a pod however we get a 404 when performing the list operation unless we add an arbitrary delay. 

_The context behind this is that we use this as a check to make sure a kind exists before performing other actions, such as creating a watch. We do not expect any resources to be present immediately after adding a CRD._





#### What did you expect to happen?

We would expect that we would not see an added event for a custom resource before the API server is able to serve the resource.

#### How can we reproduce it (as minimally and precisely as possible)?

It's hard as it's a bit racey, this is only reproducible from a pod and even then, not always.

1. Create a watch for `CustomResourceDefinitions`
2. Add a handler so when an `ADDED` event is seen it will perform a get for the kind e.g. `GET: /apis/GROUP/VERSION/KIND`
3. Add a new custom resource

If the event is received fast enough, the handler will see a 404 for it's GET call.

#### Anything else we need to know?

We are running a single master, so there is only one `etcd`

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8-gke.1095000
```

</details>


#### Cloud provider

<details>
GKE: v1.28.8-gke.1095000
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125467 Conntrack tables having stale entries for UDP connection

- Issue 链接：[#125467](https://github.com/kubernetes/kubernetes/issues/125467)

### Issue 内容

#### What happened?

We experienced an EC2 node failure within our EKS cluster. This affected node was running two CoreDNS pods, which are responsible for DNS resolution in our Kubernetes cluster. Envoy connects to CoreDNS through the UDP protocol. After these CoreDNS pods were terminated, Envoy continued to attempt connections to the terminated IP for DNS resolution.
The kube-proxy failed to update the entry in the conntrack tables, causing some Envoy pods to still connect to the terminated CoreDNS pod IP. Once we restarted the Envoy pods, the entry was refreshed, and the DNS timeout issue was resolved.

Mapping in Conntrack table for src pod ip-10.103.83.53 for UDP protocol.

Query : “conntrack -p udp -L --src 10.103.83.53”

Response : “udp  17 27 src=10.103.83.53 dst=<clusterIp> sport=21667 dport=53 [UNREPLIED] src=10.103.78.37 dst=10.103.83.53 sport=53 dport=21667 mark=0 use=1 contrack v1.4.4 (conntrack-tools): 1 flow entries have been shown”

#### What did you expect to happen?

Kubeproxy should update or refresh this conntrack table. 
conntrack shouldn't have stale UDP connection routes.

#### How can we reproduce it (as minimally and precisely as possible)?

KubeProxy version we tested with - kube-proxy:v1.29.4-minimal-eksbuild.1

which include this fix as well -  https://github.com/kubernetes/kubernetes/issues/119249

Steps we followed in our EKS cluster to stimulate this issue

- Remove podAntiAffinity on the coreDNS deployment
- Identify the node where we would want to concentrate the coreDNS to cordon it
- Evict workloads from that node
- Make that for not getting scaled down using node annotations
- Uncordon that node, and cordon the rest of the node
- Delete two coreDNS pods
- Ensure they get scheduled to that targeted node
- Login to that node, install tmux, and down all the network interfaces to simulate the 2/2 failure
- Notice the node going into NotReady state, and check for the DNS c-ares errors

#### Anything else we need to know?



#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.4-eks-036c24b
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
      NAME="Amazon Linux"
      VERSION="2"
      ID="amzn"
      ID_LIKE="centos rhel fedora"
      VERSION_ID="2"
      PRETTY_NAME="Amazon Linux 2"
      ANSI_COLOR="0;33"
      CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
      HOME_URL="https://amazonlinux.com/"
      SUPPORT_END="2025-06-30"
$ uname -a
Linux ip-10-185-97-105.ec2.internal 5.10.215-203.850.amzn2.aarch64 #1 SMP Tue Apr 23 20:32:21 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125453 kubelet unbalanced affinity pod in different numa node

- Issue 链接：[#125453](https://github.com/kubernetes/kubernetes/issues/125453)

### Issue 内容

#### What happened?

in current kubelet below algorithm, the pods/containers shall be affinity to different numa node unbalanced. 

for example: if there is 6 pods with integer cpu container configured, most of pods will be scheduled into numa0 if numa0 has enough available cpus (4 pods in numa0 and 2 pod in numa1). 

then if turboboost is enabled with OS control or static max performance, the cpu frequency will be different in callload running. for example cpus in numa0 maybe 2500 but cpus in numa1 maybe 2900 since workload is different in two numa nodes. 

in this environment, even the incoming traffic to those pods which are scheduled in numa0 and numa1 is same (round robin), the cpu usage in those pods are different. that will cause issue which traffic load can not be increased to expected values since 4 pods in numa0 is already with high cpu usage (call maybe starting drop)  but 2 pods in numa1 with low cpu usage.

       // Algorithm: topology-aware best-fit
        // 1. Acquire whole NUMA nodes and sockets, if available and the container
        //    requires at least a NUMA node or socket's-worth of CPUs. If NUMA
        //    Nodes map to 1 or more sockets, pull from NUMA nodes first.
        //    Otherwise pull from sockets first.
        acc.numaOrSocketsFirst.takeFullFirstLevel()
        if acc.isSatisfied() {
                return acc.result, nil
        }
        acc.numaOrSocketsFirst.takeFullSecondLevel()
        if acc.isSatisfied() {
                return acc.result, nil
        }

        // 2. Acquire whole cores, if available and the container requires at least
        //    a core's-worth of CPUs.
        acc.takeFullCores()
        if acc.isSatisfied() {
                return acc.result, nil
        }

        // 3. Acquire single threads, preferring to fill partially-allocated cores
        //    on the same sockets as the whole cores we have already taken in this
        //    allocation.
        acc.takeRemainingCPUs()
        if acc.isSatisfied() {
                return acc.result, nil
        }


#### What did you expect to happen?

affinity pod/container balanced to different numa instead of filling numa0 first and then numa1

#### How can we reproduce it (as minimally and precisely as possible)?

1. turboboost enabled
2. cpuManagePolicy as static 
3. Pod with GQoS and Container with integer cpu request/limit

#### Anything else we need to know?

_No response_

#### Kubernetes version

test version is 1.27


#### Cloud provider

any cloud provider


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125448 Incorrect error reporting in case of missing cgroup controllers

- Issue 链接：[#125448](https://github.com/kubernetes/kubernetes/issues/125448)

### Issue 内容

#### What happened?

If the cgroup validation fails due to missing required controllers (e.g. https://github.com/kubernetes/kubernetes/issues/122955) the error that surfaces is, 
```bash
Jun 11 20:22:16 ip-10-0-2-54 kubenswrapper[2176]: E0611 20:22:16.903259    2176 kubelet.go:1559] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubepods] doesn't exist"
```
This is incorrect. The code at, https://github.com/kubernetes/kubernetes/blob/d593c886b1fd4119204b8e5a5fc012e36f42fd7a/pkg/kubelet/cm/qos_container_manager_linux.go#L85 

calls https://github.com/kubernetes/kubernetes/blob/d593c886b1fd4119204b8e5a5fc012e36f42fd7a/pkg/kubelet/cm/cgroup_manager_linux.go#L239 but ignores the error. 

This could be quite misleading. In case of https://github.com/kubernetes/kubernetes/issues/122955, the kubelet didn't fail to start because of missing `kubepods` cgroup, but rather due to missing `cpuset`. 

The correct reported error should have been, 

```bash
 Jun 11 20:30:33 ip-10-0-2-54 kubenswrapper[2178]: E0611 20:30:33.085077    2178 kubelet.go:1559] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: error validating root container [kubepods] : cgroup [\"kubepods\"] has some missing controllers: cpuset"
 ```

#### What did you expect to happen?

Correct error to propagate when cgroup validation fails during kubelet startup. 

#### How can we reproduce it (as minimally and precisely as possible)?

Everytime kubelet fails to start due to cgroup validation. e.g https://github.com/kubernetes/kubernetes/issues/122955

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125427 When a deployment selects a node with the kubelet service not running as the nodeName, the Pods will remain in the pending state, then move to Terminating, and new Pods will be continuously created in a loop, resulting in a large number of Terminating Pods that cannot be terminated.

- Issue 链接：[#125427](https://github.com/kubernetes/kubernetes/issues/125427)

### Issue 内容

#### What happened?

When a deployment selects a node with the kubelet service not running as the nodeName, the Pods will remain in the pending state, then move to Terminating, and new Pods will be continuously created in a loop, resulting in a large number of Terminating Pods that cannot be terminated.

#### What did you expect to happen?

Waiting for the node to be ready, the Pods remain in the pending state.







#### How can we reproduce it (as minimally and precisely as possible)?

1：create app
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      nodeName: host3.example.com

2： disable kubelet
systemctl disable kubelet

3：host3.example.com 
reboot

#### Anything else we need to know?

_No response_

#### Kubernetes version

[root@host1 ~]#  kubectl version --output=yaml
clientVersion:
  buildDate: "2023-10-18T11:44:31Z"
  compiler: gc
  gitCommit: b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99
  gitTreeState: clean
  gitVersion: v1.26.10
  goVersion: go1.20.10
  major: "1"
  minor: "26"
  platform: linux/amd64
kustomizeVersion: v4.5.7
serverVersion:
  buildDate: "2023-10-18T11:33:36Z"
  compiler: gc
  gitCommit: b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99
  gitTreeState: clean
  gitVersion: v1.26.10
  goVersion: go1.20.10
  major: "1"
  minor: "26"
  platform: linux/amd64

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125412 kubectl port-forward failing for named ports in native sidecar

- Issue 链接：[#125412](https://github.com/kubernetes/kubernetes/issues/125412)

### Issue 内容

#### What happened?

```
kubectl port-forward svc/adguard-metrics metrics (named port on service)
error: Pod 'adguard-primary-7cc5d498f4-67zs4' does not have a named port 'metrics'

kubectl port-forward adguard-primary-7cc5d498f4-67zs4 metrics (named port on native sidecar)
error: Pod 'adguard-primary-7cc5d498f4-67zs4' does not have a named port 'metrics'
```

#### What did you expect to happen?

Since native sidecards support named ports, named ports on native sidecar should be supported in kubectl port-forward.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod/deployment with a named port. Example:

<details>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    app.kubernetes.io/name: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: app
  template:
    metadata:
      labels:
        app.kubernetes.io/name: app
    spec:
      initContainers:
        - name: nginx
          image: nginx
          restartPolicy: Always
          ports:
            - name: metrics
              protocol: TCP
              containerPort: 80
          livenessProbe:
            httpGet:
              port: metrics
              path: /metrics
            failureThreshold: 5
            periodSeconds: 10
          readinessProbe:
            httpGet:
              port: metrics
              path: /metrics
            failureThreshold: 5
            periodSeconds: 10
      containers:
        - name: busybox
          image: busybox
          command: ['sh', '-c', 'echo Hello Kubernetes! && sleep infinity']
```

</details>

Port forward with named port does not work:
```
kubectl port-forward app-<pod-id> 8080:metrics
error: Pod 'app-7c74c5d9db-bzmqb' does not have a named port 'metrics'
```

Port forward with port number works:
`kubectl port-forward app-<pod-id> 8080:80`
```
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
```

Same deal when port-forwarding through deployment or service:

```
kubectl port-forward deploy/app 8080:metrics
error: Pod 'app-645b84f678-tjh8j' does not have a named port 'metrics'
```

```
kubectl port-forward svc/app 8080:metrics
error: Pod 'app-645b84f678-tjh8j' does not have a named port 'metrics'
```

#### Anything else we need to know?

@aojea ping as requested

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
Talos Linux in VMs on ProxMox
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Talos"
ID=talos
VERSION_ID=v1.7.1
PRETTY_NAME="Talos (v1.7.1)"
HOME_URL="https://www.talos.dev/"
BUG_REPORT_URL="https://github.com/siderolabs/talos/issues"
VENDOR_NAME="Sidero Labs"
VENDOR_URL="https://www.siderolabs.com/"
$ uname -a
Linux version 6.6.29-talos (@buildkitsandbox) (gcc (GCC) 13.2.0, GNU ld (GNU Binutils) 2.42) #1 SMP Tue Apr 30 14:19:14 UTC 2024
```

</details>


#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI: Cilium v1.15.5
</details>


### 分析结果

不涉及

---

## Issue #125410 Job may get stuck repeatedly failing with Duplicate value message for uncountedTerminatedPods.failed

- Issue 链接：[#125410](https://github.com/kubernetes/kubernetes/issues/125410)

### Issue 内容

#### What happened?

We have encountered a situation when a Job controller is stuck constantly failing on the `syncJob`. It fails with the validation message 
like this `job_controller.go:600] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Job.batch \"pi\" is invalid: status.uncountedTerminatedPods.failed[0]: Duplicate value: \"ca8e72a0-735a-45c9-925d-d82c91256b86\"" logger="UnhandledError"`.

The Job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  generation: 1
  namespace: default
spec:
  backoffLimit: 6
  completionMode: NonIndexed
  completions: 1
  parallelism: 1
  suspend: false
  ...
status:
  failed: 1
  ready: 0
  ...
  uncountedTerminatedPods:
    succeeded:
    - xyz
  ```
  Pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  finalizers:
  - batch.kubernetes.io/job-tracking
  ...
  uid: xyz
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - ...
        weight: 100
...
status:
  conditions:
  - message: 'Taint manager: deleting due to NoExecute taint'
    reason: DeletionByTaintManager
    status: "True"
    type: DisruptionTarget
  - status: "True"
    type: Initialized
  - reason: PodFailed
    status: "False"
    type: Ready
  - reason: PodFailed
    status: "False"
    type: ContainersReady
  - status: "True"
    type: PodScheduled
  containerStatuses:
  - state:
      terminated:
        exitCode: 137
        finishedAt: null
        message: The container could not be located when the pod was terminated
        reason: ContainerStatusUnknown
        startedAt: null
  message: 'Pod was rejected: Predicate NodeAffinity failed'
  phase: Failed
  qosClass: Guaranteed
  reason: NodeAffinity

```

#### What did you expect to happen?

Job controller does not get stuck. It might be that the issue originated from user actions, or kubelet bug which transitioned the Pod first to `Succeeded`, then to `Failed`. However, Job controller should be resilient to a situation like this.

#### How can we reproduce it (as minimally and precisely as possible)?

It remains unclear what happened exactly - if the pod was transitioned to `Succeeded`, and after to `Failed`.
See below an artificial scenario which reproduces the Job controller getting stuck.



#### Anything else we need to know?

This scenario renders the Job controller stuck in v1.30.0. It involves a pod transitioning first to `Succeeded`. then to `Failed`. We had a similar issue in the past with pods being categorized first as Failed, then Succeeded: https://github.com/kubernetes/kubernetes/pull/111646/

1. Create the Job as follows:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
  labels:
    environment: test
spec:
  completions: 2
  parallelism: 2
  template:
    metadata:
      labels:
        environment: test
    spec:
      terminationGracePeriodSeconds: 1
      containers:
        - name: busybox
          image: busybox
          command: ["sleep", "5m"]
      restartPolicy: Never
  backoffLimit: 1
```
2. Create CEL validation `kubectl create -f cel.yaml` to make sure the Pod finalizer removal request fails:
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "demo-policy.example.com"
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   [""]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["pods"]
  validations:
    - expression: "size(object.metadata.finalizers) > 0"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "demo-binding-test.example.com"
spec:
  policyName: "demo-policy.example.com"
  validationActions: [Deny]
```
3. Set pod status as `Succeeded` with `kubectl edit pods/pi-766q7 --subresource=status`

You can observe that the Job status looks like this:
```yaml
  status:
    active: 2
    ready: 2
    uncountedTerminatedPods:
      succeeded:
      - 56f6d408-7949-4c66-8256-9a9c57595dab
```
4. Kill the pod that was mark succeeded: `kubectl delete pods/pi-766q7` - it makes the Pod to transition to `Failed`
5. Delete the CEL rule `kubectl delete -f cel.yaml`

Issue: Job controller does not remove the Job finalizer from the `pi-766q7` pod. It perdiodically fails with errors like this: `E0610 16:05:18.089533       1 job_controller.go:600] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Job.batch \"pi\" is invalid: status.uncountedTerminatedPods.failed[0]: Duplicate value: \"56f6d408-7949-4c66-8256-9a9c57595dab\"" logger="UnhandledError"`

#### Kubernetes version

<details>
1.27
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #125409 High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems

- Issue 链接：[#125409](https://github.com/kubernetes/kubernetes/issues/125409)

### Issue 内容

#### What happened?

Hi team,

originally this issue manifested as poor performance being observed in Redpanda but for the ease of reproducing I will only talk about [fio](https://github.com/axboe/fio) below.

Running fio on large servers "inside" of kubernetes results in less IOPS (2x in the example below) than running it  "outside" of kubernetes natively on the host.

This is caused by the cgroup settings as created by k8s effectively starving important kernel threads handling interrupts/direct-io handling.

k8s creates a cgroup hierarchy in which the pods are going to run. At the root it creates the `kubepods` cgroup. The cgroup cpu.shares/weight (v1/v2) parameter will be set to N*DEFAULT (1024 for v1/100 for v2) where N is the number of cores on the system (I think potentially `systemReserved` is being substracted from this?).

As a consequence anything running in k8s pod will get a lot higher priority than kernel threads which by default run in the `/` cgroup with a default weight (1024/100). On large many-core servers this could potentially be a very big difference - originally we discovered this on 72 core server.

Note that this problem doesn't necessarily require all cores being busy or having "allocated" all quota. Processes are being weighted against each other at their hierarchy so even if a process has been assigned just one core (1000 millicpus) and that's all it uses it will still compete with the full kubepods cgroup weight (N*DEFAULT) against kernel threads. 

Further the problem with some kernel threads is that they run on the same core as the application thread related to the work (IO etc.).



#### What did you expect to happen?

We shouldn't lose IOPS.

It's unclear to me whether the root kubepods group needs to have a non-default cpu.weight/shares. As mentioned above this shouldn't be needed for scheduling in regards to k8s pods against each other.



#### How can we reproduce it (as minimally and precisely as possible)?

The issue can be easily reproduced outside of kubernetes by just creating the needed cgroups manually:

Create a cgroup (note this is v2 so default is 100):

```
cgcreate -g cpu:/kubepods
cgset -r cpu.weight=1000 kubepods
```

Run outside of the cgroup:

```
taskset -c 11 fio --name=write_iops --directory=/mnt/xfs --size=10G
--time_based --runtime=1m --ramp_time=10s \
  --ioengine=libaio --direct=1 --verify=0 --bs=4K --iodepth=128
--rw=randwrite --group_reporting=1  --iodepth_batch_submit=128
--iodepth_batch_complete_max=128
...
   iops        : min=200338, max=200944, avg=200570.37, stdev=60.93, samples=120
...
```

Now run inside the cgroup:

```
cgexec -g cpu:kubepods -- taskset -c 11 fio --name=write_iops
--directory=/mnt/xfs --size=10G --time_based --runtime=1m \
  --ramp_time=10s --ioengine=libaio --direct=1 --verify=0 --bs=4K
--iodepth=128 --rw=randwrite --group_reporting=1
--iodepth_batch_submit=128  --iodepth_batch_complete_max=128
...
   iops        : min=113589, max=120554, avg=116073.72, stdev=1334.46,
samples=120
...
```

We see that we almost lost 50% of our IOPS (potentially worse if the disks could actually go faster).

This is on:

 - Amazon Linux 2023 / 6.1 Linux
 - i3en.3xlarge instance / 200k IOPS@4K write
 - XFS filesystem

#### Anything else we need to know?

We can also further debug of why this is happening at the kernel level (also posted [here](https://lore.kernel.org/linux-fsdevel/CAM9ScsHJ1zQ4j+0J+jQ1fUyRvxTMCF9OKC9kcvD5uyQZKxN1Pg@mail.gmail.com/T/#u)):

Comparing cpu time and context switches we see that in the bad case we are context switching a lot more. Overall the core is running at 100% in the bad case while only at something like 50% in the good case.

no-cgroup: task clock of fio:

```
perf stat -e task-clock -p 27393 -- sleep 1

 Performance counter stats for process id '27393':

            442.62 msec task-clock                       #    0.442
CPUs utilized

       1.002110208 seconds time elapsed
```

no cgroup: context switches on that core:

```
perf stat -e context-switches -C 11  -- sleep 1

 Performance counter stats for 'CPU(s) 11':

            103001      context-switches

       1.001048841 seconds time elapsed
```

Using the cgroup: task clock of fio:

```
perf stat -e task-clock -p 27456 -- sleep 1

 Performance counter stats for process id '27456':

            695.30 msec task-clock                       #    0.695
CPUs utilized

       1.001112431 seconds time elapsed
```

Using the cgroup: context switches on that core:

```
perf stat -e context-switches -C 11  -- sleep 1

 Performance counter stats for 'CPU(s) 11':

            243755      context-switches

       1.001096517 seconds time elapsed
```

So we are doing about 2.5x more context switches in the bad case. Doing the math at about ~120k IOPS we see that for every IOP we are doing two interrupts (in and out).

Finally we can also look at some perf sched traces to get an idea for what is happening (sched_stat_runtime calls omitted):

The general pattern in the good case seems to be:

```
fio 28143 [011]  2038.648954:       sched:sched_waking:
comm=kworker/11:68 pid=27489 prio=120 target_cpu=011
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0b91d5 __queue_work+0x1d5 ([kernel.kallsyms])
        ffffffff9f0b93a4 queue_work_on+0x24 ([kernel.kallsyms])
        ffffffff9f3bb04c iomap_dio_bio_end_io+0x8c ([kernel.kallsyms])
        ffffffff9f53749d blk_mq_end_request_batch+0xfd ([kernel.kallsyms])
        ffffffff9f7198df nvme_irq+0x7f ([kernel.kallsyms])
        ffffffff9f113956 __handle_irq_event_percpu+0x46 ([kernel.kallsyms])
        ffffffff9f113b14 handle_irq_event+0x34 ([kernel.kallsyms])
        ffffffff9f118257 handle_edge_irq+0x87 ([kernel.kallsyms])
        ffffffff9f033eee __common_interrupt+0x3e ([kernel.kallsyms])
        ffffffff9fa023ab common_interrupt+0x7b ([kernel.kallsyms])
        ffffffff9fc00da2 asm_common_interrupt+0x22 ([kernel.kallsyms])
        ffffffff9f297a4b internal_get_user_pages_fast+0x10b ([kernel.kallsyms])
        ffffffff9f591bdb __iov_iter_get_pages_alloc+0xdb ([kernel.kallsyms])
        ffffffff9f591ef9 iov_iter_get_pages2+0x19 ([kernel.kallsyms])
        ffffffff9f5269af __bio_iov_iter_get_pages+0x5f ([kernel.kallsyms])
        ffffffff9f526d6d bio_iov_iter_get_pages+0x1d ([kernel.kallsyms])
        ffffffff9f3ba578 iomap_dio_bio_iter+0x288 ([kernel.kallsyms])
        ffffffff9f3bab72 __iomap_dio_rw+0x3e2 ([kernel.kallsyms])
        ffffffff9f3baf8e iomap_dio_rw+0xe ([kernel.kallsyms])
        ffffffff9f45ff58 xfs_file_dio_write_aligned+0x98 ([kernel.kallsyms])
        ffffffff9f460644 xfs_file_write_iter+0xc4 ([kernel.kallsyms])
        ffffffff9f39c876 aio_write+0x116 ([kernel.kallsyms])
        ffffffff9f3a034e io_submit_one+0xde ([kernel.kallsyms])
        ffffffff9f3a0960 __x64_sys_io_submit+0x80 ([kernel.kallsyms])
        ffffffff9fa01135 do_syscall_64+0x35 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
              2500000025 [unknown] ([unknown])

fio 28143 [011]  2038.648974:       sched:sched_switch: prev_comm=fio
prev_pid=28143 prev_prio=120 prev_state=R ==> next_comm=kworker/11:68
next_pid=27489 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f135a36 exit_to_user_mode_prepare+0xa6 ([kernel.kallsyms])
        ffffffff9fa050fd syscall_exit_to_user_mode+0x1d ([kernel.kallsyms])
        ffffffff9fa01142 do_syscall_64+0x42 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
              2500000025 [unknown] ([unknown])

kworker/11:68-d 27489 [011]  2038.648984:       sched:sched_switch:
prev_comm=kworker/11:68 prev_pid=27489 prev_prio=120 prev_state=I ==>
next_comm=fio next_pid=28143 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f0ba249 worker_thread+0xb9 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])
```

fio is busy submitting aio events and gets interrupted from the nvme interrupts at which point control is yielded to the dio thread which handles the completion and yields back to fio.

Looking at the bad case there now seems to be some form of ping pong:

```
fio 28517 [011]  2702.018634:       sched:sched_switch: prev_comm=fio
prev_pid=28517 prev_prio=120 prev_state=S ==> next_comm=kworker/11:68
next_pid=27489 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f39de89 read_events+0x119 ([kernel.kallsyms])
        ffffffff9f39e042 do_io_getevents+0x72 ([kernel.kallsyms])
        ffffffff9f39e689 __x64_sys_io_getevents+0x59 ([kernel.kallsyms])
        ffffffff9fa01135 do_syscall_64+0x35 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
             11300000113 [unknown] ([unknown])

kworker/11:68+d 27489 [011]  2702.018639:       sched:sched_waking:
comm=fio pid=28517 prio=120 target_cpu=011
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0fa9d1 autoremove_wake_function+0x11 ([kernel.kallsyms])
        ffffffff9f0fbb90 __wake_up_common+0x80 ([kernel.kallsyms])
        ffffffff9f0fbd23 __wake_up_common_lock+0x83 ([kernel.kallsyms])
        ffffffff9f39f9df aio_complete_rw+0xef ([kernel.kallsyms])
        ffffffff9f0b9c35 process_one_work+0x1e5 ([kernel.kallsyms])
        ffffffff9f0ba1e0 worker_thread+0x50 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])

kworker/11:68+d 27489 [011]  2702.018642:       sched:sched_switch:
prev_comm=kworker/11:68 prev_pid=27489 prev_prio=120 prev_state=R+ ==>
next_comm=fio next_pid=28517 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d4cb preempt_schedule_common+0x1b ([kernel.kallsyms])
        ffffffff9fa0d51c __cond_resched+0x1c ([kernel.kallsyms])
        ffffffff9f0b9c56 process_one_work+0x206 ([kernel.kallsyms])
        ffffffff9f0ba1e0 worker_thread+0x50 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])
```

fio is sleeping in io_getevents waiting for all events to complete. The dio worker thread gets scheduled in handling aio completions one by one. This allows fio to wake as there are some amount of completions ready for it to process. Now because of the high weight of the fio process the kernel worker only gets a short amount of runtime and gets preempted by the scheduler yielding back to fio (notice the stack and R+ in the above trace). However because fio is waiting for all aios to complete it wakes up and goes straight back to sleep again. This ping pong continues.


#### Kubernetes version

1.25

#### Cloud provider

All / AWS EKS

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及。

---

## Issue #125397 After the kubelet restarted, the ready state of the pod should not change.

- Issue 链接：[#125397](https://github.com/kubernetes/kubernetes/issues/125397)

### Issue 内容

#### What happened?

A pod with startuoProbe/readinessProbe has been ready, it will be used as backends for Services. After the kubelet restarted, this pod will be marked `not ready`.  An additional startuoProbe needs to be executed.



#### What did you expect to happen?

After the kubelet restarted, the ready state of the pod should not change. The ready status should only be false if a probe fails.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with startupProbe 
Pod demo yaml:
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: startup
  name: startup-exec
spec:
  containers:
  - name: startup
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy;  sleep infinity
    startupProbe:
      exec:
        command:
        - /bin/sh
        - -c
        - sleep 10; cat /tmp/healthy
      initialDelaySeconds: 6
      timeoutSeconds: 15
      periodSeconds: 60
```

2. wait this pod to be ready
```
    - lastProbeTime: null
      lastTransitionTime: "2024-06-08T07:53:27Z"
      status: "True"
      type: ContainersReady
```
3. restart the kubelet 
```
kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf, 11-kind.conf
     Active: active (running) since Sat 2024-06-08 08:17:09 UTC; 1min 16s ago
```
4. this pod will be not ready. 
```
Jun 08 08:17:55 kind-control-plane kubelet[4644]: I0608 08:17:55.463964    4644 kubelet.go:2527] "SyncLoop (probe)" probe="startup" 
status="unhealthy" pod="default/startup-exec"


    - lastProbeTime: null
      lastTransitionTime: "2024-06-08T08:17:55Z"
      message: 'containers with unready status: [liveness]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
```

5.After an additional startuoProbe, it will be ready again.
```
        - lastProbeTime: null
      lastTransitionTime: "2024-06-08T08:19:05Z"
      status: "True"
      type: ContainersReady
```

#### Anything else we need to know?

It also affects readiness probe

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-05-13T22:00:36Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125394 [FG:InPlacePodVerticalScaling] Race condition setting pod resize status

- Issue 链接：[#125394](https://github.com/kubernetes/kubernetes/issues/125394)

### Issue 内容

The kubelet updates the pod status by first generating the pod status based on its internal representation, and then [merging](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/status/status_manager.go#L1049) that with the latest status from the API, based on whether the Kubelet should be the source of truth for those fields.

Currently, the PodResizeStatus is not synced with the API server status, which means that whatever the Kubelet thinks the resize status should be will overwrite whatever PodResizeStatus is currently stored in the API server. This can lead to a race condition where the Kubelet has not yet seen a resize update, and clobbers the `Proposed` resize status.

I'm not sure exactly what the right behavior is here, but it probably needs to be an intelligent state transition, or may require comparing resource versions.

/sig node
/kind bug
/cc @vinaykul @thockin 

### 分析结果

不涉及。

---

## Issue #125393 Remove Kubelet soft-admission

- Issue 链接：[#125393](https://github.com/kubernetes/kubernetes/issues/125393)

### Issue 内容

Kubelet soft admission handlers were created as a way for the Kubelet to block pods that cannot be run for reasons that can be resolved. In practice, this mechanism was only ever used by AppArmor ([here](https://github.com/kubernetes/kubernetes/blob/eef6c6082d4e34fc4a0675a36ec5cc575cd13696/pkg/kubelet/kubelet.go#L912)).

Problems with soft-admission rejection:
1. This is only used by AppArmor, and only for situations that cannot realistically be resolved (host or build does not support apparmor)
2. Pods held in the blocked state must be manually cleaned up. While there they hold reserved capacity from the scheduler and continually retry syncpod.
3. No other features work this way - usually Kubelet hard rejects pods that it knows it cannot run, or the runtime fails to start the pod/container.

This was originally added to prevent controllers from continuously recreating pods that cannot run, but that is a separate problem that should be approached globally, rather than with this 1-off AppArmor solution.

I propose we remove Kubelet soft admission entirely, and move the AppArmor admission handler to the hard-admission list.

/sig node
/milestone v1.31
/kind bug

### 分析结果

不涉及

---

## Issue #125380 Kubelet stop watching Pods from API-Server

- Issue 链接：[#125380](https://github.com/kubernetes/kubernetes/issues/125380)

### Issue 内容

#### What happened?

We have encountered several instances where certain AKS nodes fail to respond to pod updates. This issue includes:

- Terminating pods not receiving SIGTERM notifications.
- New pods not being started as scheduled.

After reviewing the API requests to the API server during one of these incidents, we noticed that the kubelet on the affected node stopped watching pods (`/api/v1/pods?allowWatchBookmarks=true&watch=true`) before the issue happened. Despite this, the kubelet continued to report themselves as healthy via heartbeat.

Kubelet cannot recover from this non-responding state by itself.
The issue can be mitigated by restarting the kubelet on the affected node.

#### What did you expect to happen?

Kubelet should not stop responding to pods while reporting healthy.

#### How can we reproduce it (as minimally and precisely as possible)?

We have not found a stable way to reproduce this issue.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.27.13

#### Cloud provider

Azure


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Common Base Linux Mariner"
VERSION="2.0.20240425"
ID=mariner
VERSION_ID="2.0"
PRETTY_NAME="CBL-Mariner/Linux"
ANSI_COLOR="1;34"
HOME_URL="https://aka.ms/cbl-mariner"
BUG_REPORT_URL="https://aka.ms/cbl-mariner"
SUPPORT_URL="https://aka.ms/cbl-mariner"

$ uname -a
# not same node, only for reference
Linux aks-agentpoolm-26294222-vmss000067 5.15.153.1-2.cm2 #1 SMP Thu Apr 25 16:39:35 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125376 Unstructured converter should produce int64 given uint input

- Issue 链接：[#125376](https://github.com/kubernetes/kubernetes/issues/125376)

### Issue 内容

#### What happened?

DeepCopy on DefaultUnstructuredConverter.ToUnstructured output from `struct { Field uint32 }` panics.

#### What did you expect to happen?

runtime/unstructured inconsistently generates int64 for uint in maps but uses uint64 in structs. They should all be using int64.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
package main

import "k8s.io/apimachinery/pkg/runtime"
import "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"

func main() {
	type Good struct { Hash map[string]uint32 }
	type Panics struct { Hash uint32 }

	obj, _ := runtime.DefaultUnstructuredConverter.ToUnstructured(&Good{Hash: map[string]uint32{"a":1}})
	uns := &unstructured.Unstructured{Object: obj}
	uns.DeepCopy()

	obj, _ = runtime.DefaultUnstructuredConverter.ToUnstructured(&Panics{Hash: 1})
	uns = &unstructured.Unstructured{Object: obj}
	uns.DeepCopy()
}
```

```text
panic: cannot deep copy uint64

goroutine 1 [running]:
k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue({0x6e3020?, 0x9c09a8?})
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:639 +0x273
k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue({0x6f6fc0?, 0xc000193a10})
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:623 +0x2b7
k8s.io/apimachinery/pkg/runtime.DeepCopyJSON(...)
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:608
k8s.io/apimachinery/pkg/apis/meta/v1/unstructured.(*Unstructured).DeepCopy(0xc00015ff30)
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/apis/meta/v1/unstructured/unstructured.go:151 +0x58
main.main()
        /data00/home/chankyin/go/src/code.byted.org/tce/federation/tmp/main.go:16 +0xed
exit status 2

```

The relevant code is found at https://github.com/kubernetes/kubernetes/blob/eef6c6082d4e34fc4a0675a36ec5cc575cd13696/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go#L838

#### Anything else we need to know?

_No response_

#### Kubernetes version

eef6c6082d4e34fc4a0675a36ec5cc575cd13696

#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

不涉及

---

## Issue #125370 Pod IP temporarily removed from status when pod transitions to a terminal state

- Issue 链接：[#125370](https://github.com/kubernetes/kubernetes/issues/125370)

### Issue 内容

#### What happened?

My pod completes (success or failure). The IP temporarily is removed on one of the status updates.

#### What did you expect to happen?

The IP is persistent .

#### How can we reproduce it (as minimally and precisely as possible)?

Apply:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: howardjohn/shell
        command:
        - bash
        - -c
        - |
          sleep 2
          exit 0
      restartPolicy: Never
  backoffLimit: 1
```

Watch:
```shell
$ kubectl get pod -ojson -w > res
$ cat res |jq -c '.status.phase + " " +.status.podIP' -r
Pending
Pending
Pending
Running 10.244.0.6
Running 10.244.0.6
Succeeded
Succeeded 10.244.0.6
Succeeded 10.244.0.6
```

The initial 'succeeded' should have the IP still.

On older versions:
```
Pending 
Pending 
Pending 
Running 10.244.0.6
Running 10.244.0.6
Succeeded 10.244.0.6
```

#### Anything else we need to know?

|Version|Status|
|-|-|
|1.22|IP retained|
|1.26|IP retained|
|1.27|IP Lost|
|1.28|IP lost|
|1.30|IP lost|

Full pod state, from 1.27:

<details>

```json
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "357",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "362",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:19Z",
                "message": "0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..",
                "reason": "Unschedulable",
                "status": "False",
                "type": "PodScheduled"
            }
        ],
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "420",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "431",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "message": "containers with unready status: [pi]",
                "reason": "ContainersNotReady",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "message": "containers with unready status: [pi]",
                "reason": "ContainersNotReady",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "image": "howardjohn/shell",
                "imageID": "",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "waiting": {
                        "reason": "ContainerCreating"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Pending",
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "483",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:23Z",
                "status": "True",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:23Z",
                "status": "True",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": true,
                "restartCount": 0,
                "started": true,
                "state": {
                    "running": {
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Running",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "495",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Running",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "498",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "501",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "503",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
```

</details>

Prior discussion https://github.com/kubernetes/kubernetes/issues/82268

#### Kubernetes version

many, see above

#### Cloud provider

Tested on kind

#### OS version

Linux

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125354 endpointslicemirroring controller not create endpointslice when the endpoints are recreate

- Issue 链接：[#125354](https://github.com/kubernetes/kubernetes/issues/125354)

### Issue 内容

#### What happened?

* create a service without selector, and manual create endpoints for this service
* endpointslicemirroring controller will create an endpointslice for this service.
* after endpoint slice created, and kube-controller-manager restart.
* during the kube-controller-manager restart, the endpoint was deleted and recreated
* after kube-controller-manager restart,  garbagecollector controller delete this endpointslice and endpointslice not recreate until endpoint/service update event or kube-controller-manager restart

#### What did you expect to happen?

endpointslice will be recreated

#### How can we reproduce it (as minimally and precisely as possible)?

* create a service without selector, and manual create endpoints for this service
* endpointslicemirroring controller will create an endpointslice for this service.
* after endpoint slice created, and kube-controller-manager restart.
* during the kube-controller-manager restart, the endpoint was deleted and recreated


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
v 1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
kind 1.30.0
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125339 readinessProbe not failing if command is not installed

- Issue 链接：[#125339](https://github.com/kubernetes/kubernetes/issues/125339)

### Issue 内容

#### What happened?

I have the following readinessProbe in my .yaml file:
```
readinessProbe:
  exec:
    command:
      - /bin/sh
      - -c
      - >
        curl -s http://127.0.0.1:8008/metrics |
        awk '!/^#/ && /^libp2p_gossipsub_healthy_peers_topics /{
          print "Found gossipsub:", $0;
          if ($2 == 1.0) {
            exit 0;  # success, healthy state
          } else {
            exit 1;  # failure, unhealthy state
          }
        }'
  successThreshold: 5
  initialDelaySeconds: 5
  periodSeconds: 1
  failureThreshold: 2
  timeoutSeconds: 5
```

The pod only has one container, and this container doesn't have `curl` installed. Still, the readinessProbe is making the pod as Ready.

I then manually entered in the container, and installed `curl` by hand. 1 second after that, the POD was marked as not ready.

If I don't manually install `curl`, and I run the same command, I got an error, as curl is not installed.

#### What did you expect to happen?

The POD should not be marked as ready.

#### How can we reproduce it (as minimally and precisely as possible)?

I edited my yaml to be reproducible and reach to the same issue.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nodes
  namespace: zerotesting
spec:
  replicas: 1
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app: zerotenkay
  template:
    metadata:
      labels:
        app: zerotenkay
    spec:
      containers:
        - name: test
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - >
                  curl -s http://127.0.0.1:8008/metrics |
                  awk 'asd'
            successThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 1
            failureThreshold: 2
            timeoutSeconds: 5
          resources:
            requests:
              memory: "64Mi"
              cpu: "150m"
            limits:
              memory: "600Mi"
              cpu: "400m"
          command:
            - sh
            - -c
            - sleep 99999
```

If I run this, I still can see that the pod is marked as Ready, even curl is not installed.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
 v1.28.9+k3s1
```

</details>


#### Cloud provider

<details>
Own deployment
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125336 volume-binding scheduler prefilter assumes that a node's metadata.name == metadata.labels["kubernetes.io/hostname"]

- Issue 链接：[#125336](https://github.com/kubernetes/kubernetes/issues/125336)

### Issue 内容

#### What happened?

Running on a system which has node names that look like FQDNs, but hostname labels which are unqualified.

The local path PV provisioner has (correctly) added nodeAffinity constraints to the PV that reference a node's `hostname` label.

A replacement pod for a statefulset that has a bound PVC cannot be re-scheduled, because the scheduler interprets  `PreFilterResult.NodeNames` as node *names*, but the code in volume_binding.go that runs the prefilter collects a set of *kubeternetes.io/hostname label values*.

#### What did you expect to happen?

Pod rescheduling should not wedge. The volume-binding scheduler plugin should resolve match constraints to a set of nodes and return their node names in its PreFilterResult.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a node with distinct name and hostname label [k8s documentation reiterates that this situation is possible]. Schedule a pod onto it with a local path PV bound. Observe the PV has a nodeAffinity constraint that contains the node's hostname label. Attempt to reschedule a pod to use this PV.

Precise behaviour may vary from 1.27 (which introduced this prefilter notion) forwards. On 1.27, the scheduler failes with a "nodeinfo not found". A workaround was backported into the prefilter loop of `schedulePod` but AFAICT the root cause was never identified. Later versions look to end up filtering out all nodes in `schedulePod` - but the root cause is the same in both cases.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.11", GitCommit:"b9e2ad67ad146db566be5a6db140d47e52c8adb2", GitTreeState:"clean", BuildDate:"2024-02-14T10:40:40Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.13-eks-3af4770", GitCommit:"4873544ec1ec7d3713084677caa6cf51f3b1ca6f", GitTreeState:"clean", BuildDate:"2024-04-30T03:31:44Z", GoVersion:"go1.21.9", Compiler:"gc", Platform:"linux/amd64"}
```

</details>

The nodes in question were older ubuntu EKS images, but that's largely irrelevant; the critical point is that nodes are registered by kubelet with a FQDN name but a short hostname. (AFAICT newer ubuntu EKS images will mask this behaviour by setting both of these to the same value, but the same erroneous assumption is baked into volume-binding still.)

#### Cloud provider

<details>
EKS, 1.27 (at present)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.8.0
</details>

but the PV record that it creates is (IMO) correct; the matchExpression attempts to identify a node by its hostname label.

### 分析结果

不涉及

---

## Issue #125332 Pod stuck at Running state due to unexpected skip of taint manager work

- Issue 链接：[#125332](https://github.com/kubernetes/kubernetes/issues/125332)

### Issue 内容

#### What happened?

1、We shutdown the worker node that our pods is running on
2、A pod stays in Running state and never change again



#### What did you expect to happen?

We expected the pod to be update to Terminating.

#### How can we reproduce it (as minimally and precisely as possible)?

It is unlikely to be reproduced since we found the 'only possible' cause to be of ridiculously low probability.

#### Anything else we need to know?

The pod is defined with tolerations as below, but doesn't seem to be a problem:
```
tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 2
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 2 
```
Strange log sequence was found in controller-manager's log
```
I0604 15:28:49.239218       1 timed_workers.go:113] Adding TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 15:28:49.239118259 +0800 CST m=+252387.993120493 to be fired at 2024-06-04 15:28:51.239118259 +0800 CST m=+252389.993120493
......
I0604 15:28:51.240319       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="dpa/infrastructure-0"
I0604 15:28:51.259631       1 taint_manager.go:416] "Noticed pod deletion" pod="dpa/infrastructure-0"
I0604 15:28:51.259642       1 timed_workers.go:132] Cancelling TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 15:28:51.259639403 +0800 CST m=+252390.013641637
......
I0604 17:38:00.725378       1 timed_workers.go:113] Adding TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 17:38:00.725348125 +0800 CST m=+260139.479350359 to be fired at 2024-06-04 17:38:02.725348125 +0800 CST m=+260141.479350359
W0604 17:38:00.725393       1 timed_workers.go:118] Trying to add already existing work for &{NamespacedName:dpa/infrastructure-0}. Skipping.
......
```
We noticed that we had restarted the node once at arround 15:28, and that trigger the eviction of pod dpa/infrastructure-0 as expected. But when we shutdown the node again at arround 17:38, the taint manager skipped the work due to the same key found in workers map.
The controller manager was never restarted or changed leader.
The log is ridiculous, but after look at the code, we did find a way that a key stays in workers map after Cancelling:
```
func (q *TimedWorkerQueue) getWrappedWorkerFunc(key string) func(ctx context.Context, args *WorkArgs) error {
	return func(ctx context.Context, args *WorkArgs) error {
		err := q.workFunc(ctx, args)
		q.Lock()
		defer q.Unlock()
		if err == nil {
			// To avoid duplicated calls we keep the key in the queue, to prevent
			// subsequent additions.
			q.workers[key] = nil
		} else {
			delete(q.workers, key)
		}
		return err
	}
}
```
In this part of code generates a WorkerFunc that would be called in a work, taint manager will run q.workFunc to delete pod from apiserver, which would trigger a podUpdate and lead to CancelWork:
```
func (q *TimedWorkerQueue) CancelWork(key string) bool {
	q.Lock()
	defer q.Unlock()
	worker, found := q.workers[key]
	result := false
	if found {
		klog.V(4).Infof("Cancelling TimedWorkerQueue item %v at %v", key, time.Now())
		if worker != nil {
			result = true
			worker.Cancel()
		}
		delete(q.workers, key)
	}
	return result
}
```
If and only if CancelWork acquired the lock before WorkerFunc, the `q.workers[key]` won't be deleted as expected, but `q.workers[key] = nil` instead, and lead to later skip of work.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+77d8f59014e1f3-dirty", GitCommit:"77d8f59014e1f3ca907c1b4a4f57900539b88dc8", GitTreeState:"dirty", BuildDate:"2024-04-29T13:26:15Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+77d8f59014e1f3-dirty", GitCommit:"77d8f59014e1f3ca907c1b4a4f57900539b88dc8", GitTreeState:"dirty", BuildDate:"2024-04-29T13:23:44Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125331 the node is in the notready state while the pod is still running 1/1. Node dynamic.7.220.110.21 is healthy again, removing all taints, but not update ready status

- Issue 链接：[#125331](https://github.com/kubernetes/kubernetes/issues/125331)

### Issue 内容

#### What happened?

node dynamic.7.220.110.21 not go into `case currentReadyCondition.Status != v1.ConditionTrue && observedReadyCondition.Status == v1.ConditionTrue:` and fallthrough -> MarkPodsNotReady.

**As a result, the node is in the notready state while the pod is still running 1/1.**

![image](https://github.com/kubernetes/kubernetes/assets/17514799/c4265bce-a983-4f42-9362-d11036b432f0)

---------
**log is below**:  **I notice `NodeCondition{Type:Ready,Status:Unknown`  is to `NodeCondition{Type:Ready,Status:False`. 

I0528 07:44:10.430657 9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125415326s. Last Ready is: &NodeCondition{Type:Ready,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-28 07:43:20 +0000 UTC,Reason:KubeletNotReady,Message:container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?,}

i guess the step is: 
1、kubelet post False state because docker is kill  
2、kill kubelet before the pod status is reported.
3、now pod status stuck in running 1/1 and never become 0/1
----------

I0527 14:24:51.746645       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996844161s. Last Ready is: &**NodeCondition{Type:Ready,Status:Unknown**,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746667       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.99686763s. Last MemoryPressure is: &NodeCondition{Type:MemoryPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746686       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996886936s. Last DiskPressure is: &NodeCondition{Type:DiskPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746704       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996904333s. Last PIDPressure is: &NodeCondition{Type:PIDPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746744       9 node_lifecycle_controller.go:960] Node dynamic.7.220.110.21 is unresponsive as of 2024-05-27 14:24:51.746735259 +0000 UTC m=+187413.002563875. Adding it to the Taint queue.
I0527 14:24:53.451556       9 controller_utils.go:120] "Update ready status of pods on node" node="dynamic.7.220.110.21"
I0527 14:24:56.798746       9 node_lifecycle_controller.go:971] **Node dynamic.7.220.110.21 is healthy again, removing all taints**
I0528 07:43:25.301189       9 node_lifecycle_controller.go:947] **Node dynamic.7.220.110.21 is NotReady as of 2024-05-28** 07:43:25.301181507 +0000 UTC m=+249725.057044658. Adding it to the Taint queue.
I0528 07:44:10.430657       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125415326s. Last Ready is: &NodeCondition{**Type:Ready,Status:False**,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-28 07:43:20 +0000 UTC,Reason:KubeletNotReady,Message:container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?,}
I0528 07:44:10.430692       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125451926s. Last MemoryPressure is: &NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,}
I0528 07:44:10.430723       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125483489s. Last DiskPressure is: &NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,}
I0528 07:44:10.430743       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125503499s. Last PIDPressure is: &NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,}


#### What did you expect to happen?

the node is in the notready state while the pod is should be running 0/1.

#### How can we reproduce it (as minimally and precisely as possible)?

see below

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125315 Kubernetes 1.30 and containerd 1.7.16 onpremise setup, pods networking is not working containerd status is failing

- Issue 链接：[#125315](https://github.com/kubernetes/kubernetes/issues/125315)

### Issue 内容

#### What happened?

I installed containerd 1.7.16 and kubernetes 1.30.0 on RHEL centos machines.The pods are unable to run.Multiple issues are noticed.
1. apiserver, coredns, controller-manager, scheduler restarted 22 times
2. Pod networking is failing(redis nodes unable to join)
3. containerd and kubelet status is showing error while fetching containerID
4. Kubectl get events display "invalid 0 image filesystem capacity"
5. Disk utiliation on one worker node goes to 84% always(with no pods)

#### What did you expect to happen?

Perfectly working cluster with
1. image and node filesystem to automatically garbage collect with default configurations
2. Able to use runc container runtime and schedule pods
3. pod networking
4. use private registry and local images

#### How can we reproduce it (as minimally and precisely as possible)?

1. Downloaded RPM'S
![image](https://github.com/kubernetes/kubernetes/assets/43172017/3bead595-7d66-4a82-8357-9b31331923a2)
Skipped rpms(glibc, iptable,iproute,systemd,system-libs,utils-linux ) as Faced conflicts while installing rpms as slightly lower versions are already present in system.
2. 1.7.16 containerd,1.4.1 CNI plugin without package manager, Containerd , stage directory (/data/containers and /data/containerd) Consumption is going to 84 percent with no pods running(state directory not GC automatically)
3. installed cluster using kubeadm 
4. CNI flannel Deployed

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
![image](https://github.com/kubernetes/kubernetes/assets/43172017/a262508f-41d5-401c-894f-f873ea959fcf)

</details>


#### Cloud provider

<details>
ON-PREMISE CENTOS RHEL 8.6
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
![image](https://github.com/kubernetes/kubernetes/assets/43172017/ab675ca7-991f-42f8-a6d4-0cfda38a6805)

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125312 False "v1 Binding is deprecated in v1.6+" warning for pods/bindings sub-resource

- Issue 链接：[#125312](https://github.com/kubernetes/kubernetes/issues/125312)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/commit/8d45bbea2b464e856ddcfe3f6ee410ddea0cee32 (new for 1.31.0 alpha 1) added this:
https://github.com/kubernetes/kubernetes/blob/ae5543e4c8f99cb1555102a8ebc310aed3c82596/staging/src/k8s.io/api/core/v1/zz_generated.prerelease-lifecycle.go#L30-L34

For some reason, this code ends up checking the `Binding` type when registering the action for the `"namespaces/{namespace}/pods/{name}/binding"` sub-resource:
https://github.com/kubernetes/kubernetes/blob/ae5543e4c8f99cb1555102a8ebc310aed3c82596/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go#L767-L776

cc @liggitt as the author of that code

As a result, the Kubernetes scheduler gets a warning from the apiserver when it calls `err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})`:

```
W0604 11:50:04.854792  276440 warnings.go:70] v1 Binding is deprecated in v1.6+
```

#### What did you expect to happen?

No warning because the sub-resource is not deprecated.

#### How can we reproduce it (as minimally and precisely as possible)?

```console
$ dlv test ./test/integration/scheduler_perf/ -- -test.run=TestScheduling/SchedulingWithResourceClaimTemplateStructured/fast
...

b installer.go:745
cond 1 action.Path == "namespaces/{namespace}/pods/{name}/binding"
```

#### Anything else we need to know?

Did not happen in 1.30.0.


#### Kubernetes version

master, shortly before v1.31.0 alpha 1.


#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #125285 Named ports in initContainer sidecars do not work with NetworkPolicies

- Issue 链接：[#125285](https://github.com/kubernetes/kubernetes/issues/125285)

### Issue 内容

#### What happened?

We have recently migrated our traditional sidecar definitions to the new recommended initContainer sidecars.

We are running a CockroachDB StatefulSet that has a Vault sidecar, such that members 0 and 1 have a "legacy" sidecar container:

```console
$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-0 | jq -r '.spec.containers[] | select(.name == "vault-credentials-agent") | .ports'
[
  {
    "containerPort": 8099,
    "name": "metrics",
    "protocol": "TCP"
  }
]

$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-1 | jq -r '.spec.containers[] | select(.name == "vault-credentials-agent") | .ports'
[
  {
    "containerPort": 8099,
    "name": "metrics",
    "protocol": "TCP"
  }
]
```

And member 2 has a new initContainer style sidecar:

```console
$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-2 | jq -r '.spec.initContainers[] | select(.name == "vault-credentials-agent") | .ports[]'
{
  "containerPort": 8099,
  "name": "metrics",
  "protocol": "TCP"
}
```

They all expose a named port `metrics`.

We also have a NetworkPolicy in this namespace such that:

```console
$ kubectl --context prod-aws -n partner-registration get netpol private-ingress-sys-prom -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"networking.k8s.io/v1","kind":"NetworkPolicy","metadata":{"annotations":{},"name":"private-ingress-sys-prom","namespace":"partner-registration"},"spec":{"ingress":[{"from":[{"namespaceSelector":{"matchLabels":{"name":"sys-prom"}}}],"ports":[{"port":"metrics"}]}],"podSelector":{},"policyTypes":["Ingress"]}}
  creationTimestamp: "2021-03-26T10:52:05Z"
  generation: 3
  name: private-ingress-sys-prom
  namespace: partner-registration
  resourceVersion: "3454111968"
  uid: 26a53b37-53e3-4a8d-92dd-8e17836b087d
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: sys-prom
    ports:
    - port: metrics
      protocol: TCP
  podSelector: {}
  policyTypes:
  - Ingress
```

Testing the connection from Prometheus I observe the following results for member 0 and 1:

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.142.58:8099/__/metrics | head -4
Connecting to 10.2.142.58:8099 (10.2.142.58:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary

$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.13.56:8099/__/metrics | head -4
Connecting to 10.2.13.56:8099 (10.2.13.56:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
```

And a timeout for member 2 (running initContainer sidecar):

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.2.53:8099/__/metrics | head -4
Connecting to 10.2.2.53:8099 (10.2.2.53:8099)
wget: download timed out
command terminated with exit code 1
```

If I apply the following patch:

```patch
diff --git a/prod-aws/partner-registration/02-network-policies.yaml b/prod-aws/partner-registration/02-network-policies.yaml
index 9d3ea45d86..c685cc8667 100644
--- a/prod-aws/partner-registration/02-network-policies.yaml
+++ b/prod-aws/partner-registration/02-network-policies.yaml
@@ -43,7 +43,7 @@ spec:
             matchLabels:
               name: sys-prom
       ports:
-        - port: metrics
+        - port: 8099
 ---
 #  Allow private Ingress
 ---
```

Then I'm able to establish a connection with cockroachdb-2:

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.2.53:8099/__/metrics | head -4
Connecting to 10.2.2.53:8099 (10.2.2.53:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
```

This is the Kyverno config that injects our sidecars the describes the whole config of the injected initContainer sidecar: https://github.com/utilitywarehouse/system-manifests/blob/master/kyverno/policies/pods/injectSidecar.yaml#L40-L88


#### What did you expect to happen?

named port in the initContainer spec allows the traffic if it matches config of the NetworkPolicy.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a StatefulSet with an initContainer sidecar that specifies a named port. Create a NetworkPolicy allowing access to the named port. You should observe timeouts trying to reach that port.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl --context prod-aws version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
AWS EC2 running our own control plane.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Flatcar Container Linux by Kinvolk"
ID=flatcar
ID_LIKE=coreos
VERSION=3815.2.0
VERSION_ID=3815.2.0
BUILD_ID=2024-02-12-2039
SYSEXT_LEVEL=1.0
PRETTY_NAME="Flatcar Container Linux by Kinvolk 3815.2.0 (Oklo)"
ANSI_COLOR="38;5;75"
HOME_URL="https://flatcar.org/"
BUG_REPORT_URL="https://issues.flatcar.org"
FLATCAR_BOARD="amd64-usr"
CPE_NAME="cpe:2.3:o:flatcar-linux:flatcar_linux:3815.2.0:*:*:*:*:*:*:*"

$ uname -a
Linux ip-10-44-18-96 6.1.77-flatcar #1 SMP PREEMPT_DYNAMIC Mon Feb 12 19:37:08 -00 2024 x86_64 AMD EPYC 7571 AuthenticAMD GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Terraform / Ignition.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI: Calico v3.27.3
</details>


### 分析结果

不涉及

---

## Issue #125283 The connection to the server Master_IP:6443 was refused - did you specify the right host or port?

- Issue 链接：[#125283](https://github.com/kubernetes/kubernetes/issues/125283)

### Issue 内容

#### What happened?

Hello All, 

I am setting up Kubernetes in my VMs for some testing, here is the info - 


Below CLI is used to initialize --> kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=Master_IP --cri-socket /run/containerd/containerd.sock

kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3



ot-master1 $ cat  /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
ot-master1 $


Whenever I try to execute kubectl CLI, it throws - The connection to the server Master_IP:6443 was refused - did you specify the right host or port?
there is No firewall/ufw enabled in the system and not sure why there' are lots of rules updated once the kubeadm is initialized

and to make it work I have set to reset the iptables, and trying to clear them out, Not sure what to do!

If I performed the steps below for all the workers and master then I can connect to API\ then after a few secs it blocked the API 

iptables -L
iptables -F
iptables -X
iptables -P INPUT ACCEPT
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -L


#### What did you expect to happen?

kubectl get nodes -v=10
I0601 11:38:36.649526  795967 loader.go:395] Config loaded from file:  /root/.kube/config
I0601 11:38:36.650037  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.650302  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650330  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.650342  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.650352  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.650401  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650414  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650497  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.650676  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650714  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.650727  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.650738  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.650773  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.651887  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.651930  795967 shortcut.go:103] Error loading discovery information: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652026  795967 round_trippers.go:466] curl -v -XGET  -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.652159  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652178  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.652188  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.652198  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.652226  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652233  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652289  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.652412  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652447  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.652459  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.652468  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.652502  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653592  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653691  795967 round_trippers.go:466] curl -v -XGET  -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.653841  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653872  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.653894  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.653903  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.653939  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653955  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653998  795967 helpers.go:264] Connection error: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
The connection to the server Master_IP:6443 was refused - did you specify the right host or port?
-->while checking the kubelet / even restarted many times, it didnt help 
ot-master1 $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Wed 2024-05-29 19:51:15 CDT; 4 days ago
       Docs: https://kubernetes.io/docs/
   Main PID: 261048 (kubelet)
      Tasks: 17 (limit: 38415)
     Memory: 51.5M
        CPU: 1h 6min 1.020s
     CGroup: /system.slice/kubelet.service
             └─261048 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9

Jun 02 22:06:52 ot-master1.internal.local kubelet[261048]: E0602 22:06:52.847762  261048 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-proxy pod=kube-proxy-vtthh_kube-system(c53eac75-2bcd-480d-a6b6-cbf635217113)\"" pod="kube-system/kube-proxy-vtthh" po>
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915602  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?resourceVersion=0&timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915811  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915923  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916140  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916329  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916348  261048 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.951079  261048 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://Master_IP:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused" interval="7s"
Jun 02 22:06:54 ot-master1.internal.local kubelet[261048]: I0602 22:06:54.846684  261048 scope.go:117] "RemoveContainer" containerID="5907d016cfb243f11019b26d636b13d3f1f1d4caa3fcfe464021f743d4087383"
Jun 02 22:06:54 ot-master1.internal.local kubelet[261048]: E0602 22:06:54.846915  261048 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-scheduler\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-ot-master1.internal.local_kube-system(7e4870baad9d42588bd86d4db89bbc3a)\"" pod="kube>
lines 1-23/23 (END)

#### How can we reproduce it (as minimally and precisely as possible)?

it is newly setup, so not sure how to reproduce it 

#### Anything else we need to know?

ot-master1 $ crictl ps -a
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a983facbb5bc1       3861cfcd7c04c       1 second ago        Running             etcd                      3903                8d5e41e38f052       etcd-ot-master1.internal.local
4e9b8fe986eb9       25a1387cdab82       21 seconds ago      Exited              kube-controller-manager   1144                28d528679ff60       kube-controller-manager-ot-master1.internal.local
e06e9abb29332       747097150317f       2 minutes ago       Exited              kube-proxy                947                 fc5c6f557c735       kube-proxy-vtthh
a19c34b9cb64d       3861cfcd7c04c       3 minutes ago       Exited              etcd                      3902                497039ab0fd10       etcd-ot-master1.internal.local
5907d016cfb24       a52dc94f0a912       4 minutes ago       Exited              kube-scheduler            3815                17c84d5c165cf       kube-scheduler-ot-master1.internal.local
e32430f6860f9       91be940803172       5 minutes ago       Exited              kube-apiserver            3598                9145c833b443c       kube-apiserver-ot-master1.internal.local
ot-master1 $

ot-master1 $ crictl logs e32430f6860f9
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I0603 03:02:32.967767       1 options.go:221] external host was not specified, using Master_IP
I0603 03:02:32.968407       1 server.go:148] Version: v1.30.1
I0603 03:02:32.968488       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0603 03:02:33.629837       1 shared_informer.go:313] Waiting for caches to sync for node_authorizer
I0603 03:02:33.632482       1 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0603 03:02:33.634263       1 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0603 03:02:33.634277       1 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0603 03:02:33.634470       1 instance.go:299] Using reconciler: lease
I0603 03:02:33.657645       1 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0603 03:02:33.657663       1 genericapiserver.go:733] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0603 03:02:33.792328       1 handler.go:286] Adding GroupVersion  v1 to ResourceManager
I0603 03:02:33.792720       1 instance.go:696] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0603 03:02:33.933622       1 instance.go:696] API group "storagemigration.k8s.io" is not enabled, skipping.
I0603 03:02:34.057807       1 instance.go:696] API group "resource.k8s.io" is not enabled, skipping.
I0603 03:02:34.070687       1 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0603 03:02:34.070714       1 genericapiserver.go:733] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.070721       1 genericapiserver.go:733] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.071041       1 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0603 03:02:34.071053       1 genericapiserver.go:733] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.071685       1 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
I0603 03:02:34.072164       1 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
W0603 03:02:34.072178       1 genericapiserver.go:733] Skipping API autoscaling/v2beta1 because it has no resources.
W0603 03:02:34.072183       1 genericapiserver.go:733] Skipping API autoscaling/v2beta2 because it has no resources.
I0603 03:02:34.073187       1 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
W0603 03:02:34.073205       1 genericapiserver.go:733] Skipping API batch/v1beta1 because it has no resources.
I0603 03:02:34.073803       1 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0603 03:02:34.073819       1 genericapiserver.go:733] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.073824       1 genericapiserver.go:733] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.074233       1 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0603 03:02:34.074245       1 genericapiserver.go:733] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.074280       1 genericapiserver.go:733] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.074674       1 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0603 03:02:34.075775       1 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0603 03:02:34.075789       1 genericapiserver.go:733] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.075794       1 genericapiserver.go:733] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.076090       1 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0603 03:02:34.076101       1 genericapiserver.go:733] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.076105       1 genericapiserver.go:733] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.076670       1 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
W0603 03:02:34.076683       1 genericapiserver.go:733] Skipping API policy/v1beta1 because it has no resources.
I0603 03:02:34.077878       1 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0603 03:02:34.077892       1 genericapiserver.go:733] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.077897       1 genericapiserver.go:733] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.078217       1 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0603 03:02:34.078230       1 genericapiserver.go:733] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.078234       1 genericapiserver.go:733] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.079820       1 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0603 03:02:34.079838       1 genericapiserver.go:733] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.079842       1 genericapiserver.go:733] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.080958       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
I0603 03:02:34.082005       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
W0603 03:02:34.082021       1 genericapiserver.go:733] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W0603 03:02:34.082042       1 genericapiserver.go:733] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.085267       1 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
W0603 03:02:34.085284       1 genericapiserver.go:733] Skipping API apps/v1beta2 because it has no resources.
W0603 03:02:34.085307       1 genericapiserver.go:733] Skipping API apps/v1beta1 because it has no resources.
I0603 03:02:34.087311       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0603 03:02:34.087330       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.087352       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.087879       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0603 03:02:34.087902       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.097441       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0603 03:02:34.097455       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.374456       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I0603 03:02:34.374501       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I0603 03:02:34.374769       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I0603 03:02:34.375033       1 secure_serving.go:213] Serving securely on [::]:6443
I0603 03:02:34.375088       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0603 03:02:34.375151       1 controller.go:78] Starting OpenAPI AggregationController
I0603 03:02:34.375181       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I0603 03:02:34.375195       1 available_controller.go:423] Starting AvailableConditionController
I0603 03:02:34.375205       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0603 03:02:34.375267       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0603 03:02:34.375273       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0603 03:02:34.375355       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0603 03:02:34.375411       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0603 03:02:34.375610       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0603 03:02:34.375674       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0603 03:02:34.375995       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0603 03:02:34.376151       1 controller.go:116] Starting legacy_token_tracking_controller
I0603 03:02:34.376216       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0603 03:02:34.376287       1 aggregator.go:163] waiting for initial CRD sync...
I0603 03:02:34.376392       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0603 03:02:34.376474       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0603 03:02:34.376555       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I0603 03:02:34.376678       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I0603 03:02:34.376984       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0603 03:02:34.377017       1 controller.go:139] Starting OpenAPI controller
I0603 03:02:34.377123       1 crd_finalizer.go:266] Starting CRDFinalizer
I0603 03:02:34.377636       1 controller.go:87] Starting OpenAPI V3 controller
I0603 03:02:34.377665       1 naming_controller.go:291] Starting NamingConditionController
I0603 03:02:34.377694       1 establishing_controller.go:76] Starting EstablishingController
I0603 03:02:34.377713       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0603 03:02:34.390422       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0603 03:02:34.390440       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0603 03:02:34.430041       1 shared_informer.go:320] Caches are synced for node_authorizer
I0603 03:02:34.433278       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0603 03:02:34.433295       1 policy_source.go:224] refreshing policies
I0603 03:02:34.475605       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0603 03:02:34.475631       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0603 03:02:34.475642       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0603 03:02:34.475729       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0603 03:02:34.475861       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0603 03:02:34.476382       1 shared_informer.go:320] Caches are synced for configmaps
I0603 03:02:34.479739       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0603 03:02:34.490818       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0603 03:02:34.490846       1 aggregator.go:165] initial CRD sync complete...
I0603 03:02:34.490852       1 autoregister_controller.go:141] Starting autoregister controller
I0603 03:02:34.490858       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0603 03:02:34.490863       1 cache.go:39] Caches are synced for autoregister controller
I0603 03:02:34.503950       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0603 03:02:35.378607       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0603 03:02:35.585354       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [Master_IP]
I0603 03:02:35.586406       1 controller.go:615] quota admission added evaluator for: endpoints
I0603 03:02:35.589649       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0603 03:02:57.793100       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0603 03:02:57.793158       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0603 03:02:57.794763       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0603 03:02:57.794797       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0603 03:02:57.796007       1 timeout.go:142] post-timeout activity - time-elapsed: 3.158512ms, GET "/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication" result: <nil>
E0603 03:02:57.838464       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
E0603 03:02:57.838541       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
E0603 03:02:57.838581       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
W0603 03:02:57.839111       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 03:02:57.839130       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 03:02:57.839170       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection 

#### Kubernetes version

kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server 16.32.12.201:6443 was refused - did you specify the right host or port?

#### Cloud provider

NA


#### OS version


ot-master1 $ cat /etc/*release*
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
ot-master1 $

#### Install tools

NA


#### Container runtime (CRI) and version (if applicable)

crictl version
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.31
RuntimeApiVersion:  v1



#### Related plugins (CNI, CSI, ...) and versions (if applicable)

NA


### 分析结果

不涉及

---

## Issue #125267 Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized

- Issue 链接：[#125267](https://github.com/kubernetes/kubernetes/issues/125267)

### Issue 内容

#### What happened?

Right after installing kubernetes control plane  using `kubeadm` and `weave net` as network plugin the API server goes down with the below error msg.
```
[root@kubemaster ~]# kubectl get pods -A --watch
error: Get "https://10.74.250.78:6443/api/v1/pods?limit=500": dial tcp 10.74.250.78:6443: connect: connection refused - error from a previous attempt: read tcp 10.74.250.78:44740->10.74.250.78:6443: read: connection reset by peer
[root@kubemaster ~]#  
```

#### What did you expect to happen?

The cluster should be working fine with `weave net` 

#### How can we reproduce it (as minimally and precisely as possible)?

```
kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=10.74.250.78 --v=5

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.74.250.78:6443 --token 7l1k0v.gnzdt2i4a0hg6uaq \
	--discovery-token-ca-cert-hash sha256:9f307309f8fd825a99a30e2643a59159499d4fd78843ac14d0c78f85849da411 
[root@kubemaster ~]# 
[root@kubemaster ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
[root@kubemaster ~]# 
```



```
[root@kubemaster ~]# kubectl apply -f weave-daemonset-k8s.yaml
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
[root@kubemaster ~]#
```
```
[root@kubemaster ~]# kubectl get pods -A  --watch
NAMESPACE     NAME                                 READY   STATUS             RESTARTS        AGE
kube-system   coredns-7db6d8ff4d-6t5cb             0/1     Pending            0               3m25s
kube-system   coredns-7db6d8ff4d-z9lvq             0/1     Pending            0               3m25s
kube-system   etcd-kubemaster                      1/1     Running            173 (90s ago)   58s
kube-system   kube-apiserver-kubemaster            1/1     Running            174 (72s ago)   3m59s
kube-system   kube-controller-manager-kubemaster   1/1     Running            12 (112s ago)   4m33s
kube-system   kube-proxy-jgnb6                     0/1     CrashLoopBackOff   2 (7s ago)      3m25s
kube-system   kube-scheduler-kubemaster            0/1     CrashLoopBackOff   181 (30s ago)   58s
kube-system   weave-net-fdvrk                      0/2     Pending            0               8s
kube-system   kube-scheduler-kubemaster            0/1     Running            182 (53s ago)   81s
kube-system   kube-proxy-jgnb6                     1/1     Running            3 (32s ago)     3m50s
kube-system   kube-scheduler-kubemaster            0/1     Completed          182 (61s ago)   89s
kube-system   kube-scheduler-kubemaster            0/1     CrashLoopBackOff   182 (2s ago)    90s
^C[root@kubemaster ~]# 

```




 Error:
====================
```
Jun  1 09:35:45 kubemaster kubelet[15562]: E0601 09:35:45.262058   15562 kubelet.go:2900] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
Jun  1 09:35:46 kubemaster kubelet[15562]: E0601 09:35:46.177829   15562 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.74.250.78:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/kubemaster?timeout=10s\": dial tcp 10.74.250.78:6443: connect: connection refused" interval="7s"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
[root@kubemaster ~]# kubectl version
Client Version: v1.30.1
```

</details>


#### Cloud provider

<details>
VM 
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
[root@kubemaster ~]# 
[root@kubemaster ~]# 
[root@kubemaster ~]# uname -r
5.14.0-427.18.1.el9_4.x86_64
[root@kubemaster ~]# 

```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Weave Net 
</details>


### 分析结果

不涉及

---

