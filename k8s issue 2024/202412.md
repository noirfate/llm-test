# Issue 安全分析报告

# 🚨 存在高风险的 Issues (3 个)

## Issue #129319 [Bug] Unexpected scheduling results due to mismatch between the inter-pod affinity rule implementation and the doc

- Issue 链接：[#129319](https://github.com/kubernetes/kubernetes/issues/129319)

### Issue 内容

#### What happened?

There is a special rule in the scheduler's `pod affinity` plugin for scheduling a group of pods with inter-pod affinity to themselves. However, the current implementation does not match the doc and the comment, causing unexpected scheduling results.  
> [[doc]](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)  
> ##### Scheduling a group of pods with inter-pod affinity to themselves  
> If the current pod being scheduled is the first in a series that have affinity to themselves, it is allowed to be scheduled if it passes all other affinity checks. This is determined by verifying that no other pod in the cluster matches the namespace and selector of this pod, that the pod matches its own terms, and the chosen node matches all requested topologies. This ensures that there will not be a deadlock even if all the pods have inter-pod affinity specified.

##### The Inconsistency

In the current version of the [documentation](https://github.com/kubernetes/website/blob/8a1bb4cf42fdd9c8065d66a1d2ebace4731ba414/content/en/docs/concepts/scheduling-eviction/assign-pod-node.md?plain=1#L292C16-L292C90) and the current version of the [code comment](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L352C55-L353C67), both say that "**no other pod in the cluster** matches the namespace and selector of this pod," which implies that the scheduler will check **all pods**.

However, after investigating the implementation, the scheduler is actually checking **all pods on nodes with at least one topology key matched**, instead of **all pods**. (For more details, please see "Anything else we need to know?".)

As a result, the current implementation leads to unexpected scheduling results.


##### The Original Intent

We have investigated the history of this special rule, and it shows:

1. [At the very beginning](https://github.com/kubernetes/kubernetes/blob/587d164307de060d271f10f2386f39153360fba9/plugin/pkg/scheduler/algorithm/predicates/predicates.go#L836C4-L848C5), the code and the comment were consistent, both executing/stating that the scheduler would check **all pods in the cluster**.

2. [Later](https://github.com/kubernetes/kubernetes/blob/587d164307de060d271f10f2386f39153360fba9/plugin/pkg/scheduler/algorithm/predicates/predicates.go#L836C4-L848C5), previous developers introduced a mechanism for pre-calculating some data structures and using them to filter pod affinity. The newly added code became inconsistent with the comment:  
   - The code was checking **all pods on nodes with at least one topology key matched**.  
   - The comment, however, still stated **all pods in the cluster**.  
   At this point, the scheduler had fallback logic to the original code if the pre-calculated data didn't exist. Therefore, the scheduler have two routes simultaneously—one consistent and the other inconsistent.

3. [Finally](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L350C2-L360C3), previous developers removed both the fallback logic and the original code. The current implementation only uses the pre-calculated data structures, which are inconsistent with the comment.


#### What did you expect to happen?

According to the history of this rule, we assume the original intend was checking **all pods** in the cluster. It's because the new added data structure, the implementation became wrong as it checks **all pods on nodes with at least one topology key matched**. 

But we think this still need developers' help to check the original / ideal intent of this rule.
1. If the intent is "check all pods" -> the implementation should be fixed.
2. If the intent is "only check pods on nodes with at least one topology key matched", then the comment in kubernetes/kubernetes and documentation in kubernetes/website should be updated.


#### How can we reproduce it (as minimally and precisely as possible)?

##### Steps:
The incoming pod affinity's selector matches itself and also the existing pod.
The incoming pod's pod affinity has 2 terms with 2 different topology keys:
- node-0 has 2 of these topology keys
- node-1 has 1 of these topology keys
- node-2 has 0 of these topology keys

1. add 3 nodes using below yaml file.
`kubectl apply -f nodes.yaml`
2. add the existing pod first, it will land on node-2.
`kubectl apply -f existing_pod.yaml`
3. add the incoming pod, it can be scheduled, although at this time there is a pod that also matches these selectors in the cluster. Because that pod is on node-2, and node-2 doesn't match any topology key.
`kubectl apply -f incoming_pod.yaml`
4. delete all pods.
`kubectl delete pod --all`
5. change the existing pod's `nodeSelector` into `node-name: node-1`,  add the existing pod, it will land on node-1.
`(change the existing pod's nodeSelector)`
`kubectl apply -f existing_pod.yaml`
6. add the incoming pod, it will not be scheduled, this time the pod that also matches these selectors is on node-1, and node-1 matches 1 topology key.
`kubectl apply -f incoming_pod.yaml`
##### Nodes:
```yaml
apiVersion: v1
kind: Node
metadata:
  labels:
    label1: value1
    label2: value2
    node-name: node-0
  name: node-0
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label2: value2
    label3: value3
    node-name: node-1
  name: node-1
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label3: value3
    node-name: node-2
  name: node-2
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
```
##### Existing pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: pod-0
  namespace: default
spec:
  containers:
  - image: nginx
    name: test-container
  nodeSelector:
    node-name: node-2
```
##### Incoming pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: test-pod
  namespace: default
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: pod-label1
            operator: In
            values:
            - value1
        topologyKey: label1
      - labelSelector:
          matchExpressions:
            - key: pod-label1
              operator: In
              values:
                - value1
        topologyKey: label2
  containers:
  - image: nginx
    name: test-container

```



#### Anything else we need to know?

##### Why the current implementation is checking pods on nodes with at least one topology key matched?
The `state.affinityCounts` is a map that maps "topology key-value pairs" to the "number of pods in the topology domain that match the namespace and selector." Below is the [code](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L104C1-L125C1) related to this rule:
```go
# Check the special rule: no other pod in the cluster matches the namespace and selector of this pod.
if len(state.affinityCounts) == 0 &&...

# update state.affinityCounts
func (m topologyToMatchedTermCount) updateWithAffinityTerms(
	terms []framework.AffinityTerm, pod *v1.Pod, node *v1.Node, value int64) {
	if podMatchesAllAffinityTerms(terms, pod) {
		for _, t := range terms {
			m.update(node, t.TopologyKey, value)
		}
	}
}

func (m topologyToMatchedTermCount) update(node *v1.Node, tk string, value int64) {
	if tv, ok := node.Labels[tk]; ok {
		pair := topologyPair{key: tk, value: tv}
		m[pair] += value
		// value could be negative, hence we delete the entry if it is down to zero.
		if m[pair] == 0 {
			delete(m, pair)
		}
	}
}
```
In the last two function, we can see that only an existing pod on a node with at least one topology key required by the incoming pod will be counted in the `state.affinityCount`.
/sig scheduling

#### Kubernetes version

1.32.0


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue描述了Kubernetes调度器中`pod affinity`插件的实现与官方文档和注释不一致，导致了意外的调度结果。具体来说，按照文档和注释的描述，当一个Pod有亲和性规则要求时，调度器在判断是否允许该Pod被调度时，应该检查“集群中所有Pod”是否匹配该Pod的命名空间和选择器。然而，当前的实现仅检查了“在至少匹配一个拓扑键的节点上的所有Pod”，而不是集群中的所有Pod。

这种不一致可能导致安全风险：

- **违反隔离策略**：Pod可能被意外地调度到不符合安全策略的节点上，导致不同租户或应用之间的隔离被打破。
- **数据泄露风险**：敏感数据可能被未经授权的Pod访问。
- **权限提升**：低权限用户可能利用该漏洞将Pod调度到高权限节点，提升其权限。

根据CVSS 3.1评分标准，我们评估该漏洞的评分如下：

- **攻击向量（AV）**：网络（N）——攻击者可以通过网络访问Kubernetes API提交恶意Pod。
- **攻击复杂度（AC）**：低（L）——攻击不需要特殊的条件或环境。
- **特权要求（PR）**：低（L）——需要能够创建Pod的权限，通常在多租户环境下，用户具有在其命名空间创建Pod的权限。
- **用户交互（UI）**：无（N）——不需要其他用户的交互。
- **作用域（S）**：改变（C）——攻击可能影响到其他组件或资源的安全性。
- **机密性（C）**：高（H）——可能导致未经授权的访问，泄露敏感数据。
- **完整性（I）**：无（N）——不会直接导致数据篡改。
- **可用性（A）**：无（N）——不会直接导致服务不可用。

根据以上指标，使用CVSS计算器计算得分：

- **基础分数（Base Score）**：**7.5（高）**

**复现过程：**

```python
# 复现脚本：演示由于Pod亲和性规则的实现与文档不一致，导致Pod被调度到不符合预期的节点上。

import subprocess
import time

def run_cmd(cmd):
    print(f"执行命令：{cmd}")
    subprocess.run(cmd, shell=True, check=True)

def apply_yaml(content, filename):
    with open(filename, 'w') as f:
        f.write(content)
    run_cmd(f"kubectl apply -f {filename}")

def delete_pods():
    run_cmd("kubectl delete pod --all --ignore-not-found")

# 定义节点的YAML
nodes_yaml = """
apiVersion: v1
kind: Node
metadata:
  labels:
    label1: value1
    label2: value2
    node-name: node-0
  name: node-0
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label2: value2
    label3: value3
    node-name: node-1
  name: node-1
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label3: value3
    node-name: node-2
  name: node-2
"""

# 定义现有的Pod，初始调度到node-2
existing_pod_yaml = """
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: pod-0
spec:
  containers:
  - image: nginx
    name: test-container
  nodeSelector:
    node-name: node-2
"""

# 定义待调度的Pod，具有Pod亲和性要求
incoming_pod_yaml = """
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: test-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: pod-label1
            operator: In
            values:
            - value1
        topologyKey: label1
      - labelSelector:
          matchExpressions:
            - key: pod-label1
              operator: In
              values:
                - value1
        topologyKey: label2
  containers:
  - image: nginx
    name: test-container
"""

def main():
    # 删除所有Pod，确保环境干净
    delete_pods()

    # 应用节点配置
    apply_yaml(nodes_yaml, "nodes.yaml")
    time.sleep(2)

    # 部署现有的Pod到node-2
    apply_yaml(existing_pod_yaml, "existing_pod.yaml")
    time.sleep(2)

    # 部署待调度的Pod
    apply_yaml(incoming_pod_yaml, "incoming_pod.yaml")
    time.sleep(5)

    # 查看Pod的调度情况
    print("第一次调度结果：")
    run_cmd("kubectl get pods -o wide")

    # 删除所有Pod
    delete_pods()
    time.sleep(2)

    # 修改现有Pod的nodeSelector为node-1
    modified_existing_pod_yaml = existing_pod_yaml.replace("node-2", "node-1")

    # 部署修改后的现有Pod到node-1
    apply_yaml(modified_existing_pod_yaml, "existing_pod.yaml")
    time.sleep(2)

    # 再次部署待调度的Pod
    apply_yaml(incoming_pod_yaml, "incoming_pod.yaml")
    time.sleep(5)

    # 查看Pod的调度情况
    print("第二次调度结果：")
    run_cmd("kubectl get pods -o wide")

    print("复现完成。请查看调度结果。")

if __name__ == "__main__":
    main()
```


**解释说明：**

**脚本功能**：

该脚本旨在复现Issue中描述的问题，演示由于调度器实现与文档不一致，导致Pod调度结果不符合预期的情况。

**执行步骤**：

1. **清理环境**：删除集群中所有的Pod，确保测试环境干净。
   
2. **创建节点**：创建三个节点`node-0`、`node-1`和`node-2`，并分别赋予不同的标签，以用于测试拓扑键。

3. **第一次测试**：

   - **部署现有Pod到`node-2`**：将`pod-0`部署到`node-2`上，`pod-0`的标签将匹配待调度Pod的亲和性选择器。
   
   - **部署待调度Pod**：部署具有Pod亲和性要求的`test-pod`。按照文档，`test-pod`应该无法被调度（因为集群中存在匹配选择器的Pod）。
   
   - **结果观察**：脚本将显示`test-pod`的实际调度结果。由于实现问题，`test-pod`实际上被成功调度，违反了预期。

4. **第二次测试**：

   - **清理环境**：再次删除所有Pod。

   - **修改现有Pod的节点**：将`pod-0`的`nodeSelector`修改为`node-1`，使其部署到`node-1`上。`node-1`具有待调度Pod亲和性要求的拓扑键。

   - **再次部署待调度Pod**：部署`test-pod`。按照实现，`test-pod`此时无法被调度，因为调度器在`node-1`上发现了匹配的Pod，且`node-1`匹配拓扑键。

   - **结果观察**：脚本将显示`test-pod`的实际调度结果，此时`test-pod`无法被调度，符合预期。

**结论**：

通过该脚本，证明了调度器在处理Pod亲和性时，仅检查了部分节点，导致了Pod被调度到不符合安全策略的节点上。这可能被恶意用户利用，绕过安全限制，访问未经授权的资源，带来高风险的安全问题。

---


## Issue #129249 devicemanager stuck in Allocate RPC and causing all new coming pods pending on admitting

- Issue 链接：[#129249](https://github.com/kubernetes/kubernetes/issues/129249)

### Issue 内容

#### What happened?

After created pod with device request, its state always be `Pending`, even though it was scheduled.
And at the same time, new coming pod (without device request) on the same node is on `Pending` state too.

After a bit investigation with goroutines, devicemanager was stucking in `Allocate` RPC, and wait device plugin's response:
![image](https://github.com/user-attachments/assets/0a97e9ef-5341-4fd7-89bb-d4d210c3f00f)

Accroding to this stack, pod admitting is stucked obviously.

#### What did you expect to happen?

Device plugins' bug or out-of-service, should not lead kubelet pod admit procedure blocking.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduct it with https://github.com/kube-HPC/k8s-dummy-device-plugin. Add some sleep time in `Allocate` call:
![image](https://github.com/user-attachments/assets/9a1b021a-27a1-43a1-af73-0f6426e7f169)

![image](https://github.com/user-attachments/assets/501dd05e-10f8-4327-825b-16554896c0b6)

pod with dummy device and new coming pod without dummy device are in `Pending` state:
![image](https://github.com/user-attachments/assets/d73c63a5-ae7c-405c-9dc6-bce45076a01e)


After running out sleep time, all pod return to `Running` state:
![image](https://github.com/user-attachments/assets/212bcc69-df04-490f-8fad-24c8be5e4ce2)


#### Anything else we need to know?

NA

#### Kubernetes version

<details>

```console
v1.25
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
CentOS
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
在该Issue中，描述了一个问题：在Kubernetes集群中，当创建了一个带有设备请求的Pod时，该Pod一直处于`Pending`状态，即使它已经被调度。同时，在同一节点上新创建的Pod（即使不需要设备请求）也处于`Pending`状态。

经过调查发现，`devicemanager`在处理`Allocate` RPC时卡住了，等待设备插件（device plugin）的响应，导致Pod的准入过程被阻塞。这意味着，如果设备插件出现问题（例如bug或无响应），会影响到节点上所有Pod的运行，包括不请求设备的Pod。

从安全角度分析，这可能导致拒绝服务（DoS）攻击。如果攻击者能够使设备插件的`Allocate`调用卡住，就能阻塞节点上所有Pod的调度和运行，影响集群的可用性。特别是在多租户环境中，这种行为可能被低权限用户利用来影响其他用户的服务。

根据CVSS 3.1标准进行评分：

- 攻击向量（AV）：网络（N）——攻击者可以通过提交特制的Pod进行远程攻击
- 攻击复杂度（AC）：低（L）——攻击不需要特殊条件
- 权限要求（PR）：低（L）——攻击者需要能够创建Pod的权限
- 用户交互（UI）：无（N）——不需要其他用户的交互
- 作用范围（S）：改变（C）——攻击影响到了其他组件（其他用户的Pod）
- 机密性（C）：无（N）
- 完整性（I）：无（N）
- 可用性（A）：高（H）——服务不可用

CVSS向量为：`CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H`，基本分为7.5（高）

**复现过程：**

```python
import os
import time
import threading
import subprocess
import sys
import signal

# Function to simulate a hanging Allocate call in device plugin
def start_dummy_device_plugin():
    # Create the device plugin directory if it doesn't exist
    plugin_dir = '/var/lib/kubelet/device-plugins'
    os.makedirs(plugin_dir, exist_ok=True)
    
    # Create a Unix socket file for the dummy device plugin
    socket_path = os.path.join(plugin_dir, 'dummy-plugin.sock')
    
    # Remove the socket file if it already exists
    if os.path.exists(socket_path):
        os.remove(socket_path)
    
    # Start a Unix socket server to simulate the device plugin
    import socket
    server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    server.bind(socket_path)
    server.listen(1)
    print("Dummy device plugin started and waiting for connections...")
    
    while True:
        conn, addr = server.accept()
        print("Connection received. Simulating hanging Allocate call...")
        time.sleep(600)  # Simulate hang for 10 minutes

def register_device_plugin():
    # Register the dummy device plugin with kubelet
    registration_message = '''
    {
        "version": "v1beta1",
        "endpoint": "dummy-plugin.sock",
        "resourceName": "example.com/dummy",
        "options": {}
    }
    '''
    plugin_dir = '/var/lib/kubelet/device-plugins'
    kubelet_socket = os.path.join(plugin_dir, 'kubelet.sock')
    if not os.path.exists(kubelet_socket):
        print("Kubelet socket not found. Is kubelet running with device-plugins enabled?")
        sys.exit(1)
    
    # Send the registration message to kubelet via Unix socket
    import socket
    client = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    client.connect(kubelet_socket)
    client.sendall(bytes(registration_message, 'utf-8'))
    client.close()
    print("Dummy device plugin registered with kubelet.")

def create_pods():
    # Create a pod that requests the dummy device
    pod_with_device = '''
apiVersion: v1
kind: Pod
metadata:
  name: dummy-pod
spec:
  containers:
  - name: dummy-container
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    resources:
      limits:
        example.com/dummy: 1
    '''
    with open('dummy-pod.yaml', 'w') as f:
        f.write(pod_with_device)
    subprocess.run(['kubectl', 'apply', '-f', 'dummy-pod.yaml'])
    print("Pod with device request created.")
    
    # Create a pod that does not request any device
    pod_without_device = '''
apiVersion: v1
kind: Pod
metadata:
  name: normal-pod
spec:
  containers:
  - name: normal-container
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    '''
    with open('normal-pod.yaml', 'w') as f:
        f.write(pod_without_device)
    subprocess.run(['kubectl', 'apply', '-f', 'normal-pod.yaml'])
    print("Normal pod without device request created.")

def monitor_pods():
    print("Monitoring pod statuses...")
    while True:
        dummy_pod_status = subprocess.getoutput('kubectl get pod dummy-pod -o jsonpath="{.status.phase}"')
        normal_pod_status = subprocess.getoutput('kubectl get pod normal-pod -o jsonpath="{.status.phase}"')
        print(f"dummy-pod status: {dummy_pod_status}, normal-pod status: {normal_pod_status}")
        time.sleep(5)

if __name__ == '__main__':
    # Check if script is run as root
    if os.geteuid() != 0:
        print("Please run this script as root.")
        sys.exit(1)
    
    # Start dummy device plugin in a separate thread
    plugin_thread = threading.Thread(target=start_dummy_device_plugin, daemon=True)
    plugin_thread.start()
    
    # Give the plugin time to start
    time.sleep(5)
    
    # Register the device plugin with kubelet
    register_device_plugin()
    
    # Create pods
    create_pods()
    
    # Monitor pod statuses
    monitor_pods()
```


**解释说明：**

该脚本模拟了一个设备插件，在`Allocate`调用中人为地加入了长时间的`sleep`，以模拟设备插件挂起的情况。

具体步骤：

1. **启动设备插件：** 使用Unix Socket模拟设备插件，等待连接后，在`Allocate`调用中`sleep` 600秒（10分钟）。

2. **注册设备插件：** 向kubelet的`kubelet.sock`发送注册消息，注册模拟的设备插件。

3. **创建Pod：** 创建两个Pod：一个请求模拟的设备（`dummy-pod`），一个不请求设备（`normal-pod`）。

4. **监控Pod状态：** 不断查询并输出两个Pod的状态。

运行该脚本需要满足以下条件：

- 在Linux系统上运行，并以`root`用户权限执行（因为需要访问`/var/lib/kubelet/device-plugins`目录和与kubelet通信）。
- Kubernetes集群已部署并运行，且`kubectl`已正确配置。
- kubelet启用了设备插件功能（使用`--feature-gates=DevicePlugins=true`启动）。

通过运行该脚本，可以观察到：

- `dummy-pod`因为设备插件的`Allocate`调用被阻塞，处于`Pending`状态。
- `normal-pod`虽然不需要设备，但也会因为设备插件的问题而处于`Pending`状态。
- 当设备插件的`Allocate`调用结束后（10分钟后），两个Pod都会进入`Running`状态。

这证明了设备插件的异常会影响到整个节点上Pod的调度和运行，可能被恶意利用来实施拒绝服务攻击。

---


## Issue #129199 Unable to verify signed images for 1.32 release

- Issue 链接：[#129199](https://github.com/kubernetes/kubernetes/issues/129199)

### Issue 内容

#### What happened?


tried to verify using command mentioned in https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures 
```
cosign verify registry.k8s.io/kube-apiserver-amd64:v1.32.0 \
  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \
  --certificate-oidc-issuer https://accounts.google.com \
  | jq .
Error: no signatures found
main.go:69: error during command execution: no signatures found
```

#### What did you expect to happen?

Should have return along with signatures details 
```
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The code-signing certificate was verified using trusted certificate authority certificates
```

#### How can we reproduce it (as minimally and precisely as possible)?

Run 
```
cosign verify registry.k8s.io/kube-apiserver-amd64:v1.32.0 \
  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \
  --certificate-oidc-issuer https://accounts.google.com 
```

#### Anything else we need to know?

happing for other 1.32 patch images

- registry.k8s.io/kube-controller-manager:v1.32.0
- registry.k8s.io/kube-proxy:v1.32.0
- registry.k8s.io/kube-scheduler:v1.32.0
- registry.k8s.io/conformance:v1.32.0




#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
NA 
</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
 cosign version
  ______   ______        _______. __    _______ .__   __.
 /      | /  __  \      /       ||  |  /  _____||  \ |  |
|  ,----'|  |  |  |    |   (----`|  | |  |  __  |   \|  |
|  |     |  |  |  |     \   \    |  | |  | |_ | |  . `  |
|  `----.|  `--'  | .----)   |   |  | |  |__| | |  |\   |
 \______| \______/  |_______/    |__|  \______| |__| \__|
cosign: A tool for Container Signing, Verification and Storage in an OCI registry.

GitVersion:    2.4.1
GitCommit:     9a4cfe1aae777984c07ce373d97a65428bbff734
GitTreeState:  "clean"
BuildDate:     2024-10-03T17:01:50Z
GoVersion:     go1.23.2
Compiler:      gc
Platform:      darwin/arm64
</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue描述了在使用cosign工具对Kubernetes 1.32版本的镜像进行签名验证时，出现了`Error: no signatures found`的错误。这意味着用户无法验证下载的镜像是否经过官方签名，存在供应链安全风险。如果攻击者替换了这些镜像，用户无法通过签名验证发现，可能导致运行被篡改或恶意的镜像，造成严重的安全后果。

根据CVSS 3.1标准进行评分：

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **特权要求（PR）**：无（N）
- **用户交互（UI）**：需要（R）
- **范围（S）**：未改变（U）
- **机密性影响（C）**：高（H）
- **完整性影响（I）**：高（H）
- **可用性影响（A）**：高（H）

计算得分为**8.8（高）**，因此风险评级为高风险。

**复现过程：**

```python
import subprocess

def verify_image_signature():
    image = "registry.k8s.io/kube-apiserver-amd64:v1.32.0"
    identity = "krel-trust@k8s-releng-prod.iam.gserviceaccount.com"
    issuer = "https://accounts.google.com"
    command = [
        "cosign", "verify", image,
        "--certificate-identity", identity,
        "--certificate-oidc-issuer", issuer
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        print("签名验证成功，输出如下：")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print("签名验证失败，错误信息如下：")
        print(e.stderr)

if __name__ == "__main__":
    verify_image_signature()
```


**解释说明：**

该脚本使用Python的`subprocess`模块调用`cosign`命令，对指定的Kubernetes镜像进行签名验证。脚本执行以下操作：

1. 定义要验证的镜像名称、证书身份和OIDC发行者。
2. 构建`cosign verify`命令的参数列表。
3. 使用`subprocess.run`执行命令，并捕获输出。
4. 如果验证成功，打印签名验证的输出信息。
5. 如果验证失败，捕获异常并打印错误信息。

该脚本复现了Issue中提到的问题，即在验证`v1.32.0`版本的Kubernetes镜像时，无法找到签名，从而提示`no signatures found`的错误。这表明该版本的镜像缺少签名，存在安全风险。

---


# ⚠️ 存在低风险的 Issues (2 个)

## Issue #129357 [FG:InPlacePodVerticalScaling] Pod CPU limit is not configured to cgroups as calculated if systemd cgroup driver is used

- Issue 链接：[#129357](https://github.com/kubernetes/kubernetes/issues/129357)

### Issue 内容

#### What happened?

As a result of #124216, which was introduced in v.1.32, a pod CPU limit calculated in `ResourceConfigForPod()` is rounded up to the nearest 10ms in `libcontainer` at resizing the pod:
- Resize a pod:
  ```
  $ kubectl patch pod resize-pod --subresource=resize --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"limits":{"cpu":"417m"}}}]}}'
  pod/resize-pod patched
  ```
- The container cgroup value is set with 1ms precision:
  ```
  $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.max
  41700 100000
  ```
- The pod cgroup value is rounded up:
  ```
  $ cat /sys/fs/cgroup/kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-burstable.slice/kubelet-kubepods-burstable-pod68a17b59_0d31_40b2_ba86_ea43f3b2f05c.slice/cpu.max
  42000 100000
  ```

When `systemd` cgroup driver is used, `libcontainer` passes the CPU Quota to `systemd` with rounding up:
https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/common.go#L304-L311

In addition, there seems to be a race in `libcontainer`. It directly writes values to the cgroup file without roundup after it passes the rounded value to `systemd`:
https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/v2.go#L489-L493

So, there is also a case where the cgroup value is set as calculated. As far as I tried, decreasing CPU limits usually hits this case though I’m not sure why:
- Decrease the CPU limits:
  ```
  $ kubectl patch pod resize-pod --subresource=resize --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"limits":{"cpu":"365m"}}}]}}'
  pod/resize-pod patched
  ```

- Both the container and the pod cgroup values are set with 1ms precision:
  ```
  $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.max
  36500 100000
  $ cat /sys/fs/cgroup/kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-burstable.slice/kubelet-kubepods-burstable-pod68a17b59_0d31_40b2_ba86_ea43f3b2f05c.slice/cpu.max
  36500 100000
  ```


#### What did you expect to happen?
This roundup looks like the intended behavior of `systemd` cgroup driver because CPU quota is also rounded up when a pod is just created with 1ms precision CPU limits. However, I have the following concerns:
- We might need to confirm this tiny gap doesn’t cause a similar issue to #128769 at resizing pods.
- We might need to clarify why the CPU quota of pod cgroup is sometimes not rounded up. This is especially necessary to complete #127192, which is going to add pod cgroup verification to resize tests.


#### How can we reproduce it (as minimally and precisely as possible)?

0. Use `systemd` cgroup driver and enable `InPlacePodVertialScaling`.
1. Resize CPU limits of a pod with 1ms precision.


#### Anything else we need to know?

_No response_

#### Kubernetes version

V1.32
<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了在启用`InPlacePodVerticalScaling`特性，并使用`systemd` cgroup驱动时，Pod的CPU限制在配置到cgroups时，与计算值存在偏差。具体表现为：

1. 当调整Pod的CPU限制时，CPU配额在传递给`systemd`时会进行向上取整（以10ms为单位），导致Pod的cgroup中配置的CPU限额略高于预期值。
2. 有时，尤其是在降低CPU限制时，Pod的cgroup中的CPU限额与计算值一致，没有向上取整，表现出不一致性。
3. 这种不一致可能导致类似于Issue [#128769](https://github.com/kubernetes/kubernetes/issues/128769)的问题，即实际分配的资源超出预期，可能影响系统的资源管理和调度。

潜在的安全风险分析：

- **资源超额使用**：由于CPU限制值向上取整，Pod可能获得比预期更多的CPU资源。在多租户环境中，这可能导致某个Pod占用过多的CPU资源，影响其他Pod的性能。
- **拒绝服务（DoS）**：资源的过度使用可能导致其他服务性能下降，存在潜在的DoS风险。

但需要注意的是：

- 该问题的触发需要具备修改Pod资源限制的权限，即需要具备创建或修改Pod的权限。
- 向上取整的偏差相对较小，通常在几个毫秒范围内，对整体系统影响有限。
- 该问题更多地影响资源的精确分配和管理，属于性能和资源管理问题，而非直接的安全漏洞。

**解释说明：**

根据风险判断标准：

- **规则5**：当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险。
- **规则3**：根据CVSS 3.1评分标准，该问题可能涉及到系统可用性的轻微影响，评分在High以下。
- **拒绝服务风险**：虽然存在资源过度使用的可能，但偏差较小，且需要特定权限。

因此，综合判断该Issue为**低风险**，无需提供复现脚本。

---


## Issue #129144 When a pod is removed, running execs stay open but frozen

- Issue 链接：[#129144](https://github.com/kubernetes/kubernetes/issues/129144)

### Issue 内容

#### What happened?

When a pod is removed, running execs stay. Their standard streams are not closed.

#### What did you expect to happen?

The root processes of the execs are killed, so the running exec should be terminated by the kubeapi-server, or at least the stdout/stderr streams should be closed.

#### How can we reproduce it (as minimally and precisely as possible)?

- create a pod with a single container
- exec into that container with `kubectl exec` an interactive command (like a shell)
- remove that pod
- the `kubectl exec` is still running but is `frozen`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
minikube or GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了在Kubernetes环境中，当一个Pod被删除后，通过`kubectl exec`建立的exec会话仍然保持打开状态，但处于冻结状态，其标准输入输出流未被关闭。

从安全角度分析：

1. **拒绝服务（DoS）风险**：攻击者可能通过重复此操作，占用API服务器或节点的资源，导致资源耗尽，影响集群的可用性。
2. **资源泄漏**：未关闭的会话可能导致系统资源（如文件描述符、内存等）未被释放。

然而：

- **权限要求高**：执行此操作需要具备对集群的访问权限，能够创建Pod并执行`kubectl exec`命令。这通常是授权用户才具备的权限。
- **影响范围有限**：由于需要高权限，且只影响自身有权限的资源，对其他用户或系统的影响较小。

根据CVSS 3.1评分标准：

- 攻击向量 (AV)：网络 (N)
- 攻击复杂度 (AC)：低 (L)
- 权限要求 (PR)：高 (H)
- 用户交互 (UI)：无 (N)
- 影响范围 (S)：未改变 (U)
- 机密性 (C)：无影响 (N)
- 完整性 (I)：无影响 (N)
- 可用性 (A)：低 (L)

经计算，基础评分为3.2，属于低风险范围。

**解释说明：**

根据分析，该问题被评估为低风险，不需要提供复现脚本。虽然存在资源泄漏和潜在的DoS风险，但由于需要高权限才能实施攻击，且影响范围有限，不属于高风险范畴。建议在系统中添加机制，确保当Pod被删除时，相关的exec会话能被正常关闭，以释放资源。

---


# ✅ 不涉及安全风险的 Issues (52 个)

## Issue #129417 apiserver healthz should check etcd override endpoints

- Issue 链接：[#129417](https://github.com/kubernetes/kubernetes/issues/129417)

### Issue 内容

#### What happened?

It seems the apiserver will fail bootstrap if the etcd override endpoint is not healthy.
But after the bootstrap completes, if the etcd override endpoint become unhealthy, the apiserver health check will still report OK while `kubectl get cs` will report etcd override endpoint is not healthy.

#### What did you expect to happen?

APIserver health check should report unhealthy when an etcd override endpoint is unhealthy.

#### How can we reproduce it (as minimally and precisely as possible)?

Run 2 etcd clusters: one for events and one for the other resources. Then configure apiserver to use the event etcd using the `--etcd-servers-overrides` flag. After the apiserver complete the bootstrap, kill the event etcd.
You will be able to see APIServer health check still reporting OK.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32. I think I have seen this issue in older version as well.

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在APIServer使用`--etcd-servers-overrides`参数配置了额外的etcd集群用于存储事件等资源的情况下，当该etcd集群出现故障（例如被关闭）后，APIServer的健康检查接口（healthz）仍然报告OK状态，而通过`kubectl get cs`可以看到etcd override endpoint状态不健康。

从问题描述来看，这是APIServer健康检查功能的缺陷，导致在etcd override endpoint不健康时，APIServer的健康检查未能准确反映实际状态。这可能会给集群的运维和监控带来不便，但并不涉及任何安全风险。

根据风险判断标准：

1. 如果Issue描述的问题非安全问题，则风险评级判断为不涉及。

因此，该问题不属于安全问题。

**解释说明：**

该Issue不涉及安全风险，是一个功能性问题，因此无需提供复现脚本。

---


## Issue #129413 Link to Borg in readme is broken

- Issue 链接：[#129413](https://github.com/kubernetes/kubernetes/issues/129413)

### Issue 内容

#### What happened?

I'm new to the Kubernetes repo and was looking through the docs and discovered a broken link to Borg in the readme file.

![image](https://github.com/user-attachments/assets/6acaa45b-fbd3-40d0-b08a-ee51d3f0cc9b)

#### What did you expect to happen?

See the page linked to Borg.

#### How can we reproduce it (as minimally and precisely as possible)?

Click on the link to Borg on the readme.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3```

</details>


#### Cloud provider

<details>

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在Kubernetes仓库的README文件中，指向Borg的链接已失效。当用户尝试点击该链接时，无法访问到预期的页面。Borg是谷歌的内部集群管理系统，Kubernetes受其启发而开发。

从Issue的描述和内容来看，这仅仅是一个文档中的链接失效问题。链接失效可能是由于目标页面已被移动、删除，或链接地址拼写错误导致。这种情况不会引起任何安全风险，不涉及代码或系统的漏洞，更不会导致信息泄露、权限提升、远程代码执行等安全问题。

根据风险判断标准：

1. Issue描述的问题非安全问题，则风险评级判断为不涉及。

**解释说明：**

该Issue仅涉及文档中一个失效的链接，属于一般的文档维护和更新问题，不涉及任何安全风险或漏洞。因此，无需提供复现脚本。

---


## Issue #129408 kubernetes 1.32: Informer with WatchClient fails to send events with Fakeclient

- Issue 链接：[#129408](https://github.com/kubernetes/kubernetes/issues/129408)

### Issue 内容

#### What happened?

I tried to start an informer with WatchClient enabled in client go and i see this error

```
W1220 19:44:49.417766   26071 reflector.go:1044] k8s.io/client-go@v1.32/tools/cache/reflector.go:243: awaiting required bookmark event for initial events stream, no events received for 10.000196792s

```

and the above error keeps coming every 10 seconds.

The informer factory is stuck at this line

`xInformerFactory.WaitForCacheSync(ctx.Done())`





#### What did you expect to happen?

Watclist feature to work and Events to propagate when a resource is created.

#### How can we reproduce it (as minimally and precisely as possible)?

```
package main

import (
	"context"
	"flag"
	"fmt"
	"os"
    "time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes/fake"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"
)

func init() {
	scheme := runtime.NewScheme()
	utilruntime.Must(corev1.AddToScheme(scheme))

}

func main() {
	os.Setenv("KUBE_FEATURE_WatchListClient", "true")
	client := fake.NewSimpleClientset()

	defer klog.Flush()
	flagSet := flag.NewFlagSet("test", flag.ExitOnError)
	klog.InitFlags(flagSet)
	_ = flagSet.Parse([]string{"--v", "6"})

	// Create an informer factory for the fake client
	informerFactory := informers.NewSharedInformerFactory(client, 0)

	// Get the Pod informer
	podInformer := informerFactory.Core().V1().Pods().Informer()

	// Add an event handler to the informer
	podInformer.AddEventHandler(&cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			pod := obj.(*corev1.Pod)
			fmt.Printf("Pod added: %s/%s\n", pod.Namespace, pod.Name)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			newPod := newObj.(*corev1.Pod)
			fmt.Printf("Pod updated: %s/%s\n", newPod.Namespace, newPod.Name)
		},
		DeleteFunc: func(obj interface{}) {
			pod := obj.(*corev1.Pod)
			fmt.Printf("Pod deleted: %s/%s\n", pod.Namespace, pod.Name)
		},
	})

	// Start the informer
	stopCh := make(chan struct{})
	defer close(stopCh)
	informerFactory.Start(stopCh)

	// Wait for cache sync
	if !cache.WaitForCacheSync(stopCh, podInformer.HasSynced) {
		fmt.Println("Failed to sync cache")
		return
	}
	// Use the client to create, update, or fetch resources
	pod := &corev1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "example-pod",
			Namespace: "default",
		},
		Spec: corev1.PodSpec{
			Containers: []corev1.Container{
				{
					Name:  "example-container",
					Image: "nginx",
				},
			},
		},
	}

	// Create a Pod
	_, err := client.CoreV1().Pods("default").Create(context.TODO(), pod, metav1.CreateOptions{})
	if err != nil {
		panic(err)
	}

	
	time.Sleep(time.Second * 5) 
	stopCh <- struct{}{}
}
```

go.mod file:

```
module myworks/watchlist-test

go 1.23.0

require (
	k8s.io/api v0.32.0
	k8s.io/apimachinery v0.32.0
	k8s.io/client-go v0.32.0
	k8s.io/klog/v2 v2.130.1
)

require (
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/fxamacker/cbor/v2 v2.7.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-openapi/jsonpointer v0.21.0 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.23.0 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/x448/float16 v0.8.4 // indirect
	golang.org/x/net v0.30.0 // indirect
	golang.org/x/oauth2 v0.23.0 // indirect
	golang.org/x/sys v0.26.0 // indirect
	golang.org/x/term v0.25.0 // indirect
	golang.org/x/text v0.19.0 // indirect
	golang.org/x/time v0.7.0 // indirect
	google.golang.org/protobuf v1.35.1 // indirect
	gopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/kube-openapi v0.0.0-20241105132330-32ad38e42d3f // indirect
	k8s.io/utils v0.0.0-20241104100929-3ea5e8cea738 // indirect
	sigs.k8s.io/json v0.0.0-20241010143419-9aa6b5e7a4b3 // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.2 // indirect
	sigs.k8s.io/yaml v1.4.0 // indirect
)
```



Error logs:
```
go run main.go
I1223 14:40:04.018354   35133 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I1223 14:40:04.018890   35133 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I1223 14:40:04.018897   35133 envvar.go:169] "Feature gate updated state" feature="WatchListClient" enabled=true
I1223 14:40:04.018901   35133 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I1223 14:40:04.018930   35133 reflector.go:313] Starting reflector *v1.Pod (0s) from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:40:04.018968   35133 reflector.go:349] Listing and watching *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
W1223 14:40:14.020097   35133 reflector.go:1052] pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: awaiting required bookmark event for initial events stream, no events received for 10.000426333s
W1223 14:40:24.019566   35133 reflector.go:1052] pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: awaiting required bookmark event for initial events stream, no events received for 20.000542791s
```




#### Anything else we need to know?


change the env var to false and you see things are working. 

```
os.Setenv("KUBE_FEATURE_WatchListClient", "false")
```

Logs:

```
go run main.go
I1223 14:45:47.374375   39780 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I1223 14:45:47.374853   39780 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I1223 14:45:47.374859   39780 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I1223 14:45:47.374864   39780 envvar.go:169] "Feature gate updated state" feature="WatchListClient" enabled=false
I1223 14:45:47.374889   39780 reflector.go:313] Starting reflector *v1.Pod (0s) from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:45:47.374927   39780 reflector.go:349] Listing and watching *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:45:47.375349   39780 reflector.go:376] Caches populated for *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
Pod added: default/example-pod
```

#### Kubernetes version

Client Version: v1.32.0

#### Cloud provider

Local

#### OS version

MacOS
Darwin Kernel Version 23.6.0

#### Install tools

golang 

```
go version
go version go1.23.0 darwin/arm64
```

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes 1.32版本中，使用client-go库的Fake Client进行测试时，当启用了`WatchListClient`特性（通过环境变量`KUBE_FEATURE_WatchListClient`设置为`true`）后，Informer无法正常工作，出现了等待bookmark事件的错误，并且缓存同步卡住。这导致了Informer无法接收到资源的事件通知。

从问题描述和提供的示例代码来看，这是一个在启用`WatchListClient`特性后，Fake Client与Informer之间的兼容性问题，导致无法正确处理事件流。

整个Issue中没有涉及任何安全方面的内容，例如：权限提升、命令执行、信息泄露、拒绝服务攻击等。因此，这只是一个功能性缺陷或Bug，而非安全漏洞。

**解释说明：**

该Issue的问题在于启用了`WatchListClient`特性后，Fake Client无法与Informer正确协作，导致Informer无法接收到资源事件。这是一个功能性的问题，可能需要在代码中修复Fake Client与`WatchListClient`特性的兼容性。由于没有涉及任何安全风险，不会对系统的安全性造成影响，因此风险评级为“不涉及”。

---


## Issue #129405 Pending should not be handled as Error in PreFilter

- Issue 链接：[#129405](https://github.com/kubernetes/kubernetes/issues/129405)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/35f584187a6d1250191aa24b0dcf735350f57508/pkg/scheduler/framework/runtime/framework.go#L735-L750

Kube Scheduler will return framework.Error when any plugin return Pending in PreFilter

#### What did you expect to happen?

Kube Scheduler should return Pending if any plugin return Pending in PreFilter.

#### How can we reproduce it (as minimally and precisely as possible)?

Implement a scheduler plugin which return Pending in PreFilter.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.32
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在Kubernetes调度器的PreFilter阶段，当任何插件返回`Pending`状态时，调度器会将其错误地处理为`Error`状态。根据链接的代码，可以看到调度器在处理插件返回的状态时，没有正确区分`Pending`和`Error`，导致预期返回`Pending`状态的插件被认为是发生了错误。

从问题描述来看，这是一个调度器逻辑处理上的问题，属于功能性错误。这个问题可能会导致调度器的行为与插件的设计预期不一致，从而影响调度决策。但整个过程中，并未涉及任何安全方面的漏洞或风险：

- **信息泄露**：不存在，问题不涉及敏感信息的处理或传输。
- **权限提升**：不存在，问题不涉及权限控制或越权操作。
- **代码/命令执行**：不存在，问题不涉及执行任意代码或命令的风险。
- **拒绝服务（DoS）**：虽然调度器的错误处理可能影响到服务的正常调度，但由于必须编写特定的插件返回`Pending`状态才能触发，且需要管理员权限来部署插件，因此不属于高风险的DoS攻击。

根据以上分析，问题属于功能性错误，对系统的安全性没有直接影响。

---


## Issue #129403 killPodOptions not showing up properly

- Issue 链接：[#129403](https://github.com/kubernetes/kubernetes/issues/129403)

### Issue 内容

#### What happened?

``` shell
I1226 15:13:00.040437 4101130 kubelet_pods.go:473] "Clean up probes for terminated pods"
I1226 15:13:00.040470 4101130 kubelet_pods.go:545] "Clean up containers for orphaned pod we had not seen before" podUID="2a07603d-dc01-4897-84f4-716127ffe399" killPodOptions="<internal error: json: unsupported type: chan<- struct {}>"
I1226 15:13:00.040502 4101130 pod_workers.go:727] "Pod is being synced for the first time" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" updateType="kill"
I1226 15:13:00.040524 4101130 pod_workers.go:808] "Pod is orphaned and must be torn down" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" updateType="kill"
I1226 15:13:00.040551 4101130 pod_workers.go:913] "Notifying pod of pending update" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" workType="terminating"
```

#### What did you expect to happen?

`killPodOptions` correctly displays parameter information

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，问题出现在`killPodOptions`参数在日志中未正确显示，输出了`<internal error: json: unsupported type: chan<- struct {}>`的错误信息。这表明在序列化`killPodOptions`参数为JSON格式时，遇到了不支持的类型`chan<- struct {}`，导致序列化失败。

从安全角度来看：

- **信息泄露**：日志中泄露的信息仅为内部错误和数据类型描述，没有包含任何敏感数据或凭据信息。
- **攻击面**：该错误属于内部日志记录问题，外部攻击者无法通过此错误直接进行攻击，也无法利用此信息进行进一步的渗透或攻击。
- **影响范围**：问题仅影响日志的正确性，对系统功能和安全性没有直接影响。

根据以上分析，该问题并未引入新的安全风险，属于功能性缺陷或日志记录问题。

---


## Issue #129402 DRA: Pod termination is stuck when DRA Driver is stopped

- Issue 链接：[#129402](https://github.com/kubernetes/kubernetes/issues/129402)

### Issue 内容

#### What happened?

A status of Pod allocated some device remains as terminating when the DRA Driver is stopped.
I don't know this is intentional or a bug.

#### What did you expect to happen?

A Pod is completely terminated.

#### How can we reproduce it (as minimally and precisely as possible)?

We can reproduce it using [dra-example-driver](https://github.com/kubernetes-sigs/dra-example-driver).

#### Summary
1. Install the `DRA Driver`(dra-example-driver) and create a `DeviceClass`.
1. Create a `ResourceClaimTemplate`.
1. Deploy a Pod allocated some device via the `ResourceClaimTemplate`.
1. Stop the DRA Driver.
1. Delete the Pod.
1. The Pod remains as terminating.

#### Procedure

<details>

Install the `DRA Driver` and create a `DeviceClass` by following [dra-example-driver demo](https://github.com/kubernetes-sigs/dra-example-driver?tab=readme-ov-file#demo).

Create a `ResourceClaimTemplate`.

`resource-claim-template-0.yaml`
```yaml
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  name: single-gpu
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.example.com
```
```bash
$ kubectl apply -f resource-claim-template-0.yaml
```

Deploy a Pod allocated some device via the `ResourceClaimTemplate`.

`sample-pod-0.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod-0
  labels:
    app: sample-pod-0
spec:
  containers:
  - name: ctr0
    image: ubuntu:22.04
    command: ["bash", "-c"]
    args: ["export; trap 'exit 0' TERM; sleep 9999 & wait"]
    resources:
      claims:
      - name: gpu
  resourceClaims:
  - name: gpu
    resourceClaimTemplateName: single-gpu
```

```bash
$ kubectl apply -f sample-pod-0.yaml
```


The current status of cluster is as follows.
```bash
$ kubectl get deviceclasses,resourceclaimtemplates,resourceclaim,pods
NAME                                          AGE
deviceclass.resource.k8s.io/gpu.example.com   54m

NAME                                               AGE
resourceclaimtemplate.resource.k8s.io/single-gpu   85s

NAME                                                   STATE                AGE
resourceclaim.resource.k8s.io/sample-pod-0-gpu-hxsn7   allocated,reserved   77s

NAME               READY   STATUS    RESTARTS   AGE
pod/sample-pod-0   1/1     Running   0          77s
```

Stop the DRA Driver.
In this case, we can uninstall the `dra-example-driver` via helm.
```bash
$ helm -n dra-example-driver uninstall dra-example-driver
```

Delete the Pod and the status remains as terminating.
```bash
$ kubectl delete po sample-pod-0
pod "sample-pod-0" deleted
(stucking...)

$ kubectl get pod
NAME           READY   STATUS        RESTARTS   AGE
sample-pod-0   0/1     Terminating   0          17m
```

</details>

#### Anything else we need to know?

The kubelet log shows the following error. 
```bash
# journalctl -xu kubelet
...
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: I1226 05:35:41.323475     231 kubelet.go:2490] "SyncLoop DELETE" source="api" pods=["default/sample-pod-0"]
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: I1226 05:35:41.323611     231 kuberuntime_container.go:809] "Killing container with a grace period" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc" containerName="ctr0" containerID="containerd://f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05" gracePeriod=30
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: E1226 05:35:41.415637     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032843     231 generic.go:358] "Generic (PLEG): container finished" podID="5598e74f-08ff-40ab-aba3-fa811874f9dc" containerID="f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05" exitCode=0
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032885     231 kubelet.go:2506] "SyncLoop (PLEG): event for pod" pod="default/sample-pod-0" event={"ID":"5598e74f-08ff-40ab-aba3-fa811874f9dc","Type":"ContainerDied","Data":"f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05"}
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032901     231 kubelet.go:2506] "SyncLoop (PLEG): event for pod" pod="default/sample-pod-0" event={"ID":"5598e74f-08ff-40ab-aba3-fa811874f9dc","Type":"ContainerDied","Data":"220eab5b9bc2277f4ac7777dabb82a92bee9d3d17bfa42f92d204cb9d85c936d"}
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032907     231 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="220eab5b9bc2277f4ac7777dabb82a92bee9d3d17bfa42f92d204cb9d85c936d"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032934     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: E1226 05:35:42.044794     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:35:53 kind-v1.32.0-worker kubelet[231]: I1226 05:35:53.469847     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:35:53 kind-v1.32.0-worker kubelet[231]: E1226 05:35:53.482748     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:36:08 kind-v1.32.0-worker kubelet[231]: I1226 05:36:08.468746     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:36:08 kind-v1.32.0-worker kubelet[231]: E1226 05:36:08.480012     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

```console
$ kind version
kind v0.26.0 go1.23.4 linux/amd64
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用DRA（Dynamic Resource Allocation）时，当DRA Driver被停止后，已分配设备的Pod在删除时会卡在"Terminating"状态，无法正常终止。根据提供的日志信息，kubelet无法找到已注册的DRA插件，导致在同步Pod状态时出现错误：

```
Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins"
```

这是因为kubelet在删除Pod时需要与DRA Driver通信以释放设备资源，但由于DRA Driver已被停止，kubelet无法完成该操作，从而导致Pod一直处于"Terminating"状态。

从安全角度分析，这个问题属于系统功能性缺陷或Bug，未涉及任何安全漏洞。该问题不会导致未授权访问、权限提升、数据泄露、远程代码执行等安全风险。

---


## Issue #129392 Creation fails when the CRD property is ServiceAccount

- Issue 链接：[#129392](https://github.com/kubernetes/kubernetes/issues/129392)

### Issue 内容

#### What happened?

My CRD property use the ServiceAccount type, When I use `kubelet apply -f <crds>` to create CRDs generated by controller-gen, get an error message: 
```text
Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property.
```

This is because ServiceAccount.Secret.Name property is in x-kubernetes-list-map-keys, but it is not have a default and not be a required property.

Structure definition:
```go
// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:...
type Foo struct {
    ServiceAccount corev1.ServiceAccount `json:"serviceAccount,omitempty"`
    ....
}
```

#### What did you expect to happen?

I expect to generate CRDs that can be created successfully.

Solutions:
- Set ObjectReference.Name to required. I think the name is required in most scenarios.
- Add default value annotations  (`//+default=""` and `//+kubebuilder:default=""`) for ObjectReference.Name.
- Modify the ServiceAccount.Secrets type to []SecretReference and set SecretReference.name to required.

#### How can we reproduce it (as minimally and precisely as possible)?

Refer to "What happened"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

v1.30+

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用controller-gen生成CRD时，CRD中的属性使用了`corev1.ServiceAccount`类型，导致在使用`kubectl apply -f <crds>`创建CRD时出现错误。错误信息为：
```
Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property.
```
这是因为`ServiceAccount.Secret.Name`属性在`x-kubernetes-list-map-keys`中，但没有默认值也不是必需属性，导致CRD的Schema验证失败。

该问题本质上是CRD定义和生成过程中的技术问题，涉及到Kubernetes CRD的Schema约束和controller-gen工具的使用，并未涉及任何安全漏洞或风险，例如命令执行、权限提升、敏感信息泄露等。

根据风险判断标准：
1. **标准2**：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。

因此，该Issue不属于安全问题，不存在潜在的安全风险。

---


## Issue #129382 Swap stats is not shown as part of the metrics/resource endpoint

- Issue 链接：[#129382](https://github.com/kubernetes/kubernetes/issues/129382)

### Issue 内容

#### What happened?

The following:
```
> kubectl get --raw "/api/v1/nodes/k8s-dev-worker/proxy/metrics/resource"
# HELP container_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the container in core-seconds
# TYPE container_cpu_usage_seconds_total counter
container_cpu_usage_seconds_total{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 1.121325 1735032838055
container_cpu_usage_seconds_total{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 1.100665 1735032838936
container_cpu_usage_seconds_total{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 7.333964 1735032837430
# HELP container_memory_working_set_bytes [STABLE] Current working set of the container in bytes
# TYPE container_memory_working_set_bytes gauge
container_memory_working_set_bytes{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 3.2923648e+07 1735032838055
container_memory_working_set_bytes{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 4.0628224e+07 1735032838936
container_memory_working_set_bytes{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 4.7026176e+07 1735032837430
# HELP container_start_time_seconds [STABLE] Start time of the container since unix epoch in seconds
# TYPE container_start_time_seconds gauge
container_start_time_seconds{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 1.7350309825441425e+09
container_start_time_seconds{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 1.7350309819809804e+09
container_start_time_seconds{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 1.7350309993126562e+09
# HELP node_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the node in core-seconds
# TYPE node_cpu_usage_seconds_total counter
node_cpu_usage_seconds_total 71.41304 1735032832343
# HELP node_memory_working_set_bytes [STABLE] Current working set of the node in bytes
# TYPE node_memory_working_set_bytes gauge
node_memory_working_set_bytes 2.134016e+08 1735032832343
# HELP pod_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the pod in core-seconds
# TYPE pod_cpu_usage_seconds_total counter
pod_cpu_usage_seconds_total{namespace="kube-system",pod="kindnet-ndczz"} 1.145182 1735032830497
pod_cpu_usage_seconds_total{namespace="kube-system",pod="kube-proxy-l5jhs"} 1.108676 1735032837395
pod_cpu_usage_seconds_total{namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 7.336168 1735032831254
# HELP pod_memory_working_set_bytes [STABLE] Current working set of the pod in bytes
# TYPE pod_memory_working_set_bytes gauge
pod_memory_working_set_bytes{namespace="kube-system",pod="kindnet-ndczz"} 3.3222656e+07 1735032830497
pod_memory_working_set_bytes{namespace="kube-system",pod="kube-proxy-l5jhs"} 4.0914944e+07 1735032837395
pod_memory_working_set_bytes{namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 4.732928e+07 1735032831254
# HELP resource_scrape_error [STABLE] 1 if there was an error while getting container metrics, 0 otherwise
# TYPE resource_scrape_error gauge
resource_scrape_error 0
```

As can be seen, swap stats is not shown here:
```
> kubectl get --raw "/api/v1/nodes/k8s-dev-worker/proxy/metrics/resource" | grep -i swap
> 
```

#### What did you expect to happen?

Swap to be included in metrics/resource endpoint stats.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bring up a cluster
2. Install metrics server
3. Run `kubectl get --raw "/api/v1/nodes/<NODE-NAME>/proxy/metrics/resource" | grep -i swap`

#### Anything else we need to know?

Swap stats were introduced in this PR: https://github.com/kubernetes/kubernetes/pull/118865.
It also shows the expected output.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0

```

</details>


#### Cloud provider

<details>
Using a kind development cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="40 (Forty)"
ID=fedora
VERSION_ID=40
VERSION_CODENAME=""
PLATFORM_ID="platform:f40"
PRETTY_NAME="Fedora Linux 40 (Forty)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:40"
DEFAULT_HOSTNAME="fedora"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f40/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=40
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=40
SUPPORT_END=2025-05-13

$ uname -a
Linux fedora40-eve 6.12.5-100.fc40.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Dec 16 15:00:58 UTC 2024 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
> crio version
Version:        1.31.3
GitCommit:      5865976a62cd3abd1b56ec21ea4fde19a730fe87
GitCommitDate:  2024-12-02T10:48:49Z
GitTreeState:   dirty
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.22.5
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  seccomp
  apparmor
  selinux
  exclude_graphdriver_devicemapper
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在使用`kubectl get --raw "/api/v1/nodes/<NODE-NAME>/proxy/metrics/resource"`命令获取节点资源指标时，没有显示swap（交换分区）的统计信息。提交者期望在该指标端点中包含swap的统计数据，以便更全面地监控节点的资源使用情况。

根据问题描述，这属于功能缺失或改进的请求，与系统性能监控和资源统计有关。但从安全风险的角度来看，缺少swap统计信息并不会导致系统存在安全漏洞或被恶意利用。它不会引起诸如权限提升、命令执行、信息泄露等安全问题。

按照风险判断标准：

1. 该Issue描述的问题并非安全问题，而是功能性的问题。
2. 不涉及攻击者、敏感信息泄露或系统被攻破的情况。

---


## Issue #129374 kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous"

- Issue 链接：[#129374](https://github.com/kubernetes/kubernetes/issues/129374)

### Issue 内容

#### What happened?

The first time you run the 1.30.8 controller, it will report insufficient permissions. Running 1.30.7 again will succeed, and then running 1.30.8 will succeed again.
```
[root@localhost ssl]# kube-controller-manager --version
Kubernetes v1.30.8
[root@localhost ssl]# kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:11:54.211385    2940 serving.go:380] Generated self-signed cert in-memory
I1224 10:11:54.590485    2940 controllermanager.go:189] "Starting" version="v1.30.8"
I1224 10:11:54.590556    2940 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:11:54.592919    2940 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:11:54.592977    2940 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:11:54.593023    2940 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:11:54.592977    2940 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:11:54.593268    2940 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:11:54.599548    2940 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
E1224 10:11:56.832867    2940 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
```

```
[root@localhost bin]# ./kube-controller-manager --version
Kubernetes v1.30.7
[root@localhost bin]# ./kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:19:45.752973    2999 serving.go:380] Generated self-signed cert in-memory
I1224 10:19:46.257829    2999 controllermanager.go:189] "Starting" version="v1.30.7"
I1224 10:19:46.257881    2999 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:19:46.260377    2999 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:19:46.260526    2999 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:19:46.260698    2999 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:19:46.260773    2999 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:19:46.260957    2999 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:19:51.261438    2999 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: Get "https://192.168.58.162:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```

```
[root@localhost bin]# kube-controller-manager --version
Kubernetes v1.30.8
[root@localhost bin]# kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:20:27.397464    3019 serving.go:380] Generated self-signed cert in-memory
I1224 10:20:28.175486    3019 controllermanager.go:189] "Starting" version="v1.30.8"
I1224 10:20:28.175526    3019 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:20:28.177207    3019 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:20:28.177235    3019 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:20:28.177628    3019 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:20:28.177668    3019 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:20:28.177829    3019 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:20:33.178872    3019 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: Get "https://192.168.58.162:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```



#### What did you expect to happen?

successful

#### How can we reproduce it (as minimally and precisely as possible)?

kube-apiserver.service
```
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
--allow-privileged=true \
--authorization-mode=Node,RBAC \
--bind-address=0.0.0.0 \
--client-ca-file=/etc/kubernetes/ssl/ca.pem \
--enable-admission-plugins=NodeRestriction \
--enable-bootstrap-token-auth=true \
--etcd-cafile=/etc/kubernetes/ssl/ca.pem   \
--etcd-certfile=/etc/kubernetes/ssl/etcd.pem   \
--etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem   \
--etcd-servers=https://127.0.0.1:2379 \
--feature-gates=RotateKubeletServerCertificate=true \
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
--proxy-client-cert-file=/etc/kubernetes/ssl/kube-proxy.pem \
--proxy-client-key-file=/etc/kubernetes/ssl/kube-proxy-key.pem \
--requestheader-allowed-names=front-proxy-client \
--requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
--requestheader-group-headers=X-Remote-Group \
--requestheader-username-headers=X-Remote-User \
--secure-port=6443 \
--service-account-issuer=https://kubernetes.default.svc.cluster.local \
--service-account-key-file=/etc/kubernetes/ssl/sa.pem \
--service-account-signing-key-file=/etc/kubernetes/ssl/sa-key.pem \
--service-cluster-ip-range=10.96.0.0/16 \
--tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem   \
--tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem   
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

```


```
export K8S_API_URL=${master01ip}

/usr/local/bin/kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=https://${K8S_API_URL}:6443 \
--kubeconfig=controller-manager.conf


/usr/local/bin/kubectl  config set-credentials system:controller-manager \
--client-certificate=/etc/kubernetes/ssl/kubernetes.pem \
--client-key=/etc/kubernetes/ssl/kubernetes-key.pem \
--embed-certs=true \
--kubeconfig=controller-manager.conf


/usr/local/bin/kubectl  config set-context system:kube-controller-manager@kubernetes \
--cluster=kubernetes \
--user=system:kube-controller-manager \
--kubeconfig=controller-manager.conf

# 设置默认上下文
/usr/local/bin/kubectl  config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.conf
```



#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl version 1.30.8

[root@localhost bin]# etcd --version
etcd Version: 3.5.17
Git SHA: 507c0de
Go Version: go1.22.9
Go OS/Arch: linux/amd64

# paste output here 
```

</details>


#### Cloud provider

vmware virtual machine kylin operating system

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
[root@localhost setup]# cat /etc/os-release 
NAME="Kylin Linux Advanced Server"
VERSION="V10 (Lance)"
ID="kylin"
VERSION_ID="V10"
PRETTY_NAME="Kylin Linux Advanced Server V10 (Lance)"
ANSI_COLOR="0;31"

# paste output here
$ uname -a
Linux k8s01.example.com 4.19.90-52.15.v2207.ky10.x86_64 #1 SMP Thu Nov 24 21:50:03 CST 2022 x86_64 x86_64 x86_64 GNU/Linux
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从提供的Issue内容来看，用户在运行`kube-controller-manager`版本`v1.30.8`时，首次启动出现了权限不足的错误，错误信息为：

```
leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
```

随后用户运行了`v1.30.7`版本，成功启动，再次运行`v1.30.8`版本也成功了。

从错误信息分析，`kube-controller-manager`在进行领导者选举（Leader Election）时，需要访问`leases`资源，但请求以`system:anonymous`用户身份发起，缺少必要的权限，导致访问被拒绝。

出现该问题的原因可能是`kube-controller-manager`的配置有误，比如`--kubeconfig`参数指向的配置文件不正确，导致认证失败，组件以匿名用户身份请求API Server。

根据风险判断标准：

- 第2条：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。
- 第4条：issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于安全风险，应当判断为不涉及。

因此，该Issue反映的是用户的配置或操作问题，不涉及项目的安全风险。

---


## Issue #129362 bug(dra): when deleting resourceclaimtemplate, pod can't running again

- Issue 链接：[#129362](https://github.com/kubernetes/kubernetes/issues/129362)

### Issue 内容

#### What happened?

```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl apply -f gpu-test2-dep.yaml
namespace/gpu-test2 created
resourceclaimtemplate.resource.k8s.io/single-gpu created
deployment.apps/gpu-deployment created
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get all -ngpu-test2
NAME                                  READY   STATUS    RESTARTS   AGE
pod/gpu-deployment-6965899554-zmq5j   2/2     Running   0          16s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/gpu-deployment   1/1     1            1           16s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/gpu-deployment-6965899554   1         1         1       16s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get resourceclaimtemplates -ngpu-test2
NAME         AGE
single-gpu   47s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart#
```
1. delete resourceclaimtemplates
2. delete pod
```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get resourceclaimtemplates -ngpu-test2
NAME         AGE
single-gpu   2m33s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl delete resourceclaimtemplates single-gpu -ngpu-test2
resourceclaimtemplate.resource.k8s.io "single-gpu" deleted
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get pods -ngpu-test2
NAME                              READY   STATUS    RESTARTS   AGE
gpu-deployment-6965899554-zmq5j   2/2     Running   0          2m50s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl delete pods gpu-deployment-6965899554-zmq5j -ngpu-test2
pod "gpu-deployment-6965899554-zmq5j" deleted
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get pods -ngpu-test2
NAME                              READY   STATUS    RESTARTS   AGE
gpu-deployment-6965899554-wl2dj   0/2     Pending   0          3s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart#
```
pod is always Pending
```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl describe pods gpu-deployment-6965899554-wl2dj -ngpu-test2
Name:             gpu-deployment-6965899554-wl2dj
Namespace:        gpu-test2
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=gpu-app
                  pod-template-hash=6965899554
Annotations:      <none>
Status:           Pending
IP:
IPs:              <none>
Controlled By:    ReplicaSet/gpu-deployment-6965899554
Containers:
  ctr0:
    Image:      ubuntu:22.04
    Port:       <none>
    Host Port:  <none>
    Command:
      bash
      -c
    Args:
      nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m26j7 (ro)
  ctr1:
    Image:      ubuntu:22.04
    Port:       <none>
    Host Port:  <none>
    Command:
      bash
      -c
    Args:
      nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m26j7 (ro)
Volumes:
  kube-api-access-m26j7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              nvidia.com/gpu.present=true
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
                             nvidia.com/gpu:NoSchedule op=Exists
Events:
  Type     Reason                       Age                 From            Message
  ----     ------                       ----                ----            -------
  Warning  FailedResourceClaimCreation  11s (x13 over 31s)  resource_claim  PodResourceClaim shared-gpu: resource claim template "single-gpu": resourceclaimtemplate.resource.k8s.io "single-gpu" not found
```

#### What did you expect to happen?

When resourceclaimtemplate is deleted, the pod can still run successfully after restart. Or resourceclaimtemplate cannot be deleted when it is in use.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-test2

---
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  namespace: gpu-test2
  name: single-gpu
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.nvidia.com

---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: gpu-test2
  name: gpu-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-app
  template:
    metadata:
      labels:
        app: gpu-app
    spec:
      containers:
      - name: ctr0
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: shared-gpu
      - name: ctr1
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: shared-gpu
      resourceClaims:
      - name: shared-gpu
        resourceClaimTemplateName: single-gpu
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      nodeSelector:
        nvidia.com/gpu.present: "true"
      restartPolicy: Always
```
use this sample yaml, and delete pod.

#### Anything else we need to know?

I'm not sure if this is by design. But according to common sense, there is a possibility that resourceclaimtemplate was accidentally deleted. If the training task is restarted, the above problem may occur. 🤔 

#### Kubernetes version



```console
root@VM-0-6-ubuntu:/home/ubuntu# kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```




#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在Kubernetes环境中，当删除`ResourceClaimTemplate`后，再删除相关的Pod，Pod无法重新运行，一直处于Pending状态。通过查看事件日志，发现Pod因为无法找到`ResourceClaimTemplate`而无法创建所需的资源声明，导致Pod无法调度运行。

这个问题涉及到资源管理和依赖关系的问题。当删除了`ResourceClaimTemplate`后，依赖于该模板的资源声明无法创建，这是预期的行为。虽然这可能给用户带来不便，但这属于功能性问题或设计缺陷，并不涉及任何安全漏洞或风险。

根据风险判断标准：

1. **如果该issue描述的问题非安全问题，则风险评级判断为不涉及。**

因此，该Issue描述的问题不属于安全问题，而是一个功能性Bug或设计问题。

---


## Issue #129356 TestServerRunWithSNI Unit Test Fails Intermittently

- Issue 链接：[#129356](https://github.com/kubernetes/kubernetes/issues/129356)

### Issue 内容

#### What happened?

The `TestServerRunWithSNI` unit test is failing intermittently. 

```
=== NAME  TestServerRunWithSNI/loopback:_bind_to_0.0.0.0_=>_loopback_uses_localhost
    serving_test.go:339: Dialing localhost:43713 as ""
    serving_test.go:372: failed to connect with loopback client: Get "https://0.0.0.0:43713/version?timeout=32s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
--- FAIL: TestServerRunWithSNI (0.00s)
    --- PASS: TestServerRunWithSNI/one_SNI_and_the_default_cert_with_the_same_name (0.23s)
    --- PASS: TestServerRunWithSNI/cert_with_multiple_alternate_names (0.28s)
    --- PASS: TestServerRunWithSNI/only_one_cert (0.30s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_on_server_cert (0.37s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_not_on_any_cert (0.38s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_on_SNI_cert (0.55s)
    --- PASS: TestServerRunWithSNI/matching_IP_in_SNI_cert_and_the_server_cert (0.55s)
    --- PASS: TestServerRunWithSNI/matching_SNI_cert (0.57s)
    --- PASS: TestServerRunWithSNI/wildcards (0.62s)
    --- FAIL: TestServerRunWithSNI/loopback:_bind_to_0.0.0.0_=>_loopback_uses_localhost (32.69s)
FAIL
I1221 18:33:43.081497   65589 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::testdata/localhost__/cert::testdata/localhost__/key"
I1221 18:33:43.081523   65589 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1221 18:33:43.081523   65589 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I1221 18:33:43.081567   65589 secure_serving.go:258] Stopped listening on 127.0.0.1:43713
FAIL	k8s.io/apiserver/pkg/server/options	32.722s
FAIL

```


#### What did you expect to happen?

The TestServerRunWithSNI test should pass consistently without any errors

#### How can we reproduce it (as minimally and precisely as possible)?

go test -v k8s.io/apiserver/pkg/server/options -run TestServerRunWithSNI

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.31

#### Cloud provider

<details>

</details>


#### OS version

windows 11



#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，`TestServerRunWithSNI`单元测试在某些情况下会间歇性失败。具体错误信息显示，在尝试使用回环客户端连接`localhost:43713`时，发生了超时错误，未能成功建立连接。

从描述来看，这更像是一个单元测试的稳定性或可靠性问题，可能与测试环境、网络配置或代码本身的逻辑有关。没有迹象表明该问题涉及安全方面的风险，例如敏感信息泄露、权限提升、代码执行等。

根据风险判断标准：

1. 如果该Issue描述的问题非安全问题，则风险评级判断为不涉及。

---


## Issue #129320 `kubectl describe pod` probe host is empty

- Issue 链接：[#129320](https://github.com/kubernetes/kubernetes/issues/129320)

### Issue 内容

#### What happened?

Using `kubectl describe po xxx` to show pod probe info, when the probe host is empty, there is no default value showed in the output, like below, so this PR add the default value.
```
    State:          Running
      Started:      Thu, 19 Dec 2024 15:25:42 +0800
    Ready:          False
    Restart Count:  0
    ...
    Liveness:       http-get http://:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
```

#### What did you expect to happen?

When the host not set, add the default value `localhost`.
```
    Liveness:       http-get http://:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    ======== to below
    Liveness:       http-get http://localhost:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
```

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          timeoutSeconds: 5

```
`kubectl apply -f xxx.yaml && kubectl describe po xxx` will reproduce this issue.

#### Anything else we need to know?

Issue tested in 1.31.

#### Kubernetes version

1.31

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue，问题是在使用`kubectl describe pod xxx`命令显示Pod的探针信息时，当探针的`host`字段为空时，输出中未显示默认值`localhost`。因此，提出了一个PR来在输出中添加默认值`localhost`。

该问题涉及的是命令行输出的信息展示问题，属于用户体验和友好度的优化，并不涉及任何安全风险。即使`host`字段为空，探针实际也会默认使用`localhost`，功能上没有问题，仅仅是输出信息中未显示默认值，可能会导致用户困惑。

根据风险判断标准：
- 如果Issue描述的问题非安全问题，则风险评级判断为“不涉及”。

---


## Issue #129310 DRA: Using All allocation mode will schedule to nodes with zero devices

- Issue 链接：[#129310](https://github.com/kubernetes/kubernetes/issues/129310)

### Issue 内容

#### What happened?

I created a resource claim template to get "All" GPUs on a node:

```yaml
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  name: all-gpus
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.nvidia.com
        allocationMode: All
```

I then created a deployment that had a Pod that used that claim. The Pod was scheduled to a node. However, my DRA driver on that node was not running, so there were no resource slices for that node.

#### What did you expect to happen?

I expected the pod to not schedule, since there were no available devices meeting the request. "All" should mean "at least one".

#### How can we reproduce it (as minimally and precisely as possible)?

Create the resource claim template as shown and a deployment, with no DRA driver running. The pod will still schedule.

#### Anything else we need to know?

/wg device-management

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0-gke.1358000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
ID=debian
HOME_URL="https://go/glinux"
SUPPORT_URL="https://go/techstop"
BUG_REPORT_URL="https://go/techstop"
$ uname -a
Linux jbelamaric.c.googlers.com 6.10.11-1rodete2-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.10.11-1rodete2 (2024-10-16) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
Issue描述了在Kubernetes中使用动态资源分配（DRA）时，当设置了`allocationMode: All`来获取节点上的所有GPU设备时，即使节点上没有运行DRA驱动（因此没有可用的GPU设备），Pod仍然会被调度到该节点上。预期行为是如果没有可用的设备，Pod不应该被调度到该节点上。

从问题的描述来看，这是一个调度器在资源不可用情况下的错误调度问题，可能导致Pod无法正常运行，影响应用程序的性能或功能。

然而，从安全风险的角度分析：

1. 该问题不会导致未授权的访问或权限提升。攻击者无法利用该漏洞获取更高的权限或访问受限资源。

2. 该问题不会导致敏感信息泄露。没有任何凭据或机密数据因此被暴露。

3. 该问题不会导致代码执行、容器逃逸或提权等高风险安全问题。

4. 即使存在拒绝服务（DoS）的可能性，但需要具有创建或修改Pod的权限才能触发，此类权限通常仅授予受信任的用户。

基于以上分析，按照给定的风险判断标准，该Issue描述的问题不属于安全问题。

---


## Issue #129309 MutatingAdmissionPolicy mutation ordering issue

- Issue 链接：[#129309](https://github.com/kubernetes/kubernetes/issues/129309)

### Issue 内容

#### What happened?

This works:
```
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers",
              value: []
            },
            JSONPatch{
              op: "add", path: "/spec/initContainers/-",
              value: Object.spec.initContainers{
                name: "mesh-proxy",
                image: "mesh-proxy/v1.0.0",
                restartPolicy: "Always"
              }
            }
          ]
```

But, this fails for a pod:
```
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers",
              value: []
            }
          ]
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers/-",
              value: Object.spec.initContainers{
                name: "mesh-proxy",
                image: "mesh-proxy/v1.0.0",
                restartPolicy: "Always"
              }
            }
          ]
```
with:
```
denied request: JSON Patch: add operation does not apply: doc is missing path: "/spec/initContainers/-": missing value
```

Seems like the ordering is wrong or the input of one isn't fed into the next

#### What did you expect to happen?

Both should work

#### How can we reproduce it (as minimally and precisely as possible)?

Try chaining the mutations together in the same object, one relying on the next as above

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

NA

#### OS version

minikube

#### Install tools

minikube

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用Kubernetes的MutatingAdmissionPolicy进行资源的变更时，遇到了mutation操作的执行顺序问题。具体表现为：

- 当将两个JSONPatch操作放在同一个表达式中时，变更能成功应用。
- 当将两个JSONPatch操作分开为两个独立的mutation时，第二个mutation未能识别到第一个mutation的结果，导致操作失败，出现错误信息：“denied request: JSON Patch: add operation does not apply: doc is missing path: "/spec/initContainers/-": missing value”。

这个问题的核心在于MutatingAdmissionPolicy在处理多个mutation操作时，未能正确地按顺序执行，或者未能将前一个mutation的结果作为后一个mutation的输入。这可能是MutatingAdmissionPolicy的实现缺陷或使用方式不当导致的。

从Issue中提供的信息来看，这属于功能性问题，并未涉及安全风险。该问题不会导致权限提升、数据泄露、越权访问等安全问题。

根据风险判断标准：

- 第2条：“如果该issue描述的问题非安全问题，则风险评级判断为不涉及”。

因此，该Issue不涉及安全风险。

---


## Issue #129294 services have not yet been read at least once, cannot construct envvars

- Issue 链接：[#129294](https://github.com/kubernetes/kubernetes/issues/129294)

### Issue 内容

#### What happened?

当我使用二进制部署kubernetes遇到一个问题，环境如下 
root@192:~/generic_architecture# kube-apiserver --version
Kubernetes v1.28.12
root@192:~/generic_architecture# containerd --version
containerd containerd.io 1.7.24 88bf19b2105c8b17560993bee28a01ddc2f97182

root@192:/etc/kubernetes# cat /etc/os-release 
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo


配置文件如下：
```
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
    cacheTTL: "2m"
  x509:
    clientCAFile: "/data/kubernetes/pki/ca-cert.pem"
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: "5m"
    cacheUnauthorizedTTL: "30s"
# DNS 配置
clusterDNS:
- "169.254.123.123"  # 使用 NodeLocal DNSCache 的 IP
clusterDomain: "cluster.local"
podCIDR: "10.244.0.0/16"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
rotateCertificates: true
serverTLSBootstrap: false
tlsCertFile: "/data/kubernetes/pki/system:node:192.168.102.230-cert.pem"
tlsPrivateKeyFile: "/data/kubernetes/pki/system:node:192.168.102.230-key.pem"
failSwapOn: false
enableSystemLogHandler: true
enableDebuggingHandlers: true
# 指定 cgroup 驱动为 systemd
cgroupDriver: systemd

# 硬驱逐阈值配置：当达到这些阈值时，pod 将被立即驱逐
# nodefs: 根文件系统 (kubelet 根目录和日志目录所在的文件系统)
# imagefs: 镜像文件系统 (容器运行时用于存储容器镜像和容器可写层的文件系统)
evictionHard:
  memory.available: "100Mi"     # 当可用内存小于 100Mi 时触发硬驱逐
  nodefs.available: "5%"        # 当根文件系统可用空间小于 1% 时触发硬驱逐
  nodefs.inodesFree: "4%"       # 当根文件系统可用 inode 小于 4% 时触发硬驱逐
  imagefs.available: "5%"      # 当镜像文件系统可用空间小于 10% 时触发硬驱逐

# 软驱逐阈值配置：当达到这些阈值时，pod 将在宽限期后被驱逐
# 软驱逐提供了一个缓冲区，让系统有时间尝试自动恢复
evictionSoft:
  memory.available: "200Mi"     # 当可用内存小于 200Mi 时触发软驱逐
  nodefs.available: "5%"       # 当根文件系统可用空间小于 10% 时触发软驱逐
  nodefs.inodesFree: "5%"       # 当根文件系统可用 inode 小于 1% 时触发软驱逐
  imagefs.available: "5%"      # 当镜像文件系统可用空间小于 1% 时触发软驱逐

# 软驱逐宽限期：在触发软驱逐后，等待多长时间才开始驱逐 pod
# 这段时间内，系统会尝试自动恢复，如果恢复失败才会驱逐 pod
evictionSoftGracePeriod:
  memory.available: "2m"        # 内存不足时的宽限期为 2 分钟
  nodefs.available: "2m"        # 磁盘空间不足时的宽限期为 2 分钟
  nodefs.inodesFree: "2m"      # inode 不足时的宽限期为 2 分钟
  imagefs.available: "2m"       # 镜像存储空间不足时的宽限期为 2 分钟

# 驱逐压力转换周期：系统必须在等待多长时间后才能转换压力状态
# 这可以防止系统在压力临界值附近频繁震荡
evictionPressureTransitionPeriod: "5m0s"

# 最小回收量：当触发驱逐时，必须要回收的最小资源量
# 这确保每次驱逐都能释放足够的资源，避免频繁驱逐
evictionMinimumReclaim:
  memory.available: "0Mi"       # 不强制内存最小回收量
  nodefs.available: "500Mi"     # 每次至少回收 500Mi 磁盘空间
  imagefs.available: "500Mi"    # 每次至少回收 500Mi 镜像存储空间

# Kubernetes 系统组件预留资源：确保 kubernetes 组件（如 kubelet）有足够资源运行
kubeReserved:
  memory: "512Mi"              # 为 kubernetes 组件预留 512Mi 内存
  ephemeral-storage: "1Gi"     # 为 kubernetes 组件预留 1Gi 临时存储

# 系统预留资源：确保系统守护进程（如 sshd、udev 等）有足够资源运行
systemReserved:
  memory: "512Mi"              # 为系统守护进程预留 512Mi 内存
  ephemeral-storage: "1Gi"     # 为系统守护进程预留 1Gi 临时存储

```

kube-proxy
```
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
mode: "ipvs"
clusterCIDR: "10.244.0.0/16"
ipvs:
  strictARP: true
```




#### What did you expect to happen?

pod状态异常
```
root@192:~/generic_architecture# k get pod -A
NAMESPACE         NAME                               READY   STATUS                       RESTARTS   AGE
tigera-operator   tigera-operator-56b74f76df-n2vpx   0/1     CreateContainerConfigError   0          13m


root@192:~/generic_architecture# k describe pod -n tigera-operator tigera-operator-56b74f76df-n2vpx 
Name:             tigera-operator-56b74f76df-n2vpx
Namespace:        tigera-operator
Priority:         0
Service Account:  tigera-operator
Node:             192.168.102.230/192.168.102.230
Start Time:       Thu, 19 Dec 2024 05:41:55 +0000
Labels:           k8s-app=tigera-operator
                  name=tigera-operator
                  pod-template-hash=56b74f76df
Annotations:      <none>
Status:           Pending
IP:               192.168.102.230
IPs:
  IP:           192.168.102.230
Controlled By:  ReplicaSet/tigera-operator-56b74f76df
Containers:
  tigera-operator:
    Container ID:  
    Image:         quay.io/tigera/operator:v1.36.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      operator
    State:          Waiting
      Reason:       CreateContainerConfigError
    Ready:          False
    Restart Count:  0
    Environment Variables from:
      kubernetes-services-endpoint  ConfigMap  Optional: true
    Environment:
      WATCH_NAMESPACE:                     
      POD_NAME:                            tigera-operator-56b74f76df-n2vpx (v1:metadata.name)
      OPERATOR_NAME:                       tigera-operator
      TIGERA_OPERATOR_INIT_IMAGE_VERSION:  v1.36.0
    Mounts:
      /var/lib/calico from var-lib-calico (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpbk7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  kube-api-access-bpbk7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoExecute op=Exists
                             :NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned tigera-operator/tigera-operator-56b74f76df-n2vpx to 192.168.102.230
  Normal   Pulled     13m (x3 over 13m)  kubelet            Container image "quay.io/tigera/operator:v1.36.0" already present on machine
  Warning  Failed     13m (x3 over 13m)  kubelet            Error: services have not yet been read at least once, cannot construct envvars


```


kubelet输出
```
ent="NodeHasSufficientPID"
I1219 05:56:58.059704   37131 kubelet_node_status.go:72] "Attempting to register node" node="192.168.102.230"
I1219 05:56:58.059860   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node 192.168.102.230 status is now: NodeHasSufficientMemory"
I1219 05:56:58.059892   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node 192.168.102.230 status is now: NodeHasNoDiskPressure"
I1219 05:56:58.059900   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node 192.168.102.230 status is now: NodeHasSufficientPID"
I1219 05:56:58.077561   37131 kubelet_node_status.go:111] "Node was previously registered" node="192.168.102.230"
I1219 05:56:58.077816   37131 kubelet_node_status.go:75] "Successfully registered node" node="192.168.102.230"
I1219 05:56:58.097306   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.098564   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.202836   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.203206   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.307148   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.307318   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.409691   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.410029   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.513072   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.513152   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.617537   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.617663   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.720189   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.720245   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.822460   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.822514   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.833937   37131 apiserver.go:50] "node sync has not completed yet"
I1219 05:56:58.833992   37131 apiserver.go:46] "node sync completed"
I1219 05:56:58.833995   37131 apiserver.go:52] "Watching apiserver"
I1219 05:56:58.834027   37131 reflector.go:305] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.834064   37131 reflector.go:341] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.836294   37131 reflector.go:368] Caches populated for *v1.Pod from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.836362   37131 config.go:292] "Setting pods for source" source="api"
I1219 05:56:58.836600   37131 config.go:397] "Receiving a new pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.836670   37131 kubelet.go:2407] "SyncLoop ADD" source="api" pods=["tigera-operator/tigera-operator-56b74f76df-kb242"]
I1219 05:56:58.836769   37131 pod_workers.go:768] "Pod is being synced for the first time" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="create"
I1219 05:56:58.836789   37131 pod_workers.go:963] "Notifying pod of pending update" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" workType="sync"
I1219 05:56:58.836807   37131 pod_workers.go:1233] "Processing pod event" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="sync"
I1219 05:56:58.836823   37131 kubelet.go:1758] "SyncPod enter" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671"
I1219 05:56:58.836846   37131 kubelet_pods.go:1774] "Generating pod status" podIsTerminal=false pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.836867   37131 kubelet_pods.go:1787] "Got phase for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242" oldPhase="Pending" phase="Pending"
I1219 05:56:58.836986   37131 status_manager.go:227] "Syncing updated statuses"
I1219 05:56:58.837068   37131 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.837089   37131 reflector.go:305] Starting reflector *v1.ConfigMap (0s) from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.837096   37131 reflector.go:341] Listing and watching *v1.ConfigMap from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.837145   37131 reflector.go:305] Starting reflector *v1.ConfigMap (0s) from object-"tigera-operator"/"kubernetes-services-endpoint"
I1219 05:56:58.837154   37131 reflector.go:341] Listing and watching *v1.ConfigMap from object-"tigera-operator"/"kubernetes-services-endpoint"
I1219 05:56:58.838431   37131 reflector.go:368] Caches populated for *v1.ConfigMap from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.838461   37131 reflector.go:368] Caches populated for *v1.ConfigMap from object-"tigera-operator"/"kubernetes-services-endpoint"
W1219 05:56:58.850981   37131 warnings.go:70] unknown field "status.containerStatuses[0].volumeMounts"
I1219 05:56:58.851284   37131 status_manager.go:872] "Patch status for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" patch="{\"metadata\":{\"uid\":\"04be5275-9b1c-465a-b701-67af82dbc671\"},\"status\":{\"containerStatuses\":[{\"image\":\"quay.io/tigera/operator:v1.36.0\",\"imageID\":\"\",\"lastState\":{},\"name\":\"tigera-operator\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}},\"volumeMounts\":[{\"mountPath\":\"/var/lib/calico\",\"name\":\"var-lib-calico\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"},{\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"name\":\"kube-api-access-qx6bh\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}]}}"
I1219 05:56:58.851321   37131 status_manager.go:881] "Status for pod updated successfully" pod="tigera-operator/tigera-operator-56b74f76df-kb242" statusVersion=1 status={"phase":"Pending","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:49Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z"},{"type":"Ready","status":"False","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z","reason":"ContainersNotReady","message":"containers with unready status: [tigera-operator]"},{"type":"ContainersReady","status":"False","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z","reason":"ContainersNotReady","message":"containers with unready status: [tigera-operator]"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z"}],"hostIP":"192.168.102.230","hostIPs":[{"ip":"192.168.102.230"}],"podIP":"192.168.102.230","podIPs":[{"ip":"192.168.102.230"}],"startTime":"2024-12-19T05:56:48Z","containerStatuses":[{"name":"tigera-operator","state":{"waiting":{"reason":"ContainerCreating"}},"lastState":{},"ready":false,"restartCount":0,"image":"quay.io/tigera/operator:v1.36.0","imageID":"","started":false,"volumeMounts":[{"name":"var-lib-calico","mountPath":"/var/lib/calico","readOnly":true,"recursiveReadOnly":"Disabled"},{"name":"kube-api-access-qx6bh","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}
I1219 05:56:58.862724   37131 desired_state_of_world.go:308] "expected volume SELinux label context" volume="var-lib-calico" label=""
I1219 05:56:58.862781   37131 desired_state_of_world.go:328] "volume does not support SELinux context mount, clearing the expected label" volume="var-lib-calico"
I1219 05:56:58.862803   37131 desired_state_of_world_populator.go:326] "Added volume to desired state" pod="tigera-operator/tigera-operator-56b74f76df-kb242" volumeName="var-lib-calico" volumeSpecName="var-lib-calico"
I1219 05:56:58.862823   37131 desired_state_of_world.go:308] "expected volume SELinux label context" volume="kube-api-access-qx6bh" label=""
I1219 05:56:58.862830   37131 desired_state_of_world.go:328] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-qx6bh"
I1219 05:56:58.862839   37131 desired_state_of_world_populator.go:326] "Added volume to desired state" pod="tigera-operator/tigera-operator-56b74f76df-kb242" volumeName="kube-api-access-qx6bh" volumeSpecName="kube-api-access-qx6bh"
I1219 05:56:58.862858   37131 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
I1219 05:56:58.900157   37131 reconciler_common.go:207] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900277   37131 reconciler_common.go:218] "operationExecutor.MountVolume started for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900346   37131 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900320   37131 projected.go:185] Setting up volume kube-api-access-qx6bh for pod 04be5275-9b1c-465a-b701-67af82dbc671 at /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh
I1219 05:56:58.906641   37131 atomic_writer.go:185] pod tigera-operator/tigera-operator-56b74f76df-kb242 volume kube-api-access-qx6bh: write required for target directory /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh
I1219 05:56:58.907199   37131 atomic_writer.go:203] pod tigera-operator/tigera-operator-56b74f76df-kb242 volume kube-api-access-qx6bh: performed write of new data to ts data directory: /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh/..2024_12_19_05_56_58.2665344692
I1219 05:56:58.907547   37131 operation_generator.go:637] "MountVolume.SetUp succeeded for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.922048   37131 kuberuntime_manager.go:432] "Retrieved pods from runtime" all=true
I1219 05:56:58.924256   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.924332   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.943971   37131 reflector.go:341] Listing and watching *v1.Service from k8s.io/client-go/informers/factory.go:160
W1219 05:56:58.945919   37131 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: "spec.clusterIP" is not a known field selector: only "metadata.name", "metadata.namespace"
E1219 05:56:58.946134   37131 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: \"spec.clusterIP\" is not a known field selector: only \"metadata.name\", \"metadata.namespace\"" logger="UnhandledError"
I1219 05:56:59.000972   37131 reconciler_common.go:207] "Starting operationExecutor.MountVolume for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.001410   37131 reconciler_common.go:218] "operationExecutor.MountVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.001482   37131 operation_generator.go:637] "MountVolume.SetUp succeeded for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.027542   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.027917   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.129801   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.130218   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.137519   37131 volume_manager.go:440] "All volumes are attached and mounted for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137624   37131 kuberuntime_manager.go:970] "Container of pod is not in the desired state and shall be started" containerName="tigera-operator" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137637   37131 kuberuntime_manager.go:1055] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [0], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137784   37131 kuberuntime_manager.go:1251] "Creating container in pod" containerType="container" container="&Container{Name:tigera-operator,Image:quay.io/tigera/operator:v1.36.0,Command:[operator],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:WATCH_NAMESPACE,Value:,ValueFrom:nil,},EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:OPERATOR_NAME,Value:tigera-operator,ValueFrom:nil,},EnvVar{Name:TIGERA_OPERATOR_INIT_IMAGE_VERSION,Value:v1.36.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:var-lib-calico,ReadOnly:true,MountPath:/var/lib/calico,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-qx6bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:kubernetes-services-endpoint,},Optional:*true,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,}" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
E1219 05:56:59.138638   37131 kuberuntime_manager.go:1274] "Unhandled Error" err="container &Container{Name:tigera-operator,Image:quay.io/tigera/operator:v1.36.0,Command:[operator],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:WATCH_NAMESPACE,Value:,ValueFrom:nil,},EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:OPERATOR_NAME,Value:tigera-operator,ValueFrom:nil,},EnvVar{Name:TIGERA_OPERATOR_INIT_IMAGE_VERSION,Value:v1.36.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:var-lib-calico,ReadOnly:true,MountPath:/var/lib/calico,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-qx6bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:kubernetes-services-endpoint,},Optional:*true,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod tigera-operator-56b74f76df-kb242_tigera-operator(04be5275-9b1c-465a-b701-67af82dbc671): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
I1219 05:56:59.138838   37131 event.go:389] "Event occurred" object="tigera-operator/tigera-operator-56b74f76df-kb242" fieldPath="spec.containers{tigera-operator}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"quay.io/tigera/operator:v1.36.0\" already present on machine"
I1219 05:56:59.140134   37131 event.go:389] "Event occurred" object="tigera-operator/tigera-operator-56b74f76df-kb242" fieldPath="spec.containers{tigera-operator}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: services have not yet been read at least once, cannot construct envvars"
I1219 05:56:59.141473   37131 kubelet.go:1764] "SyncPod exit" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" isTerminal=false
E1219 05:56:59.141512   37131 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"tigera-operator\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671"
I1219 05:56:59.142136   37131 pod_workers.go:1338] "Processing pod event done" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="sync"
I1219 05:56:59.234456   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.234485   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.337851   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.337946   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.440760   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.440798   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.542854   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.542989   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.644427   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.644553   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"

```


containerd 输出
```

UnprivilegedPorts:false EnableUnprivilegedICMP:false EnableCDI:false CDISpecDirs:[/etc/cdi /var/run/cdi] ImagePullProgressTimeout:15m0s DrainExecSyncIOTimeout:0s ImagePullWithSyncFs:false IgnoreDeprecationWarnings:[]} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
(reverse-i-search)`vim': ^Cm /etc/kibana/config/kibana.yml 
root@192:~/generic_architecture# vim /etc/containerd/config.toml
root@192:~/generic_architecture# systemctl restart containerd
root@192:~/generic_architecture# journalctl -fu containerd
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.521223723Z" level=info msg="TearDown network for sandbox \"a27e45dc9375af4ab103632c054008ca94453ce43ae64be1630b3ac440be9e0a\" successfully"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.521297276Z" level=info msg="StopPodSandbox for \"a27e45dc9375af4ab103632c054008ca94453ce43ae64be1630b3ac440be9e0a\" returns successfully"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.749025102Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:tigera-operator-56b74f76df-kb242,Uid:04be5275-9b1c-465a-b701-67af82dbc671,Namespace:tigera-operator,Attempt:0,}"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768922260Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768981468Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768992923Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.769194324Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.840085232Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:tigera-operator-56b74f76df-kb242,Uid:04be5275-9b1c-465a-b701-67af82dbc671,Namespace:tigera-operator,Attempt:0,} returns sandbox id \"8553992ad21317e7a995a33eacef59442a66080abcb42b206ff143cb23557472\""
Dec 19 05:56:53 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:53.892818421Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Dec 19 05:56:57 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:57.835999208Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."

```



containerd配置
```
gins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd" 
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    drain_exec_sync_io_timeout = "0s"
    enable_cdi = false
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    image_pull_progress_timeout = "15m0s"
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.8"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      #conf_template = "/etc/cni/net.d/10-default.conf"
      ip_pref = ""
      max_conf_num = 1
      setup_serially = false

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_blockio_not_enabled_errors = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          privileged_without_host_devices_all_devices_allowed = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          sandbox_mode = "podsandbox"
          snapshotter = ""

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = "/etc/containerd/certs.d/"

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"


  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.nri.v1.nri"]
    disable = true
    disable_connections = false
    plugin_config_path = "/etc/nri/conf.d"
    plugin_path = "/opt/nri/plugins"
    plugin_registration_timeout = "5s"
    plugin_request_timeout = "2s"
    socket_path = "/var/run/nri/nri.sock"

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.service.v1.tasks-service"]
    blockio_config_file = ""
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.blockfile"]
    fs_type = ""
    mount_options = []
    root_path = ""
    scratch_file = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    mount_options = []
    root_path = ""
    sync_remove = false
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

  [plugins."io.containerd.tracing.processor.v1.otlp"]
    endpoint = ""
    insecure = false
    protocol = ""

  [plugins."io.containerd.transfer.v1.local"]
    config_path = ""
    max_concurrent_downloads = 3
    max_concurrent_uploaded_layers = 3

    [[plugins."io.containerd.transfer.v1.local".unpack_config]]
      differ = ""
      snapshotter = "overlayfs"

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.metrics.shimstats" = "2s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0

```

#### How can we reproduce it (as minimally and precisely as possible)?

部署脚本
```
https://gitee.com/slcnx/generic_architecture/

ansible-playbook playbooks/docker/install.yml
ansible-playbook playbooks/cert/etcd.yml
cd playbooks/cert
docker compose up # 准备etcd集群
ansible-playbook playbooks/cert/kubernetes.yml
ansible-playbook playbooks/cert/worker.yml
ansible-playbook playbooks/kubernetes/deploy-master.yml
ansible-playbook playbooks/kubernetes/deploy-worker.yml
ansible-playbook playbooks/kubernetes/deploy-cni.yml
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.12
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.12

```

</details>


#### Cloud provider

<details>
vmware虚拟机
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux 192.168.102.230 6.8.0-49-generic #49-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov  4 02:06:24 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico 3.29.0
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，用户在使用二进制部署Kubernetes时遇到了问题，Pod无法正常启动，出现错误信息：“services have not yet been read at least once, cannot construct envvars”。从提供的日志和配置文件可以看出，主要问题在于CNI插件未初始化，导致网络未就绪。具体表现为：

- kubelet日志中多次出现`Network plugin returns error: cni plugin not initialized`的错误。
- containerd日志中提示`No cni config template is specified, wait for other system components to drop the config`，说明CNI配置缺失或未正确加载。

这一问题的根本原因是网络插件（如Calico）未正确安装或配置，导致Kubernetes的网络功能不可用，进而影响Pod的创建和启动。

根据风险判断标准，这是一个配置问题或部署问题，不涉及安全问题。

---


## Issue #129268 RBAC `cluster/rolebinding` created without `roleRef` and `SA` existence check

- Issue 链接：[#129268](https://github.com/kubernetes/kubernetes/issues/129268)

### Issue 内容

#### What happened?

When creating RBAC `rolebinding` and `clusterrolebinding` with subjects `ServiceAccount`, there is no `roleRef` (`Role/ClusterRole`) and `SA` existence check.

#### What did you expect to happen?

Shoud check both `roleRef` (`Role/ClusterRole`) and `SA` existence.

#### How can we reproduce it (as minimally and precisely as possible)?

rolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rb-test
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: role-not-exist # should check Role existence
subjects:
- kind: ServiceAccount
  name: sa-not-exist # should check SA existence
  namespace: default
```

clusterrolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-role-not-exist # should check ClusterRole existence
subjects:
- kind: ServiceAccount
  name: sa-not-exist # should check SA existence
  namespace: default
```

`kubectl apply -f xxx.yaml` will reproduce this issue.

#### Anything else we need to know?

Issue tested in 1.31.

#### Kubernetes version

1.31

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue指出，在创建Kubernetes的`RoleBinding`和`ClusterRoleBinding`时，如果指定的`roleRef`（即`Role`或`ClusterRole`）或`ServiceAccount`（即`SA`）不存在，系统不会进行存在性检查，仍然允许创建绑定对象。这可能导致管理员在配置RBAC权限时出现错误配置，绑定了不存在的角色或服务账户，从而导致预期的权限未能正确赋予。

从安全风险的角度来看，此问题主要涉及到配置的正确性和完整性，而不属于直接的安全漏洞。因为即使绑定了不存在的`Role`或`ServiceAccount`，也不会赋予任何实际权限，不会造成权限提升或信息泄露等安全问题。此外，攻击者无法利用该问题直接进行攻击或获取更高权限。

根据风险判断标准，此Issue描述的问题非安全问题，风险评级判断为“不涉及”。

---


## Issue #129266 informer list and watch keep str error log:too old resource version

- Issue 链接：[#129266](https://github.com/kubernetes/kubernetes/issues/129266)

### Issue 内容

#### What happened?

informer list and watch keep str error log:too old resource version
resourceVersion stay at the same value. 

#### What did you expect to happen?

when watch have error:too old resource version,resourceVersion will change other value.
informer recovery available.


#### How can we reproduce it (as minimally and precisely as possible)?

maybe etcd leader changed.

#### Anything else we need to know?


if a list request have resourceVersion and have limit or continue,apiserver will query it from etcd and return request resourceVersion.
for example 
https://qqqq6443/api/v1/nodes?resourceVersion=1111&limit=10
response resourseVersion will is 1111.


informer
1.use page.list() to list resource from apiserver.
2.get resourceVersion from 1 result.
3.use resourceVersion to watch from apiserver.


then informer page.list will always return 1111.
watch will always std err too old resource version.
#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
在这个Issue中，提出者描述了Kubernetes中Informer在进行资源的列表（List）和监视（Watch）操作时，持续输出错误日志：“too old resource version”，并且`resourceVersion`保持不变。当Watch操作出现“too old resource version”错误时，他们期望`resourceVersion`能够更新为其他值，使Informer能够恢复正常工作。

他们推测可能是由于etcd的leader变更导致的。此外，他们提到当List请求包含`resourceVersion`和`limit`或`continue`参数时，API Server会从etcd中查询并返回请求的`resourceVersion`，这可能导致`resourceVersion`停留在旧值，进而引发Watch操作的持续错误。

从上述描述可以看出，这是一个关于Kubernetes集群中Informer模块在特定情况下处理`resourceVersion`逻辑可能存在的问题，导致Informer无法正常更新资源版本，陷入错误循环。

从安全性的角度考虑：

- **机密性**：没有涉及任何敏感信息的泄露。
- **完整性**：没有出现数据被未授权篡改的情况。
- **可用性**：虽然Informer的异常可能影响到集群的部分功能，但这更偏向于可用性问题。
- **攻击可能性**：没有提到任何可能被恶意用户利用的漏洞，如未经授权的访问、提权等。

根据风险判断标准，此问题并非安全问题，而是功能性缺陷或稳定性问题。

---


## Issue #129265 RBAC `cluster/rolebinding` created without subjects and `SA` namespace check

- Issue 链接：[#129265](https://github.com/kubernetes/kubernetes/issues/129265)

### Issue 内容

#### What happened?

When creating RBAC `rolebinding` and `clusterrolebinding` with subjects `ServiceAccount`, there is no subjects empty check and namespace empty check.

#### What did you expect to happen?

Shoud check both subjects empty and namespace empty.

#### How can we reproduce it (as minimally and precisely as possible)?

rolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rb-test
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: role-test
#subjects: # subjects is required
#- kind: ServiceAccount
#  name: sa-test
#  namespace: default # namespace is required
```

clusterrolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-role-test
#subjects: # subjects is required
#- kind: ServiceAccount
#  name: sa-test
#  namespace: default # namespace is required
```

`kubectl apply -f xxx.yaml` will reproduce this issue.

#### Anything else we need to know?
Issue tested in 1.31.

#### Kubernetes version
1.31

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
在 Kubernetes 中，RoleBinding 和 ClusterRoleBinding 用于将角色（Role 或 ClusterRole）绑定到特定的主体（Subjects），这些主体可以是用户、用户组或服务账户（ServiceAccount）。在创建 RoleBinding 或 ClusterRoleBinding 时，subjects 字段是可选的，这意味着可以创建一个没有任何主体的绑定对象。

对于没有 subjects 的绑定对象，其实并不会赋予任何主体权限，因此不会导致权限泄漏或未授权访问。相反，这可能是用户配置失误，导致预期的权限未能正确分配。但这属于功能使用上的问题，而非安全漏洞。

同样地，关于 ServiceAccount 的 namespace 未指定的问题，如果在 subjects 中引用了 ServiceAccount 但未指定其命名空间，Kubernetes 默认会将其视为绑定到所在命名空间下的 ServiceAccount。如果是在 ClusterRoleBinding 中引用未指定命名空间的 ServiceAccount，可能会产生预期以外的行为，但仍然属于配置问题，不会导致未授权的权限提升或敏感信息泄露。

综上所述，该 Issue 描述的问题主要是配置验证不完善，可能导致用户误操作，但并不直接造成安全风险。

---


## Issue #129261 Kubernetes appears to use a lot of memory for its own components (≅80GiB)

- Issue 链接：[#129261](https://github.com/kubernetes/kubernetes/issues/129261)

### Issue 内容

#### What happened?

I recently installed Kubernetes on an Ubuntu 22.04 system environment. I set up the Kubernetes environment as follows: as you can see, I created one control node and two worker nodes.

```
-----------+---------------------------+--------------------------+------------
           |                           |                          |
       eth0|10.0.0.10              eth0|10.0.0.11             eth0|10.0.0.22
+----------+-----------+   +-----------+-----------+   +-----------+-----------+
|  [ ctrl.myk8s     ]  |   |  [ node01.myk8s    ]  |   |  [ node02.myk8s    ]  |
|     Control Node     |   |       Worker Node     |   |      Worker Node      |
+----------------------+   +-----------------------+   +-----------------------+
```

I started the Kubernetes service daemons and ran some pods. After a few hours, every time I ran the `free -h` command, I noticed that 80GB of memory was being used. Most of this memory usage is attributed to Kubernetes processes. 

I am curious why Kubernetes uses so much physical memory, even when running just one or two pods. I welcome any comments, hints, or information.

#### Memory usage: 

```bash
$ free -h 
    total          used        free       shared   buff/cache   available
Mem:           124Gi       85.6Gi       38Gi        21Mi       452Mi          38Gi
Swap:             0B              0B          0B

$  ps -eo uid,euser,wchan,comm,rss,vsize,%cpu,nlwp | awk '$5 != 0' | sort -k5,5rn

uid   euser    wchan  comm            rss   vsize   %cpu nlwp 
----------------------------------------------------------------------
    0 root     futex_ kubelet         117576 6850960 1.1   76
    0 root     futex_ containerd      100948 7420620 0.6   82
    0 root     futex_ calico-node     93024  5417080 0.4   56
    0 root     futex_ calico-node     83600  3494160 0.0   30
    0 root     futex_ calico-node     83192  3715612 0.0   33
    0 root     futex_ calico-node     80348  3641880 0.0   32
    0 root     futex_ calico-node     79912  3494416 0.0   30
    0 root     futex_ calico-node     74932  3493392 0.0   30
    0 root     futex_ kube-proxy      72944  1295544 0.0   32
    0 root     do_pol python3         31636   42076  0.0    1
    0 root     futex_ node-driver-reg 29436  1248032 0.0   32
    0 root     futex_ tuned           26628  257916  0.0    4
  997 polkitd  do_pol polkitd         22804  2983876 0.0   12
    0 root     futex_ csi-driver      21824  1237992 0.0   24
    0 root     do_pol NetworkManager  21480  259196  0.0    3
    0 root     futex_ containerd-shim 17492  1238184 0.0   14
    0 root     futex_ containerd-shim 16600  1238184 0.0   13
    0 root     ep_pol systemd         15192  175176  0.0    1
    0 root     futex_ containerd-shim 13688  1238184 0.0   13
    0 root     do_sel snmpd           13056   25748  0.0    1
    0 root     ep_pol systemd-journal 13056   27820  0.0    1
 1000 codegrok ep_pol systemd         13056   23928  0.0    1
    0 root     futex_ containerd-shim 12704  1238184 0.0   13

```


![k8s-85gb-mem-hog](https://github.com/user-attachments/assets/c4f4919f-db8f-467e-9f0d-73d81460d0dc)



#### What did you expect to happen?

I expected Kubernetes to use less than 8GB of memory when running just one or two pods.

#### How can we reproduce it (as minimally and precisely as possible)?

#### [1] 	Install Containerd and apply some requirements on all Nodes.
```
root@ctrl:~# apt -y install containerd
root@ctrl:~# cat > /etc/sysctl.d/99-k8s-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
EOF
root@ctrl:~# sysctl --system
root@ctrl:~# modprobe overlay; modprobe br_netfilter

root@ctrl:~# echo -e overlay\\nbr_netfilter > /etc/modules-load.d/k8s.conf
# needs [iptables-legacy] for iptables backend
# if nftables is enabled, change to [iptables-legacy]

root@ctrl:~# update-alternatives --config iptables

There are 2 choices for the alternative iptables (providing /usr/sbin/iptables).

  Selection    Path                       Priority   Status
------------------------------------------------------------
* 0            /usr/sbin/iptables-nft      20        auto mode
  1            /usr/sbin/iptables-legacy   10        manual mode
  2            /usr/sbin/iptables-nft      20        manual mode

Press <enter> to keep the current choice[*], or type selection number: 1
update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in manual mode

# disable swap

root@ctrl:~# swapoff -a

root@ctrl:~# vi /etc/fstab

# comment out
#/swap.img      none    swap    sw      0       0

# switch to Cgroup v1 (default is v2)

root@ctrl:~# vi /etc/default/grub
# line 11 : add

GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=0
"
root@ctrl:~# update-grub
```



#### [2] 	Install Kubeadm, Kubelet, Kubectl on all Nodes.
```
root@ctrl:~# curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg -o /etc/apt/keyrings/kubernetes-keyring.gpg

root@ctrl:~# echo "deb [signed-by=/etc/apt/keyrings/kubernetes-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list

root@ctrl:~# apt update

root@ctrl:~# apt -y install kubeadm kubelet kubectl
root@ctrl:~# vi /etc/default/kubelet
# create new

KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock
root@ctrl:~# systemctl edit containerd.service
# add follows

[Service]
KillMode=
KillMode=mixed
root@ctrl:~# systemctl restart containerd.service 
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:

$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux mymate 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，用户在Ubuntu 22.04系统上安装了Kubernetes集群（一个控制节点，两个工作节点），发现Kubernetes自身组件使用了大量的内存（约80GiB），即使只运行了一两个pod。用户对为什么会出现如此高的内存占用感到疑惑，并希望获得解释。

从Issue内容来看，这是一个关于Kubernetes内存消耗的性能问题，涉及Kubernetes组件（如kubelet、containerd、calico-node等）占用内存过高的问题。用户提供了内存使用情况的输出，以及安装和配置Kubernetes的步骤。

根据风险判断标准：

1. Issue描述的问题是Kubernetes组件的高内存使用，属于性能优化或资源管理的问题，而非安全问题。
2. 没有涉及任何安全漏洞或安全风险的描述，例如代码执行、容器逃逸、提权等。
3. 没有提及任何敏感信息的泄露，或可能被攻击者利用的漏洞。

因此，该Issue不涉及安全风险。

---


## Issue #129260  add a non-existent owner reference to a resource can cause it to be silently deleted

- Issue 链接：[#129260](https://github.com/kubernetes/kubernetes/issues/129260)

### Issue 内容

#### What happened?

 if we add a non-existent owner reference to a resource, the API **does not return an error when doing the modification**. However, the resource is deleted **silently** in the background.

#### What did you expect to happen?

1. Do we need to return an error when the owner dose not exist ?
2. I am aware of an issue that one can delete a resource only with update permissions by hacking a non-existent owner reference to it. This should be regarded as an authorization issue.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create an arbitary resource
2. `kubectl edit` the resource, add a non-existent owner reference to a resource.
3. `kubectl get` the resource. the resource will be not found.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.5-gke.1443001", GitCommit:"d16b6cc50798c61dc2740487abdaa656af180538", GitTreeState:"clean", BuildDate:"2024-10-07T04:15:31Z", GoVersion:"go1.22.6 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
Linux 5.15.0-1062-gcp #70~20.04.1-Ubuntu SMP Fri May 24 20:12:18 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
**问题描述**：

在 Kubernetes 中，如果对某个资源添加了一个不存在的所有者引用（Owner Reference），API 在进行修改操作时并不会返回错误提示。然而，该资源会在后台被“静默”地删除。

**安全风险分析**：

1. **权限绕过**：用户只需要拥有对资源的更新权限（update），就可以通过添加一个不存在的所有者引用来删除资源，而不需要实际的删除权限（delete）。这可能导致权限提升的问题。

2. **影响范围**：该问题可能导致非预期的资源删除，影响系统的完整性和可用性。

3. **复现难度**：攻击者只需编辑资源并添加不存在的所有者引用即可，操作简单，攻击复杂度低。

**CVSS 3.1 风险评分**：

- **攻击向量（AV）**：网络（N）——攻击者通过网络对 Kubernetes API 发起请求。
- **攻击复杂度（AC）**：低（L）——攻击不需要特殊条件或环境。
- **权限要求（PR）**：低（L）——需要具备对资源的更新权限。
- **用户交互（UI）**：无（N）——不需要其他用户的交互。
- **作用范围（S）**：未改变（U）——攻击仅影响受害者所在的权限域。
- **机密性（C）**：无（N）——不影响数据的机密性。
- **完整性（I）**：低（L）——对数据完整性有一定影响。
- **可用性（A）**：高（H）——导致资源被删除，严重影响可用性。

**综合评分**：6.1（中等）

计算结果可参考 CVSS 3.1 计算器。

---


## Issue #129259 Recreate strategy doesn't create new replicaset on its own

- Issue 链接：[#129259](https://github.com/kubernetes/kubernetes/issues/129259)

### Issue 内容

#### What happened?

We've switched some of our deployments to the recreate strategy and as a result we're seeing long delays between a replicaset scaling down and a new one scaling up when a new version is rolled out (10+ minutes between events). This can be due to a number of things but it seemed to only impact our workloads on the recreate strategy. After digging into the source code it appears that the recreate logic doesn't actually create the new replicaset even though it seems like it should. Instead it looks like it exits out early and relies on the cluster/replicaset controller to naturally resolve the out of sync state when it gets around to it (which in our case can be pretty slow). This can result in very long deploy cycles for workloads using the recreate strategy. The issue I believe is stemming from [here](https://github.com/kubernetes/kubernetes/blob/v1.31.2/pkg/controller/deployment/recreate.go#L45) where we return early after updating the deployment status if the previous replicaset has scaled down regardless of the new replicaset. Checking the history of the code this has been around for [9 years](https://github.com/kubernetes/kubernetes/commit/92798408af90569f492be3a1a4d8de02538a6787#diff-c553184997fa4c9f63e697af2f39eb2e6fb43eeff64a0ef66c9e955f61347b02R428) so either I am not interpreting what should be happening correctly or just no one has noticed but I would expect that the code doesn't return there but instead continues to spin up the new replicaset like the logic after that return statement does in that method.

#### What did you expect to happen?

I expected my deployments that use the recreate strategy to deploy their new replicasets shortly after the old ones have been scaled down.

#### How can we reproduce it (as minimally and precisely as possible)?

A unit test could probably reproduce this but I'm more curious about an explanation of the code to make sure I understand it correctly.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.31.2

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes中使用`recreate`策略进行部署时，发现旧的ReplicaSet缩容后，新的ReplicaSet在启动时存在较长的延迟（超过10分钟）。经过对源码的分析，发现可能是由于在缩容旧的ReplicaSet后，代码在更新Deployment状态后过早地返回，导致新ReplicaSet的创建依赖于集群/ReplicaSet控制器的自然状态同步，而非立即创建新的ReplicaSet。这可能导致部署过程的延迟。

从问题的描述来看，这是Kubernetes部署策略实现上的功能性问题，影响了部署的效率和性能，但并未涉及任何安全风险。具体而言：

- **未涉及未经授权的访问或权限提升**：没有提到低权限用户可以利用该问题获取更高权限或访问未经授权的资源。
- **未涉及敏感信息泄露**：没有涉及任何敏感信息的披露，如凭据、密钥等。
- **未涉及远程代码执行、容器逃逸等高危安全风险**：该问题不会导致攻击者在系统上执行任意代码或突破容器隔离。

根据风险判断标准：

- **第2条**：如果该Issue描述的问题非安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险，只是Kubernetes在特定部署策略下的功能实现问题。

**解释说明：**

该Issue不涉及安全风险，因此无需提供复现脚本。

---


## Issue #129253 The container_memory_working_set_bytes indicator corresponding to a container has three records.

- Issue 链接：[#129253](https://github.com/kubernetes/kubernetes/issues/129253)

### Issue 内容

#### What happened?

When the container_memory_working_set_bytes indicator is queried through the Cadvisor interface, three data records exist in the same container, and the corresponding IDs are nested. Why?
```
[root@master1 log]# kubectl get --raw /api/v1/nodes/master1/proxy/metrics/cadvisor | grep container_memory_working_set_bytes | grep 'container="qk-sandbox-elf-service"'
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.470353408e+09 1734442795448
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.055014912e+09 1734442803008
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.055014912e+09 1734442797894
```

In addition, the corresponding directory is found on the node.
```
[root@master1 cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope]# pwd
/sys/fs/cgroup/memory/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope
```

#### What did you expect to happen?

One container corresponds to one piece of data.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue中，用户发现通过Cadvisor接口查询容器的`container_memory_working_set_bytes`指标时，同一容器存在三条数据记录，且对应的ID是嵌套的，对此感到疑惑并询问原因。他期望每个容器只对应一条数据。

从问题描述来看，这是关于Cadvisor监控数据采集方式的技术问题，可能与容器的cgroup层级配置、Cadvisor的实现方式或Kubernetes的资源管理有关。这种情况可能是由于cgroup层级的嵌套导致的，属于运维或监控配置方面的问题。

该Issue没有涉及任何安全风险，没有提及敏感信息泄露、权限提升、命令执行、容器逃逸等安全问题。也没有迹象表明存在可能被攻击者利用的漏洞。

根据风险判断标准，此问题非安全问题。

---


## Issue #129229 fix: noderesources plugin flaw

- Issue 链接：[#129229](https://github.com/kubernetes/kubernetes/issues/129229)

### Issue 内容

#### What happened?

The score function of resourceAllocationScorer should not iterate over args.ScoringStrategy.Resources. For example: the strategy parameters are set to [gpu:2,cpu:1,mem:1]. At this time, the pod only applies for cpu and mem, but because k8s traverses parameters instead of traversing pod application, gpu becomes the final The key factor in scoring, but in fact, the pod only needs to consider resource application issues of CPU and MEM, and does not need to consider GPU. After all, it has not applied for GPU. If it is the MostRequestedPriority policy at this time, after consideration, the CPU strength will be scheduled to the GPU. On the machine, there is a fragmentation problem


#### What did you expect to happen?

Which resources will be considered for scoring based on which resources the pod has applied for,Instead of scoring all the resources configured in the configuration file, which may be counterproductive in real scenarios.


#### How can we reproduce it (as minimally and precisely as possible)?

The MostRequestedPriority policy parameters are set to [gpu:2,cpu:1,mem:1]. At this time, the pod only applies for cpu and mem. It is found that the pod is scheduled to a machine with concentrated GPUs. As a result, when there are large model GPU tasks that need to be scheduled, The machine's GPU is satisfied but the CPU is not.


#### Anything else we need to know?

_No response_

#### Kubernetes version

latest version


#### Cloud provider

only k8s

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes调度器在资源分配评分（`resourceAllocationScorer`）时的问题。当调度策略参数设置为包含GPU资源（例如`[gpu:2,cpu:1,mem:1]`），而Pod只申请了CPU和内存资源时，调度器仍会遍历所有策略参数，包括GPU资源。这可能导致Pod被调度到拥有GPU的节点上，即使Pod并不需要GPU，导致GPU资源被占用，造成资源碎片化。

从安全角度分析：

- **资源浪费问题**：这是一个资源调度优化问题，涉及资源利用率和调度效率，但并不直接构成安全风险。
- **拒绝服务（DoS）风险**：虽然资源碎片化可能间接导致GPU资源不足，影响其他需要GPU的Pod的调度，但需要具备创建Pod的权限，且影响是由于调度策略不佳引起的，不属于典型的DoS攻击。
- **多租户影响**：在多租户环境下，低权限用户提交的Pod可能影响到其他用户的资源使用，但这在Kubernetes中是通过资源配额和调度策略管理的，不属于安全漏洞。

综上所述，该Issue反映的是Kubernetes调度策略的改进需求，旨在优化资源分配，提高系统性能，不涉及安全漏洞。

---


## Issue #129228 HPA scales up despite utilization being under target

- Issue 链接：[#129228](https://github.com/kubernetes/kubernetes/issues/129228)

### Issue 内容

#### What happened?

We use Argo Rollouts to perform canary deployments of our services. During a canary deployment, new pods are brought up (the canary pods) which are included in the [Status](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/apis/autoscaling/types.go#L51) of the Rollout's scale subresource. When HPA is configured to scale on a metric with a high utilization ratio (generally memory), this results in HPA scaling out, despite the fact that the utilization is under the target. 

This seems to be a result of the behaviour of the [replica calculator](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/podautoscaler/replica_calculator.go#L65) where:
- the recommended replicas is a simple function of utilizationRatio and the total number of pods returned by the selector
- missing pods with a utilization ratio of below 1 are assumed to be because of a "scale down" and are treated as if they are [consuming 100% of resources](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/controller/podautoscaler/replica_calculator.go#L108-L113) (whereas in this case we are scaling out during a rollout)

In addition, the utilization ratio is never checked prior to updating prior to HPA increasing the desired replicas leading to [incorrect/misleading log messages](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/controller/podautoscaler/horizontal.go#L857-L859) like "New size: X; reason: memory resource utilization (percentage of request) above target". 

Further details can be found in issues logged for Argo Rollouts [here](https://github.com/argoproj/argo-rollouts/issues/2857) and [here](https://github.com/argoproj/argo-rollouts/issues/3849).

#### What did you expect to happen?

HPA should not scale up when utilization is below target (I.e. when `utilizationRatio < 1`)

#### How can we reproduce it (as minimally and precisely as possible)?

The issue can be reproduced by simulating the behaviour of Argo Rollouts and creating a 2nd `ReplicaSet` with selectors that match an existing `Deployment`. For example:

Create base `Deployment` and `HorizontalPodAutoscaler` resources:  
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx
          name: nginx
          resources:
            requests:
              memory: "10Mi"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
  namespace: test
spec:
  maxReplicas: 50
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 50
```

Wait to ensure that metrics are available and HPA is stable at 3 pods.
``` bash
$ kubectl get pods,hpa
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-7fb4f6d65f-jgdzm   1/1     Running   0          30s
pod/nginx-7fb4f6d65f-mtnxv   1/1     Running   0          30s
pod/nginx-7fb4f6d65f-tc6c8   1/1     Running   0          30s

NAME                                        REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/nginx   Deployment/nginx   memory: 42%/50%   3         50        3          30s
```

Create a 2nd replica set: 
``` yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-canary
  namespace: test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      role: canary
  template:
    metadata:
      labels:
        app: nginx
        role: canary
    spec:
      containers:
        - image: nginx
          name: nginx
          resources:
            requests:
              memory: "10Mi"
```

Observe that HPA will scale the existing `Deployment`:
``` bash
$ kubectl get pods,hpa                                                                                     
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-7fb4f6d65f-2hh7f   1/1     Running   0          2m36s
pod/nginx-7fb4f6d65f-6lfr5   1/1     Running   0          2m6s
pod/nginx-7fb4f6d65f-9wgfn   1/1     Running   0          6s
pod/nginx-7fb4f6d65f-gpzp9   1/1     Running   0          2m6s
pod/nginx-7fb4f6d65f-hdsff   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-j24kp   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-jgdzm   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-kxbvz   1/1     Running   0          66s
pod/nginx-7fb4f6d65f-l4rzr   1/1     Running   0          2m36s
pod/nginx-7fb4f6d65f-mtnxv   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-qvpqx   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-rq2r8   1/1     Running   0          96s
pod/nginx-7fb4f6d65f-s52hj   1/1     Running   0          66s
pod/nginx-7fb4f6d65f-tc6c8   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-zwpfc   1/1     Running   0          36s
pod/nginx-canary-2prb6       1/1     Running   0          3m41s
pod/nginx-canary-9l5rc       1/1     Running   0          3m41s
pod/nginx-canary-g79tk       1/1     Running   0          3m41s

NAME                                        REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/nginx   Deployment/nginx   memory: 42%/50%   3         50        14         4m36s
```

Check events on HPA to see details on why scaling occurred:
```
$ kubectl describe hpa nginx
Name:                                                     nginx
Namespace:                                                test
Labels:                                                   <none>
Annotations:                                              <none>
CreationTimestamp:                                        Mon, 16 Dec 2024 10:20:04 +0100
Reference:                                                Deployment/nginx
Metrics:                                                  ( current / target )
  resource memory on pods  (as a percentage of request):  42% (4503324444m) / 50%
Min replicas:                                             3
Max replicas:                                             50
Deployment pods:                                          8 current / 8 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type     Reason                   Age                    From                       Message
  ----     ------                   ----                   ----                       -------
  Warning  FailedGetResourceMetric  4m35s (x3 over 4m46s)  horizontal-pod-autoscaler  unable to get metric memory: no metrics returned from resource metrics API
  Normal   SuccessfulRescale        3m31s                  horizontal-pod-autoscaler  New size: 6; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        3m1s                   horizontal-pod-autoscaler  New size: 8; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        2m31s                  horizontal-pod-autoscaler  New size: 10; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        2m1s                   horizontal-pod-autoscaler  New size: 11; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        91s                    horizontal-pod-autoscaler  New size: 13; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        61s                    horizontal-pod-autoscaler  New size: 14; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        31s                    horizontal-pod-autoscaler  New size: 15; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        1s                     horizontal-pod-autoscaler  New size: 16; reason: memory resource utilization (percentage of request) above target
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，问题描述了在使用Argo Rollouts进行金丝雀部署时，HPA（Horizontal Pod Autoscaler）在资源利用率低于目标值的情况下，仍然会扩容。原因在于HPA的副本计算器在处理缺失的pods时，将其视为正在缩容，并假设这些pods正在消耗100%的资源，从而导致扩容决策。

从安全风险的角度分析：

1. **问题性质**：该问题属于HPA在特定情况下的行为异常，导致资源的不必要消耗。这更倾向于功能性错误或性能问题。

2. **攻击可能性**：要利用此问题，攻击者需要具备在集群中创建ReplicaSet或修改部署的权限。这属于较高的权限范畴。

3. **风险评估标准**：

   - 根据第2条，如果Issue描述的问题非安全问题，则风险评级判断为“不涉及”。
   - 根据第5条，在DoS攻击风险中，如果攻击者需要具备创建、修改等非只读权限，则不应判断为高风险。

综上，该Issue描述的问题不属于安全风险，而是需要修复的功能性问题。

---


## Issue #129220 kube-apiserver memory leak

- Issue 链接：[#129220](https://github.com/kubernetes/kubernetes/issues/129220)

### Issue 内容

#### What happened?

I have upgraded to kubernetes 1.31.1 using Kubespray, after the update i can observe several issues, if we observe the graph for mem usage its like 45 degrees, from the start rising up untill it crashes, not much can be seen from the logs except some GRCP errors, i doubt this to cause the crashes, but this is everything i have

#### What did you expect to happen?

Api not leak memory and crashes

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure

#### Anything else we need to know?

W1214 15:25:17.351529       1 logging.go:55] [core] [Channel #1029556 SubChannel #1029558]grpc: addrConn.createTransport failed to connect to {Addr: "REDACTED:REDACTED", ServerName: "REDACTED:REDACTED", }. Err: connection error: desc = "error reading server preface: read tcp REDACTED:REDACTED->REDACTED:REDACTED: use of closed network connection"


Those erros are filling the logs, nothing else can be observed

#### Kubernetes version

<details>

```console
$ kubectl version 
# paste output here
```
1.31.1

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# Ubuntu 24.04
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Weave
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据提供的Issue内容，用户在使用Kubespray升级到Kubernetes 1.31.1后，发现`kube-apiserver`存在内存泄漏并最终导致崩溃。日志中主要显示了一些GRPC错误，具体为：

```
W1214 15:25:17.351529       1 logging.go:55] [core] [Channel #1029556 SubChannel #1029558]grpc: addrConn.createTransport failed to connect to {Addr: "REDACTED:REDACTED", ServerName: "REDACTED:REDACTED", }. Err: connection error: desc = "error reading server preface: read tcp REDACTED:REDACTED->REDACTED:REDACTED: use of closed network connection"
```

从描述来看，这是一次正常的升级操作后出现的服务异常，导致`kube-apiserver`内存逐渐增大并最终崩溃。这更可能是由于软件Bug、配置不当或者兼容性问题引起的性能或稳定性问题。

根据风险判断标准：

1. **风险评级判断为不涉及：** 如果该Issue描述的问题非安全问题，则风险评级判断为不涉及。
2. **未涉及安全漏洞或敏感信息：** Issue中没有提及任何可被利用的安全漏洞、攻击手段或敏感信息泄露。
3. **缺乏复现步骤：** Issue中也没有提供任何可用于恶意利用的问题细节或复现步骤。

---


## Issue #129197 watcher always terminal

- Issue 链接：[#129197](https://github.com/kubernetes/kubernetes/issues/129197)

### Issue 内容

#### What happened?

when is use 
https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true

watcher will terminal

we have 100000 pods

i see #13969

this param will return all pods event

(c *cacheWatcher) processInterval  will exec process func when initEvents send to result success,but is took 5s-6s.
if process func not be exec,watcher  input chan will not have comsumer.

then watcher will be blockedWatcher,when etcd have changed
the func watcher.add(event, timer) will timeout and kill my watcher.

#### What did you expect to happen?

watcher keep to timeoutSecond

#### How can we reproduce it (as minimally and precisely as possible)?

https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true
100000 pods

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.30

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes集群中，当存在100,000个Pod时，使用Watcher（通过访问`https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true`）进行监听时，Watcher会提前终止。Issue中提到：

- 初始化事件发送到结果通道时耗时较长（5-6秒），导致Watcher的输入通道没有消费者，出现阻塞。
- 当etcd有变化时，`watcher.add(event, timer)`函数会超时，进而终止Watcher。

从上述描述来看，这个问题主要是由于在大规模Pod数量下，Watcher在处理初始事件时存在性能问题，导致Watcher无法持续工作到`timeoutSeconds`指定的时间。

在安全风险评估中，该问题属于功能性或性能问题，并未涉及到任何潜在的安全漏洞。例如：

- **未涉及未授权访问**：没有提到任何未经授权访问或数据泄露的情况。
- **未涉及权限提升**：没有提到低权限用户提升权限的情况。
- **未涉及代码执行或容器逃逸**：没有任何关于命令执行、代码注入或容器逃逸的描述。
- **未涉及拒绝服务攻击（DoS）**：虽然Watcher终止，但这是由于自身处理性能问题引起的，并非外部攻击导致的资源耗尽或服务不可用。

---


## Issue #129196 'requiredDuringSchedulingIgnoredDuringExecution' evicts Daemonset when node label removed

- Issue 链接：[#129196](https://github.com/kubernetes/kubernetes/issues/129196)

### Issue 内容

#### What happened?

The node affinity requires that the Daemonset be deployed on a node with a specified label. Affinity type is requiredDuringSchedulingIgnoredDuringExecution.
When Daemonset has been running for a while, remove the label.
Daemonset will be destroyed.

However, according to [the official doc,](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity-per-scheduling-profile) the Daemonset type is IgnoredDuringExecution, and the pod should not be destroyed after the tag is removed.
```
Note: 
In the preceding types, IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.
```

#### What did you expect to happen?

After the label is removed, Daemonset should continue to run (consistent with the IgnoredDuringExecution described in the doc).

#### How can we reproduce it (as minimally and precisely as possible)?

apply the following yaml and modify the image path. [full yaml link](https://gitee.com/OpenCloudOS/xuanwan/blob/master/os-update-operator/configs/deploy/operator/up-proxy.yaml)
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: up-proxy
  namespace: kube-system
  labels:
    control-plane: up-proxy
spec:
  selector:
    matchLabels:
      control-plane: up-proxy
  template:
    metadata:
      labels:
        control-plane: up-proxy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: xuanwan.opencloudos.org/updater-interface-version
                    operator: In
                    values:
                      - 1.0.0
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
                      - arm64
      containers:
      - name: up-proxy
        image: <up-proxy-container-path>
        imagePullPolicy: IfNotPresent
```
On the node label xuanwan.opencloudos.org/updater-interface-version= "1.0.0" and wait for Daemonset deployed and run.
Remove the node label and observe whether Daemonset continues to run

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.27.4
```

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="OpenCloudOS"
VERSION="9.2"
ID="opencloudos"
ID_LIKE="opencloudos"
VERSION_ID="9.2"
PLATFORM_ID="platform:oc9"
PRETTY_NAME="OpenCloudOS 9.2"
$ uname -a
Linux kube-master 6.6.47-12.oc9.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Sep 24 16:15:42 CST 2024 x86_64 GNU/Linux



```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了当使用 `requiredDuringSchedulingIgnoredDuringExecution` 类型的 `nodeAffinity` 时，DaemonSet 在节点标签被移除后，Pod 被驱逐的现象。根据 Kubernetes 官方文档，`IgnoredDuringExecution` 表示即使在 Pod 调度后节点标签发生变化，Pod 也应继续运行。

从安全角度分析：

1. **可能的安全影响**：如果攻击者能够移除节点上的特定标签，导致关键的 DaemonSet Pod 被驱逐，可能会造成拒绝服务（DoS）攻击，影响集群中服务的可用性。

2. **实施条件**：要移除节点标签，攻击者需要具有修改节点对象的权限。这通常需要较高的权限级别（如集群管理员或具有节点管理权限的用户）。

根据风险判断标准：

- **标准5**：在拒绝服务（DoS）攻击的风险类型中，如果攻击者需要一定权限（如创建、修改等非只读权限）才能实施攻击，则不应判断为高风险，CVSS 评级在 high 以下。

因此，尽管该问题涉及安全风险，但由于需要高权限才能实施攻击，风险评级应为**低风险**。

---


## Issue #129193 "Error syncing pod, skipping" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" pod="" podUID=""

- Issue 链接：[#129193](https://github.com/kubernetes/kubernetes/issues/129193)

### Issue 内容

#### What happened?

Pod has been terminating, error: "Error syncing pod, skipping" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" pod="" podUID=""， Actually, containerd is normal

#### What did you expect to happen?

Pod pulled up normally

#### How can we reproduce it (as minimally and precisely as possible)?

Dec  5 23: Upgrade caas from 53 onwards
Dec  5 23: 56 Node Restart
Dec 5 23:56:52 containerd starts after power on
Since the start of the kubelet process on Dec 6 00:00:23, it has not been restarted yet. It should be at this point in time that the kubelet was upgraded
Dec 6 00:00:06 Crittl Check the start and run time of the container process for nbidataserver (cinit process inside the container)
Dec 6 00:01:16 ps Check the startup time of the business process. The probe may not be ready for a short period of time before starting, and the kubelet may only sync after reaching certain conditions 
Dec 6 00:01:37 nbidataservice updataPodContainerResources first reported failure containerdsock connection rejected and kept reporting errors afterwards
Dec 6 00:01:38 containerd upgraded and started up until now



Latest progress: This issue is caused by enabling the InPlacePodVerticalScaling feature
Event replay:
Event 1:2024-12-24 22:41:33 Machine Power On/Off
Event 2:2024-12-24 22:47:53 kubelet starts to start, Pod starts to be pulled up
Event 3:2024-12-24 22:49:03 containerd starts
Event 4:2024-12-24 22:49:04 kubelet started pulling up smauthservice, but received an error message "server is not initialized yet" when accessing containerd. After containerd started, smauthservice started normally.
Event 5:2024-12-24 22:54:12: The upgrade of smauthservice started, and the Kubernetes control plane set the pod smauthservice to Terminating state, which has been consistent until now.
Cause analysis:
1. The kubelet caches real pod information (also known as pod cache), which is synchronized from containerd by PLEG (PodLifecycle Event Generator) coroutine timing. PLEG itself also caches the real pod state (known as PLEG cache). If PLEG detects a difference between the real container state and its own cache, it will synchronize the real pod information to the pod cache.
2. After enabling the InPlacePodVerticalScaling feature, during the pod startup phase, if the pod is expanding, kubelet will query the real pod status from containerd and update the pod cache directly without PLEG. At this time, because containerd cannot access it, unknown status will be saved to the pod cache (Event 4). However, PLEG will not update the pod cache again because the actual pod status is no different from its own cache.
3. Later on, when upgrading the pod, the coroutine for handling pod deletion needed to retrieve the actual pod state from the pod cache for tuning. However, an unknown state was obtained, so the deletion of the pod was stopped and the pod remained in Terminating (Event 5).


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28.1
</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 标题描述了在同步 Pod 时出现错误，无法连接 `/run/containerd/containerd.sock`，提示 "connection refused"。Issue 内容进一步分析了问题发生的原因：

1. 启用了 `InPlacePodVerticalScaling` 特性后，kubelet 在 Pod 启动阶段，如果 Pod 处于扩容状态，会直接从 containerd 查询实际的 Pod 状态并更新缓存，而不是通过 PLEG（Pod Lifecycle Event Generator）。
2. 如果此时 containerd 尚未启动，kubelet 无法访问 containerd，导致保存了未知的 Pod 状态到缓存中。
3. 因为 PLEG 不会更新缓存（因为它认为状态没有变化），在后续的 Pod 删除过程中，kubelet 获取到未知的 Pod 状态，无法正常删除 Pod，导致 Pod 一直处于 Terminating 状态。

从以上分析可以看出，这个问题导致了 Pod 无法正常删除，服务陷入不可用状态，属于潜在的拒绝服务（DoS）风险。

然而，要触发这个问题，需要满足以下条件：

- 启用了 `InPlacePodVerticalScaling` 特性。
- kubelet 和 containerd 的启动顺序特定（containerd 启动较慢，kubelet 先尝试访问）。
- 攻击者需要能够影响节点上 kubelet 和 containerd 的启动顺序或服务状态。

根据风险判断标准：

- 第5条指出，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS 评级在 high 以下。
- 此问题需要攻击者具有对节点的管理权限，才能影响 kubelet 和 containerd 的启动和配置。

因此，该问题虽属于安全问题，但风险评级应为低风险。

---


## Issue #129187 Failed to get rootfs info if the host machine uses bcachefs raid array

- Issue 链接：[#129187](https://github.com/kubernetes/kubernetes/issues/129187)

### Issue 内容

#### What happened?

If you create a host machine with bcachefs file system with multiple drives:

```bash
❯ bcachefs format /dev/nvme0n1p2 /dev/nvme1n1p2 --replicas=2
❯ mount -t bcachefs /dev/nvme0n1p2:/dev/nvme1n1p2 /mnt
```

The filesystem device is in every linux tool defined as `/dev/nvme0n1p2:/dev/nvme1n1p2`.

```bash
❯ mount
/dev/nvme0n1p2:/dev/nvme1n1p2 on / type bcachefs (rw,relatime,metadata_replicas=2,data_replicas=2,compression=lz4,background_compression=lz4,nopromote_whole_extents,fix_errors=yes,nojournal_transaction_names)

❯ df /
Filesystem                     1K-blocks       Used  Available Use% Mounted on
/dev/nvme0n1p2:/dev/nvme1n1p2 3593522709 1260627309 2297004701  36% /
```

As you might know, docker displays this file system label if you add any unnamed volumes to your container. And this brings us to the k8s issue. If you create a cluster e.g. with kind or minikube, it starts a kubelet service in the container. Kubelet tries to read all mounts, and fails to parse the bcachefs filesystem:

```
kubelet.go:1566] "Failed to start ContainerManager" err="failed to get rootfs info: failed to get mount point for device \"/dev/nvme0n1p2:/dev/nvme1n1p2\": no partition info for device \"/dev/nvme0n1p2:/dev/nvme1n1p2\""
```

What you _can_ do is to just take the first partition from the list and use that. But here we have kubelet just crashing, rendering systems such as kind or minikube not working.

#### What did you expect to happen?

Kubelet should just not care the filesystem name format. The system should start even when using bcachefs.

#### How can we reproduce it (as minimally and precisely as possible)?

Spin up a virtual machine with two disks. It's probably easiest to run on NixOS:

https://nixos.wiki/wiki/Bcachefs

Create the root volume with

```bash
❯ bcachefs format /dev/nvme0n1p2 /dev/nvme1n1p2 --replicas=2
❯ mount -t bcachefs /dev/nvme0n1p2:/dev/nvme1n1p2 /mnt
```

Continue the installation as instructed.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
```

</details>


#### Cloud provider

<details>
Local machine.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
# paste output here
Linux docker-container 6.12.3-cachyos #1-NixOS SMP PREEMPT_DYNAMIC Fri Dec  6 06:20:46 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 报告了在主机使用多个驱动器并采用 bcachefs 文件系统时，`kubelet` 服务无法启动的问题。具体表现为 `kubelet` 在尝试获取 rootfs 信息时，无法解析设备名称 `/dev/nvme0n1p2:/dev/nvme1n1p2`，导致报错并停止运行。这使得基于 Kubernetes 的系统（如 kind 或 minikube）无法正常工作。

根据风险判断标准，需要评估该问题是否存在潜在的安全风险。

首先，该问题导致的是 `kubelet` 服务无法启动，影响了系统的正常运行，这是一个可用性方面的问题，属于拒绝服务（DoS）的范畴。

根据标准第5条：

> 在风险类型为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理，当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下。

在此情境中，只有具有主机管理员权限的用户才能配置文件系统为 bcachefs 并使用多个驱动器。这意味着普通用户或外部攻击者无法通过此方式来导致 `kubelet` 无法启动。

因此，该问题并非安全漏洞，而是一个由于特殊配置导致的兼容性问题，不涉及安全风险。

---


## Issue #129177 The actual pod call to the readNamespacedPod returned a 404 NotFound error.

- Issue 链接：[#129177](https://github.com/kubernetes/kubernetes/issues/129177)

### Issue 内容

#### What happened?

This is the response from the call:
header:{audit-id=[22e1f76c-4364-41d6-a826-16f26b0df14b], cache-control=[no-cache, private], content-length=[228], content-type=[application/json], date=[Thu, 12 Dec 2024 12:53:25 GMT], x-kubernetes-pf-flowschema-uid=[38efc94f-5c70-40e9-8e97-167a23b9942a], x-kubernetes-pf-prioritylevel-uid=[09099773-afa1-4ec2-b873-29bc7bc42106]}, body:{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"pods \"iac-pod-xxx\" not found","reason":"NotFound","details":{"name":"iac-pod-xxx","kind":"pods"},"code":404}

#### What did you expect to happen?

the pod is existed，but it returned a 404 error

#### How can we reproduce it (as minimally and precisely as possible)?

This is an intermittent issue.



#### Anything else we need to know?

_No response_

#### Kubernetes version

        <dependency>
            <groupId>io.kubernetes</groupId>
            <artifactId>client-java</artifactId>
            <version>16.0.0</version>
        </dependency>

#### Cloud provider

alibaba cloud

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，用户在调用Kubernetes的`readNamespacedPod` API时，尽管Pod实际存在，但有时会返回404 NotFound错误。该问题是间歇性发生的。

从安全角度来看，这更像是一个功能性或稳定性问题，可能与以下因素有关：

- **网络延迟或不稳定**：网络问题可能导致API请求未能及时获取最新的Pod信息。
- **Kubernetes API服务器的延迟或一致性**：在集群中，数据的同步可能存在短暂的不一致性，导致偶尔无法找到Pod。
- **Pod的状态变化**：如果Pod正在被重启、迁移或短暂地被删除再创建，可能会在某些时刻无法访问。

根据风险判断标准：

1. **风险评级分为三类，不涉及，低风险和高风险**。
2. **如果该Issue描述的问题非安全问题，则风险评级判断为不涉及**。

综上所述，该Issue反映的是系统功能性问题，并不涉及安全风险。

---


## Issue #129176 Empty annotation removes all existing and valid annotations

- Issue 链接：[#129176](https://github.com/kubernetes/kubernetes/issues/129176)

### Issue 内容

#### What happened?

When creating a resource (e.g. a Service) with annotations the annotations are all removed when one of them has an empty value. The same happens on update.
It happens in our production environment as well as locally on minikube so I don't expect anything we deployed there to interfere with it.

#### What did you expect to happen?

Applying the resource should fail on empty values

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Service (notice the empty `baz` annotation)

```
apiVersion: v1
kind: Service
metadata:
  name: my-service
  annotations:
    foo: bar
    baz:
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

Apply it

```
kubectl apply -f my-service.yaml
```

Retrieve it
```
kubectl get service my-service -o yaml
```

The service also lacks the `foo` annotation
```
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"my-service","namespace":"default"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":9376}],"selector":{"app.kubernetes.io/name":"MyApp"}}}
  creationTimestamp: "2024-12-12T12:16:24Z"
  name: my-service
  namespace: default
  resourceVersion: "1776057"
  uid: a947a42c-a922-4f67-8103-c88dbb814004
spec:
  clusterIP: 169.254.131.120
  clusterIPs:
  - 169.254.131.120
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9376
  selector:
    app.kubernetes.io/name: MyApp
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```
  

#### Anything else we need to know?

This happens with `kubectl apply` but also with `helm install` 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.25.5
WARNING: version difference between client (1.28) and server (1.25) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

MacOs 15.1.1

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`kubectl apply`或`helm install`创建Kubernetes资源（例如Service）时，如果`annotations`中包含一个空值的键（如`baz`），则所有已有且有效的`annotations`都会被移除。这意味着，即使其他`annotations`具有有效的值，也会因为存在一个空的`annotation`而导致全部被删除。

从安全角度分析：

1. `annotations`通常用于存储元数据，并可能用于配置某些功能或策略。但在此案例中，问题的根源是用户提供了含有空值的`annotation`，导致所有`annotations`被移除。这更像是一个配置或使用上的错误，而非安全漏洞。

2. 该问题不会导致权限提升、命令执行、数据泄露等安全风险。

3. 根据风险判断标准第2条：“如果该issue描述的问题非安全问题，则风险评级判断为不涉及。”

---


## Issue #129169 Data race in cbor serializer

- Issue 链接：[#129169](https://github.com/kubernetes/kubernetes/issues/129169)

### Issue 内容

Seen on 2f9fb220ba69fe0d35fd151de05b769c5cb05e01 when running `TestFrontProxyConfig` with the race detector enabled and the following patch applied:

```diff
diff --git a/test/integration/examples/apiserver_test.go b/test/integration/examples/apiserver_test.go
index a50a8a846a5..837122a1847 100644
--- a/test/integration/examples/apiserver_test.go
+++ b/test/integration/examples/apiserver_test.go
@@ -45,7 +45,6 @@ import (
 	"k8s.io/apiserver/pkg/authentication/serviceaccount"
 	"k8s.io/apiserver/pkg/authentication/user"
 	genericapirequest "k8s.io/apiserver/pkg/endpoints/request"
-	"k8s.io/apiserver/pkg/features"
 	"k8s.io/apiserver/pkg/server/dynamiccertificates"
 	genericapiserveroptions "k8s.io/apiserver/pkg/server/options"
 	utilfeature "k8s.io/apiserver/pkg/util/feature"
@@ -263,7 +262,7 @@ func TestFrontProxyConfig(t *testing.T) {
 		testFrontProxyConfig(t, false)
 	})
 	t.Run("WithUID", func(t *testing.T) {
-		featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.RemoteRequestHeaderUID, true)
+		featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, "AllAlpha", true)
 		testFrontProxyConfig(t, true)
 	})
 }
```

```
WARNING: DATA RACE
Write at 0x00c005915e30 by goroutine 8814:
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getCheckerInternal()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:310 +0x1744
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getChecker()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:120 +0xc8
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.RejectCustomMarshalers()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:43 +0x90
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:155 +0xc4
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:142 +0x64
  k8s.io/apimachinery/pkg/runtime.WithVersionEncoder.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/helper.go:246 +0x2fc
  k8s.io/apimachinery/pkg/runtime.(*WithVersionEncoder).Encode()
      <autogenerated>:1 +0x94
  k8s.io/apimachinery/pkg/runtime.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/codec.go:49 +0x90
  k8s.io/client-go/rest.(*Request).Body()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/client-go/rest/request.go:503 +0x584
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*subjectAccessReviewV1Client).Create()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:490 +0x1b8
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:240 +0xe0
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:125 +0x40
  k8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtectionWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:154 +0x60
  k8s.io/apimachinery/pkg/util/wait.ExponentialBackoffWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:485 +0x7c
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:124 +0xd0
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:235 +0x504
  k8s.io/apiserver/pkg/authorization/union.unionAuthzHandler.Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/authorization/union/union.go:52 +0xf4
  k8s.io/apiserver/pkg/authorization/union.(*unionAuthzHandler).Authorize()
      <autogenerated>:1 +0x74
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:71 +0x100
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x108
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x2ac
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x88
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x44
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x9f8
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xe0
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0xecc
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x4c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x178
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x8dc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x3a8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x10c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xb8

Previous read at 0x00c005915e30 by goroutine 8816:
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getChecker()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:117 +0x88
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.RejectCustomMarshalers()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:43 +0x90
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:155 +0xc4
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:142 +0x64
  k8s.io/apimachinery/pkg/runtime.WithVersionEncoder.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/helper.go:246 +0x2fc
  k8s.io/apimachinery/pkg/runtime.(*WithVersionEncoder).Encode()
      <autogenerated>:1 +0x94
  k8s.io/apimachinery/pkg/runtime.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/codec.go:49 +0x90
  k8s.io/client-go/rest.(*Request).Body()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/client-go/rest/request.go:503 +0x584
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*subjectAccessReviewV1Client).Create()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:490 +0x1b8
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:240 +0xe0
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:125 +0x40
  k8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtectionWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:154 +0x60
  k8s.io/apimachinery/pkg/util/wait.ExponentialBackoffWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:485 +0x7c
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:124 +0xd0
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:235 +0x504
  k8s.io/apiserver/pkg/authorization/union.unionAuthzHandler.Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/authorization/union/union.go:52 +0xf4
  k8s.io/apiserver/pkg/authorization/union.(*unionAuthzHandler).Authorize()
      <autogenerated>:1 +0x74
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:71 +0x100
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x108
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x2ac
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x88
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x44
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x9f8
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xe0
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0xecc
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x4c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x178
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x8dc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x3a8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x10c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xb8

Goroutine 8814 (running) created at:
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x250
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x1b0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x170
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xbc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go:139 +0x1e8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:51 +0x1cc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:218 +0x13e0
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:74 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x194
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRoutine.func17()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/routine/routine.go:65 +0x94
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x138
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xb0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xb8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0x17c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x124
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:188 +0x5c
  net/http.serverHandler.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3210 +0x2a8
  net/http.initALPNRequest.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3819 +0x268
  net/http.(*initALPNRequest).ServeHTTP()
      <autogenerated>:1 +0x74
  net/http.Handler.ServeHTTP-fm()
      <autogenerated>:1 +0x60
  golang.org/x/net/http2.(*serverConn).runHandler()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2439 +0x194
  golang.org/x/net/http2.(*serverConn).scheduleHandler.gowrap1()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2373 +0x58

Goroutine 8816 (running) created at:
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x250
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x1b0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x170
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xbc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go:139 +0x1e8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:51 +0x1cc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:218 +0x13e0
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:74 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x194
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRoutine.func17()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/routine/routine.go:65 +0x94
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x138
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xb0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xb8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0x17c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x124
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:188 +0x5c
  net/http.serverHandler.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3210 +0x2a8
  net/http.initALPNRequest.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3819 +0x268
  net/http.(*initALPNRequest).ServeHTTP()
      <autogenerated>:1 +0x74
  net/http.Handler.ServeHTTP-fm()
      <autogenerated>:1 +0x60
  golang.org/x/net/http2.(*serverConn).runHandler()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2439 +0x194
  golang.org/x/net/http2.(*serverConn).scheduleHandler.gowrap1()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2373 +0x58
```

/kind bug
/sig api-machinery 

@kubernetes/sig-api-machinery-bugs @deads2k @jpbetz @benluddy 

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在运行`TestFrontProxyConfig`测试并启用竞态检测器（race detector）时，发现在CBOR序列化器中存在数据竞争（data race）问题。具体表现为在并发环境下，对`checkers`对象的访问没有进行适当的同步，导致读写冲突。

数据竞争可能导致程序的不确定行为，如崩溃、数据损坏等。在某些极端情况下，数据竞争可能被利用来进行拒绝服务（DoS）攻击。然而，从Issue提供的堆栈信息和问题描述来看，这个数据竞争主要影响的是序列化器的内部状态，没有直接的途径被外部攻击者利用来执行任意代码、提升权限或进行其他高危操作。

根据风险判断标准：

1. 数据竞争属于代码实现中的并发问题，可能导致程序异常或崩溃。
2. 该问题没有提到攻击者可以利用的数据输入或特定操作来触发，且需要具备一定的权限才能影响序列化过程。
3. 按照CVSS 3.1标准评估，该问题的影响主要在于可用性方面，且利用复杂度较高，攻击者需要一定权限，故得分不会达到高危等级。

---


## Issue #129157 `1.31` requires `privileged` to create a tun device, `1.30` only required `NET_ADMIN`

- Issue 链接：[#129157](https://github.com/kubernetes/kubernetes/issues/129157)

### Issue 内容

#### What happened?

Upgraded to `1.31.3` (k3s) and all of my pods failed to start with `permission denied` errors. On start up of the pod, it creates a tun device at `/dev/net/tun`. On version 1.30, this worked fine with the `NET_ADMIN` permissions. Now I have updated to `1.31` and the pods won't start unless I add `privileged` permission to the pods. 

#### What did you expect to happen?

v1.30 to be the same as v1.31 in terms of the permissions required to create tun devices.

#### How can we reproduce it (as minimally and precisely as possible)?

I am creating the tun device in go using the following:

```
err = unix.Mknod("/dev/net/tun", unix.S_IFCHR|0600, int(unix.Mkdev(10, 200)))
if err != nil {
	return nil, fmt.Errorf("failed to create /dev/net/tun: %w", err)
}

fd, err = unix.Open("/dev/net/tun", os.O_RDWR, 0)
if err != nil {
	return nil, fmt.Errorf("created /dev/net/tun, but still failed: %w", err) // <-- failing here
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.3+k3s1
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

OpenSUSE MicroOS

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)
containerd

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Flannel

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在将Kubernetes从`v1.30`升级到`v1.31`后，创建tun设备需要的权限从`NET_ADMIN`提升到了`privileged`，导致Pod无法启动。具体来说，应用在容器内创建`/dev/net/tun`设备时，在`v1.30`版本中仅需要授予`NET_ADMIN`权限即可，而在`v1.31`版本中则需要授予`privileged`权限。

根据Issue内容，这是由于Kubernetes在新版本中对权限管理进行了调整，导致了权限要求的变化。该问题主要涉及权限配置的变化，导致应用运行受到影响。

**解释说明：**

此Issue反映的是Kubernetes版本升级后权限要求的变化，导致需要为Pod授予更高的权限才能正常运行。这并不是一个安全漏洞或安全问题，而是软件升级带来的配置变更问题。

根据风险判断标准：

- **标准2**：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。

因此，风险评级为“不涉及”，不需要提供复现脚本。

---


## Issue #129152 [FG:InPlacePodVerticalScaling] Inconsistent handling of memory limit decrease

- Issue 链接：[#129152](https://github.com/kubernetes/kubernetes/issues/129152)

### Issue 内容

/kind bug

When shrinking the pod-level memory limits (sum of container limits iff all containers have limits), the Kubelet checks the current pod memory usage, and doesn't apply the new limits if the new limits < current usage. However, the Kubelet doesn't place the same restriction on containers, and we don't require container runtimes to make the same check.

In practice, this means that for a single container pod, decreasing the memory limit below usage will result in the resize being indefinitely in progress, but for a multiple container pod, if some containers have sufficient free memory, you can shrink the memory limits below usage for a container in the pod, resulting in that container being OOM-killed.

What is the desired behavior here?

https://docs.google.com/document/d/1cEFLXKwNOSNLAkzyhoJUgkBW0OiX-9bXB_aJV7OAypw/edit?tab=t.0 provides more background information and several options for how this should be handled.

/sig node
/priority important-soon

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue指出，在Kubernetes中，当缩减Pod级别的内存限制（即所有容器都有内存限制时，容器限制之和），`Kubelet`会检查当前Pod的内存使用情况，如果新的限制小于当前使用量，则不会应用新的限制。然而，`Kubelet`对容器级别没有进行同样的检查，且我们也不要求容器运行时执行此检查。

在实际操作中，这意味着：

- **单容器Pod**：如果将内存限制减少到低于当前使用量，Pod的调整将一直处于进行中状态，新限制不会生效。
- **多容器Pod**：如果某些容器有足够的空闲内存，可以将内存限制缩减到低于某个容器的当前使用量，导致该容器被OOM（Out Of Memory）杀死。

这种不一致的行为可能导致特定容器被意外地OOM杀死，造成服务中断。

根据风险判断标准：

1. 这是一个安全问题，涉及到容器可能被意外终止，属于可用性方面的安全风险。
2. 根据第五条，拒绝服务（DoS）攻击如果需要一定权限才能实施攻击，且需要创建、修改等非只读权限，则CVSS评级在High以下，不应判断为高风险。
3. 在此情况下，攻击者需要有权限修改Pod的规格（内存限制），这需要较高的权限，不是任意低权限用户都可以执行的操作。

因此，综上所述，该问题属于低风险安全问题。

---


## Issue #129146 kube-proxy: net.InterfaceAddrs may return error due to a race condition, causing the Nodeport Service to be inaccessible

- Issue 链接：[#129146](https://github.com/kubernetes/kubernetes/issues/129146)

### Issue 内容

#### What happened?

In our production environment, which verson is 1.24.4
1. kube-proxy's error log: 
2024-12-09T07:35:24.325300135+08:00 E1209 07:35:24.325193       1 proxier.go:1131] "Failed to get node IP address matching nodeport cidr" err="error listing all interfaceAddrs from host, error: route ip+net: no such network interface"
2. Then the nodeport service's backend pod on this node cannot be accessible by the Nodeport Service.
3. On the save time, in this node's /var/log/message, the log is:
Dec  9 07:35:24 VM-96-121-tencentos containerd: 2024-12-09T07:35:24+08:00 [info] cmdDel: {containerId c915c615cb43c0f7711d125e39e722581384bc45b3453e3c3c79ebe5ed994e71, netNs /var/run/netns/cni-278204fe-6400-6354-982f-ae2299bfae17, ifName eth0, args K8S_POD_INFRA_CONTAINER_ID=c915c615cb43c0f7711d125e39e722581384bc45b3453e3c3c79ebe5ed994e71;K8S_POD_UID=0a3c7874-54ae-4ec6-b4a5-b5c3b9d63a12;IgnoreUnknown=1;K8S_POD_NAMESPACE=bkmonitor-operator;K8S_POD_NAME=bcs-blackbox-job-073518-f8n4c, path /opt/cni/bin, stdinData {"capabilities":{"bandwidth":true,"portMappings":true},"cniVersion":"0.3.1","defaultDelegates":"tke-bridge","kubeconfig":"/etc/kubernetes/tke-cni-kubeconfig","logLevel":"info","name":"multus-cni","type":"multus"}}, <nil>, <nil>


#### What did you expect to happen?

The Nodeport Service should be accessible.

#### How can we reproduce it (as minimally and precisely as possible)?

In the latest version, it also have the same problem.

1, syncProxyRules->GetNodeIPs->net.InterfaceAddrs()->interfaceAddrTable(which is in interface_linux.go for Linux)->syscall.NetlinkRIB(syscall.RTM_GETADDR, syscall.AF_UNSPEC) is called.
2. Then an interface is delete by CNI.
3. Then interfaceAddrTable(which is in interface_linux.go) call interfaceTable(which is in interface_linux.go)->syscall.NetlinkRIB(syscall.RTM_GETLINK, syscall.AF_UNSPEC)
   The key point is that, RTM_GETADDR's return have the delete interface, but the RTM_GETLINK's return donot have the deleted interface.
4.  Then in interfaceAddrTable->addrTable will return err.
<img width="805" alt="Clipboard_Screenshot_1733841188" src="https://github.com/user-attachments/assets/d84160a2-2b10-4a0a-9078-faad3a010076">
5. Then the syncProxyRules will not write correct forward configuration because the GetNodeIPs's return donot have any ip.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
This production environment cluster version is v1.24.4

But any version have the same problem.

</details>


#### Cloud provider

<details>
TKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Centos8
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在kube-proxy中，由于net.InterfaceAddrs()函数可能会因为与CNI插件删除网络接口的竞争条件（race condition），导致获取节点IP地址失败，从而导致NodePort服务无法访问。这是一个因为竞争条件导致的系统可靠性问题，会影响服务的正常访问。

根据风险判断标准：

1. 该问题属于系统稳定性和可用性问题，并非安全问题。
2. 没有提及任何攻击者可以利用此漏洞进行恶意操作的情形。
3. 不涉及信息泄露、权限提升、命令执行等安全风险。

因此，该Issue描述的问题不属于安全问题。

---


## Issue #129143 Kube scheduler has a confusing error message when scheduling pods that use claims with `ReadWriteOncePod` access mode

- Issue 链接：[#129143](https://github.com/kubernetes/kubernetes/issues/129143)

### Issue 内容

#### What happened?

When we deploy a claim like:
```yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
  - ReadWriteOncePod
  resources:
    requests:
      storage: 4Gi
```

And then deploy two pods:
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
---
apiVersion: v1
kind: Pod
metadata:
  name: app2
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
```
Second pod is rightfully not scheduled.
It has the following error message:
```
 k describe po app2

Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/5 nodes are available: 5 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
  Warning  FailedScheduling  2s    default-scheduler  0/5 nodes are available: 5 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
```
This error message is misleading - there are no 5 nodes that have pods using this PersistentVolumeClaim.

Error message comes from https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L55.
It's set in the following method - https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L280
It's raised upon calling the `Filter` method - https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L314

#### What did you expect to happen?

I expect that we get an error message that reads something like:
```
  Warning  FailedScheduling  5s    default-scheduler  0/5 nodes are available: 5 nodes were not able to schedule a pod using PersistentVolumeClaim <PVC-Name> name, ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
```

or similar message, that points the operator to the actual reason for not scheduling the pod.


#### How can we reproduce it (as minimally and precisely as possible)?

Build a cluster with 3 or more nodes and apply the following manifests:
```ymal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
  - ReadWriteOncePod
  resources:
    requests:
      storage: 4Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
---
apiVersion: v1
kind: Pod
metadata:
  name: app2
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
k version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.5
```

</details>

but I believe it's applicable for other versions as well.

#### Cloud provider

Message is the same, regardless of the provider.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了在使用`ReadWriteOncePod`访问模式的PersistentVolumeClaim时，当尝试调度多个使用相同PVC的Pod时，第二个Pod无法被调度，这是预期的行为。然而，调度器返回的错误信息具有误导性，显示“5个节点上有Pod使用了相同名称和`ReadWriteOncePod`访问模式的PersistentVolumeClaim”，这与实际情况不符，可能会导致运维人员的困惑。

从安全角度来看，该问题属于错误信息描述不准确，属于用户体验和可用性的问题。它并未导致任何安全漏洞或风险，例如权限提升、敏感信息泄露、命令执行等。因此，这个Issue并不涉及安全风险。

---


## Issue #129138 NodeResourcesBalancedAllocation cause too many pods scheduled to the same node

- Issue 链接：[#129138](https://github.com/kubernetes/kubernetes/issues/129138)

### Issue 内容

#### What happened?

NodeResourcesBalancedAllocation will return different score if pod request is empty.

```
I1210 06:42:54.701779       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.50" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727588291] requestedResource=map[cpu:14580 memory:24593301504] resourceScore=96
I1210 06:42:54.701793       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.8" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727600579] requestedResource=map[cpu:15350 memory:24235737088] resourceScore=92
I1210 06:42:54.701807       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.9" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727600579] requestedResource=map[cpu:13480 memory:19474153472] resourceScore=90
```


#### What did you expect to happen?

NodeResourcesBalancedAllocation score should return zero if podRequest is zero. 

https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/scheduler/framework/plugins/noderesources/resource_allocation.go#L85-L114

The code below use `IsPrefixedNativeResource`, which ignore `cpu` and `memory`. 
https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/scheduler/util/utils.go#L158-L162

I think we should use below function.

https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/apis/core/v1/helper/helpers.go#L54-L60

#### How can we reproduce it (as minimally and precisely as possible)?

create pod whcih request is empty and enable NodeResourcesBalancedAllocation.

scheduler config:
```yaml
- name: NodeResourcesBalancedAllocation
  args:
    resources:
      - name: cpu
      - name: memory
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Server Version: v1.24.15
```

</details>


#### Cloud provider

<details>
vanilla
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用Kubernetes调度器插件NodeResourcesBalancedAllocation时，如果Pod的资源请求（resource request）为空，调度器会返回不同的评分，可能导致过多的Pod被调度到同一个节点上。这是由于在计算资源分配平衡时，对资源请求为空的Pod处理不当，导致资源使用不平衡。

从安全风险的角度来看，这个问题主要涉及到资源调度策略的优化和节点资源的有效利用，并不涉及攻击者利用漏洞进行攻击或导致系统安全性的下降。即使存在资源使用不均衡的情况，也需要具备创建Pod的权限才能提交资源请求为空的Pod。在Kubernetes中，创建Pod的操作通常受到RBAC权限控制，普通用户无法随意创建Pod。

根据风险判断标准：

- **标准2**：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。
- **标准5**：在风险类型为拒绝服务（DoS）攻击时，如果攻击者需要一定权限才能够实施该攻击，则视情况需要降级处理。当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险。

因此，该Issue不属于安全风险问题。

**解释说明：**

由于该问题不属于安全风险，因此无需提供复现脚本。

---


## Issue #129136 Preemption picks wrong victim node with higher priority pod on it after #128307

- Issue 链接：[#129136](https://github.com/kubernetes/kubernetes/issues/129136)

### Issue 内容

#### What happened?

related: #128307

After #128307 has been merged, preemption logic picks wrong victim node with higher priority pod on it.
In the following situation, `high` pod on `worker1` not `mid` on `worker2` is evicted when `very-high` pod(Priority=10000) attempts to schedule.

- `worker1`
  - `high` pod(Priority=1000)
  - `low` pod(Priority=0, preempting it will violate PDB)
- `worker2`
    - `mid` pod(Priority=100, preempting it will violate PDB)

#### What did you expect to happen?

According to here, `mid` pod on `worker2` seems to be picked as a victim.

https://github.com/kubernetes/kubernetes/blob/1148e5ee5fd95117db6c2fb92194272df574cc38/pkg/scheduler/framework/preemption/preemption.go#L411-L424

#### How can we reproduce it (as minimally and precisely as possible)?

We can reproduce it by the following steps using kind.

Preparation
<details>

Use kind.
```bash
$ kind version
kind v0.25.0 go1.22.9 linux/amd64
```

Build Node image of v1.31.3.
```bash
$ kind build node-image v1.31.3 --image ndest/node:1.31.3-build
```

Prepare kind cluster config and create cluster.

`kind-config-1.31.3.yaml`
```yaml
kind: Cluster
apiVersion: "kind.x-k8s.io/v1alpha4"
name: kind-v1.31.3
nodes:
- role: control-plane
  image: ndest/node:1.31.3-build
- role: worker
  image: ndest/node:1.31.3-build
- role: worker
  image: ndest/node:1.31.3-build
```

```bash
$ kind create cluster --config kind-config-1.31.3.yaml

$ kubectl get no
NAME                         STATUS   ROLES           AGE   VERSION
kind-v1.31.3-control-plane   Ready    control-plane   33s   v1.31.3
kind-v1.31.3-worker          Ready    <none>          18s   v1.31.3
kind-v1.31.3-worker2         Ready    <none>          18s   v1.31.3
```

Add PriorityClass to DaemonSet/kindnet to prevent it from being preempted.
```bash
$ kubectl -n kube-system patch ds kindnet --patch '{"spec": {"template": {"spec": {"priorityClassName": "system-node-critical"}}}}'
```

Modify `maxPods` of `kubelet` config on `worker` and `worker2` to trigger preemption.
```bash
# worker
$ docker exec -it kind-v1.31.3-worker /bin/bash
root@kind-v1:/# echo "maxPods: 4" >> /var/lib/kubelet/config.yaml
root@kind-v1:/# systemctl restart kubelet
root@kind-v1:/# exit

# worker2
$ docker exec -it kind-v1.31.3-worker2 /bin/bash
root@kind-v1:/# echo "maxPods: 3" >> /var/lib/kubelet/config.yaml
root@kind-v1:/# systemctl restart kubelet
root@kind-v1:/# exit
```

Now, we can schedule 2 pods on `worker` and 1 pod on `worker2`.
```bash
$ k get no -A -o='custom-columns=NAME:.metadata.name,MAXPOD:.status.capacity.pods,VERSION:.status.nodeInfo.kubeletVersion'
NAME                         MAXPOD   VERSION
kind-v1.31.3-control-plane   110      v1.31.3
kind-v1.31.3-worker          4        v1.31.3
kind-v1.31.3-worker2         3        v1.31.3

$ k get po -A -owide | grep -w kind-v1.31.3-worker | wc -l
2
$ k get po -A -owide | grep -w kind-v1.31.3-worker2 | wc -l
2
```

</details>

Create `PriorityClass`, `PodDisruptionBudget`, and `Pod`.
Applying these manifests, we can see the following situation.

<details>

`high-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "This priority class should be used for high priority pods."
```

`high.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high
  labels:
    app: high
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: high-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker
```

`mid-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: mid-priority
value: 100
globalDefault: false
description: "This priority class should be used for mid priority pods."
```

`mid.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mid
  labels:
    app: mid
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: mid-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker2
```

`low-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 0
globalDefault: false
description: "This priority class should be used for low priority pods."
```

`low.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: low
  labels:
    app: low
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: low-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker
```


`mid-pdb.yaml`
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mid-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: mid
```

`low-pdb.yaml`
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mid-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: mid
```

`very-high-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: very-high-priority
value: 10000
globalDefault: false
description: "This priority class should be used for very high priority pods."
```

</details>

`worker` and `worker2` habe reached their `maxPods` limit and evicting `low` or `mid` pod violates PDB.
```bash
$ k get po -o='custom-columns=NAME:.metadata.name,STATUS:.status.phase,PRIORITY:.spec.priority,NODE:.spec.nodeName'
NAME   STATUS    PRIORITY   NODE
high   Running   1000       kind-v1.31.3-worker
low    Running   0          kind-v1.31.3-worker
mid    Running   100        kind-v1.31.3-worker2

$ k get priorityclasses
NAME                      VALUE        GLOBAL-DEFAULT   AGE
high-priority             1000         false            26s
low-priority              0            false            26s
mid-priority              100          false            26s
system-cluster-critical   2000000000   false            17m
system-node-critical      2000001000   false            17m
very-high-priority        10000        false            26s

$ k get pdb
NAME      MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
low-pdb   1               N/A               0                     47s
mid-pdb   1               N/A               0                     47s

$ k get po -A -owide | grep -w kind-v1.31.3-worker | wc -l
4
ubuntu-user@ubuntu-server01 ~/work/kubernetes/preemption $ k get po -A -owide | grep -w kind-v1.31.3-worker2 | wc -l
3
```


Now, attempt to schedule `very-high` pod.

`very-high.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: very-high
  labels:
    app: very-high
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: very-high-priority
```

```bash
$ k apply -f very-high.yaml
```

We can see that `high` pod on `worker` is evicted instead of `mid` pod on `worker2`.
```bash
$ k get po -owide -w
NAME   READY   STATUS    RESTARTS   AGE   IP           NODE                   NOMINATED NODE   READINESS GATES
high   1/1     Running   0          6m    10.244.1.2   kind-v1.31.3-worker    <none>           <none>
low    1/1     Running   0          6m    10.244.1.3   kind-v1.31.3-worker    <none>           <none>
mid    1/1     Running   0          6m    10.244.2.2   kind-v1.31.3-worker2   <none>           <none>


very-high   0/1     Pending   0          0s    <none>       <none>                 <none>           <none>
high        1/1     Running   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>           <none>
high        1/1     Terminating   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>           <none>
very-high   0/1     Pending       0          0s     <none>       <none>                 kind-v1.31.3-worker   <none>
high        1/1     Terminating   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m10s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m10s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
very-high   0/1     Pending       0          2s      <none>       kind-v1.31.3-worker    kind-v1.31.3-worker   <none>
very-high   0/1     ContainerCreating   0          2s      <none>       kind-v1.31.3-worker    <none>                <none>
very-high   1/1     Running             0          3s      10.244.1.4   kind-v1.31.3-worker    <none>                <none>

$ k get po -owide
NAME        READY   STATUS    RESTARTS   AGE     IP           NODE                   NOMINATED NODE   READINESS GATES
low         1/1     Running   0          7m23s   10.244.1.3   kind-v1.31.3-worker    <none>           <none>
mid         1/1     Running   0          7m23s   10.244.2.2   kind-v1.31.3-worker2   <none>           <none>
very-high   1/1     Running   0          74s     10.244.1.4   kind-v1.31.3-worker    <none>           <none>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.3
```

</details>

#### Cloud provider

<details>
none
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$ kind version
kind v0.25.0 go1.22.9 linux/amd64

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes集群中，调度器的预占逻辑在特定情况下选择了错误的待驱逐Pod，导致具有更高优先级的Pod被驱逐，而不是预期的更低优先级Pod。具体而言，当一个非常高优先级（Priority=10000）的Pod尝试调度时，预占逻辑错误地选择了在`worker1`节点上的`high` Pod（Priority=1000）作为驱逐目标，而不是在`worker2`节点上的`mid` Pod（Priority=100）。

根据风险判断标准，我们需要评估该问题是否构成安全风险，以及其风险等级。

1. **权限要求**：为了触发该问题，攻击者需要能够创建具有高优先级的Pod（Priority=10000）。在Kubernetes中，设置Pod的优先级需要使用已存在的PriorityClass，或者创建新的PriorityClass。然而，创建或修改PriorityClass通常需要集群管理员权限，普通用户无法任意设置高优先级。因此，攻击者需要较高权限才能利用该问题。

2. **多租户影响**：如果攻击者能够创建高优先级Pod，并导致其他用户的高优先级Pod被驱逐，可能会对其他用户造成影响。然而，由于需要高权限才能创建高优先级的PriorityClass，且在安全配置的多租户环境中，用户权限通常受到严格控制，因此该风险可控。

3. **拒绝服务风险**：根据风险判断标准第（5）条，在拒绝服务（DoS）攻击的情况下，如果攻击者需要一定权限才能实施攻击，则应适当降低风险评级。当漏洞利用需要攻击者具备创建、修改等非只读权限时，不应判断为高风险。

综上所述，该Issue描述的问题是调度器逻辑上的错误，可能会导致资源调度的不合理，但并未涉及安全漏洞。攻击者需要高权限才能利用该问题，且其影响范围有限。

**解释说明：**

无

---


## Issue #129130 Unrealistic high cpu usage in metrics report after upgrading kubelet to 1.31 during pod restart

- Issue 链接：[#129130](https://github.com/kubernetes/kubernetes/issues/129130)

### Issue 内容

#### What happened?

We are performing the kubenetes cluster upgrade from 1.29 to 1.31

First we upgrade cluster api server to 1.31, then we upgrade all the nodes to kubelet 1.31
metrics-server version: 0.7.1
kube-state-metric version: 2.13.0
We found lots of our metric has a very high cpu usage and cpu/memory request during the pod restart phase

For example:
![image](https://github.com/user-attachments/assets/678f681b-b066-4adc-93d3-dc7e4b3f34a4)
Regularly, cpu usage should be under 100, but it's like 100M to 700M during the restart

Also, the allocatable resource:
![image](https://github.com/user-attachments/assets/84290587-350f-458c-8ced-78c8dbd709d8)




#### What did you expect to happen?

Correct metric

#### How can we reproduce it (as minimally and precisely as possible)?

upgrade kubelet from 1.29 to 1.31 not sure if 1.30 to 1.31 has this issue

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.2-eks-dfa7665
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该问题描述了在将 kubelet 从 1.29 升级到 1.31 后，pod 重启期间的指标报告中出现了不合理的高 CPU 使用率和 CPU/内存请求值。例如，通常 CPU 使用率应低于 100，但在重启期间却达到了 100M 到 700M。

从描述来看，这是由于升级 kubelet 后，导致指标采集出现异常，metrics-server 或 kube-state-metric 收到了错误的数据，导致指标显示异常高的资源使用情况。

这一问题主要是关于在升级过程中出现的性能指标不准确的问题，属于功能性缺陷或兼容性问题，并不涉及任何潜在的安全风险，也没有提到可能导致的安全后果。

---


## Issue #129122 draplugin.PublishResources() only supports enumerating resources in a single slice

- Issue 链接：[#129122](https://github.com/kubernetes/kubernetes/issues/129122)

### Issue 内容

#### What happened?

I wanted to publish multiple resource slices within the same pool, but there was no easy way to do this with the recommended helper library.

#### What did you expect to happen?

I expected the API to `draplugin.PublishResources()` to take a list of ResourceSlices, similar to the call to `resourceslice.Update()` that it wraps. 

#### How can we reproduce it (as minimally and precisely as possible)?

Examine the code here:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/dynamic-resource-allocation/kubeletplugin/draplugin.go#L427-L459
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/dynamic-resource-allocation/kubeletplugin/draplugin.go#L410

#### Anything else we need to know?

/wg device-management

#### Kubernetes version

`v1.32.0`

#### Cloud provider

None

#### OS version

Irrelevant

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用`draplugin.PublishResources()`方法时，只能发布单个资源片段（ResourceSlice），无法方便地在同一资源池中发布多个资源片段。提问者希望该API能够接收一个ResourceSlice列表，类似于它所封装的`resourceslice.Update()`调用。

这个问题属于功能改进或新特性请求，涉及API的设计和实现，并不涉及任何安全风险。因此，根据风险判断标准，该Issue非安全问题。

---


## Issue #129115 can't find way to check streams is closed 

- Issue 链接：[#129115](https://github.com/kubernetes/kubernetes/issues/129115)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blame/v1.31.3/staging/src/k8s.io/kubelet/pkg/cri/streaming/remotecommand/attach.go#L43

#### What did you expect to happen?

can find way to check streams is closed

#### How can we reproduce it (as minimally and precisely as possible)?

no method

#### Anything else we need to know?

no method

#### Kubernetes version

no method

#### Cloud provider

null

#### OS version

null

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue的标题为“can't find way to check streams is closed”，内容提到在Kubernetes代码库中 `staging/src/k8s.io/kubelet/pkg/cri/streaming/remotecommand/attach.go` 文件的第43行，无法找到检查流（streams）是否关闭的方法。提交者希望能够找到一种方法来检测流的关闭状态。

从问题描述来看，这属于功能上的改进需求或缺陷报告，涉及到代码的可用性和健壮性，但并不直接涉及安全问题。无法检测流是否关闭，可能会导致资源未及时释放，从而影响系统性能，但这属于可靠性问题，而非安全漏洞。

根据风险判断标准：

1. 该Issue描述的问题非安全问题，应当判断为不涉及。

---


## Issue #129104 CoreDNS service is running but IPs are not assigned

- Issue 链接：[#129104](https://github.com/kubernetes/kubernetes/issues/129104)

### Issue 内容

/sig Network
#### What happened?

I started a new cluster with 1 control plane node and 1 worker node. None of the default pods are getting IPs and it seems like there is no associated node with core dns pods. Some logs are as below:

$ kubectl get services -A
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  9m21s
kube-system   kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   8m48s

$ kubectl get pods -o wide -A
NAMESPACE     NAME                                                       READY   STATUS    RESTARTS   AGE     IP       NODE                               NOMINATED NODE   READINESS GATES
kube-system   coredns-55cb58b774-h4vbk                                   0/1     Pending   0          3m59s   <none>   <none>                             <none>           <none>
kube-system   coredns-55cb58b774-zfxmn                                   0/1     Pending   0          3m59s   <none>   <none>                             <none>           <none>
kube-system   etcd-bsocialtoday-control-plane-s7n99                      1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-apiserver-bsocialtoday-control-plane-s7n99            1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-controller-manager-bsocialtoday-control-plane-s7n99   1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-proxy-64tgt                                           1/1     Running   0          81s     <none>   bsocialtoday-md-0-dwwnv-8r6sj      <none>           <none>
kube-system   kube-proxy-q7l7s                                           1/1     Running   0          3m59s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-scheduler-bsocialtoday-control-plane-s7n99            1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>

#### What did you expect to happen?

I expected core dns to work fine with default instructions as followed in [here](https://syself.com/docs/caph/getting-started/quickstart/creating-a-workload-cluster).

#### How can we reproduce it (as minimally and precisely as possible)?

Follow instructions [here](https://syself.com/docs/caph/getting-started/quickstart/creating-a-workload-cluster).

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.30.7
```

</details>


#### Cloud provider

<details>
Hetzner
</details>


#### OS version

Ubuntu 22.04

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes集群中，CoreDNS服务正在运行，但Pods未被分配IP地址，且未关联到任何节点。这导致CoreDNS等Pods处于Pending状态。Issue中提供的日志显示，CoreDNS Pods的`NODE`字段为`<none>`，`IP`字段为`<none>`，这通常是由于网络插件（如CNI插件）未正确安装或配置所导致的。

根据风险判断标准：

1. **风险评级判断为不涉及**：该Issue反映的是Kubernetes集群的配置问题，尤其是网络配置方面的问题，如CNI插件未正确安装。这属于功能性错误或配置错误。

2. **Issue提交者的问题**：Issue中没有提及任何安全漏洞、敏感信息泄露或其他可能导致安全风险的因素。

3. **不涉及安全风险**：没有迹象表明该问题会导致权限提升、任意代码执行、拒绝服务攻击等安全风险。

因此，该Issue不涉及安全风险，风险评级判断为“不涉及”。

---


## Issue #129102 Loss of Kind and APIVersion in Errors from Request.Stream

- Issue 链接：[#129102](https://github.com/kubernetes/kubernetes/issues/129102)

### Issue 内容

#### What happened?

When invoking the func (r *Request) Stream(ctx context.Context) (io.ReadCloser, error) function in the Kubernetes client-go library, the error returned by the apiserver originally includes fields such as kind and apiVersion (e.g., "kind":"Status","apiVersion":"v1"). However, after passing through the rest-client’s processing logic, these fields (Kind and APIVersion) are stripped from the error object.

This leads to the following issues:

Users lose structured metadata (Kind, APIVersion) from the original server response, which is essential for identifying and categorizing the error type.
It becomes challenging to programmatically distinguish between different error types or versions without additional error handling logic.


#### What did you expect to happen?

same as apiserver

#### How can we reproduce it (as minimally and precisely as possible)?

Make a request using the Kubernetes client-go library that results in an error, e.g., an invalid resource request.
Observe the response from the apiserver (e.g., using a network debugging tool like kubectl or other HTTP debugging methods) to verify that the Kind and APIVersion fields are present in the error payload.
Check the error returned by the Request.Stream function to observe the absence of Kind and APIVersion.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用 Kubernetes client-go 库的 `Request.Stream` 函数时，来自 apiserver 的错误响应中原本包含的 `kind` 和 `apiVersion` 字段在经过 rest-client 的处理后被去除了。这样导致用户无法获取原始服务器响应中的结构化元数据，影响对错误类型的识别和分类，增加了处理错误的难度。

从安全角度分析：

- **信息完整性**：错误信息的完整性受到影响，但这主要是对开发和调试造成不便，不会直接导致安全风险。
- **攻击面**：攻击者无法通过该问题获取敏感信息、提升权限或执行未授权操作。
- **影响范围**：该问题影响的是客户端对错误的处理方式，对系统的安全性没有实质性影响。

根据以上分析，该问题不涉及安全风险。

---


## Issue #129097 Namespace deletion does not trigger container lifecycle hooks

- Issue 链接：[#129097](https://github.com/kubernetes/kubernetes/issues/129097)

### Issue 内容

#### What happened?

I want to ensure that certain commands are run on graceful deletion of a pod, so I have set up a container lifecycle prestop hook to do so. Deleting the pod does trigger the prestop hook as expected; however, when deleting the namespace that the pod is in, I would expect this to also be a graceful shutdown event, which should trigger the prestop hook, but this does not happen. In addition, helm uninstalling our helm chart with the relevant statefulset has the same issue. See the test example below where we have setup RBAC for kubectl access within the container:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: <>
  namespace: <>
  labels:
    component: <>
    app: <>
spec:
  ...
  template:
    metadata:
      labels:
    spec:
      containers:
        - name: testContainer
          image: <>
          lifecycle:
           preStop:
             exec:
               command:
                  - /bin/bash
                  - -c
                  - "kubectl label cm test-configmap -n test-prestop-hook testlabel=8 --overwrite"
             
```

#### What did you expect to happen?

Prestop hook should be triggered by namespace deletion. 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bring up a pod with a container that has a prestop hook configured
2. Exec into the pod and label the ConfigMap to see that it works (no RBAC or image issues)
3. Delete the namespace that the pod is in (expected to trigger the prestop hook)
4. Wait 30 seconds and see that the prestop hook did not run, configmap label did not change to `testlabel: "8"`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10-eks-7f9249a
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

NA

</details>


#### Install tools

<details>
Using helm to install the helm chart with statefulset:

$ helm version
version.BuildInfo{Version:"v3.12.3", GitCommit:"3a31588ad33fe3b89af5a2a54ee1d25bfe6eaa5e", GitTreeState:"clean", GoVersion:"go1.20.7"}

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes环境中，当删除Pod所在的命名空间时，容器的生命周期钩子（preStop）未被触发的问题。具体来说，单独删除Pod会如预期触发preStop钩子，但删除命名空间或通过Helm卸载StatefulSet时，preStop钩子未被执行。

从安全角度分析：

1. **行为描述**：容器的preStop钩子用于在容器终止前执行特定操作。未触发preStop钩子可能导致一些清理或状态更新操作未执行。

2. **潜在影响**：未执行preStop钩子可能会导致资源未及时释放、未完成的事务或日志未记录等问题，但这些都属于应用程序的生命周期管理问题。

3. **安全性考虑**：该问题不会导致未经授权的访问、权限提升、敏感信息泄露等安全风险。攻击者无法利用该行为进行恶意操作，因为该过程需要对命名空间或资源具有删除权限，属于高权限操作。

因此，该Issue描述的是Kubernetes在特定情况下的预期行为与实际行为不一致的问题，属于功能性缺陷或改进需求，而非安全漏洞。

---


## Issue #129093 Link for cluster bootstrap is broken

- Issue 链接：[#129093](https://github.com/kubernetes/kubernetes/issues/129093)

### Issue 内容

#### What happened?

Link for Cluster-bootstrap is broken in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/cluster-bootstrap/README.md

#### What did you expect to happen?

Link should be working

#### How can we reproduce it (as minimally and precisely as possible)?

clicking on link

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0



```
</details>


#### Cloud provider

<details>
baremetal
</details>


#### OS version

<details>

```console
# On Linux:
  $ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian

$ uname -a
# paste output here
Linux XXXXXXXXXXXXX 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，报告者在Kubernetes项目的README文档中发现了一个失效的链接（broken link）。具体地说，链接指向`cluster-bootstrap`，但无法正常访问。这是一个文档维护问题，与项目的源代码或部署无关。

根据风险判断标准：

1. **风险评级分为三类，不涉及，低风险和高风险**。
2. **如果该Issue描述的问题非安全问题，则风险评级判断为不涉及**。

因此，该Issue不涉及任何安全风险，只是一个文档链接错误的问题。

---


## Issue #129086 flowcontrol: maxCL is unreachable

- Issue 链接：[#129086](https://github.com/kubernetes/kubernetes/issues/129086)

### Issue 内容

#### What happened?

With the current implementation of `maxCL`, it is possible that the value reported by `apiserver_flowcontrol_upper_limit_seats` exceeds the total concurrency limit. Since the concurrency limit of a given priority level is bound by the total concurrency limit, this effectively means the value reported by the metric is too loose.

Moreover, in practice a priority level of type `Limited` cannot borrow more than the total number of seats that other priority levels are configured to lend. This means that the total concurrency limit is generally speaking a loose upper bound that can be restricted even further.

Here below are the values of `apiserver_flowcontrol_upper_limit_seats` for an apiserver running with the default total concurrency limit of 600.

```bash
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_upper_limit_seats
# HELP apiserver_flowcontrol_upper_limit_seats [ALPHA] Configured upper bound on number of execution seats available to each priority level
# TYPE apiserver_flowcontrol_upper_limit_seats gauge
apiserver_flowcontrol_upper_limit_seats{priority_level="catch-all"} 613
apiserver_flowcontrol_upper_limit_seats{priority_level="exempt"} 600
apiserver_flowcontrol_upper_limit_seats{priority_level="global-default"} 649
apiserver_flowcontrol_upper_limit_seats{priority_level="leader-election"} 625
apiserver_flowcontrol_upper_limit_seats{priority_level="node-high"} 698
apiserver_flowcontrol_upper_limit_seats{priority_level="system"} 674
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-high"} 698
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-low"} 845
```

#### What did you expect to happen?

With the default priority levels and default values, that is total concurrency limit of 600 and 343 borrowable seats in total, we expect the following upper limits

```console
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_upper_limit_seats
# HELP apiserver_flowcontrol_upper_limit_seats [ALPHA] Configured upper bound on number of execution seats available to each priority level
# TYPE apiserver_flowcontrol_upper_limit_seats gauge
apiserver_flowcontrol_upper_limit_seats{priority_level="catch-all"} 356        # = lower limit ( 13 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="exempt"} 600
apiserver_flowcontrol_upper_limit_seats{priority_level="global-default"} 367   # = lower limit ( 24 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="leader-election"} 368  # = lower limit ( 25 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="node-high"} 416        # = lower limit ( 73 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="system"} 393           # = lower limit ( 50 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-high"} 392    # = lower limit ( 49 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-low"} 367     # = lower limit ( 24 ) + borrowable ( 343 )
```

#### How can we reproduce it (as minimally and precisely as possible)?

This behavior can be reproduced with a kind cluster using the default configuration
The total number of lendable seats sums up to 343, this is the maximum number of seats any limited priority level can borrow

```console
$ kind create cluster
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_nominal_limit_seats
# HELP apiserver_flowcontrol_nominal_limit_seats [BETA] Nominal number of execution seats configured for each priority level
# TYPE apiserver_flowcontrol_nominal_limit_seats gauge
apiserver_flowcontrol_nominal_limit_seats{priority_level="catch-all"} 13        #  0% lendable =   0 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="exempt"} 0
apiserver_flowcontrol_nominal_limit_seats{priority_level="global-default"} 49   # 50% lendable =  25 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="leader-election"} 25  #  0% lendable =   0 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="node-high"} 98        # 25% lendable =  25 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="system"} 74           # 33% lendable =  24 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="workload-high"} 98    # 50% lendable =  49 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="workload-low"} 245    # 90% lendable = 220 lendable seats
```

#### Anything else we need to know?

An accurate `apiserver_flowcontrol_upper_limit_seats` is useful to compare against `apiserver_flowcontrol_current_limit_seats`. If they are equal, operators can infer that the priority level cannot borrow any more seats, helping debug faster throttling issues.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kind-control-plane 6.10.14-linuxkit #1 SMP Thu Oct 24 19:28:55 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
https://kind.sigs.k8s.io/#installation-and-usage
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

**风险定级：**  
**不涉及**

**判断依据：**  
该Issue报告了Kubernetes API服务器中的流控实现问题，具体来说，`apiserver_flowcontrol_upper_limit_seats`指标可能报告超过总并发限制的值。这意味着该指标可能过于松散，无法准确反映实际的并发限制。这可能会导致运维人员在监控和调试流控问题时产生误解。

从Issue的描述来看，该问题涉及以下方面：

- **指标不准确**：`apiserver_flowcontrol_upper_limit_seats`指标值超过了实际的总并发限制，无法准确反映每个优先级级别可用的最大执行席位数。
- **运维影响**：不准确的指标可能会导致运维人员在处理请求被限流等问题时无法快速定位和解决问题。

然而，该问题并未涉及任何安全方面的漏洞，例如权限提升、敏感信息泄露、拒绝服务攻击等。它主要是一个功能性和可观测性的问题。

**解释说明：**

根据风险判断标准第2条：“如果该Issue描述的问题非安全问题，则风险评级判断为不涉及。”该Issue讨论的是流控指标的准确性问题，对系统的安全性没有直接影响，因此风险评级为“不涉及”。无需提供复现脚本。

---


## Issue #129078 cpumanager:staticpolicy:smtalign: pod admission failed after kubelet restart

- Issue 链接：[#129078](https://github.com/kubernetes/kubernetes/issues/129078)

### Issue 内容

#### What happened?

When Kubelet is configured with `static` CPUManager policy + `full-pcpus-only` option, after kubelet restart, pod is not get admitted and  I'm getting the following error under kubelet logs:

```
Dec 04 12:42:25 kubenswrapper[2410667]: I1204 12:42:25.173086 2410667 kubelet.go:2320] "Pod admission denied" podUID="4355ee04-54b0-4755-b23e-d05ce12e54c1" pod="my-app-namespace/app-deployment" reason="SMTAlignmentError" message="SMT Alignment Error: not enough free physical CPUs: available physical CPUs = 1, requested CPUs = 2, CPUs per core = 1"
```

After manually deleting and recreating the pod, it gets admitted, so the claim that there are node enough free physical CPUs is wrong.


Pod spec:

```yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-12-04T12:50:20Z"
  name: app-deployment
  namespace: my-app-namespace
  resourceVersion: "11109192"
  uid: 16737fd6-45e1-4371-b10e-3eff1377c224
spec:
  containers:
  - args:
    - while true; do sleep 10000; done;
    command:
    - /bin/sh
    - -c
    image: quay.io/jitesoft/alpine
    imagePullPolicy: Always
    name: app-container2
    resources:
      limits:
        cpu: "2"
        memory: 100Mi
      requests:
        cpu: "2"
        memory: 100Mi
  qosClass: Guaranteed

```

#### What did you expect to happen?

After kubelet restart pod should readmitted 

#### How can we reproduce it (as minimally and precisely as possible)?
On a node with 4 CPUs:
 `1` reserved (`reserverSystemCPUs`)
 `3` allocatable

1. Configure CPUManager to `static` and `full-pcpus-only` option to `true`
2. Create a Guaranteed QoS class pod requesting 2  exclusive CPUs:
``` yaml
kind: Pod
...
spec:
  containers:
...
    resources:
      limits:
        cpu: "2"
        memory: 100Mi
      requests:
        cpu: "2"
        memory: 100Mi
```
3. wait for pod to start
4. restart kubelet
5. Pod will failed with SMTAlignment error

NOTE:
It's possible to reproduce the issue on a system with any size of CPUs.
A general formula would be:
On a node with `N` CPUs:
 `1` reserved 
 `N-1` allocatable
Create a Guaranteed QoS class pod requesting `N / 2`  exclusive CPUs:

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.2
WARNING: version difference between client (1.29) and server (1.31) exceeds the supported minor version skew of +/-1

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在设置Kubelet的CPUManager策略为`static`并启用`full-pcpus-only`选项的情况下，重启Kubelet后，Pod无法被重新接纳，出现了“SMT Alignment Error”的错误信息。手动删除并重新创建Pod后，Pod可以正常运行。这表明在Kubelet重启后，CPUManager对于CPU资源的分配出现了问题。

从问题描述来看，这是一个由于Kubelet在特定配置下重启导致的资源调度错误，属于功能性缺陷或Bug，并未涉及任何安全漏洞。没有提及攻击者可以利用此漏洞进行未授权的访问、提升权限、远程代码执行或其他安全风险。

根据风险判断标准：

- **第2条**：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。

因此，该Issue不涉及安全风险。

---


## Issue #129069 Is it necessary for the kube-apiserver to periodically list services?

- Issue 链接：[#129069](https://github.com/kubernetes/kubernetes/issues/129069)

### Issue 内容

#### What happened?

Is it necessary for the kube-apiserver to periodically list services?
https://github.com/kubernetes/kubernetes/blob/810e9e212ec5372d16b655f57b9231d8654a2179/pkg/registry/core/service/ipallocator/controller/repair.go#L125-L131
https://github.com/kubernetes/kubernetes/blob/810e9e212ec5372d16b655f57b9231d8654a2179/pkg/registry/core/service/ipallocator/controller/repair.go#L211

#### What did you expect to happen?

The kube-apiserver does not periodically list services.

#### How can we reproduce it (as minimally and precisely as possible)?

Currently, metrics statistics show that the kube-apiserver frequently performs list services operations.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.31.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，提问者关注的是kube-apiserver是否有必要定期列出服务（services）。他发现kube-apiserver频繁执行了List services的操作，并质疑这种行为是否必要。

从提供的链接可以看到，kube-apiserver在执行Service IP分配的修复操作时，会定期列出所有的服务。这可能会引起对性能的担忧，但从安全角度来看，没有直接的安全风险。

根据风险判断标准：

1. 如果该issue描述的问题非安全问题，则风险评级判断为不涉及。

该Issue并未提到任何与安全相关的问题，如权限提升、信息泄露、远程代码执行等。而是关注kube-apiserver的操作行为是否合理，属于性能优化或功能改进的范畴。

---


## Issue #129065 Fake client does not validate if the namespace exists when creating resources

- Issue 链接：[#129065](https://github.com/kubernetes/kubernetes/issues/129065)

### Issue 内容

#### What happened?

When using the fake client provided by the Kubernetes client-go library, I observed that it does not validate whether the namespace exists before creating resources.

For example, in the test case below, I attempted to create a Deployment in a non-existent namespace (test-namespace). However, no error was returned, and the operation succeeded without the namespace being created or verified:

```go
import (
	clientFake "k8s.io/client-go/kubernetes/fake"
	v1 "k8s.io/api/apps/v1"
        "testing"
)

func Test1(t *testing.T) {
    k8sClient := clientFake.NewSimpleClientset()
    _, err := k8sClient.AppsV1().Deployments("test-namespace").Create(context.Background(), &v1.Deployment{
        ObjectMeta: metav1.ObjectMeta{Name: "test-deployment"},
    }, metav1.CreateOptions{})
    fmt.Println(err)
}
```

Running this test produces the following output:
```bash
<nil>
PASS
ok      portrait/offline/pkg/prometheus 0.018s
```

I expected the fake client to return an error indicating that the namespace does not exist. However, it silently allowed the creation of the resource.


#### What did you expect to happen?

The fake client should validate whether the specified namespace exists before allowing the creation of a resource within it. If the namespace does not exist, it should return an error similar to the behavior of a real Kubernetes API server.

#### How can we reproduce it (as minimally and precisely as possible)?

* Create a fake client using clientFake.NewSimpleClientset().
* Attempt to create a resource (e.g., Deployment) in a non-existent namespace.
* Observe that no error is returned, and the resource is created without namespace validation.

#### Anything else we need to know?

* The fake client is commonly used for unit testing in Kubernetes-related projects. If it does not mimic the real Kubernetes API server's behavior accurately, it can lead to tests passing incorrectly and bugs being introduced into production code.
* I understand that the fake client is not a full Kubernetes API server implementation. However, basic validations like namespace existence checks are critical for accurate testing.
* I am happy to submit a PR to address this issue if it is agreed that this is a bug or missing feature. Please let me know if this behavior is intentional or if I should proceed with the fix.
* If the maintainers decide this is not a bug, I would appreciate clarification on whether this behavior is by design or if there are alternative approaches to handle namespace validation in unit tests.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.21.1
WARNING: version difference between client (1.31) and server (1.21) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux MI-20240719YHKN 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue指出，当使用Kubernetes client-go库提供的fake client时，在创建资源时不会验证命名空间是否存在。具体来说，即使尝试在一个不存在的命名空间中创建Deployment，也不会返回错误，操作会无错误地成功执行。这与真实的Kubernetes API服务器的行为不一致，后者会在命名空间不存在时返回错误。

这个问题的主要影响是可能导致单元测试通过，但在真实环境中会失败，从而引入潜在的bug。开发者可能误以为代码在处理不存在的命名空间时能够正常工作，但实际上在生产环境中会遇到错误。

然而，这一问题属于测试准确性和代码质量的问题，并不涉及直接的安全风险。它不会导致未经授权的访问、敏感信息泄露、权限提升或远程代码执行等安全问题。

根据风险判断标准：

- **标准2**：如果该issue描述的问题非安全问题，则风险评级判断为不涉及。
- **标准9**：如果issue中提供的内容不够充分，则根据issue可能导致的后果判断风险评级。

综合以上分析，认为该问题不属于安全风险，而是功能性问题。

---


## Issue #129060 Compatibility Versions E2E tests failing for v1.33 with "emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]"

- Issue 链接：[#129060](https://github.com/kubernetes/kubernetes/issues/129060)

### Issue 内容

#### What happened?

Currently the Compatibility Versions E2E tests are failing when run against v1.33 with the below error:
```
> 2024-11-27T02:30:24.881212795Z stderr F E1127 02:30:24.881143       1 run.go:72] "command failed" err="emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]"
```
This test is catching a bug where currently it seems that there is some logic in k8s with the validation for --emulated-version where it is not allowing n-2 versions even though Compatibility Versions supports up to n-3

Related kube-apiserver logs link:
https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-compatibility-versions/1861594576398061568/artifacts/kind-control-plane/containers/kube-apiserver-kind-control-plane_kube-system_kube-apiserver-842342e203722dea42f37c832c5b10193a08bca734ad290c3a1ee3c32921e560.log


Associated TestGrid Alert Information Below + Additional Links:
New TestGrid alerts at [sig-testing-kind#compatibility-version-test](https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test) since commit/0e1abc4d1.

Found 1 failing rows in compatibility-version-test: 1 new, 0 other 1 new Test Failures in compatibility-version-test.
Test [ci-kubernetes-e2e-kind-compatibility-versions.Overall](https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test) has failed 3 times since it last passed at commit/8770bd58d. It started failing between commit/8770bd58d (Tue Nov 26 12:16 PST) and commit/0e1abc4d1 (Tue Nov 26 18:16 PST): .

TestGrid Link: https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test

Prow CI Link For First Failing Test:
https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-compatibility-versions/1861594576398061568a

Commits that caused test breakage:
https://github.com/kubernetes/kubernetes/compare/8770bd58d...0e1abc4d1
^ this is associated with creating a new git tag for the v1.32 RC which modifies the k8s version output by `git describe`/`hack/print-workspace-status.sh`.  Eg: master is now v1.33.0-alpha.0.1+0e1abc4d18e353 with this commit

#### What did you expect to happen?

K8s v1.33 should work with --emulated-version=1.31

#### How can we reproduce it (as minimally and precisely as possible)?

See test-grid test above.  I am not sure why but this doesn't repro locally using hack/local-up-cluster.sh (perhaps the current version of k8s is calculated differently using this script?)

#### Anything else we need to know?

/assign
/sig api-machinery

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在Kubernetes v1.33版本中，Compatibility Versions E2E测试失败，错误信息为：
```
emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]
```
这表明在使用`--emulated-version`参数进行版本模拟时，版本验证逻辑存在问题，导致无法支持预期的n-2版本兼容性。Issue中提到Compatibility Versions应该支持到n-3版本，但当前的验证逻辑不允许。这是一个功能性错误，涉及版本兼容性和版本验证逻辑的问题。

从安全风险的角度来看，此问题并不涉及任何安全漏洞或安全风险。它不会导致权限提升、信息泄露、命令执行、拒绝服务（DoS）攻击等安全问题。该问题仅影响到测试流程，属于软件的功能性缺陷。

---


## Issue #129056 kubelet: PodRejectionStatus Kubelet should reject pod when the node didn't have enough resource test error

- Issue 链接：[#129056](https://github.com/kubernetes/kubernetes/issues/129056)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/pull/128403 merged to move PodRejectionStatus into e2e/node from e2e/common/node. In certain cases this test fails because the test is comparing the entire status object. The test needs to be changed to validate the fields we care about.

```
Expected
    <v1.PodStatus>: 
        conditions:
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "False"
          type: PodReadyToStartContainers
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "True"
          type: Initialized
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          reason: PodFailed
          status: "False"
          type: Ready
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          reason: PodFailed
          status: "False"
          type: ContainersReady
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "True"
          type: PodScheduled
        containerStatuses:
        - image: registry.k8s.io/pause:3.10
          imageID: ""
          lastState: {}
          name: pod-out-of-cpu
          ready: false
          restartCount: 0
          started: false
          state:
            waiting:
              reason: ContainerCreating
          volumeMounts:
          - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
            name: kube-api-access-4dd6q
            readOnly: true
            recursiveReadOnly: Disabled
        hostIP: 10.0.128.3
        hostIPs:
        - ip: 10.0.128.3
        message: 'Pod was rejected: Node didn''t have enough resource: cpu, requested: 1000000000000000,
          used: 491, capacity: 3500'
        phase: Failed
        qosClass: Burstable
        reason: OutOfcpu
        startTime: "2024-11-29T14:10:06Z"
to equal
    <v1.PodStatus>: 
        message: 'Pod was rejected: Node didn''t have enough resource: cpu, requested: 1000000000000000,
          used: 491, capacity: 3500'
        phase: Failed
        qosClass: Burstable
        reason: OutOfcpu
        startTime: "2024-11-29T14:10:06Z"
```



#### What did you expect to happen?

Expect the test case to run successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

See reproduced output.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31+
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
Linux
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述的是在合并了PR #128403后，`PodRejectionStatus`测试从`e2e/common/node`移动到了`e2e/node`中。在某些情况下，该测试失败了，原因是测试在比较整个`PodStatus`对象，而实际上应该只验证我们关心的特定字段。Issue中提供了预期的`PodStatus`和实际的`PodStatus`，可以看出实际的`PodStatus`包含了更多的字段信息，导致测试失败。

这个问题涉及到测试用例的编写和验证问题，需要修改测试代码，以确保只比较关心的字段，而不是整个对象。这是一个代码质量和测试可靠性的问题。

根据风险判断标准：

1. 该Issue描述的问题并非安全问题，如漏洞、敏感信息泄露等。
2. 没有涉及攻击者可以利用的安全漏洞或风险。

---


## Issue #129050 failed to create patch: unable to find api field in struct Probe for the json field "grpc"

- Issue 链接：[#129050](https://github.com/kubernetes/kubernetes/issues/129050)

### Issue 内容

#### What happened?

When running `helm upgrade ...` for a certain release, the following error has occurs:
```
$ helm upgrade --debug --install <REDACTED>
history.go:53: [debug] getting history for release <REDACTED>
upgrade.go:121: [debug] preparing upgrade for <REDACTED>
upgrade.go:428: [debug] reusing the old release's values
upgrade.go:129: [debug] performing update for <REDACTED>
upgrade.go:296: [debug] creating upgraded release for <REDACTED>
client.go:173: [debug] checking 6 resources for changes
client.go:436: [debug] Looks like there are no changes for Service "<REDACTED>"
client.go:205: [debug] error updating the resource "<REDACTED>":
 	 failed to create patch: unable to find api field in struct Probe for the json field "grpc"
 upgrade.go:355: [debug] warning: Upgrade "<REDACTED>" failed: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
 ##[error]history.go:53: [debug] getting history for release <REDACTED>
upgrade.go:121: [debug] preparing upgrade for <REDACTED>
upgrade.go:428: [debug] reusing the old release's values
upgrade.go:129: [debug] performing update for <REDACTED>
upgrade.go:296: [debug] creating upgraded release for <REDACTED>
client.go:173: [debug] checking 6 resources for changes
client.go:436: [debug] Looks like there are no changes for Service "<REDACTED>"
client.go:205: [debug] error updating the resource "<REDACTED>":
	 failed to create patch: unable to find api field in struct Probe for the json field "grpc"
upgrade.go:355: [debug] warning: Upgrade "<REDACTED>" failed: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
Error: UPGRADE FAILED: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
helm.go:94: [debug] failed to create patch: unable to find api field in struct Probe for the json field "grpc"
helm.sh/helm/v3/pkg/kube.(*Client).Update
	/home/circleci/helm.sh/helm/pkg/kube/client.go:218
helm.sh/helm/v3/pkg/action.(*Upgrade).performUpgrade
	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:310
helm.sh/helm/v3/pkg/action.(*Upgrade).Run
	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:130
main.newUpgradeCmd.func2
	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:154
github.com/spf13/cobra.(*Command).execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
github.com/spf13/cobra.(*Command).ExecuteC
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
main.main
	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
runtime.main
	/usr/local/go/src/runtime/proc.go:203
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1373
UPGRADE FAILED
main.newUpgradeCmd.func2
	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:156
github.com/spf13/cobra.(*Command).execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
github.com/spf13/cobra.(*Command).ExecuteC
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
main.main
	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
runtime.main
	/usr/local/go/src/runtime/proc.go:203
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1373

2024-12-01T16:36:53.8872455Z Error: UPGRADE FAILED: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
2024-12-01T16:36:53.8882936Z helm.go:94: [debug] failed to create patch: unable to find api field in struct Probe for the json field "grpc"
2024-12-01T16:36:53.8883982Z helm.sh/helm/v3/pkg/kube.(*Client).Update
2024-12-01T16:36:53.8884509Z 	/home/circleci/helm.sh/helm/pkg/kube/client.go:218
2024-12-01T16:36:53.8884968Z helm.sh/helm/v3/pkg/action.(*Upgrade).performUpgrade
2024-12-01T16:36:53.8885425Z 	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:310
2024-12-01T16:36:53.8885874Z helm.sh/helm/v3/pkg/action.(*Upgrade).Run
2024-12-01T16:36:53.8886315Z 	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:130
2024-12-01T16:36:53.8886733Z main.newUpgradeCmd.func2
2024-12-01T16:36:53.8887154Z 	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:154
2024-12-01T16:36:53.8887599Z github.com/spf13/cobra.(*Command).execute
2024-12-01T16:36:53.8888049Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
2024-12-01T16:36:53.8888497Z github.com/spf13/cobra.(*Command).ExecuteC
2024-12-01T16:36:53.8888948Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
2024-12-01T16:36:53.8889382Z github.com/spf13/cobra.(*Command).Execute
2024-12-01T16:36:53.8889825Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
2024-12-01T16:36:53.8890231Z main.main
2024-12-01T16:36:53.8890638Z 	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
2024-12-01T16:36:53.8891035Z runtime.main
2024-12-01T16:36:53.8891429Z 	/usr/local/go/src/runtime/proc.go:203
2024-12-01T16:36:53.8891825Z runtime.goexit
2024-12-01T16:36:53.8892223Z 	/usr/local/go/src/runtime/asm_amd64.s:1373
2024-12-01T16:36:53.8892633Z UPGRADE FAILED
2024-12-01T16:36:53.8893026Z main.newUpgradeCmd.func2
2024-12-01T16:36:53.8893464Z 	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:156
2024-12-01T16:36:53.8893917Z github.com/spf13/cobra.(*Command).execute
2024-12-01T16:36:53.8894386Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
2024-12-01T16:36:53.8894933Z github.com/spf13/cobra.(*Command).ExecuteC
2024-12-01T16:36:53.8895407Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
2024-12-01T16:36:53.8895846Z github.com/spf13/cobra.(*Command).Execute
2024-12-01T16:36:53.8896265Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
2024-12-01T16:36:53.8896669Z main.main
2024-12-01T16:36:53.8897065Z 	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
2024-12-01T16:36:53.8897469Z runtime.main
2024-12-01T16:36:53.8897823Z 	/usr/local/go/src/runtime/proc.go:203
2024-12-01T16:36:53.8898178Z runtime.goexit
2024-12-01T16:36:53.8898541Z 	/usr/local/go/src/runtime/asm_amd64.s:1373
2024-12-01T16:36:53.8976539Z ##[section]Finishing: helm upgrade
```

I suspect the issue is coming from the `kubectl patch` command which helm uses, as you can see in the error message:
`failed to create patch`

This only happens when there's a deployment's pod spec with a *Probe (startupProbe, readinessProbe, etc...), with the `grpc` attribute, but the `grpc.service` attribute is an empty string.
When I put something in the service, this works fine.
e.g:
This works:
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: "readiness"
```
These don't work:
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: ""
```
```yaml
readinessProbe:
  grpc:
    port: 8080
    service:
```

#### What did you expect to happen?

The *Probe struct should be able to receive an "empty" value for the `grpc.service` endpoint.

#### How can we reproduce it (as minimally and precisely as possible)?

 - Create a deployment with these values (both should behave the same imo):
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: ""
```
```yaml
readinessProbe:
  grpc:
    port: 8080
    service:
```
 - Apply the deployment to a cluster
 - Make a change to something unrelated to the `grpc` struct
 - Try to `kubectl patch` it

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9
```

</details>


#### Cloud provider

<details>
Azure AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                          OSArchitecture  Version
26100        Microsoft Windows 11 Enterprise  64-bit          10.0.26100
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了在使用Helm执行升级操作时遇到的错误：

```
failed to create patch: unable to find api field in struct Probe for the json field "grpc"
```

具体情况是，当Deployment的Pod规范中包含了*Probe（例如`readinessProbe`），并且使用了`grpc`属性，但`grpc.service`字段为空字符串或未设置时，执行`helm upgrade`会失败。而当`grpc.service`字段有有效值时，升级操作正常。

问题可能出在Helm内部调用的`kubectl patch`命令，因为错误信息中提到了`failed to create patch`。这表明在生成补丁时，无法找到对应的API字段。

从描述来看，这是由于Kubernetes的API在处理Probe结构时，对`grpc.service`字段为空的情况处理不当，导致无法生成正确的补丁文件。这是一个功能性问题，涉及到Kubernetes资源的配置和更新流程。

根据提供的信息，该问题并未涉及任何安全漏洞，例如未授权的访问、权限提升、敏感信息泄露等。

---


