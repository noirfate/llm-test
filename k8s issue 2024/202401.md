# Issue 安全分析报告

# 🚨 存在安全风险的 Issues (9 个)

## Issue #123029 messages with sensitive information should not be printed to the log file because of information security?

- Issue 链接：[#123029](https://github.com/kubernetes/kubernetes/issues/123029)

### Issue 内容

#### What happened?

we found some sensitive information in log file:

1. in Audit.log
```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"RequestResponse"，"auditID":"","stage":"ResponseComplete","requestURI":"/apis/certificates.k8s.io/v1/certificatesigningrequests","verb":"create","user":{"username":"kubelet-bootstrap","uid":"10001","groups":
["system:bootstrappers","system:authenticated"J}，"sourceiPs":["192.108.18.135 ⌋，"usergent :Kubelet/（Linux/amd64)kubernetes/77d727f"，"objectRef":{"resource":"certificatesigningrequests","apiGroup":"certificates.k8s.io","apiVersion":"v1"},"responseStatus":{"metadata":{}，"code":201},"requestObject": {"kind":"CertificateSigningRequest","apiVersion":"certificates.k8s.io/v1","metadata":
{"generateName":"csr-","creationTimestamp":null},"spec":{"request":"LSetLS1CRUd38880 XXXXXXXXX U2NxaGtqT1B.  

```
spec.request  `"LSetLS1CRUd38880 XXXXXXXXX U2NxaGtqT1B.  ` looks like a private key

2. in kubelet.log
```
E0116 19:35:32.748517 4920 kuberuntime_manager.go:1007] container&Container{Name:installer,Image:engine-hub-x86 64:23.99.1,Comand:[],Args:[],WorkingDir:,Ports[]ContainerPort{ContainerPort{Name:daemon,HostPort:0,ContainerPort:9902,Protocol:TCP,HostIP:,},},Env: []EnvVar{EnVar{Name:INSTALL_APP,Value:engine-hub,ValueFrom:nil,},EnvVar{Name:INSTALL_SET_image_repository,Value:engine-hub-B6,ValueFrom:nil,},EnvVar{Name:INSTALL_SET_Cluster_replicas,Value:3,ValueFrom:nil,},EnvVar{Name:INSTALL_SET_cluster_secret,Value:00000xxxxxxxxxx, ValueFrom:nil}

```
kubelet will print pod information including container environment variable when startContainer failed.
if pod uses some secrets as environment variable, kubelet will print this secret content just like above-mentioned
`EnvVar{Name:INSTALL_SET_cluster_secret,Value:00000xxxxxxxxxx, ValueFrom:nil}` 

so will this content in log file result in information leakage?  and do we have a better log print policy?



#### What did you expect to happen?

no sensitive information in log file

#### How can we reproduce it (as minimally and precisely as possible)?

all k8s version

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因：**

在日志文件中打印了敏感信息，例如私钥和敏感的环境变量（如Secrets）。这些信息不应出现在日志文件中，因为一旦攻击者获得对日志文件的访问权限，就可能获取这些敏感数据，导致信息泄漏。

**可能的影响：**

- **私钥泄漏：** 攻击者获得私钥后，可以冒充服务器或用户，进行中间人攻击，解密通信内容，甚至获取系统的完全控制权。
- **Secrets泄漏：** 攻击者获取Secrets后，可以访问受保护的资源、数据库、API或其他服务，进一步扩大攻击范围。

根据 **CVSS 3.1** 评分标准，此类信息泄漏的漏洞可能评分为 **High** 甚至 **Critical**，因为它对机密性、完整性和可用性都有高影响，且容易被攻击者利用。

**Proof of Concept：**

1. **Audit.log** 中包含了编码后的证书签名请求（CSR）：

   ```
   spec.request: "LSetLS1CRUd38880 XXXXXXXXX U2NxaGtqT1B..."
   ```

   这里的 `spec.request` 字段包含了 CSR 的 Base64 编码内容，解码后可能包含明文的公钥信息，甚至可能包含私钥信息。

2. **kubelet.log** 中打印了包含敏感环境变量的容器信息：

   ```
   EnvVar{Name: INSTALL_SET_cluster_secret, Value: 00000xxxxxxxxxx, ValueFrom: nil}
   ```

   这里的 `INSTALL_SET_cluster_secret` 环境变量的值直接在日志中明文显示，暴露了敏感的集群密钥。

**建议：**

- **日志过滤：** 修改日志记录机制，避免在日志中输出敏感信息。对于可能包含敏感数据的字段，进行过滤或掩码处理。
- **日志级别控制：** 调整日志级别，在非调试模式下，不记录详细的请求和环境变量信息。
- **访问控制：** 确保日志文件的访问权限最小化，仅授权的管理员可以访问日志文件。
- **安全审计：** 定期审查日志文件和代码，确保未意外记录或暴露敏感信息。

通过以上措施，可以降低敏感信息在日志中泄漏的风险，提升系统的整体安全性。

---

## Issue #122940 kube-apiserver v1.28.3 panics with fatal error: concurrent map iteration and map write

- Issue 链接：[#122940](https://github.com/kubernetes/kubernetes/issues/122940)

### Issue 内容

#### What happened?

I am doing v1.28.3 kube-apiserver testing in the test environment, and I often encounter kube-apiserver panic, especially when kube-apiserver call webhook fails.

The following are the captured panic logs:
```
Jan 18 08:31:00 VMS71884 kube-apiserver[913778]: E0118 08:31:00.777233  913778 dispatcher.go:213] failed calling webhook "validation.istio.io": failed to call webhook: Post "https://istiod.istio-system.svc:443/validate?timeout=30s": service "istiod" not found
Jan 18 08:31:01 VMS71884 kube-apiserver[913778]: E0118 08:31:01.703058  913778 dispatcher.go:213] failed calling webhook "validation.istio.io": failed to call webhook: Post "https://istiod.istio-system.svc:443/validate?timeout=30s": service "istiod" not found
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: fatal error: concurrent map iteration and map write
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: goroutine 177317973 [running]:
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: golang.org/x/net/http2.cloneHeader(0xc0aa793b00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/golang.org/x/net/http2/server.go:2944 +0xb0
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: golang.org/x/net/http2.(*responseWriterState).writeHeader(0xc1257f9200, 0x1f8)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/golang.org/x/net/http2/server.go:2938 +0x3df
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: golang.org/x/net/http2.(*responseWriter).WriteHeader(0xc140fdbf08, 0x1f8)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/golang.org/x/net/http2/server.go:2900 +0x39
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader(0xc12261ff20, 0x1f8)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/wrap.go:94 +0x63
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: github.com/felixge/httpsnoop.(*rw).WriteHeader(0xc0e082c640, 0x1f8)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/github.com/felixge/httpsnoop/wrap_generated_gteq_1.8.go:372 +0xe3
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/server/httplog.(*respLogger).WriteHeader(0xc0e19e8370, 0x1f8)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/k8s.io/apiserver/pkg/server/httplog/httplog.go:301 +0x47
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/server/filters.(*baseTimeoutWriter).timeout(0xc0aa793f50, 0xc0e13f3cc0)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/k8s.io/apiserver/pkg/server/filters/timeout.go:258 +0x11d
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP(0xc00964eb58, {0x6adf0f0, 0xc1095cc860}, 0xc0e0831b00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/k8s.io/apiserver/pkg/server/filters/timeout.go:149 +0x555
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/endpoints/filters.withRequestDeadline.func1({0x6adf0f0, 0xc1095cc820}, 0xc0e0831b00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x73e
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: net/http.HandlerFunc.ServeHTTP(0xc0165d3a40, {0x6adf0f0, 0xc1095cc820}, 0xc0e0831a00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: /usr/local/go/src/net/http/server.go:2122 +0x43
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/server/filters.withWaitGroup.func1({0x6adf0f0, 0xc1095cc820}, 0xc0e0831a00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: vendor/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x2fe
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: net/http.HandlerFunc.ServeHTTP(0xc0165e02c0, {0x6adf0f0, 0xc1095cc820}, 0xc0e0831a00)
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: /usr/local/go/src/net/http/server.go:2122 +0x43
Jan 18 08:31:06 VMS71884 kube-apiserver[913778]: k8s.io/apiserver/pkg/server/filters.(*goaway).ServeHTTP(0xc0165b15a0, {0x6adf0f0, 0xc1095cc820}, 0xc0e0831a00)





Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: E0107 13:58:37.697995   78343 dispatcher.go:214] failed calling webhook "chaosblade-operator.chaosblade.svc": failed to call webhook: Post "https://chaosblade-webhook-server.chaosblade.svc:443/mutating-pods?timeout=10s": dial tcp 10.7.153.222:443: connect: connection refused
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: fatal error: concurrent map iteration and map write
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: goroutine 589955230 [running]:
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*responseWriterState).promoteUndeclaredTrailers(0xc0bb23a780)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2750 +0x1ca
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*responseWriterState).writeChunk(0xc0bb23a780, {0xc0197e1000, 0x98, 0x1000})
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2611 +0xa6
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.chunkWriter.Write({0xc0bb23a780}, {0xc0197e1000, 0x98, 0x1000})
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2564 +0x6c
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: bufio.(*Writer).Flush(0xc1669b2f00)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: /usr/local/go/src/bufio/bufio.go:628 +0x11f
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*responseWriter).FlushError(0xc0e2a32038)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2829 +0x8c
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*responseWriter).Flush(0xc0e2a32038)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2819 +0x1e
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*responseWriter).handlerDone(0xc0e2a32038)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2997 +0x46
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*serverConn).runHandler.func1()
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2366 +0x445
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: golang.org/x/net/http2.(*serverConn).runHandler(0xc103848680, 0xc0e2a32038, 0xc09a809b00, 0xc1b2789f68)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2370 +0x1ec
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: created by golang.org/x/net/http2.(*serverConn).scheduleHandler
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/golang.org/x/net/http2/server.go:2303 +0x1c9
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: goroutine 1 [chan receive, 7572 minutes]:
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: k8s.io/apiserver/pkg/server.preparedGenericAPIServer.Run({0xc00551a300}, 0xc0004601e0)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/k8s.io/apiserver/pkg/server/genericapiserver.go:674 +0xdba
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: k8s.io/kube-aggregator/pkg/apiserver.preparedAPIAggregator.Run({0xc0049d54a0, {0x6aba220, 0xc00551a300}}, 0xc0004601e0)
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: vendor/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go:467 +0x45
Jan 07 13:58:37 VMS71884 kube-apiserver[78343]: k8s.io/kubernetes/cmd/kube-apiserver/app.Run({0xc000dda000}, 0xc0004601e0)
Jan 07 13:58:38 VMS71884 kube-apiserver[78343]: cmd/kube-apiserver/app/server.go:188 +0x711
Jan 07 13:58:38 VMS71884 kube-apiserver[78343]: k8s.io/kubernetes/cmd/kube-apiserver/app.NewAPIServerCommand.func2(0xc000221800, {0xc000004900, 0x0, 0x2e})
Jan 07 13:58:38 VMS71884 kube-apiserver[78343]: cmd/kube-apiserver/app/server.go:136 +0x34e

```

and:

```
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: E0124 06:10:56.887453   86280 watch.go:287] unable to encode watch object *v1.WatchEvent: client disconnected (&streaming.encoderWithAllocator{writer:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*metrics.ResponseWriterDelegator)(0xc0e77aa900), InnerCloseNotifierFlusher:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*filters.auditResponseWriter)(0xc0ef2d8600), InnerCloseNotifierFlusher:responsewriter.outerWithCloseNotifyAndFlush{UserProvidedDecorator:(*httplog.respLogger)(0xc0d0bba2c0), InnerCloseNotifierFlusher:struct { httpsnoop.Unwrapper; http.ResponseWriter; http.Flusher; http.CloseNotifier; http.Pusher }{Unwrapper:(*httpsnoop.rw)(0xc148e971d0), ResponseWriter:(*httpsnoop.rw)(0xc148e971d0), Flusher:(*httpsnoop.rw)(0xc148e971d0), CloseNotifier:(*httpsnoop.rw)(0xc148e971d0), Pusher:(*httpsnoop.rw)(0xc148e971d0)}}}}, encoder:(*versioning.codec)(0xc11b9448c0), memAllocator:(*runtime.Allocator)(0xc0cec163d8)})
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: E0124 06:10:56.892439   86280 timeout.go:142] post-timeout activity - time-elapsed: 10.377525ms, GET "/api/v1/namespaces/istio-system/configmaps/istio-leader" result: Header called after Handler finished
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: goroutine 691243071 [running]:
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1.1()
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/server/filters/timeout.go:110 +0x11b
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: panic({0x59b79e0, 0x6ab0310})
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: /usr/local/go/src/runtime/panic.go:890 +0x262
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: golang.org/x/net/http2.(*responseWriter).Header(0xc12abe8148)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/golang.org/x/net/http2/server.go:2870 +0xa8
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).Header(0xc1350eb980)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/wrap.go:73 +0x38
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: github.com/felixge/httpsnoop.(*rw).Header(0xc1349a3450)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/github.com/felixge/httpsnoop/wrap_generated_gteq_1.8.go:364 +0xee
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/server/httplog.(*respLogger).Header(0xc1343cf8c0)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/server/httplog/httplog.go:284 +0x3f
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/endpoints/filters.(*recorder).AddWarning(0xc1349a34a0, {0x0, 0x0}, {0x676ffc6, 0x7b})
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/endpoints/filters/warning.go:110 +0x5a6
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/warning.AddWarning({0x6ae4e78, 0xc13f02a390}, {0x0, 0x0}, {0x676ffc6, 0x7b})
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/warning/context.go:59 +0xd8
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/token/cache.(*cachedTokenAuthenticator).AuthenticateToken(0xc0007ba370, {0x6ae4e78, 0xc13f02a390}, {0xc03a9c3007, 0x3b5})
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/token/cache/cached_token_authenticator.go:139 +0x2a5
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/request/bearertoken.(*Authenticator).AuthenticateRequest(0xc0001f6420, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/request/bearertoken/bearertoken.go:63 +0x339
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/request/union.(*unionAuthRequestHandler).AuthenticateRequest(0xc0004f01e0, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/request/union/union.go:56 +0x118
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/group.(*AuthenticatedGroupAdder).AuthenticateRequest(0xc0001f6440, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/group/authenticated_group_adder.go:40 +0x86
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/request/union.(*unionAuthRequestHandler).AuthenticateRequest(0xc0004f0220, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/request/union/union.go:56 +0x118
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/authentication/request/union.(*unionAuthRequestHandler).AuthenticateRequest(0xc000658740, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/authentication/request/union/union.go:56 +0x118
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1({0x6ae3d50, 0xc1659ea040}, 0xc13d18fd00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:66 +0x1b2
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: net/http.HandlerFunc.ServeHTTP(0xc00d7b75e0, {0x6ae3d50, 0xc1659ea040}, 0xc13d18fc00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: /usr/local/go/src/net/http/server.go:2122 +0x43
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x6ae3d50, 0xc1659ea040}, 0xc13d18fc00)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x36f
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: net/http.HandlerFunc.ServeHTTP(0xc0187ba870, {0x6ae3d50, 0xc1659ea040}, 0xc13f02d600)
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: /usr/local/go/src/net/http/server.go:2122 +0x43
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd6
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: created by k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: vendor/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x3b9
Jan 24 06:10:56 VMS71884 kube-apiserver[86280]: fatal error: concurrent map iteration and map write

```

#### What did you expect to happen?

kube-apiserver does not panic even if calling webhook times out or fails

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a webhook, remove the service corresponding to the webhook, and then create pods in batches to trigger webhook calls

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647", GitTreeState:"clean", BuildDate:"2023-05-17T14:20:07Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.3-4-rc1-apiserver-mr", GitCommit:"f8ac9363684f399a102efd1027a7db7a6ffa9f3c", GitTreeState:"clean", BuildDate:"2024-01-18T06:21:10Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
private cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

在上述Issue中，**存在潜在的安全风险**。

**原因及可能的影响：**

- **攻击者利用方式：** 攻击者可以通过触发kube-apiserver调用失败的Webhook，导致kube-apiserver发生panic崩溃。具体而言，如果攻击者能够使Webhook服务不可用（例如，删除Webhook对应的Service），然后诱使kube-apiserver调用该Webhook，就会触发并发map读写异常，导致kube-apiserver崩溃。
  
- **可能的影响：** kube-apiserver是Kubernetes集群的核心组件，负责处理所有的API请求。一旦kube-apiserver崩溃，整个集群的API服务将不可用，影响集群的正常运行。这种情况下，攻击者可以通过重复触发该漏洞，造成持续的拒绝服务（DoS）攻击。

**符合风险判断标准：**

1. **该风险能被攻击者利用：** 是的，攻击者可以通过构造特定的请求或操作，诱使kube-apiserver调用失败的Webhook，导致panic。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在high以上：**

   - **攻击向量（AV）：网络（N）**——攻击可以通过网络远程发起。
   - **攻击复杂度（AC）：低（L）**——不需要复杂的攻击条件。
   - **所需权限（PR）：低（L）**——需要能够创建资源触发Webhook调用的权限（普通用户通常具有）。
   - **用户交互（UI）：无（N）**——不需要额外的用户交互。
   - **作用范围（S）：未改变（U）**——攻击仅影响kube-apiserver所在的系统。
   - **机密性影响（C）：无（N）**——不影响数据的保密性。
   - **完整性影响（I）：无（N）**——不影响数据的完整性。
   - **可用性影响（A）：高（H）**——导致kube-apiserver崩溃，影响服务可用性。

   **CVSS 3.1 基本评分：** 7.5（高）

**Proof of Concept（概念验证）：**

1. **部署一个Webhook：** 创建一个MutatingAdmissionWebhook或ValidatingAdmissionWebhook，使其指向一个特定的服务。

2. **使Webhook服务不可用：** 删除或停止Webhook对应的Service，确保kube-apiserver无法成功调用Webhook。

3. **触发Webhook调用：** 普通用户在集群中批量创建Pod等资源，触发Admission Webhook的调用。

   ```bash
   for i in {1..100}; do
       kubectl run test-pod-$i --image=busybox -- sleep 3600 &
   done
   ```

4. **观察kube-apiserver崩溃：** kube-apiserver在处理失败的Webhook调用时，由于并发map读写，触发panic并崩溃。

5. **重复攻击：** 攻击者可以反复执行上述步骤，导致kube-apiserver频繁崩溃，造成拒绝服务。

**总结：**

该问题可能导致攻击者通过构造特定的条件，诱发kube-apiserver崩溃，从而对集群实施拒绝服务攻击。鉴于kube-apiserver的重要性，这一漏洞的影响较大，需尽快修复。

---

## Issue #122828 e2e tests slated for removal when we drop cloud providers 

- Issue 链接：[#122828](https://github.com/kubernetes/kubernetes/issues/122828)

### Issue 内容

When we compile e2e.test with `providerless` tag, there are a bunch of tests that will get dropped. Here's how to generate the list of tests.

```
KUBE_PROVIDERLESS=y make WHAT=test/e2e/e2e.test
_output/local/go/bin/e2e.test --list-tests > list-tests-providerless.txt
rm -rf _output/
KUBE_PROVIDERLESS=n make WHAT=test/e2e/e2e.test
_output/local/go/bin/e2e.test --list-tests > list-tests-regular.txt
diff -Bbwu list-tests-regular.txt list-tests-providerless.txt
```

here's the list of tests on the chopping block:

```
$ diff -Bbwu list-tests-regular.txt list-tests-providerless.txt | grep -E "^-" | sort
-    k8s.io/kubernetes/test/e2e/cloud/gcp/node/gpu.go:46: [sig-node] gpu Upgrade [Feature:GPUUpgrade] master upgrade should NOT disrupt gpu pod [Feature:GPUMasterUpgrade]
-    k8s.io/kubernetes/test/e2e/cloud/gcp/node/gpu.go:59: [sig-node] gpu Upgrade [Feature:GPUUpgrade] cluster upgrade should be able to run gpu pod after upgrade [Feature:GPUClusterUpgrade]
-    k8s.io/kubernetes/test/e2e/cloud/gcp/node/gpu.go:72: [sig-node] gpu Upgrade [Feature:GPUUpgrade] cluster downgrade should be able to run gpu pod after downgrade [Feature:GPUClusterDowngrade]
-    k8s.io/kubernetes/test/e2e/cloud/gcp/recreate_node.go:98: [sig-cloud-provider-gcp] Recreate [Feature:Recreate] recreate nodes and ensure they function upon restart
-    k8s.io/kubernetes/test/e2e/instrumentation/monitoring/accelerator.go:63: [sig-instrumentation] Stackdriver Monitoring should have accelerator metrics [Feature:StackdriverAcceleratorMonitoring]
-    k8s.io/kubernetes/test/e2e/network/firewall.go:212: [sig-network] Firewall rule control plane should not expose well-known ports
-    k8s.io/kubernetes/test/e2e/network/firewall.go:77: [sig-network] Firewall rule [Slow] [Serial] should create valid firewall rules for LoadBalancer type service
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:119: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should conform to Ingress spec
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:164: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should conform to Ingress spec
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:179: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to switch between IG and NEG modes
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:225: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to create a ClusterIP service
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:239: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints to NEG
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:283: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] rolling update backend pods should not cause service disruption
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:342: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints for both Ingress-referenced NEG and standalone NEG
-    k8s.io/kubernetes/test/e2e/network/ingress_gce.go:426: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should create NEGs for all ports with the Ingress annotation, and NEGs for the standalone annotation otherwise
-    k8s.io/kubernetes/test/e2e/network/ingress_scale.go:67: [sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1077: [sig-network] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on the same nodes
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1209: [sig-network] LoadBalancers should not have connectivity disruption during rolling update with externalTrafficPolicy=Cluster [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1218: [sig-network] LoadBalancers should not have connectivity disruption during rolling update with externalTrafficPolicy=Local [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1253: [sig-network] LoadBalancers ESIPP [Slow] should work for type=LoadBalancer
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1310: [sig-network] LoadBalancers ESIPP [Slow] should work for type=NodePort
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1342: [sig-network] LoadBalancers ESIPP [Slow] should only target nodes with endpoints
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1418: [sig-network] LoadBalancers ESIPP [Slow] should work from pods
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:143: [sig-network] LoadBalancers should be able to change the type and ports of a TCP service [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:1476: [sig-network] LoadBalancers ESIPP [Slow] should handle updates to ExternalTrafficPolicy field
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:336: [sig-network] LoadBalancers should be able to change the type and ports of a UDP service [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:530: [sig-network] LoadBalancers should only allow access from service loadbalancer source ranges [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:614: [sig-network] LoadBalancers should be able to create an internal type load balancer [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:744: [sig-network] LoadBalancers should have session affinity work for LoadBalancer service with ESIPP on [Slow] [LinuxOnly]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:755: [sig-network] LoadBalancers should be able to switch session affinity for LoadBalancer service with ESIPP on [Slow] [LinuxOnly]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:766: [sig-network] LoadBalancers should have session affinity work for LoadBalancer service with ESIPP off [Slow] [LinuxOnly]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:777: [sig-network] LoadBalancers should be able to switch session affinity for LoadBalancer service with ESIPP off [Slow] [LinuxOnly]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:793: [sig-network] LoadBalancers should handle load balancer cleanup finalizer for service [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:825: [sig-network] LoadBalancers should be able to create LoadBalancer Service without NodePort and change it [Slow]
-    k8s.io/kubernetes/test/e2e/network/loadbalancer.go:945: [sig-network] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on different nodes
-    k8s.io/kubernetes/test/e2e/network/network_tiers.go:71: [sig-network] Services GCE [Slow] should be able to create and tear down a standard-tier load balancer [Slow]
-    k8s.io/kubernetes/test/e2e/scheduling/nvidia-gpus.go:231: [sig-scheduling] [Feature:GPUDevicePlugin] run Nvidia GPU Device Plugin tests
-    k8s.io/kubernetes/test/e2e/scheduling/nvidia-gpus.go:333: [sig-scheduling] GPUDevicePluginAcrossRecreate [Feature:Recreate] run Nvidia GPU Device Plugin tests with a recreation
-    k8s.io/kubernetes/test/e2e/storage/pd.go:137: [sig-storage] Pod Disks [Feature:StorageProvider] schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of "default (30s)"
-    k8s.io/kubernetes/test/e2e/storage/pd.go:137: [sig-storage] Pod Disks [Feature:StorageProvider] schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of "immediate (0s)"
-    k8s.io/kubernetes/test/e2e/storage/pd.go:137: [sig-storage] Pod Disks [Feature:StorageProvider] schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of "default (30s)"
-    k8s.io/kubernetes/test/e2e/storage/pd.go:137: [sig-storage] Pod Disks [Feature:StorageProvider] schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of "immediate (0s)"
-    k8s.io/kubernetes/test/e2e/storage/pd.go:257: [sig-storage] Pod Disks [Feature:StorageProvider] schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 1 containers and 2 PDs
-    k8s.io/kubernetes/test/e2e/storage/pd.go:257: [sig-storage] Pod Disks [Feature:StorageProvider] schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 4 containers and 1 PDs
-    k8s.io/kubernetes/test/e2e/storage/pd.go:351: [sig-storage] Pod Disks [Feature:StorageProvider] detach in a disrupted environment [Slow] [Disruptive] when node's API object is deleted
-    k8s.io/kubernetes/test/e2e/storage/pd.go:351: [sig-storage] Pod Disks [Feature:StorageProvider] detach in a disrupted environment [Slow] [Disruptive] when pod is evicted
-    k8s.io/kubernetes/test/e2e/storage/pd.go:452: [sig-storage] Pod Disks [Feature:StorageProvider] should be able to delete a non-existent PD without error
-    k8s.io/kubernetes/test/e2e/storage/pd.go:461: [sig-storage] Pod Disks [Feature:StorageProvider] [Serial] attach on previously attached volumes should work
-    k8s.io/kubernetes/test/e2e/storage/persistent_volumes-gce.go:134: [sig-storage] PersistentVolumes GCEPD [Feature:StorageProvider] should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach
-    k8s.io/kubernetes/test/e2e/storage/persistent_volumes-gce.go:151: [sig-storage] PersistentVolumes GCEPD [Feature:StorageProvider] should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach
-    k8s.io/kubernetes/test/e2e/storage/persistent_volumes-gce.go:167: [sig-storage] PersistentVolumes GCEPD [Feature:StorageProvider] should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk
-    k8s.io/kubernetes/test/e2e/storage/regional_pd.go:100: [sig-storage] Regional PD RegionalPD should failover to a different zone when all nodes in one zone become unreachable [Slow] [Disruptive]
-    k8s.io/kubernetes/test/e2e/storage/regional_pd.go:82: [sig-storage] Regional PD RegionalPD should provision storage [Slow]
-    k8s.io/kubernetes/test/e2e/storage/regional_pd.go:86: [sig-storage] Regional PD RegionalPD should provision storage with delayed binding [Slow]
-    k8s.io/kubernetes/test/e2e/storage/regional_pd.go:91: [sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies [Slow]
-    k8s.io/kubernetes/test/e2e/storage/regional_pd.go:95: [sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies with delayed binding [Slow]
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:302: [sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] [Feature:StorageProvider] should provision storage with non-default reclaim policy Retain
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:349: [sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] [Feature:StorageProvider] should test that deleting a claim before the volume is provisioned deletes the volume.
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:395: [sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] [Feature:StorageProvider] deletion should be idempotent
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:465: [sig-storage] Dynamic Provisioning DynamicProvisioner External should let an external dynamic provisioner create and delete persistent volumes [Slow]
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:529: [sig-storage] Dynamic Provisioning DynamicProvisioner Default should create and delete default persistent volumes [Slow]
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:553: [sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by changing the default annotation [Serial] [Disruptive]
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:590: [sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by removing the default annotation [Serial] [Disruptive]
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:630: [sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV
-    k8s.io/kubernetes/test/e2e/storage/volume_provisioning.go:93: [sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] [Feature:StorageProvider] should provision storage with different parameters

```


### 分析结果

该Issue涉及潜在的安全风险。

**原因和可能的影响：**

在使用`providerless`标签编译`e2e.test`时，会有一系列测试被移除，其中包括与安全性直接相关的测试。例如：

- `k8s.io/kubernetes/test/e2e/network/firewall.go:212: [sig-network] Firewall rule control plane should not expose well-known ports`

这个测试的目的是确保Kubernetes控制平面没有暴露一些众所周知的端口。如果在`providerless`模式下，这个测试被移除，那么在没有云提供商的环境下，可能不会检测到控制平面错误地暴露了重要的网络端口。

攻击者可以利用这一潜在漏洞，扫描并发现控制平面暴露的端口，进而尝试通过已知的服务漏洞或弱口令等方式获取未授权访问。这可能导致集群被入侵，数据泄露，服务中断等严重后果。

根据CVSS 3.1评分标准，此类漏洞可能涉及以下评分项：

- **攻击向量（AV）：网络（Network）**
- **攻击复杂度（AC）：低（Low）**
- **特权要求（PR）：无（None）**
- **用户交互（UI）：无（None）**
- **影响（CI、II、AI）：高（High）**

综合评分可能达到High以上的级别。

**Proof of Concept（概念验证）：**

1. **环境准备：**
   - 使用`providerless`标签编译Kubernetes，部署一个没有云提供商的集群。
   - 由于相关的防火墙测试被移除，集群的控制平面可能错误地暴露了某些端口。

2. **攻击步骤：**
   - 攻击者对控制平面的IP地址进行端口扫描，发现开放的众所周知端口（例如22端口）。
   - 尝试使用SSH连接到控制平面，如果存在默认或弱密码，则可能获得控制平面的访问权限。
   - 获取访问权限后，攻击者可以执行任意命令，窃取敏感数据，或进一步渗透集群内部网络。

3. **结果验证：**
   - 证明由于缺少安全测试，控制平面存在未授权的端口暴露，确实可以被攻击者利用。

**结论：**

由于关键的安全测试被移除，可能导致控制平面的安全漏洞未被发现，给攻击者可乘之机，符合风险判断标准，需要引起重视并进行修复。

---

## Issue #122725 kube-scheduler memory leak issue in v1.28.3

- Issue 链接：[#122725](https://github.com/kubernetes/kubernetes/issues/122725)

### Issue 内容

#### What happened?

We have identified a memory leak issue in kube-scheduler after upgrading to version v1.28.3. Despite a stable number of pods/nodes, we have noticed a continuous increase in kube-scheduler memory usage over time.

![image](https://github.com/kubernetes/kubernetes/assets/95034/4f363977-720f-468a-860f-833fb4ee4059)

Upon analyzing the pprof dump, it was observed that a significant portion of the memory is consumed by watch.(*StreamWatcher).receive

![image](https://github.com/kubernetes/kubernetes/assets/95034/73508580-5403-4480-832f-c6182f9f0d0a)

![image](https://github.com/kubernetes/kubernetes/assets/95034/7b8ed04d-6253-4d33-8263-303592bf24bc)

[profile.pb.gz](https://github.com/kubernetes/kubernetes/files/13916077/profile.pb.gz)





#### What did you expect to happen?

The kube-scheduler(v1.28.3)memory usage is expected to keep stable

#### How can we reproduce it (as minimally and precisely as possible)?

Using the script below to scale out/in pods, we should see the memory leak issue

```
for (( ; ; ))
do
    kubectl scale deployment nginx-deployment --replicas=0 
    sleep 60
    kubectl scale deployment nginx-deployment --replicas=30
    sleep 120
done
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.3-1", GitCommit:"d0509afab1df7f17cc670b6c617b3d747da6e207", GitTreeState:"clean", BuildDate:"2020-11-11T12:27:59Z", GoVersion:"go1.15.2", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.3-2-rc2-apiserver-compatiable-all-01", GitCommit:"c2078abccc565d633be3a31da29c9f8d683f8327", GitTreeState:"clean", BuildDate:"2023-12-27T08:36:21Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"linux/amd64"}
```

</details>

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
$ uname -a
Linux VMS48219 4.14.67-2dev917.el7.x86_64 #1 SMP Sun Sep 16 19:46:27 CST 2018 x86_64 x86_64 x86_64 GNU/Linux

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

- **利用方式：** 攻击者（具有低权限的用户）可以通过持续反复地创建和删除 Pods，触发 kube-scheduler 的内存泄漏。这种行为会导致 kube-scheduler 的内存占用不断增加。

- **影响：** 随着内存的增加，kube-scheduler 可能会变得不稳定，甚至崩溃。一旦调度器不可用，新的 Pods 将无法被调度到节点上，影响整个 Kubernetes 集群的服务可用性，造成拒绝服务（DoS）攻击。

**根据 CVSS 3.1 评分标准：**

- 攻击向量（AV）：网络（N）——攻击者通过网络连接进行攻击。
- 攻击复杂度（AC）：低（L）——攻击不需要特殊条件，方法简单。
- 权限要求（PR）：低（L）——攻击者只需要具备创建和删除 Pods 的基本权限。
- 用户交互（UI）：无（N）——不需要其他用户的交互。
- 作用域（S）：改变（C）——攻击影响到 kube-scheduler，并进而影响整个集群的可用性。
- 机密性（C）：无（N）——不影响数据机密性。
- 完整性（I）：无（N）——不影响数据完整性。
- 可用性（A）：高（H）——严重影响系统的可用性。

**计算得出的基准分数为 7.5，属于高危等级。**

**Proof of Concept：**

攻击者可以编写如下脚本，不断创建和删除 Pods，诱发 kube-scheduler 的内存泄漏：

```bash
#!/bin/bash
for (( ; ; ))
do
    kubectl run malicious-pod --image=nginx
    sleep 1
    kubectl delete pod malicious-pod
done
```

**说明：**

- 该脚本持续创建名为 `malicious-pod` 的 Pod，然后删除。
- 循环操作会导致 kube-scheduler 的内存使用量不断增加。
- 长时间运行后，可能导致 kube-scheduler 内存耗尽，影响集群的调度功能。

**建议：**

- 尽快定位并修复内存泄漏问题。
- 在修复之前，限制用户对 Pods 的创建和删除操作频率。
- 监控 kube-scheduler 的资源使用情况，设置警报以便及时处理。

---

## Issue #122699 kubectl logs does not show output

- Issue 链接：[#122699](https://github.com/kubernetes/kubernetes/issues/122699)

### Issue 内容

#### What happened?

kubectl logs does not show output of the pod but nerdctl/crictl works well.

```
root@10-20-1-20:/home/cyclinder/network-operator/deployment/network-operator# kubectl logs  -n kube-system network-operator-78ccc765d5-49sfk




^C
root@10-20-1-20:/home/cyclinder/network-operator/deployment/network-operator# nerdctl -n k8s.io logs 734269658eb1 | head -n 10
2024-01-11T07:39:49Z	INFO	controller-runtime.metrics	Metrics server is starting to listen	{"addr": ":8080"}
2024-01-11T07:39:49Z	DEBUG	setup.migrate	NVIPAM config map not found, skip remove state label
2024-01-11T07:39:49Z	DEBUG	setup.controller.NicClusterPolicy.StateManager	Creating a new State manager with	{"states:": ["state-pod-security-policy", "state-multus-cni", "state-container-networking-plugins", "state-ipoib-cni", "state-whereabouts-cni", "state-OFED", "state-SRIOV-device-plugin", "state-RDMA-device-plugin", "state-ib-kubernetes", "state-nv-ipam-cni", "state-nic-feature-discovery"]}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.DaemonSet"}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.ConfigMap"}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.Deployment"}
2024-01-11T07:39:49Z	DEBUG	setup.controller.MacVlanNetwork.StateManager	Creating a new State manager with	{"states:": ["state-Macvlan-Network"]}
2024-01-11T07:39:49Z	INFO	setup.controller.MacVlanNetwork	Watching	{"Kind": "*v1alpha1.MacvlanNetwork"}
2024-01-11T07:39:49Z	INFO	setup.controller.MacVlanNetwork	Watching	{"Kind": "*v1.NetworkAttachmentDefinition"}
2024-01-11T07:39:49Z	DEBUG	setup.controller.HostDeviceNetwork.StateManager	Creating a new State manager with	{"states:": ["state-host-device-network"]}
2024-01-11T07:39:49Z	INFO	setup.controller.HostDeviceNetwork	Watching	{"Kind": "*v1alpha1.HostDeviceNetwork"}
2024-01-11T07:39:49Z	INFO	setup.controller.HostDeviceNetwork	Watching	{"Kind": "*v1.NetworkAttachmentDefinition"}


root@10-20-1-20:/home/cyclinder/network-operator/deployment/network-operator# crictl logs 734269658eb1b | head -n 10
2024-01-11T07:39:49Z	INFO	controller-runtime.metrics	Metrics server is starting to listen	{"addr": ":8080"}
2024-01-11T07:39:49Z	DEBUG	setup.migrate	NVIPAM config map not found, skip remove state label
2024-01-11T07:39:49Z	DEBUG	setup.controller.NicClusterPolicy.StateManager	Creating a new State manager with	{"states:": ["state-pod-security-policy", "state-multus-cni", "state-container-networking-plugins", "state-ipoib-cni", "state-whereabouts-cni", "state-OFED", "state-SRIOV-device-plugin", "state-RDMA-device-plugin", "state-ib-kubernetes", "state-nv-ipam-cni", "state-nic-feature-discovery"]}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.DaemonSet"}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.ConfigMap"}
2024-01-11T07:39:49Z	INFO	setup.controller.NicClusterPolicy	Watching	{"Kind": "*v1.Deployment"}
2024-01-11T07:39:49Z	DEBUG	setup.controller.MacVlanNetwork.StateManager	Creating a new State manager with	{"states:": ["state-Macvlan-Network"]}
2024-01-11T07:39:49Z	INFO	setup.controller.MacVlanNetwork	Watching	{"Kind": "*v1alpha1.MacvlanNetwork"}
2024-01-11T07:39:49Z	INFO	setup.controller.MacVlanNetwork	Watching	{"Kind": "*v1.NetworkAttachmentDefinition"}
2024-01-11T07:39:49Z	DEBUG	setup.controller.HostDeviceNetwork.StateManager	Creating a new State manager with	{"states:": ["state-host-device-network"]}
2024-01-11T07:39:49Z	INFO	setup.controller.HostDeviceNetwork	Watching	{"Kind": "*v1alpha1.HostDeviceNetwork"}
2024-01-11T07:39:49Z	INFO	setup.controller.HostDeviceNetwork	Watching	{"Kind": "*v1.NetworkAttachmentDefinition"}
2024-01-11T07:39:49Z	DEBUG	setup.controller.IPoIBNetwork.StateManager	Creating a new State manager with	{"states:": ["state-IPoIB-Network"]}
2024-01-11T07:39:49Z	INFO	setup.controller.IPoIBNetwork	Watching	{"Kind": "*v1alpha1.IPoIBNetwork"}
2024-01-11T07:39:49Z	INFO	setup.controller.IPoIBNetwork	Watching	{"Kind": "*v1.NetworkAttachmentDefinition"}
```

#### What did you expect to happen?

kubectl logs works well

#### How can we reproduce it (as minimally and precisely as possible)?

- Use kubeadm create a cluster
- Deploy a pod and see the logs 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Linux 10-20-1-20 6.2.0-39-generic #40-Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 14 14:18:00 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

存在潜在的安全风险。

**原因和可能的影响：**

- **日志不可见性**：`kubectl logs` 无法显示 Pod 的日志，但通过 `nerdctl` 或 `crictl` 可以查看。这表明日志在容器运行时存在，但 Kubernetes 的日志收集和呈现机制可能存在问题。
  
- **攻击者利用**：攻击者可能利用此漏洞，绕过 Kubernetes 的日志收集机制，隐藏其在 Pod 内的恶意活动。由于 `kubectl logs` 是运维人员常用的日志查看工具，如果日志被隐藏，攻击者的行为可能不会被及时发现。

- **日志篡改或丢失**：如果攻击者能够控制日志输出，使得 `kubectl logs` 无法获取日志，这可能导致日志篡改或丢失的问题，影响系统的可审计性和追溯性。

- **安全审计困难**：由于日志是安全审计和问题排查的重要依据，无法通过 `kubectl logs` 获取日志可能导致安全事件的检测和响应延迟。

**Proof of Concept：**

1. **环境准备：**

   - 使用 kubeadm 创建一个 Kubernetes 集群。
   - 部署一个含有恶意容器的 Pod。

2. **攻击步骤：**

   - 攻击者在容器内修改日志配置，或以某种方式阻止日志被 Kubernetes 收集，例如更改日志输出路径，或使用非标准输出（stdout/stderr）来记录日志。

3. **结果验证：**

   - 运维人员使用 `kubectl logs` 查看 Pod 日志时，无法看到任何输出。
   - 攻击者使用 `nerdctl logs` 或 `crictl logs` 直接访问容器运行时，可获取完整日志，包含恶意活动的记录。

**可能的 CVSS 3.1 评分：**

- **攻击向量（AV）**：网络（N）
- **攻击复杂度（AC）**：低（L）
- **特权要求（PR）**：低（L）
- **用户交互（UI）**：无（N）
- **影响范围（S）**：未改变（U）
- **机密性影响（C）**：高（H）
- **完整性影响（I）**：高（H）
- **可用性影响（A）**：中（L）

根据以上指标，综合评分可能在 **High**（高）级别。

---

## Issue #122661  Kube-scheduler High Memory usage

- Issue 链接：[#122661](https://github.com/kubernetes/kubernetes/issues/122661)

### Issue 内容

#### What happened?

When I was patrolling the kubernetes cluster, I found that the Pod memory usage of the kube scheduler on a node was abnormal. The following are the resource usage and log outputs：
Logs:
[root@lb ~]# kubectl logs -f -n kube-system kube-scheduler-k8s-master-03
I0910 13:47:01.536621       1 serving.go:348] Generated self-signed cert in-memory
W0910 13:47:03.253799       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0910 13:47:03.253828       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0910 13:47:03.253841       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0910 13:47:03.253853       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0910 13:47:03.307574       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.1"
I0910 13:47:03.307597       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0910 13:47:03.311804       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0910 13:47:03.313958       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0910 13:47:03.314014       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0910 13:47:03.314028       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0910 13:47:03.414494       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0910 13:47:03.414543       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I1221 06:22:01.501294       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
Resouces:
[root@lb ~]# kubectl top po -n kube-system |grep sch
kube-scheduler-k8s-master-01            1m           31Mi            
kube-scheduler-k8s-master-02            0m           29Mi            
kube-scheduler-k8s-master-03            76m          3450Mi

#### What did you expect to happen?

How can I solve my abnormal memory usage situation and seek a solution

#### How can we reproduce it (as minimally and precisely as possible)?

![ee91b3fe0610869118c6f6d3010af24](https://github.com/kubernetes/kubernetes/assets/87749576/886ccf51-0b08-4c99-a781-a53de7c00766)
![Uploading 1704816893072.png…]()


#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes: 1.28.1
kubeadm: 1.28.1
kubelet: 1.28.1


#### Cloud provider

N/A


#### OS version

os:  CentOS Linux release 7.9.2009 (Core)

kernel：3.10.0-1160.76.1.el7.x86_64

#### Install tools

kubeadm


#### Container runtime (CRI) and version (if applicable)

[root@lb ~]# crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.22
RuntimeApiVersion:  v1

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue中存在潜在的安全风险，具体分析如下：

**问题分析：**

从提供的日志信息可以看出，`kube-scheduler`无法获取名为`extension-apiserver-authentication`的ConfigMap，导致认证配置失败：

```
W0910 13:47:03.253799       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.
W0910 13:47:03.253828       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0910 13:47:03.253841       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
```

由于`kube-scheduler`无法获取必要的认证配置，导致其继续在没有正确认证的情况下运行，并将所有请求视为匿名。这可能导致未经授权的访问和操作，存在安全风险。

**潜在的安全风险：**

1. **未经认证的请求被接受：** 调度器将所有请求视为匿名请求，攻击者可能利用这一点发送恶意请求，干扰调度器的正常工作。

2. **权限提升和未授权访问：** 攻击者可能通过匿名请求执行需要高权限的操作，如修改调度策略、影响Pod的调度，从而达到权限提升的目的。

3. **集群稳定性和安全性受影响：** 未经授权的调度操作可能导致关键服务中断、资源耗尽或数据泄露等严重后果。

**可能的影响：**

- **拒绝服务（DoS）攻击：** 攻击者可能提交大量调度请求，消耗调度器资源，导致集群不可用。

- **数据泄露或篡改：** 关键服务被恶意调度或重新调度，可能导致数据被窃取或篡改。

- **权限滥用：** 攻击者利用匿名访问执行高权限操作，危害整个集群的安全。

**概念验证（PoC）：**

假设攻击者能够访问`kube-scheduler`的API接口，由于调度器将请求视为匿名，攻击者可以发送未经认证的请求：

```bash
# 攻击者发送恶意调度请求
curl -k -X POST https://<scheduler-host>:10259/some-scheduler-endpoint -d '<malicious-payload>'
```

由于调度器未正确配置认证，此请求可能被接受并执行。

**CVSS 3.1评分：**

根据上述分析，初步评估该漏洞的CVSS 3.1评分如下：

- **攻击向量（AV）：网络（N）** - 攻击者通过网络访问
- **攻击复杂度（AC）：低（L）** - 攻击不需要复杂的条件
- **所需特权（PR）：无（N）** - 攻击者无需任何权限
- **用户交互（UI）：无（N）** - 不需要用户交互
- **作用范围（S）：未改变（U）**
- **机密性（C）：高（H）** - 可能导致敏感数据泄露
- **完整性（I）：高（H）** - 可能篡改数据
- **可用性（A）：高（H）** - 可能导致服务不可用

根据公式计算，综合评分为9.8，高危（High）。

**结论和建议：**

- **存在安全风险：** 该风险可以被攻击者利用，满足分配CVE编号的条件，且CVSS评分在高危以上。

- **建议措施：**
  1. **恢复正确的权限配置：** 确保`kube-scheduler`拥有读取`extension-apiserver-authentication` ConfigMap的权限。可以创建适当的RoleBinding：
     ```bash
     kubectl create rolebinding kube-scheduler-auth --clusterrole=extension-apiserver-authentication-reader --serviceaccount=kube-system:kube-scheduler -n kube-system
     ```
  2. **强制认证配置成功：** 在`kube-scheduler`的启动参数中，设置`--authentication-tolerate-lookup-failure=false`，使其在认证配置失败时停止运行，避免匿名请求被接受。
  3. **限制网络访问：** 确保`kube-scheduler`的API接口不对外暴露，仅限于必要的内部通信。

**总结：**

该Issue涉及严重的安全风险，应立即采取措施修复权限配置问题，确保`kube-scheduler`的认证机制正常运行，防止潜在的攻击和安全隐患。

---

## Issue #122602 InPlacePodVerticalScaling can make node requests be over 100 percent

- Issue 链接：[#122602](https://github.com/kubernetes/kubernetes/issues/122602)

### Issue 内容

#### What happened?

Before I patch pod to enlarge CPU requests:

```
kubectl describe nodes node-x
...
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                4350m (13%)  5800m (18%)
  memory             6384Mi (3%)  8532Mi (5%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:              <none>
```

Patch the pod to change CPU requests, and it is successful:
```
[root@node-x test]# kubectl patch pod tonyaw-pod2 --patch '{"spec":{"containers":[{"name":"test","resources":{"limits":{"cpu":"31000m"}, "requests":{"cpu":"30000m"}}}]}}'
pod/tonyaw-pod2 patched
[root@node-x test]#
```

node request is over 100%:
```
kubectl describe nodes node-x
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests       Limits
  --------           --------       ------
  # tonyaw: requests is over 100%
  cpu                33850m (105%)  36 (112%)
  memory             6384Mi (3%)    8532Mi (5%)
  ephemeral-storage  0 (0%)         0 (0%)
  hugepages-1Gi      0 (0%)         0 (0%)
  hugepages-2Mi      0 (0%)         0 (0%)
Events:              <none>
```

If I use same CPU request to recreate this pod, it will fail because of lacking of resource:
```
Warning  FailedScheduling  43s   default-scheduler  0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
```

#### What did you expect to happen?

Patch to change resources shall be failed.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod, and patch its CPU request to over the total request of node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
local server
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

该Issue涉及潜在的安全风险，具体分析如下：

**原因及可能的影响：**

此问题描述了在使用InPlacePodVerticalScaling（Pod垂直扩缩容）功能时，允许用户通过`kubectl patch pod`命令增加Pod的CPU requests，甚至可以使节点的资源请求总和超过100%。这可能导致以下安全风险：

1. **资源滥用与拒绝服务（DoS）攻击：**
   攻击者可以利用此漏洞，恶意地增加Pod的资源请求，占用大量的节点资源，导致节点上的其他Pod无法获得足够的资源，甚至使其他Pod无法调度到该节点上。这将影响集群中其他用户或服务的可用性，造成拒绝服务攻击。

2. **绕过资源配额限制：**
   在多租户环境中，管理员通常会使用ResourceQuota或LimitRange等机制来限制用户的资源使用。然而，通过这种方式，普通用户可能绕过了集群的资源限制策略，获得超出其应有权限的资源。

3. **影响集群稳定性：**
   超额的资源请求可能会导致调度器的异常行为，甚至可能影响整个集群的稳定运行。

**满足风险判断标准：**

1. **该风险能被攻击者利用：**
   攻击者只需拥有对Pod的修改权限（例如对自己命名空间下的Pod进行修改的权限），即可执行`kubectl patch pod`命令，恶意增加资源请求。

2. **该风险有可能成为一个漏洞，并被分配CVE编号，使用CVSS 3.1评分标准打分，结果在High以上：**

   - **攻击向量（AV）：网络（N）** - 攻击者可以通过网络访问API服务器进行攻击。
   - **攻击复杂度（AC）：低（L）** - 攻击不需要特殊条件或复杂步骤。
   - **权限要求（PR）：低（L）** - 需要对Pod的修改权限，通常在多租户环境下，用户对自己资源有修改权限。
   - **用户交互（UI）：无（N）** - 不需要其他用户的交互。
   - **作用范围（S）：变更（C）** - 攻击影响超出了最初的权限范围，影响到其他资源。
   - **机密性（C）：无（N）** - 不影响机密性。
   - **完整性（I）：无（N）** - 不影响完整性。
   - **可用性（A）：高（H）** - 严重影响可用性，导致拒绝服务。

   综合上述指标，CVSS评分为：**8.6（高）**

**Proof of Concept：**

1. **创建一个普通用户，并赋予其对特定命名空间的操作权限：**

   ```bash
   # 创建用户凭据、绑定Role等操作，确保用户只能操作自己命名空间下的资源
   ```

2. **用户创建一个Pod：**

   ```bash
   kubectl run my-pod --image=nginx --requests='cpu=100m' --limits='cpu=200m' -n user-namespace
   ```

3. **用户使用patch命令，恶意增加Pod的CPU requests，使其超过节点的总CPU容量：**

   ```bash
   kubectl patch pod my-pod -n user-namespace --patch '{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"50000m"}}}]}}'
   ```

4. **查看节点的Allocated resources，发现CPU requests超过100%：**

   ```bash
   kubectl describe node <node-name>
   ```

   输出显示：

   ```
   Allocated resources:
     (Total limits may be over 100 percent, i.e., overcommitted.)
     Resource           Requests       Limits
     --------           --------       ------
     cpu                50500m (150%)  xxxxx
     ...
   ```

5. **其他用户尝试在该节点上调度Pod，发现因为资源不足而失败：**

   ```bash
   kubectl run another-pod --image=nginx -n other-namespace
   ```

   事件显示：

   ```
   Warning  FailedScheduling  ...  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.
   ```

**总结：**

该问题允许拥有一定权限的用户通过修改Pod的资源请求，导致节点资源超分配，进而影响其他用户的可用性。这在多租户环境中尤为严重，可能被恶意利用来进行拒绝服务攻击。因此，建议尽快修复此漏洞，或者通过Admission Controller等手段限制Pod资源的动态修改。

---

## Issue #122598 fail to connect to pod network

- Issue 链接：[#122598](https://github.com/kubernetes/kubernetes/issues/122598)

### Issue 内容

#### What happened?

I init a kubernetes cluster in amazon-ec2 by kubeadm.Flannel is it's network plugin.
``
kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.24.17 --service-cidr=10.96.0.0/16
``

use default flannel yaml
``
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
``

The security rules of ec2. I opened all ports.
![image](https://github.com/kubernetes/kubernetes/assets/53106233/34da597c-513d-42ba-bd7d-819ea5ce1738)


create dep and svc
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-node-wu
      containers:
      - name: nginx-pod
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-svic
  namespace: wxtest
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    nodePort: 32080
  type: NodePort
```

```
kubectl get no -owide
NAME             STATUS     ROLES           AGE   VERSION                     INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                  CONTAINER-RUNTIME
k8s-master       Ready      control-plane   73d   v1.24.17                    172.31.22.234   <none>        Amazon Linux 2023       6.1.55-75.123.amzn2023.x86_64   containerd://1.7.2
k8s-node2     Ready      <none>          73d   v1.24.17                    172.31.16.122   <none>        Amazon Linux 2023       6.1.55-75.123.amzn2023.x86_64   containerd://1.7.2
k8s-node1        Ready      <none>          73d   v1.24.17                    172.31.22.26    <none>        Amazon Linux 2023       6.1.55-75.123.amzn2023.x86_64   containerd://1.7.2

k get po -ntest -owide
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
nginx-79db656995-shx4q   1/1     Running   0          17h   10.244.2.28   k8s-node1   <none>           <none>

 kubectl describe svc nginx-svc -ntest
Name:                     nginx-svic
Namespace:                test
Labels:                   <none>
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.28.164
IPs:                      10.96.28.164
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32080/TCP
Endpoints:                10.244.2.28:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
```

Pod of the dep is dispatched to k8s-node2 and run successfully. But when I curl Endpoints or NodePort of master, the connect is failed.
```
curl localhost:32080
curl: (28) Failed to connect to localhost port 32080 after 132843 ms: Couldn't connect to server

curl 10.244.2.28:80
curl: (28) Failed to connect to 10.244.2.28 port 80 after 129717 ms: Couldn't connect to server
```

curl by k8s-node2's real ip, successfully.
```
curl 52.83.66.62:32080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

```

Kubernetes core pods, flannel pod as well as kubelet are Running normally, and no correlative error log.
```
k get po -nkube-system -owide  
NAME                                 READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
coredns-7f74c56694-ctgnp             1/1     Running   0          73d   10.244.2.2      k8s-node2   <none>           <none>
coredns-7f74c56694-rgjg8             1/1     Running   0          73d   10.244.1.2      k8s-node1     <none>           <none>
etcd-k8s-master                      1/1     Running   8          73d   172.31.22.234   k8s-master    <none>           <none>
kube-apiserver-k8s-master            1/1     Running   3          73d   172.31.22.234   k8s-master    <none>           <none>
kube-controller-manager-k8s-master   1/1     Running   3          73d   172.31.22.234   k8s-master    <none>           <none>
kube-proxy-87btg                     1/1     Running   0          71d   172.31.22.26    k8s-node1     <none>           <none>
kube-proxy-hfz82                     1/1     Running   0          71d   172.31.22.234   k8s-master    <none>           <none>
kube-proxy-qv2hm                     1/1     Running   0          71d   172.31.16.122   k8s-node2   <none>           <none>
kube-scheduler-k8s-master            1/1     Running   9          73d   172.31.22.234   k8s-master    <none>           <none>

 k get po -nkube-flannel -owide
NAME                         READY   STATUS    RESTARTS      AGE   IP              NODE             NOMINATED NODE   READINESS GATES
kube-flannel-ds-9g7m8        1/1     Running   0             71d   172.31.16.122   k8s-node2      <none>           <none>
kube-flannel-ds-m7vh2        1/1     Running   0             71d   172.31.22.26    k8s-node1        <none>           <none>
kube-flannel-ds-w4hp8        1/1     Running   0             71d   172.31.22.234   k8s-master       <none>           <none>
```

Other anomalies have not been found. kubectl logs、exec, etc are executed successfully.

#### What did you expect to happen?

apply NodePort successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

see What Happend

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2023"
ID="amzn"
ID_LIKE="fedora"
VERSION_ID="2023"
PLATFORM_ID="platform:al2023"
PRETTY_NAME="Amazon Linux 2023"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2023"
HOME_URL="https://aws.amazon.com/linux/"
BUG_REPORT_URL="https://github.com/amazonlinux/amazon-linux-2023"
SUPPORT_END="2028-03-01"
$ uname -a
Linux k8s-master 6.1.55-75.123.amzn2023.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Sep 26 20:06:16 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
ctr -v
ctr github.com/containerd/containerd 1.7.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
see What Happend
</details>


### 分析结果

存在潜在的安全风险。

**原因：**

在Issue内容中，作者提到：“我开放了所有端口。”并附上了EC2的安全组配置截图，显示所有端口对外开放。这意味着Kubernetes集群的所有节点的所有端口都暴露在互联网上，没有任何访问控制。

**可能的影响：**

攻击者可以通过互联网直接访问集群中的所有节点，扫描开放的端口和服务，利用已知漏洞或弱密码对节点进行未授权的访问。例如：

- **未经授权的Kubernetes API访问：** 如果Kubernetes API服务器未进行充分的身份验证控制，攻击者可以直接与API服务器交互，获取集群的敏感信息，甚至执行任意命令。

- **利用已知服务漏洞：** 节点上运行的其他服务（如SSH、HTTP等）可能存在已知漏洞，攻击者可以利用这些漏洞获取系统权限。

- **网络攻击：** 攻击者可能进行拒绝服务攻击、中间人攻击等，影响集群的可用性和数据完整性。

**Proof of Concept：**

攻击者可以执行以下步骤：

1. **端口扫描：** 使用工具（如nmap）对EC2实例的公网IP进行端口扫描，发现所有开放的端口和对应的服务。

   ```
   nmap -p- <EC2实例公网IP>
   ```

2. **服务探测：** 确定运行在开放端口上的服务类型和版本。

   ```
   nmap -sV <EC2实例公网IP>
   ```

3. **漏洞利用：** 例如，如果发现Kubernetes API服务器端口（默认6443）开放且未做访问控制，攻击者可以直接与API服务器交互：

   ```
   kubectl --server=<EC2实例公网IP>:6443 get pods --all-namespaces
   ```

4. **获取权限：** 利用服务或操作系统的已知漏洞，获取节点的管理权限，进一步渗透整个集群。

**CVSS 3.1评分：**

- 攻击向量（AV）：网络（N）
- 攻击复杂度（AC）：低（L）
- 权限要求（PR）：无（N）
- 用户交互（UI）：无（N）
- 作用范围（S）：改变（C）
- 机密性（C）：高（H）
- 完整性（I）：高（H）
- 可用性（A）：高（H）

**综合评分：10.0（Critical）**

因此，该风险能够被攻击者利用，可能成为一个漏洞，并被分配CVE编号，CVSS评分在High以上。

**建议：**

- **严格控制安全组规则：** 仅开放必要的端口和IP地址范围，避免将所有端口对全网开放。

- **加强身份验证和授权：** 确保Kubernetes API服务器等关键服务有严格的身份验证和授权机制。

- **定期安全检查：** 定期扫描和评估系统的安全性，及时修复已知漏洞。

---

## Issue #122585  kubelet creates two duplicate containers

- Issue 链接：[#122585](https://github.com/kubernetes/kubernetes/issues/122585)

### Issue 内容

#### What happened?


In our k8s cluster, I created a pod, but kubelet pulled up two containers，as shown below:
![pod](https://github.com/kubernetes/kubernetes/assets/62041640/be6c78cf-9241-4273-8896-588c84f04664)
![container](https://github.com/kubernetes/kubernetes/assets/62041640/628f4ad5-ca48-4cbf-afdb-de55eb40a63e)

Judging from the kubelet log, a timeout occurred when kubelet created the first container.kubelet may think container creation failed, then kubelet created a second container.
When the second container is created successfully, the first container is also created successfully, so container duplication occurs.

#### What did you expect to happen?

I think kubelet should not create duplicate containers.

#### How can we reproduce it (as minimally and precisely as possible)?

This problem occurs occasionally, and I don’t know how to reproduce it yet. Please give me some help, thanks.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
[root@master1 ~]# kubectl version
Client Version: version.Info{Major:"", Minor:"", GitVersion:"v1.19.4-20230630+556102af12e5153b871b39ef59108f08312e826e", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"1970-01-01T00:00:00Z", GoVersion:"go1.15.13", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.4-20230630+556102af12e5153b871b39ef59108f08312e826e", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"1970-01-01T00:00:00Z", GoVersion:"go1.15.13", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

经过分析，该 Issue 存在潜在的安全风险，具体原因和可能的影响如下：

**问题描述：**

在创建 Pod 时，kubelet 因为在创建第一个容器时发生了超时错误，误以为容器创建失败，继而再次尝试创建容器，结果导致了两个相同的容器被创建。

**可能的安全风险及影响：**

1. **资源耗尽（拒绝服务攻击，DoS）：**

   攻击者可以利用该漏洞，通过重复触发 kubelet 的容器创建超时机制，导致 kubelet 不断创建重复的容器。这样会消耗大量的计算和存储资源，可能导致节点资源耗尽，从而影响其他正常服务的运行，甚至导致整个集群的服务不可用。

2. **数据一致性和完整性风险：**

   如果这些重复的容器共享同一个卷或数据存储，可能会同时对数据进行读写操作，导致数据不一致或损坏。这对依赖数据准确性的应用来说，可能造成严重后果。

3. **安全隔离被打破：**

   在多租户环境下，重复的容器可能打破预期的资源和权限隔离，导致某些容器获得了不该有的访问权限，进而可能导致敏感信息泄露或权限提升。

**利用方式（Proof of Concept）：**

1. **触发条件：**

   攻击者需要找到一种方法，诱使 kubelet 在创建容器时发生超时。可能的方法包括：

   - 提供一个启动时间非常长的容器镜像。
   - 使用复杂的初始化脚本，导致容器创建时间延长。
   - 人为制造网络延迟，使得 kubelet 与容器运行时之间的通信超时。

2. **攻击步骤：**

   - 攻击者提交特制的 Pod 配置，使得 kubelet 在创建容器时发生超时。
   - kubelet 误以为容器创建失败，重新尝试创建容器。
   - 实际上，第一个容器仍然成功创建，导致出现多个相同的容器实例。
   - 攻击者重复上述过程，导致节点上运行大量重复容器，耗尽系统资源。

3. **影响验证：**

   - 观察节点资源使用情况，发现 CPU、内存等资源被大量占用。
   - 其他正常的 Pods 无法调度或运行，影响正常服务。

**漏洞评级（根据 CVSS 3.1）：**

- **攻击向量（AV）：网络（N）**
- **攻击复杂度（AC）：低（L）**
- **所需特权（PR）：低（L）**
- **用户交互（UI）：无（N）**
- **影响范围（S）：改变（C）**
- **机密性影响（C）：低（L）**
- **完整性影响（I）：低（L）**
- **可用性影响（A）：高（H）**

根据以上指标，综合评分为 **7.5（High）**。

**结论：**

该问题满足安全风险的判断标准：

1. **可被攻击者利用**：攻击者可以通过构造特定的 Pod，使 kubelet 产生重复容器。
2. **可能成为漏洞并被分配 CVE 编号，CVSS 评分高于 High**。

建议开发团队尽快修复该问题，确保 kubelet 在容器创建超时后正确处理，避免重复创建容器。

---

# 📌 不涉及安全风险的 Issues (56 个)

## Issue #123057 Difference in observed pod status in regular vs static pod

- Issue 链接：[#123057](https://github.com/kubernetes/kubernetes/issues/123057)

### Issue 内容

#### What happened?

I was looking create a reproducer to test https://github.com/kubernetes/kubernetes/pull/122778, when I stumbled upon something that didn't make sense to me. Maybe I am mistaken, please feel free to close this issue if it isn't a real bug. 

I was trying to force the pod to stay in `ContainerCreating` state for debugging https://github.com/kubernetes/kubernetes/issues/122721 when using standard `Generic PLEG`.  So I modified `Generic PLEG` in my local setup to ignore `PodLifeCycle ContainerStarted` event for a specific pod under test using this patch, 

<details>
  <summary>pleg diff</summary>

```diff
diff --git a/pkg/kubelet/pleg/generic.go b/pkg/kubelet/pleg/generic.go
index 80da5bb059c..3ec180ec0a6 100644
--- a/pkg/kubelet/pleg/generic.go
+++ b/pkg/kubelet/pleg/generic.go
@@ -19,6 +19,7 @@ package pleg
 import (
        "context"
        "fmt"
+       "strings"
        "sync"
        "sync/atomic"
        "time"
@@ -176,7 +177,7 @@ func (g *GenericPLEG) Healthy() (bool, error) {
        return true, nil
 }
 
-func generateEvents(podID types.UID, cid string, oldState, newState plegContainerState) []*PodLifecycleEvent {
+func generateEvents(podID types.UID, cid string, oldState, newState plegContainerState, pod *kubecontainer.Pod) []*PodLifecycleEvent {
        if newState == oldState {
                return nil
        }
@@ -184,6 +185,9 @@ func generateEvents(podID types.UID, cid string, oldState, newState plegContaine
        klog.V(4).InfoS("GenericPLEG", "podUID", podID, "containerID", cid, "oldState", oldState, "newState", newState)
        switch newState {
        case plegContainerRunning:
+               if strings.Contains(pod.Name, "nginx") {
+                       return []*PodLifecycleEvent{{ID: podID, Type: ContainerChanged, Data: cid}}
+               }
                return []*PodLifecycleEvent{{ID: podID, Type: ContainerStarted, Data: cid}}
        case plegContainerExited:
                return []*PodLifecycleEvent{{ID: podID, Type: ContainerDied, Data: cid}}
@@ -388,14 +392,17 @@ func getContainersFromPods(pods ...*kubecontainer.Pod) []*kubecontainer.Containe
 
 func computeEvents(oldPod, newPod *kubecontainer.Pod, cid *kubecontainer.ContainerID) []*PodLifecycleEvent {
        var pid types.UID
+       var pod *kubecontainer.Pod
        if oldPod != nil {
                pid = oldPod.ID
+               pod = oldPod
        } else if newPod != nil {
                pid = newPod.ID
+               pod = newPod
        }
        oldState := getContainerState(oldPod, cid)
        newState := getContainerState(newPod, cid)
-       return generateEvents(pid, cid.ID, oldState, newState)
+       return generateEvents(pid, cid.ID, oldState, newState, pod)
 }

 
 func (g *GenericPLEG) cacheEnabled() bool { 
```
</details>


While this works well for regular pod (which end up getting stuck, as expected, in `ContainerCreating` state), 

```bash
 kubectl get pods 
NAME    READY   STATUS              RESTARTS   AGE
nginx   0/1     ContainerCreating   0          2s
```
but if I deploy the same pod as a static pod it ends up running even though Generic PLEG is clearly not sending any `ContainerStarted` PodLifeCycle event. 

```bash
$ kubectl get pods 
NAME              READY   STATUS    RESTARTS   AGE
nginx-127.0.0.1   1/1     Running   0          2s
```

I always thought the pod status should be similar for regular and static pod, but not sure why do they differ here though. 

<details>
<summary> pod yaml </summary>

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
```
</details>

#### What did you expect to happen?

Pod statuses should remain same for regular as well as static pods. 

#### How can we reproduce it (as minimally and precisely as possible)?

You will have to apply the patch given in the description. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

master 


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123053 Duplicate OwnerReferences Can Deny Service To SSA Clients

- Issue 链接：[#123053](https://github.com/kubernetes/kubernetes/issues/123053)

### Issue 内容

#### What happened?

When some actor manages to add duplicate `OwnerReferences` to an object, any other client attempting to use server-side apply to mutate the object will fail client-side.

Consider some `Service`:

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-07-10T22:57:31Z"
  name: service-name
  namespace: service-namespace
  ownerReferences:
  - apiVersion: group.whatever.com/v1
    blockOwnerDeletion: false
    controller: false
    kind: SomeKind
    name: someName
    uid: 1ea7605c-001a-11ea-9a90-027feb72be7e
  - apiVersion: group.whatever.com/v1
    blockOwnerDeletion: false
    controller: false
    kind: SomeKind
    name: someName
    uid: 1ea7605c-001a-11ea-9a90-027feb72be7e
  resourceVersion: "304470743"
  uid: 8cf44d74-d698-450f-abb6-b52f3511a85b
spec:
  clusterIP: 172.30.249.81
  clusterIPs:
  - 172.30.249.81
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 443
    protocol: TCP
    targetPort: 5443
  selector:
    app: whatever
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

When a client attempts to send a server-side apply PATCH, client-side validation fails:

```
E0131 12:33:54.677801       1 queueinformer_operator.go:319] sync {"update" "service-namespace/service-name"} failed: failed to create manager for existing fields: failed to convert new object (service-namespace/service-name; /v1, Kind=Service) to smd typed: errors:
  .metadata.ownerReferences: duplicate entries for key [uid="1ea7605c-001a-11ea-9a90-027feb72be7e"]
  .metadata.ownerReferences: duplicate entries for key [uid="1ea7605c-001a-11ea-9a90-027feb72be7e"]
```

#### What did you expect to happen?

If unique `OwnerReferences` is an invariant required of objects, perhaps we can add some ratcheting validation for objects in `etcd` that enforces it. I can't see a good reason any client would want duplicate entries, but we must have uniqueness for SSA to function here.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a resource with a duplicate `OwnerReference` - don't use `kubectl edit` as that client will helpfully dedupe things for you. Then try using SSA.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.0
WARNING: version difference between client (1.29) and server (1.24) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
not important
```

</details>


#### Install tools

<details>
none
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
n/a
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
n/a
</details>


### 分析结果

不涉及

---

## Issue #123049 Kubelet unable to collect `node_cpu_usage_seconds_total` and `node_memory_working_set_bytes`

- Issue 链接：[#123049](https://github.com/kubernetes/kubernetes/issues/123049)

### Issue 内容

#### What happened?

Some nodes have kubelet successfully collecting node_cpu_usage_seconds_total and node_memory_working_set_bytes, while on some nodes, it fails to collect them.
![kubelet_error](https://github.com/kubernetes/kubernetes/assets/102716628/aae5d07c-48af-485b-b6bf-c3ec0bfe98bf)


#### What did you expect to happen?

All nodes successfully collecting the two metrics.

#### How can we reproduce it (as minimally and precisely as possible)?

sorry, Not sure exactly what caused it.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.7", GitCommit:"07a61d861519c45ef5c89bc22dda289328f29343", GitTreeState:"clean", BuildDate:"2023-10-18T11:42:32Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.7", GitCommit:"07a61d861519c45ef5c89bc22dda289328f29343", GitTreeState:"clean", BuildDate:"2023-10-18T11:33:23Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
onpremise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux node1 5.4.0-169-generic #187-Ubuntu SMP Thu Nov 23 14:52:28 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray 2.23.1
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.5
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123027 Readiness probe success during preStop hook causes replicas scaling 

- Issue 链接：[#123027](https://github.com/kubernetes/kubernetes/issues/123027)

### Issue 内容

#### What happened?

We are using Readiness and liveness probes in the deployment while `preStop` hook is set to `sleep 60` in order to gracefully shutdown pods.

When we update a deployment by setting a new image, a new Pod is created.  Due to slow start of the JVM, it accidentally failed all initial Readiness and liveness probes attempts. Then the preStop hook is called and the pod is scheduled to be killed after a period of time (for example 60 seconds in our case).

According to the current design, https://github.com/kubernetes/kubernetes/issues/122824#issuecomment-1896099927, the Readiness probe will not be stopped. And in some cases, we've observed the Readiness probe passed and the new RS was scaled up while the old RS was scaled down.

But this Pod will be terminated after 60 seconds. So after some time, no Pod is healthy.

#### What did you expect to happen?

The Readiness probe does not lead to the reconcile of deployment-controller.

#### How can we reproduce it (as minimally and precisely as possible)?

The timeline is designed as following,

![image](https://github.com/kubernetes/kubernetes/assets/2568208/65a55d67-a169-48ec-a5c3-778be0771350)

Any readiness probe attempt pass within the orange zone will cause this issue.

First, we can start a Kubernetes version `1.22` with kind,

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.22.17@sha256:f5b2e5698c6c9d6d0adc419c0deae21a425c07d81bbf3b6a6834042f25d4fba2
```

Then write a simple Go application to control when the probes can pass,

```go
package main

import (
	"flag"
	"fmt"
	"log"
	"net/http"
	"time"
)

var (
	_                      http.Handler = (*healthHandler)(nil)
	startTime                           = time.Now()
	healthThresholdSeconds int
)

type healthHandler struct{}

func (h healthHandler) ServeHTTP(writer http.ResponseWriter, request *http.Request) {
	if time.Now().Sub(startTime).Seconds() > float64(healthThresholdSeconds) {
		writer.WriteHeader(200)
		writer.Write([]byte("ok"))
		return
	}
	writer.WriteHeader(500)
	writer.Write([]byte("not healthy"))
}

func main() {
	flag.IntVar(&healthThresholdSeconds, "health-threshold-seconds", 30, "delay")
	flag.Parse()
	log.Println(fmt.Sprintf("health probe will start after %d seconds", healthThresholdSeconds))
	log.Fatal(http.ListenAndServe(":8080", &healthHandler{}))
}
```

Then deploy this application to the cluster,

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  namespace: default
spec:
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: kubernetes-prober:latest
          resources:
            limits:
              memory: "64Mi"
              cpu: "10m"
          args:
            - -health-threshold-seconds=55
          ports:
            - containerPort: 8080
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                  - sleep
                  - "30"
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /check
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /check
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
```

#### Anything else we need to know?

Startup Probe does help in this scenario if we patch the deployment above,

```yaml
          startupProbe:
            httpGet:
              path: /check
              port: 8080
            failureThreshold: 60
            periodSeconds: 5
```

1.27.x also suffers from this issue,

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.22.17@sha256:f5b2e5698c6c9d6d0adc419c0deae21a425c07d81bbf3b6a6834042f25d4fba2
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.22.17
WARNING: version difference between client (1.29) and server (1.22) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #123024 Kubernetes Node Resources may not have provider ID populated when using external cloud provider 

- Issue 链接：[#123024](https://github.com/kubernetes/kubernetes/issues/123024)

### Issue 内容

#### What happened?

When using an external/out of band cloud providers, the responsibility of adding the provider ID to a Node resource moves from kubelet to the node controller. However because of this change, kubelet will create a node without the provider ID specified and may mark the node as ready. This has become an issue in GKE where workloads are scheduled on the node, but external networking (Ingress and Gateway) cannot proceed because the providerID is missing leading to possible downtimes every time a new node is created.

Another consequence is that if populating the [provider ID fails for any reason](https://github.com/kubernetes/kubernetes/blob/ae185414f4e94e969399c40b23995af6f85fa37f/staging/src/k8s.io/cloud-provider/controllers/node/node_controller.go#L434-L437), the Node object is never updated which means information such as the zone topology and provider ID don't exist. EPS Controller uses the zone topology to populate the EPS.

#### What did you expect to happen?

When using an external cloud provider, I would expect that the provider ID should gate whether the Node is considered ready or not. This would ensure that workloads are not scheduled on to a Node, until it has all relevant information populated.



#### How can we reproduce it (as minimally and precisely as possible)?

On any cluster that uses an external cloud provider, there will be two API calls made when a new node is created. A create call from kubelet where provider ID isn't populated and an update by the node controller.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Relevant code seems quite old and should be the same in all supported Kubernetes Versions.

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及安全风险。

---

## Issue #123018 How can I compile Kubernetes with gollvm and generate IR?

- Issue 链接：[#123018](https://github.com/kubernetes/kubernetes/issues/123018)

### Issue 内容

#### What happened?

Hello everyone, I am nanzi yang. I want to compile Kubernetes source code to LLVM IR. So, I just install gollvm. My installation is fine and I can use commands such as go build/go-doc to compile a simple go program to IR (such as Hello world.go). But when I try to compile the Kubernetes source code. I have some issues:
1. When I try to use go build, there are no main.go under Kubernetes main directory, so it is failed.
2. I tried to use go build under each separate directory. It still failed.
3. After 1 and 2, I try to modify the Makefile of LLVM and make it use my specific gollvm compiler, not the default go compiler. But I am not familiar with the Makefile of LLVM and I can't find the location where I can specify the go compiler.

#### What did you expect to happen?

Does anyone can give me some comments about compiling Kubernetes source code to LLVM IR?

#### How can we reproduce it (as minimally and precisely as possible)?

You can try to install gollvm following its official documents [here](https://go.googlesource.com/gollvm/#building). However, please notice that the gollvm is not compatible with the latest LLVM main branch. It takes me about one week and now I follow the commit [here](https://groups.google.com/g/golang-nuts/c/EzY7FFlDINM). These commits work on my local ubuntu 22.04 LTS. I will be very glad if anyone can help me to fix these "terrible" problem.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
the master branch now
# paste output here
```

</details>


#### Cloud provider

<details>

</details>
no provider

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123013 e2e_node help message about "--kubeconfig" flag is not correct 

- Issue 链接：[#123013](https://github.com/kubernetes/kubernetes/issues/123013)

### Issue 内容

#### What happened?

When I try to do e2e test on my local environment which is built using `kind`, `unknown flag` error occusr though this flag is explained in help message.

#### What did you expect to happen?

the explanation about flag will be fixed

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction steps:
```
$ cd $GOPATH/src/k8s.io/kubernetes
$ make WHAT="test/e2e_node/e2e_node.test" 
$ ./_output/bin/e2e_node.test --kubeconfig ~/.kube/config
```

When I do above steps, error message `unknown flag: --kubeconfig` is shown.
But When I execute `./_output/bin/e2e_node.test --help` , below message is shown:
```
--host string      The host, or apiserver, to connect to. Will default to https://127.0.0.1:6443 if this argument and --kubeconfig are not set.
```

Can I use `--kubeconfig` flag or not?

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29+", GitVersion:"v1.29.0-alpha.0.594+efb8fe8ba90c01", GitCommit:"efb8fe8ba90c01c488fd980f6fff494ae7edff36", GitTreeState:"clean", BuildDate:"2024-01-16T08:23:24Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
local cluster built using `kind`
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
6.5.0-14-generic #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #123002 Kubernetes Job - If no readiness probe defined for Pod, readiness probe checked on container restart

- Issue 链接：[#123002](https://github.com/kubernetes/kubernetes/issues/123002)

### Issue 内容

#### What happened?

I am running an application that submits Jobs to a Kubernetes Cluster. 

I've provided a stripped down Job manifest as follows.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: container-notready
  namespace: default 
spec:
  template:
    spec:
      containers:
        - name: container-notready
          image: busybox:latest
          command: ["/bin/sh"]
          args: ["-c", "sleep 180 && exit 1"]
      restartPolicy: OnFailure
  backoffLimit: 1
```

When this Job runs and the process exits, when the container restarts, the default `terminationGracePeriodSeconds` will start due to no readiness check passing. 

Below is the kubernetes APIServer audit logs. I've scrubbed confidential information from them so please let me know if you need more info. 

First I demonstrate that the container was running but had an exit code 139 from the main program.

```json
        "status": {
            "phase": "Running",
            "conditions": [
                {
                    "type": "Initialized",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:36:13Z"
                },
                {
                    "type": "Ready",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:45:55Z"
                },
                {
                    "type": "ContainersReady",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:45:55Z"
                },
                {
                    "type": "PodScheduled",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:36:13Z"
                }
            ],
            "hostIP": "10.29.64.105",
            "podIP": "100.64.37.171",
            "podIPs": [
                {
                    "ip": "100.64.37.171"
                }
            ],
            "startTime": "2024-01-25T20:36:13Z",
            "containerStatuses": [
                {
                    "name": "job-pod-name",
                    "state": {
                        "running": {
                            "startedAt": "2024-01-25T20:45:54Z"
                        }
                    },
                    "lastState": {
                        "terminated": {
                            "exitCode": 139,
                            "reason": "Error",
                            "startedAt": "2024-01-25T20:36:14Z",
                            "finishedAt": "2024-01-25T20:45:53Z",
                            "containerID": "containerd://6f7462ce5b91abce3d6847f49ca7326fdfee4ec9d021f6ab71d81d707ae37fed"
                        }
                    },
                    "ready": true,
                    "restartCount": 1,
                    "image": "foo",
                    "imageID": "bar",
                    "containerID": "containerd://a4d1e14b1bb2b30d0f9802590e52ec861e96e8bcd1befd27d6c2f4027c7fa5f7",
                    "started": true
                }
            ],
            "qosClass": "Guaranteed"
        }
    },
    "requestReceivedTimestamp": "2024-01-25T20:45:55.104371Z",
    "stageTimestamp": "2024-01-25T20:45:55.146533Z",
    "annotations": {
        "authorization.k8s.io/decision": "allow",
        "authorization.k8s.io/reason": "RBAC: allowed by ClusterRoleBinding \"system:controller:job-controller\" of ClusterRole \"system:controller:job-controller\" to ServiceAccount \"job-controller/kube-system\""
    }
}
```

The container restarts very quickly but then ~30 seconds later (which seems to line up with the terminationGracePeriodSeconds default value of 30s), the container receives a SIGKILL (exit code 137) message. 

```json
        "status": {
            "phase": "Running",
            "conditions": [
                {
                    "type": "Initialized",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:36:13Z"
                },
                {
                    "type": "Ready",
                    "status": "False",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:46:27Z",
                    "reason": "ContainersNotReady",
                    "message": "containers with unready status: [job-pod-name]"
                },
                {
                    "type": "ContainersReady",
                    "status": "False",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:46:27Z",
                    "reason": "ContainersNotReady",
                    "message": "containers with unready status: [job-pod-name]"
                },
                {
                    "type": "PodScheduled",
                    "status": "True",
                    "lastProbeTime": null,
                    "lastTransitionTime": "2024-01-25T20:36:13Z"
                }
            ],
            "hostIP": "10.29.64.105",
            "podIP": "100.64.37.171",
            "podIPs": [
                {
                    "ip": "100.64.37.171"
                }
            ],
            "startTime": "2024-01-25T20:36:13Z",
            "containerStatuses": [
                {
                    "name": "job-pod-name",
                    "state": {
                        "terminated": {
                            "exitCode": 137,
                            "reason": "Error",
                            "startedAt": "2024-01-25T20:45:54Z",
                            "finishedAt": "2024-01-25T20:46:26Z",
                            "containerID": "containerd://a4d1e14b1bb2b30d0f9802590e52ec861e96e8bcd1befd27d6c2f4027c7fa5f7"
                        }
                    },
                    "lastState": {
                        "terminated": {
                            "exitCode": 139,
                            "reason": "Error",
                            "startedAt": "2024-01-25T20:36:14Z",
                            "finishedAt": "2024-01-25T20:45:53Z",
                            "containerID": "containerd://6f7462ce5b91abce3d6847f49ca7326fdfee4ec9d021f6ab71d81d707ae37fed"
                        }
                    },
                    "ready": false,
                    "restartCount": 1,
                    "image": "foo",
                    "imageID": "bar",
                    "containerID": "containerd://a4d1e14b1bb2b30d0f9802590e52ec861e96e8bcd1befd27d6c2f4027c7fa5f7",
                    "started": false
                }
            ],
            "qosClass": "Guaranteed"
        }
    },
    "requestReceivedTimestamp": "2024-01-25T20:46:27.448717Z",
    "stageTimestamp": "2024-01-25T20:46:27.590303Z",
    "annotations": {
        "authorization.k8s.io/decision": "allow",
        "authorization.k8s.io/reason": ""
    }
}

```


#### What did you expect to happen?

The first time the container starts up there is no readiness probe. On container restart, we **should not** check for readiness.
 
That way the restarted container does not suffer from SIGKILL due to not being ready from a readiness check that is not defined.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Job with the following features
- no readiness probe
- the main process can exit with a non-zero exit code in > 60 seconds (more than terminationGracePeriodSeconds)
- backoffLimit >= 1 (so container can restart on non-zero exit)

#### Anything else we need to know?

I believe this to be a bug in Kubernetes because this edge case had unexpected behaviour. 

My reference for this behaviour are the preStop hook defined in:
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination


#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.28.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.11-eks-8cb36c9
```

</details>


#### Cloud provider

<details>
Amazon EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122968 kubectl config set-credentials w/ --exec-api-version=...v1 omits setting required interactiveMode

- Issue 链接：[#122968](https://github.com/kubernetes/kubernetes/issues/122968)

### Issue 内容

#### What happened?

`kubectl config set-credentials --exec-api-version=client.authentication.k8s.io/v1` with an exec command creates a kubeconfig which is invalid and when I attempt to access my cluster with it I get this error:
```
error: interactiveMode must be specified for user to use exec authentication plugin
```

#### What did you expect to happen?

Ideally, interactiveMode would be set to a default value and a flag would be provided to set it to a different value.

#### How can we reproduce it (as minimally and precisely as possible)?
```
$ kubectl config set-credentials my-user --exec-api-version=client.authentication.k8s.io/v1 --exec-command=echo --exec-arg=hello

$ cat ~/.kube/config
# Observe that no interactiveMode is set tho one is required for client.authentication.k8s.io/v1  exec plugin
- name: my-user
  user:
      exec:
         apiVersion: client.authentication.k8s.io/v1
         args:
         - hello
         command: echo
         env: null
         provideClusterInfo: false
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: 1.26.8
Server Version: 1.27.7-gke
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122960 [Kubevirt] virt-launcher pod stuck in Terminating after node power cycle

- Issue 链接：[#122960](https://github.com/kubernetes/kubernetes/issues/122960)

### Issue 内容

#### What happened?

- Created a centos VM using the kubevirt with a disk as block device.
- Rebooted the Node on which the virt-launcher pod is running.
- Pod goes in Terminating state.
- All nodes were in healthy state.
- User kubernetes version v1.27.7.
- Found below error in the kubelet log. Kubelet tries to unmount the path which is not exists.
```
Jan 16 01:43:40 appserv56 kubelet[6715]: E0116 01:43:40.722571    6715 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/dcx.csi.diamanti.com^pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe podName:2fb65628-7467-45d4-8dc1-4f77e5cc64db nodeName:}" failed. No retries permitted until 2024-01-16 01:45:42.722524679 -0800 PST m=+15150.170124605 (durationBeforeRetry 2m2s). Error: UnmapVolume.UnmapBlockVolume failed for volume "pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe" (UniqueName: "kubernetes.io/csi/dcx.csi.diamanti.com^pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe") pod "2fb65628-7467-45d4-8dc1-4f77e5cc64db" (UID: "2fb65628-7467-45d4-8dc1-4f77e5cc64db") : blkUtil.DetachFileDevice failed. globalUnmapPath:, podUID: 2fb65628-7467-45d4-8dc1-4f77e5cc64db, bindMount: true: failed to unmap device from map path. mapPath is empty
Jan 16 01:45:42 appserv56 kubelet[6715]: I0116 01:45:42.800589    6715 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe\" (UniqueName: \"kubernetes.io/csi/dcx.csi.diamanti.com^pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe\") pod \"2fb65628-7467-45d4-8dc1-4f77e5cc64db\" (UID: \"2fb65628-7467-45d4-8dc1-4f77e5cc64db\") "
Jan 16 01:45:42 appserv56 kubelet[6715]: W0116 01:45:42.800664    6715 volume_path_handler_linux.go:62] couldn't find loopback device which takes file descriptor lock. Skip detaching device. device path: "2fb65628-7467-45d4-8dc1-4f77e5cc64db"
Jan 16 01:45:42 appserv56 kubelet[6715]: W0116 01:45:42.800689    6715 volume_path_handler.go:217] Warning: Unmap skipped because symlink does not exist on the path: /var/lib/kubelet/pods/2fb65628-7467-45d4-8dc1-4f77e5cc64db/volumeDevices/kubernetes.io~csi/pvc-6a70e1b4-8b7f-435a-bf5c-fba2fddea7fe
```
```
$ kgp                                                                                                                                            
NAME                         READY   STATUS        RESTARTS   AGE   IP       NODE        NOMINATED NODE   READINESS GATES                                                  
virt-launcher-centos-rnkvz   0/1     Terminating   0          51m      appserv92              1/1                           
```
- **Workaround**: Restart the kubelet service or delete the pod forcefully.

#### What did you expect to happen?

The virt-launcher po should be in running state.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create centos VM with 1 disk(block device) using below spec on kubevirt. 
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: centos-disk
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 100Gi
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine                                                                                                                                                       
metadata:                                                                                                                                                                  
  name: centos                                                                                                                                                             
spec:                                                                                                                                                                      
  running: false                                                                                                                                                           
  template:                                                                                                                                                                
    metadata:                                                                                                                                                              
      annotations:                                                                                                                                                         
        diamanti.com/endpoint0: '{"network":"default","perfTier":"high"}'                                                                                                  
      labels:                                                                                                                                                              
        kubevirt.io/domain: centos                                                                                                                                         
    spec:                                                                                                                                                                  
      domain:                                                                                                                                                              
        ioThreadsPolicy: auto                                                                                                                                              
        cpu:                                                                                                                                                               
          cores: 8                                                                                                                                                         
        devices:                                                                                                                                                           
          blockMultiQueue: true                                                                                                                                            
          disks:                                                                                                                                                           
          - disk:                                                                                                                                                          
              bus: virtio                                                                                                                                                  
            name: harddrive                                                                                                                                                
          - disk:                                                                                                                                                          
              bus: virtio                                                                                                                                                  
            name: harddrive2                                                                                                                                               
            dedicatedIOThread: true                                                                                                                                        
        machine:                                                                                                                                                           
          type: q35                                                                                                                                                        
        resources:                                                                                                                                                         
          requests:                                                                                                                                                        
            memory: 16G                                                                                                                                                    
      volumes:                                                                                                                                                             
      - name: harddrive                                                                                                                                                    
        persistentVolumeClaim:                                                                                                                                             
          claimName: centos-os                                                                                                                                             
      - name: harddrive2                                                                                                                                                   
        persistentVolumeClaim:                                                                                                                                             
          claimName: centos-disk
```
2. Restart the node on which the virt-launcher pod is running.
3. Wait till the node is up and goos state.
4. Observe the pod status, it would be in terminating state.
5. Try step 1-4 if not reproduced.
6. See the kubelet logs.

#### Anything else we need to know?

There was the same issue resolved for the nfs mount https://github.com/kubernetes/kubernetes/issues/114546

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.7", GitCommit:"07a61d861519c45ef5c89bc22dda289328f29343", GitTreeState:
"clean", BuildDate:"2023-10-18T11:42:32Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.7", GitCommit:"07a61d861519c45ef5c89bc22dda289328f29343", GitTreeState:
"clean", BuildDate:"2023-10-18T11:33:23Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Rocky Linux"
VERSION="8.6 (Green Obsidian)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="8.6"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Rocky Linux 8.6 (Green Obsidian)"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:rocky:rocky:8:GA"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
ROCKY_SUPPORT_PRODUCT="Rocky Linux"
ROCKY_SUPPORT_PRODUCT_VERSION="8"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8"

$ uname -a
Linux appserv64 4.18.0-372.9.1.el8.x86_64 #1 SMP Tue May 10 14:48:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
$ crio version
INFO[2024-01-25 02:02:40.557038574-08:00] Starting CRI-O, version: 1.25.4, git: unknown(clean) 
Version:        1.25.4
GitCommit:      unknown
GitCommitDate:  unknown
GitTreeState:   clean
BuildDate:      2023-08-10T08:50:26Z
GoVersion:      go1.19
Compiler:       gc
Platform:       linux/amd64
Linkmode:       dynamic
BuildTags:      
  rpm_crashtraceback
  exclude_graphdriver_btrfs
  btrfs_noversion
  exclude_graphdriver_devicemapper
  libdm_no_deferred_remove
  seccomp
  containers_image_openpgp
LDFlags:          -X github.com/cri-o/cri-o/internal/pkg/criocli.DefaultsPath= -X  github.com/cri-o/cri-o/internal/version.buildDate=2023-08-10T08:50:26Z -X  github.com/cri-o/cri-o/internal/version.gitCommit=3ad059f673902a0b502da5e9ec80714df08f3f4e -X  github.com/cri-o/cri-o/internal/version.version=1.25.4 -X  github.com/cri-o/cri-o/internal/version.gitTreeState=clean  -B 0x0ae0f03614d4cdbdf46118d31839208bbeb1c635 -extldflags '-Wl,-z,relro  -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld ' -compressdwarf=false
SeccompEnabled:   true
AppArmorEnabled:  false
Dependencies:     
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122955 Kubelet - failed to initialize top level QOS containers: root container [kubepods] doesn't exist

- Issue 链接：[#122955](https://github.com/kubernetes/kubernetes/issues/122955)

### Issue 内容

#### What happened?

Starting the kubelet the first time after a system reboot,  the kubelet fails with:
`E0125 00:20:56.003890    2172 kubelet.go:1466] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubepods] doesn't exist"`

Which, sort of is a lie.   The root cause is actually that `cpuset` does not exist in `/sys/fs/cgroup/cgroup.subtree_control` 

After the kubelet exits, systemd restarts in,  and then all is fine after that,  until the next time the OS is restarted.  

I tried to track it down,  create.go does add the cpuset to `cgroup.subtree_control`,  and it remains there until `common.go:setUnitProperties()` is executed,  at which time `cpuset` appears to be disabled and disappears.  The other controllers are unaffected.   On the subsequent kubelet restart, `cpuset` does get added again,  and remains after the call to setUnitProperties().   

#### What did you expect to happen?

I would expect the kubelet to start and not exit on all starts of the kubelet. 

#### How can we reproduce it (as minimally and precisely as possible)?

The OS I am using is RHEL 9 on s390x.   
1) Boot the OS
2) Start the kubelet (I start with `kubeadm init` or `kubeadm join`.
3) Observer the kubelet starts,  fails, and then restarts.   Subsequent restarts of the kubelet are successful until the OS rebooted. (Likely due to after a reboot, the /sys/fs/cgroups is fresh,  unlike subsequent restarts of the kubelet)


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.9", GitCommit:"d1483fdf7a0578c83523bc1e2212a606a44fd71d", GitTreeState:"clean", BuildDate:"2023-09-13T11:32:41Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/s390x"}
Kustomize Version: v4.5.7
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

RHEL 9 kernel 5.14.0-162.22.2.el9.s390x
```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
CRI-O 1.26.0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122948 http probes that do return 200 but hang afterwards do not fail if their size is above 10 Kb

- Issue 链接：[#122948](https://github.com/kubernetes/kubernetes/issues/122948)

### Issue 内容

#### What happened?

http probes that do return 200 but hang afterwards do not fail

recently we had an issue that a service was getting stuck, it returned http code 200, but it never fully returned as it would be a streaming response, however it just wasn't finished, imagine a html document whose last </html> hasn't been written yet but </body> has

resulting in an request whose response hasn't been fully read by k8s, thus not running in a timeout, thus not a failing probe

if the 10 Kb limit wouldn't be in place we would have detected that issue
the relevant code can be found here https://github.com/kubernetes/kubernetes/blob/master/pkg/probe/http/http.go#L34

i am aware that returning html or similar isn't suitable for a probe, however it was our last resort measurement and there are probably a lot of more services out there where this is the default situation

- **this limitation is not documented**
- to me this is an issue of k8s closing the request before it is fully read, although debatable

_as a workaround we switched to an exec probe that calls a wget with a timeout ... really not the way to go_


if you need further info let me know

#### What did you expect to happen?

failing probes

#### How can we reproduce it (as minimally and precisely as possible)?

have a service whose response exceeds 10Kb and that did not close the socket yet
```
        readinessProbe:
          httpGet:
            path: /login
            port: 3100
          timeoutSeconds: 4
          failureThreshold: 3
          periodSeconds: 10
```

#### Anything else we need to know?

the relevant code can be found here
https://github.com/kubernetes/kubernetes/blob/master/pkg/probe/http/http.go#L34

#### Kubernetes version

<details>

```console
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.10
```

</details>


#### Cloud provider

<details>
ionos
</details>


#### OS version

<details>
-
</details>


#### Install tools

<details>
-
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
-
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
-
</details>


### 分析结果

不涉及

---

## Issue #122947 why remove VolumeSchedulingStageLatency metrics from scheduler?

- Issue 链接：[#122947](https://github.com/kubernetes/kubernetes/issues/122947)

### Issue 内容

#### What happened?

we need to know how much time cost in volume binding stage, there is an old metrics called `VolumeSchedulingStageLatency`. it has been deprecated since 1.19. 
Just want to know why? i have found related pr but don't know why to deprecated this metric. https://github.com/kubernetes/kubernetes/pull/92160

#### What did you expect to happen?

add metrics for scheduler to show the time cost of volume binding

#### How can we reproduce it (as minimally and precisely as possible)?

create a pod with csi, when pod scheduled successfully, there is no related metrics to show the time cost of volume binding

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
master
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122938 The `pod` is always in `ContainerCreating`

- Issue 链接：[#122938](https://github.com/kubernetes/kubernetes/issues/122938)

### Issue 内容

#### What happened?

```
Type     Reason                  Age                 From               Message
  ----     ------                  ----                ----               -------
  Normal   Scheduled               5m28s               default-scheduler  Successfully assigned ci/drone-0zob84hw8iawdxzayx93 to newipaas-k8s-app-002
  Warning  FailedCreatePodSandBox  2m7s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "00502d98ba4c09d1415624e7080dcd2c4f020653afb7ab97a3846d0f8f4114a3" network for pod "drone-0zob84hw8iawdxzayx93": networkPlugin cni failed to set up pod "drone-0zob84hw8iawdxzayx93_ci" network: the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces ci)
  Normal   SandboxChanged          36s (x2 over 2m6s)  kubelet            Pod sandbox changed, it will be killed and re-created.
```

```
Events:
  Type     Reason                  Age                  From               Message
  ----     ------                  ----                 ----               -------
  Normal   Scheduled               10m                  default-scheduler  Successfully assigned ci/nginx-85b98978db-6lhm5 to newipaas-k8s-app-001
  Warning  FailedCreatePodSandBox  6m44s                kubelet            Failed to create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded
  Normal   SandboxChanged          89s (x2 over 5m13s)  kubelet            Pod sandbox changed, it will be killed and re-created.
  Warning  FailedCreatePodSandBox  89s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "3a4873398d4c6244983b4f1ff6d1763c2c78bf5455f78593638b359f006bdb28" network for pod "nginx-85b98978db-6lhm5": networkPlugin cni failed to set up pod "nginx-85b98978db-6lhm5_ci" network: netplugin failed: "2024-01-24 15:23:32.632 [INFO][15967] plugin.go 265: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {newipaas--k8s--app--001-k8s-nginx--85b98978db--6lhm5-eth0 nginx-85b98978db- ci  8c8bdd17-9f9a-4e75-ba6b-38d07f7e1f87 103790314 0 2024-01-24 15:14:53 +0800 CST <nil> <nil> map[app:nginx pod-template-hash:85b98978db projectcalico.org/namespace:ci projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  newipaas-k8s-app-001  nginx-85b98978db-6lhm5 eth0 default [] []   [kns.ci ksa.ci.default] calic30ecf867b3  []}} ContainerID=\"3a4873398d4c6244983b4f1ff6d1763c2c78bf5455f78593638b359f006bdb28\" Namespace=\"ci\" Pod=\"nginx-85b98978db-6lhm5\" WorkloadEndpoint=\"newipaas--k8s--app--001-k8s-nginx--85b98978db--6lhm5-\"\n2024-01-24 15:23:32.632 [INFO][15967] k8s.go 71: Extracted identifiers for CmdAddK8s ContainerID=\"3a4873398d4c6244983b4f1ff6d1763c2c78bf5455f78593638b359f006bdb28\" Namespace=\"ci\" Pod=\"nginx-85b98978db-6lhm5\" WorkloadEndpoint=\"newipaas--k8s--app--001-k8s-nginx--85b98978db--6lhm5-eth0\"\n"
```

```
Events:
  Type     Reason                  Age                    From               Message
  ----     ------                  ----                   ----               -------
  Normal   Scheduled               16m                    default-scheduler  Successfully assigned ci/drone-r8753gheujfr8vkvozjq to newipaas-k8s-app-002
  Warning  FailedCreatePodSandBox  8m29s (x2 over 12m)    kubelet            Failed to create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded
  Normal   SandboxChanged          8m29s (x2 over 12m)    kubelet            Pod sandbox changed, it will be killed and re-created.
  Warning  FailedSync              2m26s (x7 over 4m27s)  kubelet            error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded
  Warning  FailedKillPod           22s                    kubelet            error killing pod: failed to "KillPodSandbox" for "34969f82-8e91-4d66-8013-35728bab87cb" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"
```

#### What did you expect to happen?

I want to be able to start the `pod` properly.

#### How can we reproduce it (as minimally and precisely as possible)?

I'm not sure how it happened.

Using `client-go` to call the `api`, a timeout also occurs....

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

```
% kubectl get nodes -o wide
NAME                      STATUS   ROLES                  AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION              CONTAINER-RUNTIME
newipaas-k8s-app-001      Ready    <none>                 232d   v1.23.8   172.29.2.229   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-app-002      Ready    <none>                 232d   v1.23.8   172.29.2.230   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-app-003      Ready    <none>                 120d   v1.23.8   172.29.2.236   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-app-004      Ready    <none>                 120d   v1.23.8   172.29.2.238   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-app-005      Ready    <none>                 120d   v1.23.8   172.29.2.240   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-master-001   Ready    control-plane,master   232d   v1.23.8   172.29.2.228   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-master002    Ready    control-plane,master   119d   v1.23.8   172.29.2.239   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
newipaas-k8s-master003    Ready    control-plane,master   119d   v1.23.8   172.29.2.237   <none>        CentOS Linux 7 (Core)   5.4.8-1.el7.elrepo.x86_64   docker://19.3.14
```
```
# kubectl get pods -n calico-system
NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-cd467d985-kr26b   1/1     Running   4          232d
calico-node-4h4cg                         1/1     Running   2          232d
calico-node-7q7rv                         1/1     Running   0          232d
calico-node-csk5m                         1/1     Running   3          232d
calico-node-gj7sw                         1/1     Running   1          120d
calico-node-l5zqj                         1/1     Running   2          120d
calico-node-nwf2j                         1/1     Running   0          120d
calico-node-rqjmb                         1/1     Running   2          120d
calico-node-tqp9w                         1/1     Running   0          120d
calico-typha-84d9fcbf6d-cl87g             1/1     Running   2          120d
calico-typha-84d9fcbf6d-fkbvr             1/1     Running   2          232d
calico-typha-84d9fcbf6d-rhrmv             1/1     Running   3          232d
```

#### OS version

<details>

```console
# On Linux:
# cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
# uname -a
Linux newipaas-k8s-master-001 5.4.8-1.el7.elrepo.x86_64 #1 SMP Sat Jan 4 15:29:03 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122935 validatingadmissionpolicystatus reports error when using spec.variables

- Issue 链接：[#122935](https://github.com/kubernetes/kubernetes/issues/122935)

### Issue 内容

#### What happened?

When I load a policy definition like this

```yaml
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicy
metadata:
  name: loadbalancer.servicenow.net
spec:
  matchConstraints:
    resourceRules:
    - apiVersions: ["v1"]
      apiGroups:   [""]
      resources:   ["services"]
      operations:  ["CREATE", "UPDATE"]
  matchConditions:
  - name: "only-loadbalancer-services"
    expression: object.spec.type == "LoadBalancer"
  paramKind:
    apiVersion: control.servicenow.net/v1beta1
    kind: TenantPermission
  variables:
  - name: loadbalancerid
    expression: "'ccm.snowsk8s.servicenow.net/lb-service-id' in object.metadata.annotations ? object.metadata.annotations['ccm.snowsk8s.servicenow.net/lb-service-id'] : 'none'"
  validations:
    - expression: variables.loadbalancerid in params.permissions.loadbalancerid
      reason: Forbidden
      messageExpression: "'Namespace ' + string(namespaceObject.metadata.name) + ' does not have permissions to loadbalancer ' + string(variables.loadbalancerid)"
  failurePolicy: Fail
```


The object is updated with the following status
```yaml
status:
  observedGeneration: 2
  typeChecking:
    expressionWarnings:
    - fieldRef: spec.validations[0].expression
      warning: |
        /v1, Kind=Service: ERROR: <input>:1:1: undeclared reference to 'variables' (in container '')
         | variables.loadbalancerid in params.permissions.loadbalancerid
         | ^
    - fieldRef: spec.validations[0].messageExpression
      warning: |
        /v1, Kind=Service: ERROR: <input>:1:111: undeclared reference to 'variables' (in container '')
         | 'Namespace ' + string(namespaceObject.metadata.name) + ' does not have permissions to loadbalancer ' + string(variables.loadbalancerid)
         | ..............................................................................................................^
```

While this error is reported, the policy does seem to be fully functioning

```
// Try to claim someone elses loadbalancer in the a different namespace
k apply -f testing/someones-loadbalancer.yaml   
Error from server (Forbidden): error when creating "testing/someones-loadbalancer.yaml": services "idonthavealoadbalancer" is forbidden: ValidatingAdmissionPolicy 'loadbalancer.servicenow.net' with binding 'deny-non-owning-loadbalancer' denied request: Namespace workload1 does not have permissions to loadbalancer 123someoneelses123

// Try to claim a loadbalancer without any valid ID
 k apply -f testing/idonthavemyown.yaml
Error from server (Forbidden): error when creating "testing/idonthavemyown.yaml": services "soneone-elses-loadbalancer" is forbidden: ValidatingAdmissionPolicy 'loadbalancer.servicenow.net' with binding 'deny-non-owning-loadbalancer' denied request: Namespace workload1 does not have permissions to loadbalancer none

// A valid request
k apply -f testing/my-loadbalancer.yaml 
service/my-loadbalancer configured
```

This is similar to the documented examples
https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/#variable-composition

#### What did you expect to happen?

No error reported on a valid spec

#### How can we reproduce it (as minimally and precisely as possible)?

Create a a policy that uses variables
Observe the status of the newly created policy

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
k version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122913 apiserver: command exec drops individual characters

- Issue 链接：[#122913](https://github.com/kubernetes/kubernetes/issues/122913)

### Issue 内容

#### What happened?

While investigating https://github.com/kubernetes/kubernetes/issues/112834, an E2E test was written which sends one byte to `cat` in a container via stdin and checks what comes back on stdout. It does that for each byte from 0 to 255.

The following bytes all get lost:
```
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] "\f" (0c) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] "\n" (0a) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] "\t" (09) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] "\v" (0b) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] " " (20) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
  [FAIL] [sig-api-machinery] Pod exec can transfer ASCII character [It] "\r" (0d) [sig-api-machinery]
  /nvme/gopath/src/k8s.io/kubernetes/test/e2e/apimachinery/execwithoptions_stress.go:107
```

#### What did you expect to happen?

Stdin/stdout should be 8-bit clean and support arbitrary chunks of data, including single byte.

#### How can we reproduce it (as minimally and precisely as possible)?

- Check out https://github.com/kubernetes/kubernetes/pull/122911
- `make` and `hack/local-up-cluster.sh`
- `_output/bin/ginkgo -focus="Pod exec can transfer ASCII character" ./test/e2e`


#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.29.1

#### Cloud provider

n/a

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122909 Topology spreading can become imbalanced during pod eviction

- Issue 链接：[#122909](https://github.com/kubernetes/kubernetes/issues/122909)

### Issue 内容

#### What happened?

We saw topology spreading constraints violated when a pod was evicted from a node and rescheduled on another node. In our deployment we want the 3 pods to be distributed in 3 availability zones but we saw 0/1/2 distributions multiple times. 

When a node is drained it evicts all pods on it and k8s schedules new pods on other nodes to satisfy deployment's replicas spec. In our environment, when this event happens the newly scheduled pod is always created at least 10 seconds before the corresponding old pod is terminated. When scheduling the new pod k8s sees evenly distributed pods without knowing a pod will be terminated soon, so it schedules pod to an arbitrary domain which is sometimes different from the domain in which the old pod is running. 

This is similar to https://github.com/kubernetes/kubernetes/issues/105661 but it happens during node draining so the new and old pods are in the same replica, which means we can't use the `MatchLabelKeys` to distinguish the old an new pods. 

#### What did you expect to happen?

I am not sure this is a bug or limitation of topology spreading constraints. If it is a limitation please document this limitation.

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a 3 replica service to a k8s cluster running in 3 AZ region with the following topologySpreadConstraints

```
topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.k8s.io/zone
        whenUnsatisfiable: DoNotSchedule
```

Drain the node in one zone and check the pods distribution after

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.6-dd.1
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122904 missing events for some of pod containers

- Issue 链接：[#122904](https://github.com/kubernetes/kubernetes/issues/122904)

### Issue 内容

#### What happened?

After running the Pod with 9+ containers (1 regular container + minimum of 8 init containers) there are Events available only for 8 containers, anything above is not registered in the Pod's events.

#### What did you expect to happen?

I would expect to have `Pulled`/`Created`/`Started` events for all the containers of the Pod.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a job with 9 init containers and 1 further container:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example
spec:
  backoffLimit: 0
  completions: 1
  parallelism: 1
  suspend: false
  template:
    spec:
      restartPolicy: Never
      initContainers:
      - image: busybox:1.36.1
        name: example-1
      - image: busybox:1.36.1
        name: example-2
      - image: busybox:1.36.1
        name: example-3
      - image: busybox:1.36.1
        name: example-4
      - image: busybox:1.36.1
        name: example-5
      - image: busybox:1.36.1
        name: example-6
      - image: busybox:1.36.1
        name: example-7
      - image: busybox:1.36.1
        name: example-8
      - image: busybox:1.36.1
        name: example-9
      containers:
      - image: busybox:1.36.1
        name: example-10
```

and later:

```console
$ kubectl get events | grep pod/example
```

It won't display events for `example-9` and `example-10` containers:
* on K3S and Minikube the last event is `Pulled` from `example-9`
* on GKE the last event is `Started` from `example-8`
<details>
  <summary>Example events log</summary>

```console
$ kubectl get events | grep pod/example
55s         Normal    Scheduled            pod/example-k4jfw                                    Successfully assigned testkube/example-k4jfw to homelab
55s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
55s         Normal    Created              pod/example-k4jfw                                    Created container example-1
55s         Normal    Started              pod/example-k4jfw                                    Started container example-1
54s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
54s         Normal    Created              pod/example-k4jfw                                    Created container example-2
54s         Normal    Started              pod/example-k4jfw                                    Started container example-2
53s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
53s         Normal    Created              pod/example-k4jfw                                    Created container example-3
53s         Normal    Started              pod/example-k4jfw                                    Started container example-3
52s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
52s         Normal    Created              pod/example-k4jfw                                    Created container example-4
52s         Normal    Started              pod/example-k4jfw                                    Started container example-4
51s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
51s         Normal    Created              pod/example-k4jfw                                    Created container example-5
51s         Normal    Started              pod/example-k4jfw                                    Started container example-5
50s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
50s         Normal    Created              pod/example-k4jfw                                    Created container example-6
50s         Normal    Started              pod/example-k4jfw                                    Started container example-6
49s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
49s         Normal    Created              pod/example-k4jfw                                    Created container example-7
49s         Normal    Started              pod/example-k4jfw                                    Started container example-7
48s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
48s         Normal    Created              pod/example-k4jfw                                    Created container example-8
48s         Normal    Started              pod/example-k4jfw                                    Started container example-8
47s         Normal    Pulled               pod/example-k4jfw                                    Container image "busybox:1.36.1" already present on machine
```
</details>

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# K3S
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.5+k3s1

# GKE
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.25.10-gke.2700
```
</details>


#### Cloud provider

<details>
Tested on GKE, K3S and Minikube
</details>


#### OS version

<details>

```console
# K3S
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 23.04"
NAME="Ubuntu"
VERSION_ID="23.04"
VERSION="23.04 (Lunar Lobster)"
VERSION_CODENAME=lunar
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=lunar
LOGO=ubuntu-logo
$ uname -a
Linux homelab 6.2.0-20-generic #20-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr  6 07:48:48 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122903 kubelet: watch of *v1.Pod ended with: very short watch: k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:67: Unexpected watch close - watch lasted less than a second and no items received

- Issue 链接：[#122903](https://github.com/kubernetes/kubernetes/issues/122903)

### Issue 内容

#### What happened?

We encountered the following situation:

When the performance of etcd decreases, it causes some nodes in the cluster to show "failed to update lease, error: etcdserver: too many requests". At the same time, the client go module reports "watch of * v1. Pod ended with very short watch: k8s. io/kubernetes/pkg/kubelet/configure/apiserver. go: 67: Unexpected watch close - watch last less than a second and no items received"

#### What did you expect to happen?

The client go module should not display a watch of * v1. Pod ended with: very short watch: k8s. io/kubernetes/pkg/kubelet/configure/apiserver. go: 67: Unexpected watch close - watch last less than a second and no items received

#### How can we reproduce it (as minimally and precisely as possible)?

1. Disable etcd port
2. wait 1 minute
3. got failed to update lease, error: etcdserver: too many requests in kubelet log
4. got ""watch of * v1. Pod ended with very short watch: k8s. io/kubernetes/pkg/kubelet/configure/apiserver. go: 67: Unexpected watch close - watch last less than a second and no items received" in kubelet log
5. the kubelet list pod/node resource from apiserver.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
kubelet --version
Kubernetes v1.20.13-266+1d34e1abb653af-dirty
```

</details>


#### Cloud provider

<details>
nothing
</details>


#### OS version

<details>

```console
# On Linux:
CentOS Linux release 8.2.2004 (Core)
# paste output here
$ uname -a
4.18.0-193.6.3.el8_2.v1.2.x86_64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122902 sidecar container readniess result to be used in Pod ready state also for pod.spec.restartPolicy = Never or OnFailure

- Issue 链接：[#122902](https://github.com/kubernetes/kubernetes/issues/122902)

### Issue 内容

#### What happened?

I created a pod with sidecar container with following manifest:
```
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: webapp
spec:
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoSchedule"
  initContainers:
    - name: init-service
      image:  busybox
      command:
        - sleep
        - 5s
    - name: sidecar
      image:  busybox
      command:
        - sleep
        - 30s
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
      readinessProbe:
        exec:
          command:
          - cat
        initialDelaySeconds: 5
        periodSeconds: 5
      imagePullPolicy: IfNotPresent
      restartPolicy: Always
  containers:
  - name: main-application
    image: nginx
    volumeMounts:
      - name: shared-logs
        mountPath: /var/log/nginx
    resources:
      requests:
        memory: "100Mi"
        cpu: "0.5"
  volumes:
    - name: shared-logs
      emptyDir: {}
  restartPolicy: Never

```

watch the pod readiness. 
```
#before the sidecar container be restarted , the readiness is 2/2
watch kubectl  get pod
Every 2.0s: kubectl get pod                                                     

NAME                    READY   STATUS    RESTARTS   AGE
simple-webapp           2/2     Running   0          36s

#after the sidecar container be restarted, the pod readiness is 1/2, sidecar readiness result is not be used to the pod. 
NAME                    READY   STATUS    RESTARTS   AGE
simple-webapp           1/2     Running   2          2m15s

```
in [Sidecar containers and Pod lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle), it's mentioned "If a readinessProbe is specified for this init container, its result will be used to determine the ready state of the Pod."

it's true only for pod.spec.restartPolicy = Always, sidecar container readniess result will be used in Pod ready state, neither "Never" or "OnFailure".

I tested with v1.29.0

#### What did you expect to happen?

I expect "If a readinessProbe is specified for this init container, its result will be used to determine the ready state of the Pod." also applicable to  pod.spec.restartPolicy = "Never" or "OnFailure".

#### How can we reproduce it (as minimally and precisely as possible)?

Set pod.spec.restartPolicy = "Never" or "OnFailure".

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux node-10-210-171-200 5.15.0-87-generic #97-Ubuntu SMP Mon Oct 2 21:09:21 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
 containerd --version
containerd github.com/containerd/containerd v1.7.11 64b8a811b07ba6288238eefc14d898ee0b5b99ba

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122899 kube-proxy healthcheck all zero listening

- Issue 链接：[#122899](https://github.com/kubernetes/kubernetes/issues/122899)

### Issue 内容

#### What happened?

I have deployed a Kubernetes cluster with version 1.28 in a dual-stack environment. The main configuration for kube-proxy is as follows:
```yaml
...
--cluster-cidr=172.22.0.0/16,1111::/48
--nodeport-addresses=192.168.30.0/24
...
```
Then, I deployed a service with healthCheckNodePort: 30955 and other settings as follows:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-deploy-svc
spec:
  healthCheckNodePort: 30955
  ipFamilyPolicy: PreferDualStack
  type: LoadBalancer
  externalTrafficPolicy: Local
...
```
However, port 30955 is listening on all zeros, as shown below:
```shell
tcp6       0      0 :::30955                :::*                    LISTEN      110720/kube-proxy
```
When I restarted kube-proxy, the all-zeros listening issue was resolved, and it now looks like this:
```shell
tcp        0      0 192.168.30.89:30955      0.0.0.0:*               LISTEN      25411/kube-proxy
```
I would like to know whether, in a dual-stack environment, to achieve non-all-zeros listening, both IPv4 and IPv6 address ranges must be configured for the `--nodeport-addresses` parameter. Otherwise, it may lead to the all-zeros listening issue, or it could be a kube-proxy bug. Is it related to the following code:
https://github.com/kubernetes/kubernetes/blob/a07b1aaa5b39b351ec8586de800baa5715304a3f/pkg/proxy/metaproxier/meta_proxier.go#L55-L58
Additionally, I believe that in a dual-stack environment, if `--nodeport-addresses` is configured with one IP address range and the other IP address range is not configured, then the other IP address range cannot have all-zeros listening. Otherwise, the already configured IP address range may lose the non-all-zeros listening.


#### What did you expect to happen?

In a dual-stack environment, if `--nodeport-addresses` is configured with one IP address range and the other IP address range is not configured, then the other IP address range cannot have all-zeros listening. 

#### How can we reproduce it (as minimally and precisely as possible)?

In a dual-stack environment, `--nodeport-addresses` is configured with only one IP address range.Additionally, a Service with healthCheckNodePort is created.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.28.3
```

</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122895  PVC storage allocation erroneously assigns the entire size of the PV instead of the specified size.

- Issue 链接：[#122895](https://github.com/kubernetes/kubernetes/issues/122895)

### Issue 内容

#### What happened?

I created a PV using below manifest file - where PV size is 120Gi

`kubectl apply -f pv.yml`

root@control-plane:/home/vboxuser# cat pv.yml 
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 120Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Delete
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /nfs/data
    server: 192.168.56.1
```

Now I would like to create a PVC using with size 2Gi which is not happening and is creating 120 Gi of PVC

`kubectl apply -f pvc.yml`

root@control-plane:/home/vboxuser# cat pvc.yml 
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
  volumeMode: Filesystem
  volumeName: nfs-pv
```
when I do get pvc, I see its creating 120Gi of size which is not the desired state.

```bash
root@control-plane:/home/vboxuser# kubectl get pvc
NAME       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
test-pvc   Bound    nfs-pv   120Gi      RWX                           <unset>                 19h
root@control-plane:/home/vboxuser# 
```


#### What did you expect to happen?

I supposed to see a 2Gi of PVC created instead 120Gi

#### How can we reproduce it (as minimally and precisely as possible)?

When I tried to re-size to 2Gi using `kubectl edit` command and I see it as 2Gi only but the status shows 120Gi

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"test-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteMany"],"resources":{"requests":{"storage":"2Gi"}},"volumeMode":"Filesystem","volumeName":"nfs-pv"}}
    pv.kubernetes.io/bind-completed: "yes"
  creationTimestamp: "2024-01-20T15:37:02Z"
  finalizers:
  - kubernetes.io/pvc-protection
  name: test-pvc
  namespace: default
  resourceVersion: "541821"
  uid: 4b95bfa3-d631-4d1a-84f9-ed63ce807b66
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
  volumeMode: Filesystem
  volumeName: nfs-pv
status:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 120Gi
  phase: Bound
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0

```

</details>


#### Cloud provider

<details>
On-prem
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122835 Accessing NodePort on K8S host has latency when reaching Pods across different hosts

- Issue 链接：[#122835](https://github.com/kubernetes/kubernetes/issues/122835)

### Issue 内容

#### What happened?

I have deployed a Kubernetes cluster on Debian 12.4 operating system. I am using a test image, exposing a port to obtain the container's name. I use this method to determine which Pod the request is assigned to.

As shown in the screenshot below, accessing the Pod on the local machine works fine. However, if the request is assigned to a Pod on another host, there is a 1-second delay before receiving a response. Upon inspecting the packets using tcpdump, I observed that the first request resulted in a connection reset, and I am not quite sure about the cause of this issue.

![image](https://github.com/kubernetes/kubernetes/assets/104751498/d2fb1b0e-d2ea-4627-b691-9d6bfbc1398f)

![image](https://github.com/kubernetes/kubernetes/assets/104751498/84aa407e-4387-47fe-94f1-384950f0909a)



#### What did you expect to happen?

Deployed Kubernetes environment is functioning properly on CentOS 7.9 operating system.

![image](https://github.com/kubernetes/kubernetes/assets/104751498/7f9ed3e6-c0be-4458-b6fd-b6c05afa6842)


#### How can we reproduce it (as minimally and precisely as possible)?

I can reproduce it on Debian 12, Debian 11, and Ubuntu 22.04.

Here is my YAML file.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: centos:7.9.2009
        command:
        - sh
        - -c
        - |
          echo $(hostname) > hostname.txt
          python -m SimpleHTTPServer
        resources:
          limits:
            memory: 512Mi
            cpu: 1
          requests:
            memory: 64Mi
            cpu: 0.01
        volumeMounts:
        - name: tz-config
          mountPath: /etc/localtime
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Etc/GMT-8

---

apiVersion: v1
kind: Service
metadata:
  name: test
  namespace: default
spec:
  selector:
    app: test
  ports:
  - name: external-test
    port: 8000
    targetPort: 8000
    nodePort: 8000
  type: NodePort

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:29:58Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}

```
</details>


#### Cloud provider

<details>
Aliyun
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux debian-1 6.1.0-16-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.67-1 (2023-12-12) x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd -v
containerd containerd.io 1.6.27 a1496014c916f9e62104b33d1bb5bd03b0858e59

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
mirrored-flannelcni-flannel:v0.20.1
</details>


### 分析结果

不涉及

---

## Issue #122824 Readiness probes are called even when pod is in terminating state

- Issue 链接：[#122824](https://github.com/kubernetes/kubernetes/issues/122824)

### Issue 内容

#### What happened?

When i deleted a pod using readiness probe saw below events

Events:
  Type     Reason     Age                 From     Message
  ----     ------     ----                ----     -------
  Normal   Killing    3m3s                kubelet  Stopping container xxxxxxx
  Warning  Unhealthy  35s (x10 over 14d)  kubelet  Readiness probe failed:


#### What did you expect to happen?

Based on the issue https://github.com/kubernetes/kubernetes/issues/52817 and PR https://github.com/kubernetes/kubernetes/pull/98571 I expected readiness probe should not be called for a terminating pod similar to startup and liveness probe. Any specific reason we did not include readiness probe in this check.

	// Graceful shutdown of the pod.
	if w.pod.ObjectMeta.DeletionTimestamp != nil && (w.probeType == liveness || w.probeType == startup) {
		klog.V(3).Infof("Pod deletion requested, setting %v probe result to success: %v - %v",
			w.probeType.String(), format.Pod(w.pod), w.container.Name)
		if w.probeType == startup {
			klog.Warningf("Pod deletion requested before container has fully started: %v - %v",
				format.Pod(w.pod), w.container.Name)
		}
		// Set a last result to ensure quiet shutdown.
		w.resultsManager.Set(w.containerID, results.Success, w.pod)

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod using readiness probe and a preStop hook so that deletion takes time. Delete the pod. You can still see readiness probe getting called for this pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:29:58Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here

NAME="Red Hat Enterprise Linux"
VERSION="8.6 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.6"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.6 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/red_hat_enterprise_linux/8/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.6
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.6"


$ uname -a
# paste output here

Linux infdevlnx001 4.18.0-372.32.1.el8_6.x86_64 #1 SMP Fri Oct 7 12:35:10 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122805 Watch-List feature does not work as expected when ResourceVersion is not set.

- Issue 链接：[#122805](https://github.com/kubernetes/kubernetes/issues/122805)

### Issue 内容

#### What happened?

Kube-apiserver with [Watch-List](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/3157-watch-list#design-details) feature enabled does not send back `k8s.io/initial-events-end` BOOKMARK to client when  resourceVersion is not set.

```bash
curl 'http://localhost:8001/api/v1/namespaces/default/pods?allowWatchBookmarks=true&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&timeoutSeconds=565&watch=true' -v 8
* About to connect() to localhost port 8001 (#0)
*   Trying ::1...
* Connected to localhost (::1) port 8001 (#0)
> GET /api/v1/namespaces/default/pods?allowWatchBookmarks=true&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&timeoutSeconds=565&watch=true HTTP/1.1
> User-Agent: curl/7.29.0
> Host: localhost:8001
> Accept: */*
> 
< HTTP/1.1 200 OK
< Audit-Id: 7234c88c-a068-4479-8c4e-7780d252ba66
< Cache-Control: no-cache, private
< Content-Type: application/json
< Date: Tue, 16 Jan 2024 08:25:17 GMT
< X-Kubernetes-Pf-Flowschema-Uid: c164aa1f-bd17-4fd7-8b8e-ca061207c6dc
< X-Kubernetes-Pf-Prioritylevel-Uid: 71b16c3c-cc3a-4485-adce-76c22bb29aad
< Transfer-Encoding: chunked
< 
```

and when resourceVersion is randomly set as seen in the following, the response from kube-apiserver does have the designed BOOKMARK.

```bash
curl 'http://localhost:8001/api/v1/namespaces/default/pods?allowWatchBookmarks=true&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&timeoutSeconds=565&watch=true&resourceVersion=1000' -v 8
* About to connect() to localhost port 8001 (#0)
*   Trying ::1...
* Connected to localhost (::1) port 8001 (#0)
> GET /api/v1/namespaces/default/pods?allowWatchBookmarks=true&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&timeoutSeconds=565&watch=true&resourceVersion=1000 HTTP/1.1
> User-Agent: curl/7.29.0
> Host: localhost:8001
> Accept: */*
> 
< HTTP/1.1 200 OK
< Audit-Id: eaf9a084-642c-444c-882f-dd2d5d4ced3d
< Cache-Control: no-cache, private
< Content-Type: application/json
< Date: Tue, 16 Jan 2024 08:25:27 GMT
< X-Kubernetes-Pf-Flowschema-Uid: c164aa1f-bd17-4fd7-8b8e-ca061207c6dc
< X-Kubernetes-Pf-Prioritylevel-Uid: 71b16c3c-cc3a-4485-adce-76c22bb29aad
< Transfer-Encoding: chunked
< 
{"type":"BOOKMARK","object":{"kind":"Pod","apiVersion":"v1","metadata":{"resourceVersion":"3089","creationTimestamp":null,"annotations":{"k8s.io/initial-events-end":"true"}},"spec":{"containers":null},"status":{}}}
```

Is this phenomenon a bug to this alpha phase [feature](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/3157-watch-list#design-details) or just by design?

#### What did you expect to happen?

send back  `k8s.io/initial-events-end` BOOKMARK to client as well when resource version is not set in request parameter.

#### How can we reproduce it (as minimally and precisely as possible)?

use kubectl proxy to connect apiserver
```bash
kubectl proxy &
```
and then curl
```
curl 'http://localhost:8001/api/v1/namespaces/default/pods?allowWatchBookmarks=true&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&timeoutSeconds=565&watch=true' -v 8
```

#### Anything else we need to know?

etcd version is 3.5.11

#### Kubernetes version

<details>

```console
$ kubectl version
I have tried k&k with tag of v1.28.4, v1.29.0 and master branch.
```

</details>


#### Cloud provider

<details>
bare-metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122803 PostStart is never executed

- Issue 链接：[#122803](https://github.com/kubernetes/kubernetes/issues/122803)

### Issue 内容

#### What happened?

If the kubelet is restarted when a pod with a postStart lifecycle hook is started but poststart still not be executed, then the postStart hook will never be executed when the kubelet start.

#### What did you expect to happen?

when kubelet restart, pod's postStart hook can always be run.

#### How can we reproduce it (as minimally and precisely as possible)?

I'm not quite clear on how to reproduce this; it might require some luck.
However, maybe we can reproduce it by adding a sleep before executing postStart in the code.Then we can restart kubelet when container has run.
```golang
	// Step 4: execute the post start hook.
	if container.Lifecycle != nil && container.Lifecycle.PostStart != nil {
		kubeContainerID := kubecontainer.ContainerID{
			Type: m.runtimeName,
			ID:   containerID,
		}
		msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)
		if handlerErr != nil {
			klog.ErrorS(handlerErr, "Failed to execute PostStartHook", "pod", klog.KObj(pod),
				"podUID", pod.UID, "containerName", container.Name, "containerID", kubeContainerID.String())
			m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg)
			if err := m.killContainer(pod, kubeContainerID, container.Name, "FailedPostStartHook", reasonFailedPostStartHook, nil); err != nil {
				klog.ErrorS(err, "Failed to kill container", "pod", klog.KObj(pod),
					"podUID", pod.UID, "containerName", container.Name, "containerID", kubeContainerID.String())
			}
			return msg, ErrPostStartHook
		}
	}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122777 CEL has misleading error message for fields called "namespace"

- Issue 链接：[#122777](https://github.com/kubernetes/kubernetes/issues/122777)

### Issue 内容

#### What happened?

See the CRD at https://gist.github.com/mattcary/353b202240c250b5cb4371058f233476

in summary it is

```
spec:
  ref:
    name: myName
    namespace: myNamespace
```

One can create a x-kubernetes-validations rule that spec.ref.name != "", but one cannot create a rule that spec.ref.namespace != "". The former applies successfully, but applying the latter (the gist as written now):

```
The CustomResourceDefinition "validations.mattcary.info" is invalid: spec.validation.openAPIV3Schema.properties[spec].x-kubernetes-validations[0].rule: Invalid value: apiextensions.ValidationRule{Rule:"self.ref.namespace != \"\"", Message:"", MessageExpression:"", Reason:(*apiextensions.FieldValueErrorReason)(nil), FieldPath:"", OptionalOldSelf:(*bool)(nil)}: compilation failed: ERROR: <input>:1:9: undefined field 'namespace'
 | self.ref.namespace != ""
 | ........^
```

#### What did you expect to happen?

The error message should say improper use of reserved word "namespace" rather than say that the field does not exist.

#### How can we reproduce it (as minimally and precisely as possible)?

Save the gist https://gist.github.com/mattcary/353b202240c250b5cb4371058f233476 to a file, then `kubectl apply -f FILENAME`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0-gke.1492000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
ID=debian
HOME_URL="https://go/glinux"
SUPPORT_URL="https://go/techstop"
BUG_REPORT_URL="https://go/techstop"
$ uname -a
Linux mattcary.c.googlers.com 6.5.13-1rodete1-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.5.13-1rodete1 (2023-12-06) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122766 kubectl fails to aggregate cluster role selectors when specifying multiple --aggregation-rule flags

- Issue 链接：[#122766](https://github.com/kubernetes/kubernetes/issues/122766)

### Issue 内容

#### What happened?

I've created 3 cluster roles, one that has read access to pods, one that has access to delete pods and one that will aggregate the rules between the first two. When creating the aggregator role with imperative commands, the resulted role fails to aggregate the permissions properly even with separate arguments for the aggregation rules, the issue happens because the resulted selector seems to look for roles that have a combination of all the labels instead of roles that have either of the labels. Editing the cluster role selector to look for roles that have either of the labels seems to fix the issue.

#### What did you expect to happen?

Creating the cluster role that aggregates the rules should work properly with imperative commands without me having to manually edit the resource.

#### How can we reproduce it (as minimally and precisely as possible)?

This was a step by step process of me reproducing the issue:

```
kind-learning/default  k create clusterrole test --verb=get,watch,list --resource=pods                                                                          
clusterrole.rbac.authorization.k8s.io/test created

kind-learning/default  k create clusterrole test2 --verb=delete --resource=pods                                                                                 
clusterrole.rbac.authorization.k8s.io/test2 created

kind-learning/default  k label clusterrole test reader=true                                                                                                     
clusterrole.rbac.authorization.k8s.io/test labeled

kind-learning/default  k label clusterrole test2 delete=true                                                                                                    
clusterrole.rbac.authorization.k8s.io/test2 labeled

kind-learning/default  k describe clusterrole test                                                                                                              
Name:         test
Labels:       reader=true
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [get watch list]

kind-learning/default  k describe clusterrole test2                                                                                                             
Name:         test2
Labels:       delete=true
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [delete]

kind-learning/default  k create clusterrole test3 --aggregation-rule=reader=true --aggregation-rule=delete=true                                                
clusterrole.rbac.authorization.k8s.io/test3 created

kind-learning/default  k describe clusterrole test3                                                                                                             
Name:         test3
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
```

The issue lies in the way the cluster role is generated:

```
kind-learning/default  k get clusterrole test3 -o yaml                                                                                                          
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      delete: "true"
      reader: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: "2024-01-14T14:19:25Z"
  name: test3
  resourceVersion: "1508"
  uid: b138a790-d2b0-4bae-9e1f-7d51179599ec
rules: null
```

The first matchLabels rule will look for roles that have both the delete=true and reader=true labels, but instead these should be separate rules, as specified in the creation of the clusterrole called test3 with two separate --aggregation-rule arguments.

Editing the cluster role to have two separate matchLabels selectors fixes the issue:

```
kind-learning/default  k edit clusterrole test3                                                                                                                 
clusterrole.rbac.authorization.k8s.io/test3 edited

kind-learning/default  k get clusterrole test3 -o yaml                                                                                                   
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      delete: "true"
  - matchLabels:
      reader: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: "2024-01-14T14:19:25Z"
  name: test3
  resourceVersion: "2066"
  uid: b138a790-d2b0-4bae-9e1f-7d51179599ec
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - watch
  - list

kind-learning/default  k describe clusterrole test3                                                                                                             
Name:         test3
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [delete get watch list]
```

The performed edit was this:

```
- matchLabels:
-     delete: "true"
-     reader: "true"
+ matchLabels:
+     delete: "true"
+ - matchLabels:
+     reader: "true"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
No cloud provider, I am using Kind locally with the following configuration:

```
cat kind-cluster-config.yaml                                                                                                             
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
kubeadmConfigPatches:
- |
  apiVersion: kubelet.config.k8s.io/v1beta1
  kind: KubeletConfiguration
  evictionHard:
    nodefs.available: "0%"
kubeadmConfigPatchesJSON6902:
- group: kubeadm.k8s.io
  version: v1beta3
  kind: ClusterConfiguration
  patch: |
    - op: add
      path: /apiServer/certSANs/-
      value: my-hostname
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 8080
    protocol: TCP
  - containerPort: 443
    hostPort: 8443
    protocol: TCP
- role: worker
- role: worker
- role: worker
  extraPortMappings:
  - containerPort: 30080
    hostPort: 30080
    protocol: TCP
```
</details>


#### OS version

<details>

```console

$ uname -a
uname -a                                                                                                                              
Darwin Alexandrus-MacBook-Pro.local 21.6.0 Darwin Kernel Version 21.6.0: Wed Aug 10 14:28:23 PDT 2022; root:xnu-8020.141.5~2/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122755 Mutation webhook fails with unsupported patch type error

- Issue 链接：[#122755](https://github.com/kubernetes/kubernetes/issues/122755)

### Issue 内容

#### What happened?

Patch operations on the custom resource fails when I use a mutation webhook to add an annotation. I expected "application/json-patch+json" patch type to work for mutation webhooks.

error: mycustomresources.sample.com "mycustomresource" could not be patched: Internal error occurred: failed calling webhook "mutation-webhook.myns.svc.cluster.local": unsupported patch type "application/json-patch+json"

#### What did you expect to happen?

Expected kube-apiserver to apply the annotation on the custom resource.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a CRD
Create a Custom resource
Write a mutation webhook that simply adds an annotation to the CR.
Update the custom resource using `kubectl edit`.

#### Anything else we need to know?

If I do not add any annotations to the JSONPatch in my webhook code, things work fine.

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.7
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122740 UDP connections of Services are not preserved after restarting kube-proxy

- Issue 链接：[#122740](https://github.com/kubernetes/kubernetes/issues/122740)

### Issue 内容

#### What happened?

When running kube-proxy with log level 4, I found it always clears all UDP contrack entries for all service IPs and NodePorts unconditionally after it's restarted even when the Services and Endpoints/EndpointSlices don't change.

Clearing UDP conntrack entries could cause UDP connections to switch to other endpoints. Although UDP is stateless, endpoint change may unnecessarily disrupt connections for some protocols built on UDP, e.g. QUIC. 

From the code, the reason why these UDP conntrack entries need to be deleted seems to be: if service traffic towards a service IP arrive before the iptables/nftables rules for the services IP, it could produce a conntrack entry which will keep black hole subsequent packets after the iptables/nftables rules are installed. And it uses zero endpoints -> non-zero endpoints change as the trigger for the cleanup. However, when kube-proxy restarts, in the first sync, all services will meet the condition, so all UDP conntrack entries of services are deleted.

After investigating what's the "black hole" conntrack entry, I realize it may be just a normal conntrack entry with no destination IP translation. When a packet to a service IP arrives, if there is no DNAT rule for it (because iptables/nftables rules are not installed yet), it will likely be forwarded to the primary network interface by matching the default route, creating a conntrack entry with no destination IP translation, which causes subsequent packets bypass NAT table.

The log after kube-proxy starts:
```
2024-01-08T11:10:41.453415412Z stderr F I0108 11:10:41.453282       1 proxier.go:951] "Syncing nftables rules"
2024-01-08T11:10:41.486367354Z stderr F I0108 11:10:41.486233       1 proxier.go:1552] "Reloading service nftables data" numServices=218 numEndpoints=225
2024-01-08T11:10:41.584646255Z stderr F I0108 11:10:41.584466       1 cleanup.go:51] "Newly-active UDP service may have stale conntrack entries" servicePortName="kube-system/kube-dns:dns"
2024-01-08T11:10:41.584672722Z stderr F I0108 11:10:41.584498       1 cleanup.go:51] "Newly-active UDP service may have stale conntrack entries" servicePortName="conntrack-7585/svc-udp:udp"
2024-01-08T11:10:41.58468002Z stderr F I0108 11:10:41.584516       1 cleanup.go:66] "Deleting conntrack stale entries for services" IPs=["10.96.0.10","10.96.216.235"]
2024-01-08T11:10:41.584711168Z stderr F I0108 11:10:41.584585       1 conntrack.go:69] "Clearing conntrack entries" parameters=["-D","--orig-dst","10.96.0.10","-p","udp"]
2024-01-08T11:10:41.616062012Z stderr F I0108 11:10:41.615350       1 conntrack.go:74] "Conntrack entries deleted" output=<
2024-01-08T11:10:41.616083667Z stderr F 	udp      17 26 src=10.244.1.63 dst=10.96.0.10 sport=49756 dport=53 src=10.244.0.3 dst=10.244.1.63 sport=53 dport=49756 mark=0 use=1
2024-01-08T11:10:41.616142772Z stderr F 	udp      17 11 src=10.244.1.63 dst=10.96.0.10 sport=42236 dport=53 src=10.244.0.3 dst=10.244.1.63 sport=53 dport=42236 mark=0 use=1
2024-01-08T11:10:41.616148744Z stderr F 	udp      17 14 src=10.244.1.62 dst=10.96.0.10 sport=53756 dport=53 src=10.244.0.3 dst=10.244.1.62 sport=53 dport=53756 mark=0 use=1
2024-01-08T11:10:41.616153535Z stderr F 	udp      17 12 src=10.244.1.62 dst=10.96.0.10 sport=49317 dport=53 src=10.244.0.4 dst=10.244.1.62 sport=53 dport=49317 mark=0 use=1
2024-01-08T11:10:41.616180643Z stderr F 	udp      17 13 src=10.244.1.63 dst=10.96.0.10 sport=41286 dport=53 src=10.244.0.4 dst=10.244.1.63 sport=53 dport=41286 mark=0 use=1
2024-01-08T11:10:41.616197187Z stderr F 	udp      17 16 src=10.244.1.63 dst=10.96.0.10 sport=36633 dport=53 src=10.244.0.4 dst=10.244.1.63 sport=53 dport=36633 mark=0 use=1
2024-01-08T11:10:41.616202282Z stderr F 	udp      17 14 src=10.244.1.63 dst=10.96.0.10 sport=47472 dport=53 src=10.244.0.3 dst=10.244.1.63 sport=53 dport=47472 mark=0 use=1
2024-01-08T11:10:41.616207641Z stderr F 	udp      17 10 src=10.244.1.63 dst=10.96.0.10 sport=38918 dport=53 src=10.244.0.4 dst=10.244.1.63 sport=53 dport=38918 mark=0 use=1
2024-01-08T11:10:41.616212369Z stderr F 	udp      17 29 src=10.244.1.63 dst=10.96.0.10 sport=44212 dport=53 src=10.244.0.4 dst=10.244.1.63 sport=53 dport=44212 mark=0 use=1
2024-01-08T11:10:41.616217015Z stderr F 	udp      17 0 src=10.244.1.59 dst=10.96.0.10 sport=59289 dport=53 src=10.244.0.3 dst=10.244.1.59 sport=53 dport=59289 mark=0 use=1
```

/sig network
/area kube-proxy


#### What did you expect to happen?

Restarting kube-proxy should preserve existing UDP connections of Services when they and their endpoints don't change.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a UDP Service backed by several Pods running UDP server.
2. Connect to the UDP service and keep sending requests to maintain the connection, record which backend Pod it connects to.
3. Restart kube-proxy on the Node which the UDP connection originates from.
4. Check the connction again, it may be forwarded to a different backend Pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.29.0 (but it seems to exist for earlier versions too)
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122736 Printing CLI parameters for Kubelet is misleading to users.

- Issue 链接：[#122736](https://github.com/kubernetes/kubernetes/issues/122736)

### Issue 内容

#### What happened?

While working on https://github.com/kubernetes/kubernetes/pull/122241, I ran into an issue where the CLI was reporting that `--fail-swap-on="true"`. But in the code it was actually set to "false". This is because the flag was set in the KubeletConfig.

We have users also noticing that the logs in Kubelet for CLI don't report the actual value as they are actually being read from the KubeletConfig.

#### What did you expect to happen?

We should bring the values of these configurations that they are rather than the CLI fields.

#### How can we reproduce it (as minimally and precisely as possible)?

Log at logs for a CLI parameter (swap, eviction, etc) and change those values in the KubeletConfig. 

Kubelet logs will display the CLI parameter rather than the KubeletConfig value.

#### Anything else we need to know?

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/

CLI parameters are deprecated and replaced for KubeletConfig but I'm not sure if we bring the actual values in the logs.

#### Kubernetes version

master

#### Cloud provider

NA


#### OS version

NA

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

Container/CRIO

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122709 RecreatingTerminatedPod/RecreatingFailedPod/SuccessfulDelete events leak

- Issue 链接：[#122709](https://github.com/kubernetes/kubernetes/issues/122709)

### Issue 内容

#### What happened?

The event `RecreatingTerminatedPod` is emitted ~10 times simultaneously after evicting pods of a Statefulset.



#### What did you expect to happen?

The event should only be emitted at most once.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a 1.29 kind cluster:
```
kind create cluster --image kindest/node:v1.29
```
 (tested with `sha256:eaa1450915475849a73a9227b8f201df25e55e268e5d619312131292e324d570`)

- Deploy a statefulset:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
```

- Get the initial `RecreatingTerminatedPod` event count for that sts:
```
kubectl get events  --field-selector involvedObject.kind=StatefulSet,involvedObject.name=web,reason=RecreatingTerminatedPod -o json | jq '.items[].count'
```
(shoud return nothing)

- Taint the node:
```
kubectl taint nodes kind-control-plane foo=bar:NoExecute
```

- Get the event count now:
```
kubectl get events  --field-selector involvedObject.kind=StatefulSet,involvedObject.name=web,reason=RecreatingTerminatedPod -o json | jq '.items[].count'
```
(got: `10`, `11`,`9`...)

#### Anything else we need to know?

From the kube-controller logs:

```
2024-01-11T11:26:09.134534094Z I0111 11:26:09.134420       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.135572053Z I0111 11:26:09.135506       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.141711678Z E0111 11:26:09.141587       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.142157720Z I0111 11:26:09.142069       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.143250511Z I0111 11:26:09.143147       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.145130470Z E0111 11:26:09.145043       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.145508511Z I0111 11:26:09.145440       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.146506011Z I0111 11:26:09.146423       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.150412678Z E0111 11:26:09.150314       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.150731261Z I0111 11:26:09.150652       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.151777928Z I0111 11:26:09.151690       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.154159220Z E0111 11:26:09.154073       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.154551136Z I0111 11:26:09.154479       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.155519386Z I0111 11:26:09.155424       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.157276011Z E0111 11:26:09.157186       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.171016053Z I0111 11:26:09.170929       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.172533345Z I0111 11:26:09.172453       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.175124386Z E0111 11:26:09.175049       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.337640386Z I0111 11:26:09.337476       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.339178428Z I0111 11:26:09.339061       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.341355011Z E0111 11:26:09.341228       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.664705011Z I0111 11:26:09.664319       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.668555303Z I0111 11:26:09.668306       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.676319053Z E0111 11:26:09.676042       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.677364720Z I0111 11:26:09.677120       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.679484886Z I0111 11:26:09.679242       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.684279428Z E0111 11:26:09.684135       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.688194220Z I0111 11:26:09.687885       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.690645803Z I0111 11:26:09.690430       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.695824386Z E0111 11:26:09.695522       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.696631386Z I0111 11:26:09.696397       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.698303595Z I0111 11:26:09.698136       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.701026845Z E0111 11:26:09.700893       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: object is being deleted: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.701639136Z I0111 11:26:09.701494       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="RecreatingTerminatedPod" message="StatefulSet default/web is recreating terminated Pod web-0"
2024-01-11T11:26:09.703167928Z I0111 11:26:09.703010       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"
2024-01-11T11:26:09.708887345Z E0111 11:26:09.708730       1 stateful_set.go:430] error syncing StatefulSet default/web, requeuing: pods "web-0" already exists, the server was not able to generate a unique name for the object
2024-01-11T11:26:09.711943720Z I0111 11:26:09.711700       1 event.go:376] "Event occurred" object="default/web" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod web-0 in StatefulSet web successful"
```

The deletion of the Pod fails which make the controller enqueue the sts again and ask for another deletion and so on until the Pod is really deleted from the store.


`fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="delete Pod web-0 in StatefulSet web successful"` is confusing as well.

- The same problem occurs with the event `RecreatingFailedPod`, deploy the statefulet:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        command:
        - /bin/sh
        - -c
        - |
          sleep 3;
          exit 1
        ports:
        - containerPort: 80
          name: web
```

When the Pods start `CrashLoopBackOff`, delete them, then ~10 `RecreatingFailedPod` events will be emitted.

- A simple update to the statefulset's spec also triggers a bulk of RecreatingTerminatedPod events.

- the `SuccessfulDelete` event leaks as well.

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.0", GitCommit:"3f7a50f38688eb332e2a1b013678c6435d539ae6", GitTreeState:"clean", BuildDate:"2023-12-14T19:20:14Z", GoVersion:"go1.21.5", Compiler:"gc", Platform:"linux/arm64"}
```

</details>

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### 分析结果

不涉及

---

## Issue #122686 kubelet no container metric issues

- Issue 链接：[#122686](https://github.com/kubernetes/kubernetes/issues/122686)

### Issue 内容

#### What happened?

```shell
[root@app-k8s-master-1 ~]# journalctl -u kubelet -f
Jan 10 22:41:24 app-k8s-master-1 kubelet[12421]: E0110 22:41:24.391540   12421 cadvisor_stats_provider.go:415] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/system.slice/kubelet.service\": RecentStats: unable to find data in memory cache], [\"/system.slice/docker.service\": RecentStats: unable to find data in memory cache], [\"/kubepods.slice/kubepods-besteffort.slice\": RecentStats: unable to find data in memory cache]"
Jan 10 22:41:27 app-k8s-master-1 kubelet[12421]: E0110 22:41:27.901155   12421 cadvisor_stats_provider.go:415] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/system.slice/docker.service\": RecentStats: unable to find data in memory cache], [\"/system.slice/kubelet.service\": RecentStats: unable to find data in memory cache], [\"/kubepods.slice/kubepods-besteffort.slice\": RecentStats: unable to find data in memory cache]"
Jan 10 22:41:27 app-k8s-master-1 kubelet[12421]: E0110 22:41:27.901220   12421 summary_sys_containers.go:82] "Failed to get system container stats" err="failed to get cgroup stats for \"/system.slice/kubelet.service\": failed to get container info for \"/system.slice/kubelet.service\": partial failures: [\"/system.slice/kubelet.service\": RecentStats: unable to find data in memory cache]" containerName="/system.slice/kubelet.service"
Jan 10 22:41:27 app-k8s-master-1 kubelet[12421]: E0110 22:41:27.901242   12421 summary_sys_containers.go:82] "Failed to get system container stats" err="failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": partial failures: [\"/system.slice/docker.service\": RecentStats: unable to find data in memory cache]" containerName="/system.slice/docker.service"
Jan 10 22:41:27 app-k8s-master-1 kubelet[12421]: E0110 22:41:27.901289   12421 cadvisor_stats_provider.go:415] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-besteffort.slice\": RecentStats: unable to find data in memory cache], [\"/system.slice/kubelet.service\": RecentStats: unable to find data in memory cache], [\"/system.slice/docker.service\": RecentStats: unable to find data in memory cache]"
Jan 10 22:41:27 app-k8s-master-1 kubelet[12421]: E0110 22:41:27.901342   12421 summary_sys_containers.go:82] "Failed to get system container stats" err="failed to get cgroup stats for \"/system.slice/kubelet.service\": failed to get container info for \"/system.slice/kubelet.service\": partial failures: [\"/system.slice/kubelet.service\": RecentStats: unable to find data in memory cache]" containerName="/system.slice/kubelet.service"

[root@app-k8s-master-1 ~]# journalctl -u kubelet -f | grep -i Prometheus
Jan 10 22:54:48 app-k8s-master-1 kubelet[12421]: W0110 22:54:48.274346   12421 prometheus.go:1856] Couldn't get containers: partial failures: ["/kubepods.slice/kubepods-besteffort.slice": containerDataToContainerInfo: unable to find data in memory cache], ["/system.slice/kubelet.service": containerDataToContainerInfo: unable to find data in memory cache], ["/system.slice/docker.service": containerDataToContainerInfo: unable to find data in memory cache]
Jan 10 22:54:51 app-k8s-master-1 kubelet[12421]: W0110 22:54:51.526487   12421 prometheus.go:1856] Couldn't get containers: partial failures: ["/system.slice/kubelet.service": containerDataToContainerInfo: unable to find data in memory cache], ["/system.slice/docker.service": containerDataToContainerInfo: unable to find data in memory cache], ["/kubepods.slice/kubepods-besteffort.slice": containerDataToContainerInfo: unable to find data in memory cache]


```

#### What did you expect to happen?

Prometheus is able to collect monitoring indicators



#### How can we reproduce it (as minimally and precisely as possible)?

No response

#### Anything else we need to know?

```shell
[root@app-k8s-master-1 ~]# cat /etc/kubernetes/kubelet-conf.yml
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 10%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s

[root@app-k8s-master-1 ~]# cat /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --config=/etc/kubernetes/kubelet-conf.yml --pod-infra-container-image=k8s.gcr.io/pause:3.1 --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cert-dir=/etc/kubernetes/pki --cgroup-driver=systemd --v=2

CPUAccounting=true
MemoryAccounting=true
Restart=always
RestartSec=10s
CPUAccounting=true
MemoryAccounting=true
[Install]
WantedBy=multi-user.target

```

#### Kubernetes version

```shell
[root@app-k8s-master-1 ~]# kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.3", GitCommit:"ca643a4d1f7bfe34773c74f79527be4afd95bf39", GitTreeState:"clean", BuildDate:"2021-07-15T21:04:39Z", GoVersion:"go1.16.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.3", GitCommit:"ca643a4d1f7bfe34773c74f79527be4afd95bf39", GitTreeState:"clean", BuildDate:"2021-07-15T20:59:07Z", GoVersion:"go1.16.6", Compiler:"gc", Platform:"linux/amd64"}

```



#### Cloud provider

<details>

</details>


#### OS version


```console
[root@app-k8s-master-1 ~]# cat /etc/issue

Authorized users only. All activities may be monitored and reported.
[root@app-k8s-master-1 ~]# uname -a
Linux app-k8s-master-1 5.10.0-153.37.0.114.oe2203sp2.x86_64 #1 SMP Thu Dec 28 10:18:02 CST 2023 x86_64 x86_64 x86_64 GNU/Linux

```
openeuler 2203 sp2


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

```shell
[root@app-k8s-master-1 ~]# docker info
Client:
 Context:    default
 Debug Mode: false

Server:
 Containers: 6
  Running: 2
  Paused: 0
  Stopped: 4
 Images: 8
 Server Version: 20.10.24
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 2806fc1057397dbaeefbea0e4e17bddfbd388f38
 runc version: v1.1.5-0-gf19387a6
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 5.10.0-153.37.0.114.oe2203sp2.x86_64
 Operating System: openEuler 22.03 (LTS-SP2)
 OSType: linux
 Architecture: x86_64
 CPUs: 8
 Total Memory: 15.08GiB
 Name: app-k8s-master-1
 ID: WHXS:FWB6:UMT7:IJSF:ZD2F:PSVZ:JH2C:6SWG:NVIM:UF65:4AKU:ZGT5
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  172.26.67.9:39090
  127.0.0.0/8
 Registry Mirrors:
  https://docker.mirrors.ustc.edu.cn/
 Live Restore Enabled: false
 Product License: Community Engine


```


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122681 Ubuntu Remove Snap LXD + Vanilla Kubernetes + Restart = kubelet fails

- Issue 链接：[#122681](https://github.com/kubernetes/kubernetes/issues/122681)

### Issue 内容

#### What happened?

Ubuntu has a package manager called snapd.  I do not use snaps, so I decided to remove it.
After removing snapd, kubelet fails to start.
Update: Snap runs lxd which appears to be a dependency.

#### What did you expect to happen?

No impact as Kubernetes should not depend on snapd or lxd.

#### How can we reproduce it (as minimally and precisely as possible)?

Install Ubuntu 22.04
Install Kubernetes 1.29
Install cri-o 1.28

Use kubeadm init vanilla.
kubeadm init --apiserver-advertise-address=xxx --control-plane-endpoint=xxx --pod-network-cidr=10.244.0.0/16

Upon completion kubelet is running:
```
`● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Wed 2024-01-10 10:34:39 UTC; 20min ago`
```
Now remove snapd:
`apt-get autoremove -y --purge snapd && apt-mark hold snapd`

Or alternatively:
`snap remove --purge lxd`

Power off the machine (cold).
Power on the machine.

On boot, kubelet will no longer run:
```
`● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Wed 2024-01-10 11:12:46 UTC; 8s ago
       Docs: https://kubernetes.io/docs/
    Process: 1193 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (>
   Main PID: 1193 (code=exited, status=1/FAILURE)`
```

#### Anything else we need to know?

Kubelet journal logs:
```
Jan 10 11:13:47 dev-master systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 8.
Jan 10 11:13:47 dev-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Jan 10 11:13:47 dev-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Jan 10 11:13:47 dev-master kubelet[1565]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Jan 10 11:13:47 dev-master kubelet[1565]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.410924    1565 server.go:204] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.413602    1565 server.go:487] "Kubelet version" kubeletVersion="v1.29.0"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.413631    1565 server.go:489] "Golang settings" GOGC="" GOMAXPROCS="6" GOTRACEBACK=""
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.413773    1565 server.go:919] "Client rotation is on, will bootstrap in background"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.414836    1565 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.416533    1565 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.441677    1565 server.go:745] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.441893    1565 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442020    1565 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":"2Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":"10Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"memory.available","Operator":"LessThan","Value":{"Quantity":"100Mi","Percentage":0},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442053    1565 topology_manager.go:138] "Creating topology manager with none policy"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442062    1565 container_manager_linux.go:301] "Creating device plugin manager"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442086    1565 state_mem.go:36] "Initialized new in-memory state store"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442160    1565 kubelet.go:396] "Attempting to sync node with API server"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442171    1565 kubelet.go:301] "Adding static pod path" path="/etc/kubernetes/manifests"       
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442184    1565 kubelet.go:312] "Adding apiserver pod source"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442192    1565 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Jan 10 11:13:47 dev-master kubelet[1565]: W0110 11:13:47.442683    1565 reflector.go:539] vendor/k8s.io/client-go/informers/factory.go:159: failed to list *v1.Node: Get "https://10.255.0.100:6443/api/v1/nodes?fieldSelector=metadata.name%3Ddev-master&limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.442736    1565 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:159: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://10.255.0.100:6443/api/v1/nodes?fieldSelector=metadata.name%3Ddev-master&limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442771    1565 kuberuntime_manager.go:258] "Container runtime initialized" containerRuntime="cri-o" version="1.28.2" apiVersion="v1"
Jan 10 11:13:47 dev-master kubelet[1565]: W0110 11:13:47.442909    1565 reflector.go:539] vendor/k8s.io/client-go/informers/factory.go:159: failed to list *v1.Service: Get "https://10.255.0.100:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused    
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.442968    1565 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:159: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.255.0.100:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.442982    1565 kubelet.go:809] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.443281    1565 server.go:1256] "Started kubelet"
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.443723    1565 event.go:355] "Unable to write event (may retry after sleeping)" err="Post \"https://10.255.0.100:6443/api/v1/namespaces/default/events\": dial tcp 10.255.0.100:6443: connect: connection refused" event="&Event{ObjectMeta:{dev-master.17a8f814dc9135fa  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:dev-master,UID:dev-master,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:dev-master,},FirstTimestamp:2024-01-10 11:13:47.443271162 +0000 UTC m=+0.067505367,LastTimestamp:2024-01-10 11:13:47.443271162 +0000 UTC m=+0.067505367,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:dev-master,}" 
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.443749    1565 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.443765    1565 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.444183    1565 server.go:461] "Adding debug handlers to kubelet server"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.443752    1565 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.445311    1565 server.go:233] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.445369    1565 volume_manager.go:291] "Starting Kubelet Volume Manager"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.445618    1565 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.445841    1565 factory.go:221] Registration of the systemd container factory successfully      
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.445993    1565 factory.go:221] Registration of the crio container factory successfully
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.446026    1565 factory.go:219] Registration of the containerd container factory failed: unable to create containerd client: containerd: cannot unix dial containerd api service: dial unix /run/containerd/containerd.sock: connect: no such file or directory
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.447567    1565 reconciler_new.go:29] "Reconciler: start to sync state"
Jan 10 11:13:47 dev-master kubelet[1565]: W0110 11:13:47.447730    1565 reflector.go:539] vendor/k8s.io/client-go/informers/factory.go:159: failed to list *v1.CSIDriver: Get "https://10.255.0.100:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.447764    1565 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:159: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://10.255.0.100:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.446026    1565 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.255.0.100:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/dev-master?timeout=10s\": dial tcp 10.255.0.100:6443: connect: connection refused" interval="200ms"
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.449909    1565 kubelet.go:1462] "Image garbage collection failed once. Stats initialization may not have completed yet" err="failed to get imageFs info: unable to find data in memory cache"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.453737    1565 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"      
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455038    1565 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"      
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455053    1565 cpu_manager.go:214] "Starting CPU manager" policy="none"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455059    1565 status_manager.go:217] "Starting to sync pod status with apiserver"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455062    1565 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455071    1565 kubelet.go:2329] "Starting kubelet main sync loop"
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.455100    1565 kubelet.go:2353] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455072    1565 state_mem.go:36] "Initialized new in-memory state store"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455186    1565 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455198    1565 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455202    1565 policy_none.go:49] "None policy: Start"
Jan 10 11:13:47 dev-master kubelet[1565]: W0110 11:13:47.455413    1565 reflector.go:539] vendor/k8s.io/client-go/informers/factory.go:159: failed to list *v1.RuntimeClass: Get "https://10.255.0.100:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.455442    1565 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:159: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://10.255.0.100:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.255.0.100:6443: connect: connection refused
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455717    1565 memory_manager.go:170] "Starting memorymanager" policy="None"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455731    1565 state_mem.go:35] "Initializing new in-memory state store"
Jan 10 11:13:47 dev-master kubelet[1565]: I0110 11:13:47.455836    1565 state_mem.go:75] "Updated machine memory state"
Jan 10 11:13:47 dev-master kubelet[1565]: E0110 11:13:47.504259    1565 kubelet.go:1542] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubepods] doesn't exist"
Jan 10 11:13:47 dev-master systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jan 10 11:13:47 dev-master systemd[1]: kubelet.service: Failed with result 'exit-code'.
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```
</details>

Note:
I can also replicate this on 1.26, 1.27, 1.28, 1.29.


#### Cloud provider

<details>
None - Using Hyper V VM for testing.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux dev-master 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o 1.28
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Antrea 1.14.1
</details>


### 分析结果

不涉及

---

## Issue #122675 `kubelet_pod_start_sli_duration_seconds_sum` getting negative values

- Issue 链接：[#122675](https://github.com/kubernetes/kubernetes/issues/122675)

### Issue 内容

#### What happened?

We wanted to start using the SLI for kubelet pod start duration, as per:
- https://github.com/kubernetes/community/blob/master/sig-scalability/slos/pod_startup_latency.md
- https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#steady-state-slisslos
- https://github.com/kubernetes/community/pull/2204
- https://github.com/kubernetes/kubernetes/pull/111930

...but it turns out that it seems to report unexpected/unusable **negative** values in the `sum` metric of the histogram, i.e:
`kubelet_pod_start_sli_duration_seconds_sum`

#### What did you expect to happen?

As it's a duration, only positive values would be expected.

#### How can we reproduce it (as minimally and precisely as possible)?

`count(kubelet_pod_start_sli_duration_seconds_sum < 0) / count(kubelet_pod_start_sli_duration_seconds_sum)` is `1` over time, i.e. all values are negative.

For example:
`rate(kubelet_pod_start_sli_duration_seconds_sum[10m])` gives negative durations:
><img width="1060" alt="Skärmavbild 2024-01-10 kl  09 35 23" src="https://github.com/kubernetes/kubernetes/assets/62675/1be54f59-a855-4379-9e6a-55d96dd1c6a7">


#### Anything else we need to know?

Let me know if there's any more context I can provide so we can understand this better.

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.11", GitCommit:"3cd242c51317aed8858119529ccab22079f523b1", GitTreeState:"clean", BuildDate:"2023-11-15T16:50:12Z", GoVersion:"go1.20.11", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
Cluster API OpenStack
</details>


#### OS version

<!--
<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
-->

#### Install tools

<!--
<details>

</details>
-->

#### Container runtime (CRI) and version (if applicable)

<!--
<details>

</details>
-->

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<!--
<details>

</details>
-->

### 分析结果

不涉及。

---

## Issue #122674 kubelet starts workloads on a node that is currently in process of being shut down after it is restarted

- Issue 链接：[#122674](https://github.com/kubernetes/kubernetes/issues/122674)

### Issue 内容

#### What happened?

Currently, when the [Graceful Node Shutdown](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2000-graceful-node-shutdown/README.md) is enabled, kubelet will then connect to Dbus and subscribe to the `org.freedesktop.systemd1.Manager` interface (using object path `/org/freedesktop/login1`) to listen for the `PrepareForShutdown` event.

Additionally, aside from listening for an event, kubelet will also place a so-called "inhibitor lock" (see [Inhibitor Locks](https://www.freedesktop.org/wiki/Software/systemd/inhibit) for more information) that allows for it to stagger a potential system shutdown or restart up to some configured amount of time, and as such, allowing for workloads to complete their work and terminate gracefully.

Thus, should there be a shutdown or a restart of the underlying node initiated using compliant mechanisms (such as more modern tools that correctly handle the interaction between systemd, systemd-inhibit and Dbus), then kubelet would receive the `PrepareForShutdown` with its boolean attribute set to `true`—this would be a clear indication that the node is due to interrupt its operation, either a shutdown or a restart, and kubelet should remove ability for the node to receive new workloads (scheduling on it will be disabled) and then start gracefully terminating current workloads.

Then, once the last workload is terminated, kubelet will release the "inhibitor lock", allowing for the shutdown or restart to progress uninterrupted. This mechanism works well, given no interruptions to Dbus or kubelet operations.

However, if kubelet had been restarted, let's say as part of an ongoing upgrade or crash, then it would start to schedule workloads again on a currently shutting down node. This might not be desirable and problematic, as some nodes can have a substantial number of workloads (in terms of volume of Pods and containers running), or the workloads can be sparse but very resource intensive, especially since the node is about to be shut down, restarted or even completely retired.

This is because kubelet does not detect the current status (in terms of a potentially pending shutdown or restart) of the underlying node and only listens for an incoming event, which might have already been sent.

That said, depending on how the node will be shut down or terminated, there is a potential saving grace since if the shutdown or restart is a scheduled event, then for the remainder of the time before the main event, the `PrepareForShutdown` event might be broadcasted again (but might also not). But there is a delay between each new event broadcast, during which kubelet could promptly schedule workloads back on the node before it notices the current node status again.

Thus, this might still not be enough to ensure that workloads won't be scheduled again on such a node.

#### What did you expect to happen?

kubelet should avoid starting workloads, both static and otherwise, again on a node that is currently being shut down or waiting to be restarted. The underlying node should also not be marked as schedulable again.

#### How can we reproduce it (as minimally and precisely as possible)?

Only a few steps are needed to reproduce this behaviour:

- Configure kubelet on a single worker node to enable Graceful Node Shutdown

This can be done using the following kubelet configuration properties:


```yaml
shutdownGracePeriod: 3600s
shutdownGracePeriodCriticalPods: 1800s
```

A very simple kubelet configuration file as an example:

<details>

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 172.17.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 3600s
shutdownGracePeriodCriticalPods: 1800s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
```

</details>

- Prepare a simple test workload both as a static and normal Pod

An example `test.yaml`:

<details>

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
  namespace: default
spec:
  containers:
    - name: test
      image: docker.io/library/ubuntu:22.04
      command:
        - /bin/sh
        - -c
        - |
          ( while true ; do date ; sleep 1 ; done ) &
          pid=$!
          echo "Starting!"
          _term () {
              kill $pid
              echo "Caught SIGTERM!"
          }
          trap _term TERM
          wait $pid
          trap - TERM
          wait $pid
          exit 0
  restartPolicy: Always
```

</details>

Usually, static Pods manifests are stored within the `/etc/kubernetes/manifests` directory.

- Simulate the node shutdown

Emit the `org.freedesktop.login1.Manager.PrepareForShutdown` signal to simulate the node being shut down:

An example using the `gdbus` utility:

```console
# gdbus emit --system --object-path /org/freedesktop/login1 --signal 'org.freedesktop.login1.Manager.PrepareForShutdown' true
```
You should see the following entries in the kubelet log (provided a sufficient log level is set):

<details>

```console
Jan 10 07:28:44 worker-node01 kubelet[6188]: I0110 07:28:44.397678    6188 nodeshutdown_manager_linux.go:265] "Shutdown manager detected new shutdown event, isNodeShuttingDownNow" event=true
Jan 10 07:28:44 worker-node01 kubelet[6188]: I0110 07:28:44.397692    6188 nodeshutdown_manager_linux.go:273] "Shutdown manager detected new shutdown event" event="shutdown"
Jan 10 07:28:44 worker-node01 kubelet[6188]: I0110 07:28:44.397703    6188 nodeshutdown_manager_linux.go:322] "Shutdown manager processing shutdown event"
Jan 10 07:28:44 worker-node01 kubelet[6188]: I0110 07:28:44.398563    6188 event.go:376] "Event occurred" object="worker-node01" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Shutdown" message="Shutdown manager detected shutdown event"
````

</details>

Plus, kubelet should already be working towards terminating workloads on the given node.

Alternatively, you can schedule an actual restart, albeit delayed. This can be done using the following command:

```console
shutdown -r +10
```

The above would defer node restart to **10 minutes** in the future from the time when the command was issued.

- Restart the kubelet process

Provided that the static Pod manifest is in place, you can restart kubelet using the `systemctl restart kubelet` command.

This should restart the current kubelet process—it will start a new one, a fresh one. Shortly after the restart, kubelet should bring the static Pod up alongside other workloads that should already start being scheduled again on the given node.

Why static Pod? Sometimes, as I have noticed (and which might be a potential issue), certain workloads are often immediately scheduled on the node that is supposed to be shutting down, and as such, on a smaller cluster, this can result in scheduling failures (and if the CNI-related workloads are affected, then nothing would work as the overlay network will be most likely down). A static Pod does not have this problem, of course.

#### Anything else we need to know?

There do not seem to be a lot of reliable mechanisms presented to kubelet from Dbus or systemd that would allow it to ascertain the current status of the node. The API offered by Dbus is somewhat lacking in that regard.

kubelet could pick the current status again, provided that the `PrepareForShutdown` event will be broadcast again—which is the case for scheduled shutdowns. However, this is not very reliable and might still result in kubelet starting workloads again.

An idea would be for kubelet to check either the `PreparingForShutdown` property of the `org.freedesktop.login1.Manager` interface, which will be eventually set to `true` to indicate the current status or the `ScheduledShutdown` property (same interface) that when non-empty (a reason and future time has been set) could be used as an indicator that the node stats is due to change.

That said, the `PreparingForShutdown` does not seem to be reliably handled—for example, would cancelling a pending node restart clear this property (set to back to `false`)? Should kubelet clear this property, if at all possible, should the shutdown be cancelled? The `ScheduledShutdown` property seems a little more reliable as it will be set to an empty state should the scheduled shutdown be cancelled.

A different approach would be to persist some property via the API Server as part of the Node object or use a "tag file" somewhere safe on the file system and check for its presence on startup. Both methods would require some way to reconcile the state to avoid stale information and such.

The `org.freedesktop.login1.Manager` interface for reference:

<details>

```
interface org.freedesktop.login1.Manager {
  methods:
    GetSession(in  s session_id,
               out o object_path);
    GetSessionByPID(in  u pid,
                    out o object_path);
    GetUser(in  u uid,
            out o object_path);
    GetUserByPID(in  u pid,
                 out o object_path);
    GetSeat(in  s seat_id,
            out o object_path);
    ListSessions(out a(susso) sessions);
    ListUsers(out a(uso) users);
    ListSeats(out a(so) seats);
    ListInhibitors(out a(ssssuu) inhibitors);
    @org.freedesktop.systemd1.Privileged("true")
    CreateSession(in  u uid,
                  in  u pid,
                  in  s service,
                  in  s type,
                  in  s class,
                  in  s desktop,
                  in  s seat_id,
                  in  u vtnr,
                  in  s tty,
                  in  s display,
                  in  b remote,
                  in  s remote_user,
                  in  s remote_host,
                  in  a(sv) properties,
                  out s session_id,
                  out o object_path,
                  out s runtime_path,
                  out h fifo_fd,
                  out u uid,
                  out s seat_id,
                  out u vtnr,
                  out b existing);
    @org.freedesktop.systemd1.Privileged("true")
    ReleaseSession(in  s session_id);
    ActivateSession(in  s session_id);
    ActivateSessionOnSeat(in  s session_id,
                          in  s seat_id);
    LockSession(in  s session_id);
    UnlockSession(in  s session_id);
    LockSessions();
    UnlockSessions();
    KillSession(in  s session_id,
                in  s who,
                in  i signal_number);
    KillUser(in  u uid,
             in  i signal_number);
    TerminateSession(in  s session_id);
    TerminateUser(in  u uid);
    TerminateSeat(in  s seat_id);
    SetUserLinger(in  u uid,
                  in  b enable,
                  in  b interactive);
    AttachDevice(in  s seat_id,
                 in  s sysfs_path,
                 in  b interactive);
    FlushDevices(in  b interactive);
    PowerOff(in  b interactive);
    PowerOffWithFlags(in  t flags);
    Reboot(in  b interactive);
    RebootWithFlags(in  t flags);
    Halt(in  b interactive);
    HaltWithFlags(in  t flags);
    Suspend(in  b interactive);
    SuspendWithFlags(in  t flags);
    Hibernate(in  b interactive);
    HibernateWithFlags(in  t flags);
    HybridSleep(in  b interactive);
    HybridSleepWithFlags(in  t flags);
    SuspendThenHibernate(in  b interactive);
    SuspendThenHibernateWithFlags(in  t flags);
    CanPowerOff(out s result);
    CanReboot(out s result);
    CanHalt(out s result);
    CanSuspend(out s result);
    CanHibernate(out s result);
    CanHybridSleep(out s result);
    CanSuspendThenHibernate(out s result);
    ScheduleShutdown(in  s type,
                     in  t usec);
    CancelScheduledShutdown(out b cancelled);
    Inhibit(in  s what,
            in  s who,
            in  s why,
            in  s mode,
            out h pipe_fd);
    CanRebootParameter(out s result);
    SetRebootParameter(in  s parameter);
    CanRebootToFirmwareSetup(out s result);
    SetRebootToFirmwareSetup(in  b enable);
    CanRebootToBootLoaderMenu(out s result);
    SetRebootToBootLoaderMenu(in  t timeout);
    CanRebootToBootLoaderEntry(out s result);
    SetRebootToBootLoaderEntry(in  s boot_loader_entry);
    SetWallMessage(in  s wall_message,
                   in  b enable);
  signals:
    SessionNew(s session_id,
               o object_path);
    SessionRemoved(s session_id,
                   o object_path);
    UserNew(u uid,
            o object_path);
    UserRemoved(u uid,
                o object_path);
    SeatNew(s seat_id,
            o object_path);
    SeatRemoved(s seat_id,
                o object_path);
    PrepareForShutdown(b start);
    PrepareForSleep(b start);
  properties:
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    @org.freedesktop.systemd1.Privileged("true")
    readwrite b EnableWallMessages = true;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    @org.freedesktop.systemd1.Privileged("true")
    readwrite s WallMessage = '';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly u NAutoVTs = 6;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly as KillOnlyUsers = [];
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly as KillExcludeUsers = [];
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly b KillUserProcesses = false;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly s RebootParameter = '';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b RebootToFirmwareSetup = false;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly t RebootToBootLoaderMenu = 18446744073709551615;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly s RebootToBootLoaderEntry = '';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly as BootLoaderEntries = [];
    readonly b IdleHint = false;
    readonly t IdleSinceHint = 0;
    readonly t IdleSinceHintMonotonic = 0;
    readonly s BlockInhibited = '';
    readonly s DelayInhibited = 'shutdown';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t InhibitDelayMaxUSec = 300000000;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t UserStopDelayUSec = 10000000;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandlePowerKey = 'poweroff';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandleSuspendKey = 'suspend';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandleHibernateKey = 'hibernate';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandleLidSwitch = 'suspend';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandleLidSwitchExternalPower = '';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s HandleLidSwitchDocked = 'ignore';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t HoldoffTimeoutUSec = 30000000;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly s IdleAction = 'ignore';
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t IdleActionUSec = 1800000000;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b PreparingForShutdown = true;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b PreparingForSleep = false;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly (st) ScheduledShutdown = ('', 0);
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b Docked = false;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b LidClosed = false;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly b OnExternalPower = true;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly b RemoveIPC = true;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t RuntimeDirectorySize = 410025984;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t RuntimeDirectoryInodesMax = 100104;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t InhibitorsMax = 8192;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly t NCurrentInhibitors = 1;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("const")
    readonly t SessionsMax = 8192;
    @org.freedesktop.DBus.Property.EmitsChangedSignal("false")
    readonly t NCurrentSessions = 1;
};
```

</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>

```
N/A
```

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux worker-node01 5.15.0-83-generic #92-Ubuntu SMP Mon Aug 14 09:30:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

```console
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.0", GitCommit:"3f7a50f38688eb332e2a1b013678c6435d539ae6", GitTreeState:"clean", BuildDate:"2023-12-13T08:50:10Z", GoVersion:"go1.21.5", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
$ crio --version
crio version 1.29.0
Version:        1.29.0
GitCommit:      d59bbdc252837107c9f5d235b8fb2650ff2b9d93
GitCommitDate:  2023-12-21T16:23:14Z
GitTreeState:   clean
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.21.1
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:      
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  exclude_graphdriver_devicemapper
  seccomp
  apparmor
  selinux
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false
```

```console
$ conmon --version
conmon version 2.1.10
commit: 09bcded8e9c49cf1ff1fda403feac5a08f22535f-dirty
````

```console
$ crun --version
crun version 1.12
commit: ce429cb2e277d001c2179df1ac66a470f00802ae
rundir: /run/user/1000/crun
spec: 1.0.0
+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL
````

```console
$ runc --version
runc version 1.1.11
commit: v1.1.11-0-g4bccb38c
spec: 1.0.2-dev
go: go1.20.12
libseccomp: 2.5.4
````



</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

```console
$ crictl exec -it ee8826f196d0a calicoctl version
Client Version:    v3.25.0
Git commit:        3f7fe4d29
Cluster Version:   v3.25.0
Cluster Type:      k8s,kdd,bgp,kubeadm
```

</details>


### 分析结果

不涉及

---

## Issue #122673 kubernetes v1.24: systemd[14798]: Failed to set up mount unit: Invalid argument

- Issue 链接：[#122673](https://github.com/kubernetes/kubernetes/issues/122673)

### Issue 内容

#### What happened?

After upgrading the kubernetes version from 1.23 to 1.24, the OS  /var/log/syslog  log prompts: systemd[14798]: Failed to set up mount unit: Invalid argument, The pod on this host has some rbd volumes mounted.
![err](https://github.com/kubernetes/kubernetes/assets/35327558/0190cc72-64a3-4e11-8ac8-27ed74993b17)


#### What did you expect to happen?

Mounting the rbd volume is normal and such logs should not appear, The ceph rbd volume used by my cluster

#### How can we reproduce it (as minimally and precisely as possible)?

My operation was to upgrade kubernetes version v 1.23 to v1.24, and these logs appeared.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.16", GitCommit:"51e33fadff13065ae5518db94e84598293965939", GitTreeState:"clean", BuildDate:"2023-07-19T12:26:21Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.16", GitCommit:"51e33fadff13065ae5518db94e84598293965939", GitTreeState:"clean", BuildDate:"2023-07-19T12:19:24Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="18.04.6 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.6 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

$ uname -a
Linux aukeyit-k8s1-master01 4.15.0-169-generic #177-Ubuntu SMP Thu Feb 3 10:50:38 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122671 Incorrect URL to Install Docker Desktop for Mac at build/README.md

- Issue 链接：[#122671](https://github.com/kubernetes/kubernetes/issues/122671)

### Issue 内容

#### What happened?

The link in the following sentence is incorrect
macOS Install Docker for Mac. See installation instructions [here](https://docs.docker.com/docker-for-mac/).

It redirects to [Sign in to Docker Desktop](https://docs.docker.com/desktop/get-started/) instead of [Install Docker Desktop on Mac](https://docs.docker.com/desktop/install/mac-install/)

#### What did you expect to happen?

The link should get redirected to [Install Docker Desktop on Mac](https://docs.docker.com/desktop/install/mac-install/)

#### How can we reproduce it (as minimally and precisely as possible)?

Navigate to [build/README.md](https://github.com/kubernetes/kubernetes/blob/master/build/README.md)

Click on the hyperlink in the below section
macOS Install Docker for Mac. See installation instructions [here](https://docs.docker.com/docker-for-mac/).

#### Anything else we need to know?

N/A

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

不涉及

---

## Issue #122668 OpenAPI handler fails on duplicated path

- Issue 链接：[#122668](https://github.com/kubernetes/kubernetes/issues/122668)

### Issue 内容

#### What happened?

When installing the metrics-server to implement the metrics.k8s.io API and the prometheus-adapter to implement the custom.metrics.k8s.io API, it is possible to make the OpenAPI handler fail to initialize with this error log in apiserver: https://github.com/kubernetes/kube-openapi/blob/master/pkg/aggregator/aggregator.go#L343
```
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/namespaces/{namespace}/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes/{name}
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
```
This will result in kubectl discovery calls to /openapi/v2 to fail with a 503:
```
{
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "863d36c0-3797-4f19-bc74-df571a671a00",
  "stage": "ResponseComplete",
  "requestURI": "/openapi/v2?timeout=31s",
  "verb": "get",
...
  "userAgent": "kubectl/v1.28.2 (linux/amd64) kubernetes/cd9e211",
  "responseStatus": {
    "metadata": {},
    "code": 503
  },
  "requestReceivedTimestamp": "2023-12-19T02:50:12.882602Z",
  "stageTimestamp": "2023-12-19T02:50:12.913981Z",
  "annotations": {
    "authorization.k8s.io/decision": "allow",
    "authorization.k8s.io/reason": ""
  }
}
```


#### What did you expect to happen?

The OpenAPI handler should not fail to initialize if both metrics.k8s.io and custom.metrics.k8s.io are serving the same path for different API groups/versions.

#### How can we reproduce it (as minimally and precisely as possible)?

I can provide exact installation steps if needed, however, a high-level repro is:
- install metrics-server to serve v1beta1.metrics.k8s.io  
- install prometheus-adapter to serve v1beta1.custom.metrics.k8s.io
- ensure resourceRules is populated in the prometheus-adapter config so that resource metrics are also served by v1beta1.custom.metrics.k8s.io: https://github.com/kubernetes-sigs/prometheus-adapter/blob/7a421bb91e9ea52be4e26db22392c36c8af38bc8/deploy/manifests/custom-metrics-config-map.yaml#L73-L98
- restarting apiserver will result in the OpenAPI failure and 503 responses to /openapi/v2 path

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.4-eks-8cb36c9", GitCommit:"b4ddd686b005f445825bd0afd5dba24d4414d4cf", GitTreeState:"clean", BuildDate:"2023-11-22T21:52:14Z", GoVersion:"go1.20.11", Compiler:"gc", Platform:"linux/amd64"}
```
</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122658 ValidatingAdmissionPolicy using CRDs as paramKind always fails after restarting kube-apiserver

- Issue 链接：[#122658](https://github.com/kubernetes/kubernetes/issues/122658)

### Issue 内容

#### What happened?

I'm using ValidatingAdmissionPolicy(VAP) with setting my own CRD to paramKind in my on-premise cluster. It works fine, but once I restarted one of the kube-apiserver instances it changed to reject both valid and invalid requests with the error message like:

```
error: failed to create rolebinding: rolebindings.rbac.authorization.k8s.io "view" is forbidden: ValidatingAdmissionPolicy 'test-vap' denied request: failed to configure policy: failed to find resource referenced by paramKind: 'example.com/v1alpha1, Kind=Foo'
```

#### What did you expect to happen?

VAP using CRDs as paramKind should continue to work fine even when one of kube-apiserver instances is restarted.

#### How can we reproduce it (as minimally and precisely as possible)?

You can reproduce it locally using kind cluster by following procedure.

<details>

```
$ cat kind.yaml         
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        feature-gates: "ValidatingAdmissionPolicy=true"
        runtime-config: "admissionregistration.k8s.io/v1beta1=true"

$ kind create cluster --image=kindest/node:v1.29.0 --config kind.yaml
Creating cluster "kind" ...
 ✓ Ensuring node image (kindest/node:v1.29.0) 🖼
 ✓ Preparing nodes 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
Set kubectl context to "kind-kind"
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Thanks for using kind! 😊

$ cat << EOF | kubectl apply -f -                                                                  
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: foos.example.com
spec:
  group: example.com
  names:
    kind: Foo
    listKind: FooList
    plural: foos
    singular: foo
  scope: Namespaced
  versions:
  - name: v1alpha1
    schema:
      openAPIV3Schema:
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            properties:
              foo:
                type: string
            type: object
        type: object
    served: true
    storage: true
EOF
customresourcedefinition.apiextensions.k8s.io/foos.example.com created

$ kubectl create ns test                                                                           
namespace/test created

$ cat << EOF | kubectl apply -f -
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicy
metadata:
  name: test-vap
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: example.com/v1alpha1
    kind: Foo
  matchConstraints:
    resourceRules:
    - apiGroups: ["rbac.authorization.k8s.io"]
      apiVersions: ["v1"]
      operations: ["CREATE", "UPDATE"]
      resources: ["rolebindings"]
  validations:
  - expression: "object.subjects.all(s, s.name == params.spec.foo)"
    messageExpression: "'subject must be ' + string(params.spec.foo)"
---
apiVersion: example.com/v1alpha1
kind: Foo
metadata:
  name: for-vap
  namespace: test
spec:
  foo: foo
---
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: test-vap-binding
spec:
  policyName: test-vap
  validationActions: [Deny]
  paramRef:
    name: for-vap
    namespace: test
    parameterNotFoundAction: Deny
EOF
validatingadmissionpolicy.admissionregistration.k8s.io/test-vap created
foo.example.com/for-vap created
validatingadmissionpolicybinding.admissionregistration.k8s.io/test-vap-binding created

# Before restart, you can create the RoleBinding that matches to the VAP rule.
$ kubectl create rolebinding view --clusterrole=view --group=foo -n test 
rolebinding.rbac.authorization.k8s.io/view created

$ kubectl delete rolebinding view -n test
rolebinding.rbac.authorization.k8s.io "view" deleted

$ docker exec -it kind-control-plane sh -c 'crictl stop $(crictl ps | grep kube-apiserver | awk '\''{print $1}'\'')'
$ kubectl get po -n kube-system  | grep kube-apiserver
kube-apiserver-kind-control-plane            1/1     Running   1 (33s ago)   5m

# After restarting of the kube-apiserver, you cannot create the same RoleBinding.
$ kubectl create rolebinding view --clusterrole=view --group=foo -n test 
error: failed to create rolebinding: rolebindings.rbac.authorization.k8s.io "view" is forbidden: ValidatingAdmissionPolicy 'test-vap' denied request: failed to configure policy: failed to find resource referenced by paramKind: 'example.com/v1alpha1, Kind=Foo'
```

</details>

#### Anything else we need to know?

I recovered this failure situation by mutating `spec.paramKind` field of all existing VAPs and running reconciliation of this field explicitly. I've also tried to mutate `metadata.annotations`, but it is not enough.
Without the above operation, all requests are rejected. 

I might be able to avoid this error by setting `spec.paramRef.parameterNotFoundAction` to Allow, but I don't want to do that to enforce this validation check.

#### Kubernetes version

v1.28.3, v1.29.0

<details>

```console
$ kubectl version                                                       
Client Version: v1.28.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### 分析结果

不涉及

---

## Issue #122650 [Regression in master] e2e.test is no longer compilable on macOS: `imports github.com/opencontainers/runc/libcontainer/userns: C source files not allowed when not using cgo or SWIG: userns_maps.c`

- Issue 链接：[#122650](https://github.com/kubernetes/kubernetes/issues/122650)

### Issue 内容

#### What happened?

e2e.test is no longer compilable on macOS in the master branch.

It was compilable with Kubernetes v1.29.0 and v1.30.0-alpha.0.

#### What did you expect to happen?

It should be compilable.

#### How can we reproduce it (as minimally and precisely as possible)?

```console
$ go version
go version go1.21.5 darwin/amd64

$ make WHAT=test/e2e/e2e.test
+++ [0109 10:06:53] Building go targets for darwin/amd64
    k8s.io/kubernetes/test/e2e/e2e.test (test)
package k8s.io/kubernetes/test/e2e
        imports k8s.io/kubernetes/test/e2e/common
        imports k8s.io/kubernetes/test/e2e/common/node
        imports k8s.io/kubernetes/pkg/kubelet
        imports github.com/opencontainers/runc/libcontainer/userns: C source files not allowed when not using cgo or SWIG: userns_maps.c
!!! [0109 10:06:54] Call tree:
!!! [0109 10:06:54]  1: /Users/suda/gopath/src/k8s.io/kubernetes/hack/lib/golang.sh:948 kube::golang::build_binaries_for_platform(...)
!!! [0109 10:06:54]  2: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0109 10:06:54] Call tree:
!!! [0109 10:06:54]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0109 10:06:54] Call tree:
!!! [0109 10:06:54]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
make: *** [all] Error 1
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

master (9e0eccabb47d9044b8ec72ff27df848692b88823)

(This is just the latest commit, not the commit that introduced the regression)
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
macOS 14.2.1 (Intel)
</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### 分析结果

不涉及

---

## Issue #122648 Unexpected annotations merge instead of replace with server-side PATCH using pure API

- Issue 链接：[#122648](https://github.com/kubernetes/kubernetes/issues/122648)

### Issue 内容

#### What happened?

This looks to me like a bug. Whenever annotations are managed by the same field manager the server-side PATCH that includes annotation change, the changes are merged instead of complete replacement of the annotations. 

#### What did you expect to happen?

Annotations to be replaced when PATCHed on the server-side by the same field manager as the one that introduced them.
If the behavior is correct, what then is the mechanism to replace the entire mapping with a *server-side* PATCH?

#### How can we reproduce it (as minimally and precisely as possible)?

The test case to be provided

#### Anything else we need to know?

Here is the conversation capture:

In the below example there is a namespace with annotations. Namespace is added with annotations "a: x" and the PATCH is supposed to replace them with "b: y" (same field manager). Instead they are merged: `"annotations":{"a":"x","b":"y"}`. 

Send:

```
POST https://192.168.121.2:8443/api/v1/namespaces?fieldManager=kubernator&fieldValidation=Warn

Accept: application/json
User-Agent: OpenAPI-Generator/28.1.0/python
Content-Type: application/json

{"apiVersion": "v1", "kind": "Namespace", "metadata": {"name": "ns1", "annotations": {"a": "x"}}}
```

Recv:
```

201 Created

Audit-Id: 53d3660e-11f8-4acb-8614-66ee7ef69451
Cache-Control: no-cache, private
Content-Type: application/json
X-Kubernetes-Pf-Flowschema-Uid: 5c8f085c-4351-4402-a445-6dd0bd78700c
X-Kubernetes-Pf-Prioritylevel-Uid: 48c26814-1353-4e9f-b65b-5d5542e5c295
Date: Sun, 07 Jan 2024 23:23:03 GMT
Content-Length: 566

{"kind":"Namespace","apiVersion":"v1","metadata":{"name":"ns1","uid":"0a95b535-34ad-48a4-9fe3-8812d09b4343","resourceVersion":"276","creationTimestamp":"2024-01-07T23:23:03Z","labels":{"kubernetes.io/metadata.name":"ns1"},"annotations":{"a":"x"},"managedFields":[{"manager":"kubernator","operation":"Update","apiVersion":"v1","time":"2024-01-07T23:23:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:a":{}},"f:labels":{".":{},"f:kubernetes.io/metadata.name":{}}}}}]},"spec":{"finalizers":["kubernetes"]},"status":{"phase":"Active"}}
```

Send:
```
PATCH https://192.168.121.2:8443/api/v1/namespaces/ns1?dryRun=All&fieldManager=kubernator&fieldValidation=Warn&force=True

Accept: application/json
Content-Type: application/apply-patch+yaml
User-Agent: OpenAPI-Generator/28.1.0/python

{"apiVersion": "v1", "kind": "Namespace", "metadata": {"name": "ns1", "annotations": {"b": "y"}}}
```

Recv:
```
200 OK

Audit-Id: 8caa380e-e3cf-4379-8188-12b1b73304c9
Cache-Control: no-cache, private
Content-Type: application/json
X-Kubernetes-Pf-Flowschema-Uid: 5c8f085c-4351-4402-a445-6dd0bd78700c
X-Kubernetes-Pf-Prioritylevel-Uid: 48c26814-1353-4e9f-b65b-5d5542e5c295
Date: Sun, 07 Jan 2024 23:23:05 GMT
Content-Length: 746

{"kind":"Namespace","apiVersion":"v1","metadata":{"name":"ns1","uid":"0a95b535-34ad-48a4-9fe3-8812d09b4343","resourceVersion":"276","creationTimestamp":"2024-01-07T23:23:03Z","labels":{"kubernetes.io/metadata.name":"ns1"},"annotations":{"a":"x","b":"y"},"managedFields":[{"manager":"kubernator","operation":"Apply","apiVersion":"v1","time":"2024-01-07T23:23:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:b":{}}}}},{"manager":"kubernator","operation":"Update","apiVersion":"v1","time":"2024-01-07T23:23:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:a":{}},"f:labels":{".":{},"f:kubernetes.io/metadata.name":{}}}}}]},"spec":{"finalizers":["kubernetes"]},"status":{"phase":"Active"}}
```

#### Kubernetes version

1.28.4


#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122639 api logs not very precise in regards to cert issues on endpoints

- Issue 链接：[#122639](https://github.com/kubernetes/kubernetes/issues/122639)

### Issue 内容

#### What happened?

our kube-apiserver just started failing to start.. The log from containerd only says:
2023-12-16T07:35:01.426019865+01:00 stderr F }. Err: connection error: desc = "transport: authentication handshake failed: x509: certificate has expired or is not yet valid: current time 2023-12-16T06:35:01Z is after 2023-12-03T03:37:19Z"

we checked apiserver certs etc. -and they were all fine.
We finally checked etcd endpoint - and that had an old cert

/sig api-machinery

a repost - for only log issue - per @neolit123 suggestion https://github.com/kubernetes/kubeadm/issues/2989#issuecomment-1879221174

#### What did you expect to happen?

a logmessage that told me which endpoint had the bad cert and details about cert.

#### How can we reproduce it (as minimally and precisely as possible)?

renew api certs and not etcd certs - and then move time forward so etcd certs have run out - and restart kubelet

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.26.4

#### Cloud provider

hetzner.com - physical servers

#### OS version

ubuntu 22.04

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122632 confusing CRI-O error from cadvisor for nodes that use containerd

- Issue 链接：[#122632](https://github.com/kubernetes/kubernetes/issues/122632)

### Issue 内容

#### What happened?

After a fresh installation of CentOS Stream 9 with Kubernetes, I am encountering a registration failure for the CRI-O container factory. The error message indicates a problem with connecting to the CRI-O socket: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory. Despite not having CRI-O installed and using Containerd, Kubernetes is attempting to register the CRI-O container factory, resulting in the failure.

- `kubelet` is starting successfully, but there is an error related to CRI-O in the logs.
- `containerd` is running without any apparent issues.
- CNI configuration for Calico is present in `/etc/cni/net.d`.
- CNI binaries are available in `/opt/cni/bin`.
- The `kubeadm-config-custom.yaml` and related configuration files seem to be correctly configured for containerd.
- The configuration for `containerd` in `/etc/containerd/config.toml` also appears to be correct.
- The `crictl` configuration in `/etc/crictl.yaml` specifies the correct runtime endpoint.

```
$ systemctl status kubelet.service

Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.946014   25417 status_manager.go:217] "Starting to sync pod status with apiserver"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.946027   25417 kubelet.go:2329] "Starting kubelet main sync loop"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: E0107 18:34:33.946075   25417 kubelet.go:2353] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.947489   25417 factory.go:221] Registration of the systemd container factory successfully
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.947624   25417 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.951631   25417 factory.go:221] Registration of the containerd container factory successfully
```

#### What did you expect to happen?

I expected a clean Kubernetes installation without errors related to CRI-O, as CRI-O was not prior installed and configuration set accordingly.

#### How can we reproduce it (as minimally and precisely as possible)?

Perform a fresh installation of CentOS Stream 9 with Kubernetes (sudo kubeadm init --config /etc/kubernetes/kubam-config-custom.yaml), kubectl, Containerd, Runc, CNI Plugins, and Calico. Check the status of the kubelet.service and observe the error related to CRI-O container factory registration.

#### Anything else we need to know?

```
$ systemctl status kubelet.service

Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.946014   25417 status_manager.go:217] "Starting to sync pod status with apiserver"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.946027   25417 kubelet.go:2329] "Starting kubelet main sync loop"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: E0107 18:34:33.946075   25417 kubelet.go:2353] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.947489   25417 factory.go:221] Registration of the systemd container factory successfully
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.947624   25417 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Jan 07 18:34:33 k8s-control-plane-1 kubelet[25417]: I0107 18:34:33.951631   25417 factory.go:221] Registration of the containerd container factory successfully
```

```
$ journalctl -u containerd.service

Jan 07 18:34:33 k8s-control-plane-1 systemd[1]: Started containerd container runtime.
Jan 07 18:34:33 k8s-control-plane-1 containerd[25248]: time="2024-01-07T18:34:33.836477300+11:00" level=info msg="containerd successfully booted in 0.186740s"
Jan 07 18:34:34 k8s-control-plane-1 containerd[25248]: time="2024-01-07T18:34:34.059567600+11:00" level=info msg="No cni config template is specified, wait for other system components to drop the config."
```

```
$ ll /etc/cni/net.d
total 8
-rw-r--r--. 1 root root  679 Jan  7 17:47 10-calico.conflist
-rw-------. 1 root root 2741 Jan  7 17:47 calico-kubeconfig
```

```
$ ll /opt/cni/bin
total 244832
-rwxr-xr-x. 1 root root  4041623 Jan  7 17:47 bandwidth
-rwxr-xr-x. 1 root root  3248936 Sep 11 21:26 bridge
-rwsr-xr-x. 1 root root 62044175 Jan  7 17:47 calico
-rwsr-xr-x. 1 root root 62044175 Jan  7 17:47 calico-ipam
-rwxr-xr-x. 1 root root  8021392 Sep 11 21:26 dhcp
-rwxr-xr-x. 1 root root  2971752 Sep 11 21:26 dummy
-rwxr-xr-x. 1 root root  3333560 Sep 11 21:26 firewall
-rwxr-xr-x. 1 root root  2491746 Jan  7 17:47 flannel
-rwxr-xr-x. 1 root root  2888056 Sep 11 21:26 host-device
-rwxr-xr-x. 1 root root  3521392 Jan  7 17:47 host-local
-rwsr-xr-x. 1 root root 62044175 Jan  7 17:47 install
-rwxr-xr-x. 1 root root  2989072 Sep 11 21:26 ipvlan
-rwxr-xr-x. 1 root root  3602373 Jan  7 17:47 loopback
-rwxr-xr-x. 1 root root  3015104 Sep 11 21:26 macvlan
-rwxr-xr-x. 1 root root  4039793 Jan  7 17:47 portmap
-rwxr-xr-x. 1 root root  3112536 Sep 11 21:26 ptp
-rwxr-xr-x. 1 root root  2642160 Sep 11 21:26 sbr
-rwxr-xr-x. 1 root root  2158744 Sep 11 21:26 static
-rwxr-xr-x. 1 root root  3035352 Sep 11 21:26 tap
-rwxr-xr-x. 1 root root  3760798 Jan  7 17:47 tuning
-rwxr-xr-x. 1 root root  2984672 Sep 11 21:26 vlan
-rwxr-xr-x. 1 root root  2665880 Sep 11 21:26 vrf
```

```
kubeadm-config-custom.yaml (output below condensed)

apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  criSocket: "unix:///var/run/containerd/containerd.sock"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: "systemd"
cgroupRoot: "/"
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"

```

```
$ cat /var/lib/kubelet/kubeadm-flags.env
KUBELET_KUBEADM_ARGS=""
```

```
$ cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

```
$ config default > /etc/containerd/config.toml
$ cat /etc/containerd/config.toml

root = "/var/lib/containerd"
state = "/var/run/containerd"

[grpc]
  address = "/var/run/containerd/containerd.sock"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
```

```
$ cat /etc/crictl.yaml

runtime-endpoint: unix:///var/run/containerd/containerd.sock
image-endpoint: unix:///var/run/containerd/containerd.sock
timeout: 2
debug: true
pull-image-on-create: false
```

```
$ sudo crictl info
  "lastCNILoadStatus": "OK",
  "lastCNILoadStatus.default": "OK"
```

#### Kubernetes version

<details>

```
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>

#### Cloud provider

<details>



</details>

#### OS version

<details>

```
# On Linux:
$ cat /etc/os-release
NAME="CentOS Stream"
VERSION="9"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="9"
PLATFORM_ID="platform:el9"
PRETTY_NAME="CentOS Stream 9"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:centos:centos:9"
HOME_URL="https://centos.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_SUPPORT_PRODUCT_VERSION="CentOS Stream"

$ uname -a
Linux k8s-control-plane-1 5.14.0-402.el9.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Dec 21 19:46:35 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

<details>



</details>

#### Container runtime (CRI) and version (if applicable)

<details>

```
$ containerd --version
containerd github.com/containerd/containerd v1.7.11 64b8a811b07ba6288238eefc14d898ee0b5b99ba
```

</details>

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz

</details>

### 分析结果

不涉及

---

## Issue #122626 Assigned Pod add/update events don't requeue Pods without PodAffinity

- Issue 链接：[#122626](https://github.com/kubernetes/kubernetes/issues/122626)

### Issue 内容

#### What happened?

/sig scheduling
/kind bug

- AssignedPodAdded(): https://github.com/kubernetes/kubernetes/blob/b529e6ff1c7edb3bb1eaca185b9662d8e42ac2d0/pkg/scheduler/internal/queue/scheduling_queue.go#L1044
- AssignedPodUpdated(): https://github.com/kubernetes/kubernetes/blob/b529e6ff1c7edb3bb1eaca185b9662d8e42ac2d0/pkg/scheduler/internal/queue/scheduling_queue.go#L1069


When assigned Pods are created/updated, cluster events (Pod/Add or Pod/Update) should be delivered for all plugins that subscribe those events. But, actually `getUnschedulablePodsWithMatchingAffinityTerm` pre-filters Pods to receive those events based on Pods' affinity and `pod.Status.Resize`. - meaning only PodAffinity/NodeResourceFit receive them actually.

The problem here is similar to #110175 - in in-tree plugins, PodAffinity, PodTopologySpread and NodeResourceFit subscribe Pod add and/or update events. And, Pods rejected by PodTopologySpread will never be requeued to activeQ by Pod-related events. The same happens for custom plugins registering pod add and/or update events.


#### What did you expect to happen?

All Pods will get chances to get back to activeQ by Pod-related events.

#### How can we reproduce it (as minimally and precisely as possible)?

Run ↓ in https://github.com/sanposhiho/kubernetes/tree/pod-bug-repro.

```
 make test-integration WHAT=./test/integration/scheduler KUBE_TEST_ARGS="-run=^TestCoreResourceEnqueue$" 
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

master

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122623 E2E test is not setting the EndpointSlices conditions to ready

- Issue 链接：[#122623](https://github.com/kubernetes/kubernetes/issues/122623)

### Issue 内容

The test create custome EndpointSlices to validate some behaviors sending traffic, so it is clear that it expects the EndpointSlices to be ready, 

https://github.com/kubernetes/kubernetes/blob/b529e6ff1c7edb3bb1eaca185b9662d8e42ac2d0/test/e2e/network/endpointslice.go#L698-L712

The API doc is vague about this situation and leaves room to interpret the "unknown behavior", quoting @danwinship

> I was going to say that the API docs say that Ready: nil means it is ready, but actually it says "A nil value indicates an unknown state. In most cases consumers should interpret this unknown state as ready". FTR, kube-proxy interprets nil as true for Ready and Serving, and false for Terminating. It's possible we should update the API docs to make that official. Or if not, probably we should change the e2e tests to not rely on undefined values

Since the test expects the EndpointSlices to be Ready we should set them, if we decide to change the API behavior, then we should add an additional e2e test to validate this behavior, but not use the same test to validate two different things

/kind bug
/kind failing-testing
/sig network

Slack thread https://kubernetes.slack.com/archives/C09QYUH5W/p1704500806325289

### 分析结果

不涉及

---

## Issue #122621 Kubernetes Scheduling Framework, "kubernetes-sigs/scheduler-plugins" [Warnings] for "./hack/update-codegen.sh"

- Issue 链接：[#122621](https://github.com/kubernetes/kubernetes/issues/122621)

### Issue 内容

#### What happened?

I am trying to create a new plugin for "kubernetes-sigs/scheduler-plugins" using links

https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/doc/develop.md
https://github.com/wangchen615/scheduler-plugins/tree/kubecon22/pkg/scorebylabel
But getting following Warnings for command ./hack/update-codegen.sh

Warnings:
Generating deepcopy funcs
Generating defaulters
Generating conversions
WARNING: generate-groups.sh is deprecated.
WARNING: Please use k8s.io/code-generator/kube_codegen.sh instead.

WARNING: Specifying "all" as a generator is deprecated.
WARNING: Please list the specific generators needed.
WARNING: "all" is now an alias for "applyconfiguration,client,deepcopy,informer,lister"; new code generators WILL NOT be added to this set

WARNING: generate-internal-groups.sh is deprecated.
WARNING: Please use k8s.io/code-generator/kube_codegen.sh instead.

Generating deepcopy funcs
Generating apply configuration for scheduling:v1alpha1 at sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration
Generating clientset for scheduling:v1alpha1 at sigs.k8s.io/scheduler-plugins/pkg/generated/clientset
Generating listers for scheduling:v1alpha1 at sigs.k8s.io/scheduler-plugins/pkg/generated/listers
Generating informers for scheduling:v1alpha1 at sigs.k8s.io/scheduler-plugins/pkg/generated/informers

How to solve this?

I have added the plugin "ScoreByLabel" by referring https://github.com/wangchen615/scheduler-plugins/tree/kubecon22/pkg/scorebylabel and executed command "./hack/update-codegen.sh"

#### What did you expect to happen?

Expectation was to update the generated files with functions as DeepCopy for the added structures.

#### How can we reproduce it (as minimally and precisely as possible)?

by: "git clone https://github.com/kubernetes-sigs/scheduler-plugins.git"
and executing command:  "./hack/update-codegen.sh"

Reference document: https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/doc/develop.md

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Not applicable
</details>



#### Cloud provider

<details>

</details>
Not applicable

#### OS version

<details>
Not applicable
```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122611 `--proxy-mode=nftables` fails if non-canonical (but standardized) IPv6 addresses are used

- Issue 链接：[#122611](https://github.com/kubernetes/kubernetes/issues/122611)

### Issue 内容

#### What happened?

When a non-canonical IPv6 address, e.g. `fd00::10.0.0.34`, is used as a loadBalancerIP, communication fails.

/sig network
/area kube-proxy

#### What did you expect to happen?

Addresses like `fd00::10.0.0.34` should work, as they do for proxy-mode ipvs and iptables

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a dual-stack service with type loadBalancer
2. Assign a non-canonical ipv6 loadBalancerIP
3. Try to access the service with the non-canonical ipv6 address

I will add a manifest for test, and more instructions later.

#### Anything else we need to know?

What "canonical" really is can be debated. The [rfc5952](https://datatracker.ietf.org/doc/html/rfc5952) seems to accept "embedded ipv4", as long as the prefix follows the rules. So, as I understand it, `fd00::10.0.0.34` *is* a canonical IPv6 address.

@danwinship If you don't mind I would like to take a shot at this myself to get acquainted with the code

#### Kubernetes version

v1.29.0

#### Cloud provider

None

#### OS version

Linux

#### Install tools

None

#### Container runtime (CRI) and version (if applicable)

N/A (cri-o)

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A (but tested with calico and flannel)

### 分析结果

不涉及

---

## Issue #122609 kubelet.service entered failed state  (code=exited, status=1/FAILURE)

- Issue 链接：[#122609](https://github.com/kubernetes/kubernetes/issues/122609)

### Issue 内容

#### What happened?

I had followed the steps from [install-kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) to install the tools
Enabled kubelet service, started it and checked the status
systemctl status kubelet

#### What did you expect to happen?

The service should be active and running. Got below error

```
# systemctl status kubelet

● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: activating (auto-restart) (Result: exit-code) since Fri 2024-01-05 13:21:38 UTC; 6s ago
     Docs: https://kubernetes.io/docs/
  Process: 7019 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)
 Main PID: 7019 (code=exited, status=1/FAILURE)

Jan 05 13:21:38 ip-10-34-114-123 systemd[1]: Unit kubelet.service entered failed state.
Jan 05 13:21:38 ip-10-34-114-123 systemd[1]: kubelet.service failed.
```

#### How can we reproduce it (as minimally and precisely as possible)?

Followed the steps from [install-kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) to install the tools
Enable kubelet service, start it and check the status

System Info: Amazon Linux 2 AMI on a t3.medium instance

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"

$ uname -a
Linux ip-xx-xx-xxx-xxx 5.10.201-191.748.amzn2.x86_64 #1 SMP Mon Nov 27 18:28:14 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
kubeadm, kubectl, kubelet, containerd, runc
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI - 1.4.0
crictl - 1.29.0
</details>


### 分析结果

不涉及。

---

## Issue #122604 kube-proxy nftables mode failed to delete rules due to "Device or resource busy" consistently in some cases

- Issue 链接：[#122604](https://github.com/kubernetes/kubernetes/issues/122604)

### Issue 内容

#### What happened?

While validating nftable mode using conformance test, the following error logs were produced consistently:
```
E0104 17:21:56.957351       1 proxier.go:941] "Unable to delete stale chains; will retry later" err=<
        /dev/stdin:1:28-90: Error: Could not process rule: Device or resource busy
        delete chain ip kube-proxy service-QZNVUP4F-endpointslice-3291/example-named-port/tcp/http
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        /dev/stdin:3:28-109: Error: Could not process rule: Device or resource busy
        delete chain ip kube-proxy endpoint-A6Z5NSSV-endpointslice-3291/example-named-port/tcp/http__10.244.2.28/3001
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

I0104 18:43:05.900199       1 proxier.go:935] "Deleting stale nftables chains" numChains=1
E0104 18:43:05.945300       1 proxier.go:941] "Unable to delete stale chains; will retry later" err=<
        /dev/stdin:1:28-93: Error: Could not process rule: Device or resource busy
        delete chain ip kube-proxy endpoint-EVHUKXWZ-services-4440/externalsvc/tcp/__10.244.1.80/9376
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
I0104 18:43:06.986202       1 proxier.go:935] "Deleting stale nftables chains" numChains=3
I0104 18:43:16.462808       1 proxier.go:935] "Deleting stale nftables chains" numChains=1
E0104 18:43:16.517339       1 proxier.go:941] "Unable to delete stale chains; will retry later" err=<
        /dev/stdin:1:28-99: Error: Could not process rule: Device or resource busy
        delete chain ip kube-proxy endpoint-FJWEGLAR-services-6954/affinity-nodeport/tcp/__10.244.2.70/9376
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

I0104 18:50:42.968934       1 proxier.go:935] "Deleting stale nftables chains" numChains=8
I0104 18:50:55.643712       1 proxier.go:935] "Deleting stale nftables chains" numChains=1
E0104 18:50:55.677064       1 proxier.go:941] "Unable to delete stale chains; will retry later" err=<
        /dev/stdin:1:28-100: Error: Could not process rule: Device or resource busy
        delete chain ip kube-proxy endpoint-TK32CM73-services-3530/affinity-clusterip/tcp/__10.244.2.99/9376
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
```

I thought it was because a known issue described here: https://github.com/kubernetes/kubernetes/blob/09a5049ca785024edd4955eb82e855d9b5657491/pkg/proxy/nftables/proxier.go#L1519-L1524
However, the error was produced for every run and every kube-proxy instance, which doesn't seem related to the kernel bug.

After some investigation, I think it's caused by a bug: in some cases a chain could change from stale to active, but once it's added to `staleChains` it will always be deleted once. When the proxier tries to delete a previously stale but currently active chain, it would fail and lead to these errors, though it won't cause real problem thanks to kernel's validation.

cc @danwinship 

#### What did you expect to happen?

kube-proxy shouldn't try to delete active chains and produce such error logs.

#### How can we reproduce it (as minimally and precisely as possible)?

The e2e test case can reproduce it consistantly:
```
sonobuoy run --e2e-focus "should create Endpoints and EndpointSlices for Pods matching a Service"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.29.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及安全风险。

---

## Issue #122591 Liveness probe with an error doesn't trigger container restart

- Issue 链接：[#122591](https://github.com/kubernetes/kubernetes/issues/122591)

### Issue 内容

#### What happened?

We have a specific case when a health check would error out with `500 INTERNAL SERVER ERROR`. In this particular scenario, the readiness probe will correctly mark pod unready but the liveness probe does not trigger a container restart.

Here are some more details:
1. Kubernetes Server Version: v1.24.16.
2. An application pod is running a single container.
3. Both readiness and liveness probes are `httpGet` and are hitting the same `/healthcheck` endpoint.
4. Healthcheck endpoint is responding with `500 INTERNAL SERVER ERROR`.
5. The readiness probe does mark the container as "unready" as expected.
6. The liveness probe does not trigger a container restart, even after the multiple consecutive failures.

I confirmed that `restartPolicy: Always` and when I exec into the container and run curl, I do get the 500 INTERNAL SERVER ERROR, the same as what I see in the application logs. As per documentation, a failing liveness probe should be triggering a restart but it is not happening.

Here's the log from kubelet:
```
Jan 03 17:34:20 ip-1-1-1-1 kubelet[5845]: E0103 17:34:20.352585    5845 prober.go:118] "Probe errored" err="unexpected EOF" probeType="Liveness" pod="mynamespace/application-abc
```

Also, looking at the code [here](https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/kubelet/prober/prober.go#L115-L124) the error does seem to be captured correctly by the probe but restart is not initiated.

Somehow the error is not being handled gracefully I assume. If I change it to an invalid port and cause a timeout the container gets restarted as expected. Also, switching from `httpGet` to `exec` type of healthcheck and running `curl http://localhost:8000/healthcheck` does trigger a restart so I suspect there is some bug with the httpGet probe itself.

This is what I see in the events:
```
Liveness probe errored: unexpected EOF
Readiness probe errored: unexpected EOF
```

#### What did you expect to happen?

A failing healthcheck in a liveness probe would trigger a container restart.

```
containerMessage="Container abc  failed liveness probe, will be restarted"
```

#### How can we reproduce it (as minimally and precisely as possible)?

Create a deployment with an app that is crashing and returns 500

Configure healthceck like so:
```
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthcheck
        port: healthcheck
        scheme: HTTP
      initialDelaySeconds: 3
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
    name: abc
    ports:
    - containerPort: 8000
      name: healthcheck
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthcheck
        port: healthcheck
        scheme: HTTP
      initialDelaySeconds: 3
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.24.16
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.5 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.5 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
# Linux ip-1-1-1-1 5.15.0-1028-aws #32~20.04.1-Ubuntu SMP Mon Jan 9 18:02:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122586 kube-apiserver doesn't correctly apply /etc/kubernetes/manifests/kube-apiserver.yaml

- Issue 链接：[#122586](https://github.com/kubernetes/kubernetes/issues/122586)

### Issue 内容

#### What happened?

kube-apiserver complains about missing parameters not exist while being set correctly upon intial installation on  /etc/kubernetes/manifests/kube-apiserver.yaml, [-etc-kubernetes-manifests-kube-apiserver.yaml.txt](https://github.com/kubernetes/kubernetes/files/13832109/-etc-kubernetes-manifests-kube-apiserver.yaml.txt)

$kube-apiserver
W0104 17:23:45.519496   30064 services.go:37] No CIDR for service cluster IPs specified. Default value which was 10.0.0.0/24 is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.
I0104 17:23:45.519693   30064 server.go:554] external host was not specified, using 192.168.0.214
W0104 17:23:45.519698   30064 authentication.go:525] AnonymousAuth is not allowed with the AlwaysAllow authorizer. Resetting AnonymousAuth to false. You should use a different authorizer
E0104 17:23:45.519889   30064 run.go:74] "command failed" err="[--etcd-servers must be specified, service-account-issuer is a required flag, --service-account-signing-key-file and --service-account-issuer are required flags]"


#### What did you expect to happen?

kub-apiserver shall apply yaml configurations correctly but not happens!


#### How can we reproduce it (as minimally and precisely as possible)?

Fedora 39 Linux localhost.localdomain 6.6.8-200.fc39.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Dec 21 04:01:49 UTC 2023 x86_64 GNU/Linux

sudo dnf install git go firewalld iptables iproute-tc cri-o containernetworking-plugins kubernetes-master kubernetes-client kubernetes-node kubernetes-kubeadm

#### Anything else we need to know?

is there a work around or stable working version

#### Kubernetes version

<details>

```console
$ kubectl version
kubectl version --output=yaml
clientVersion:
  buildDate: "2023-12-20T00:00:00Z"
  compiler: gc
  gitCommit: d15213f69952c79b317e635abff6ff4ec81475f8
  gitTreeState: archive
  gitVersion: v1.27.9
  goVersion: go1.21.5
  major: "1"
  minor: "27"
  platform: linux/amd64
kustomizeVersion: v5.0.1

The connection to the server localhost:8080 was refused - did you specify the right host or port?

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="39 (Server Edition)"
ID=fedora
VERSION_ID=39
VERSION_CODENAME=""
PLATFORM_ID="platform:f39"
PRETTY_NAME="Fedora Linux 39 (Server Edition)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:39"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f39/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=39
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=39
SUPPORT_END=2024-11-12
VARIANT="Server Edition"
VARIANT_ID=server
$ uname -a
Linux localhost.localdomain 6.6.8-200.fc39.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Dec 21 04:01:49 UTC 2023 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
$sudo dnf install git go firewalld iptables iproute-tc cri-o containernetworking-plugins kubernetes-master kubernetes-client kubernetes-node kubernetes-kubeadm

Last metadata expiration check: 1:17:08 ago on Thu 04 Jan 2024 04:28:19 PM EET.
Package git-2.43.0-1.fc39.x86_64 is already installed.
Package golang-1.21.5-1.fc39.x86_64 is already installed.
Package firewalld-2.0.2-1.fc39.noarch is already installed.
Package iptables-nft-1.8.9-5.fc39.x86_64 is already installed.
Package iproute-tc-6.4.0-2.fc39.x86_64 is already installed.
Package cri-o-1.27.2-2.fc39.x86_64 is already installed.
Package containernetworking-plugins-1.3.0-3.fc39.x86_64 is already installed.
Package kubernetes-master-1.27.9-1.fc39.x86_64 is already installed.
Package kubernetes-client-1.27.9-1.fc39.x86_64 is already installed.
Package kubernetes-node-1.27.9-1.fc39.x86_64 is already installed.
Package kubernetes-kubeadm-1.27.9-1.fc39.x86_64 is already installed.
Dependencies resolved.
Nothing to do.
Complete!

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$crio --version
crio version 1.27.2
Version:        1.27.2
GitCommit:      unknown
GitCommitDate:  unknown
GitTreeState:   clean
GoVersion:      go1.21.4
Compiler:       gc
Platform:       linux/amd64
Linkmode:       dynamic
BuildTags:      
  rpm_crashtraceback
  seccomp
  selinux
LDFlags:           -X github.com/cri-o/cri-o/version=1.27.2 -B 0xc72bf2ae30fdb0b6d2150587804157413ef2e444 -compressdwarf=false -linkmode=external -extldflags '-Wl,-z,relro -Wl,--as-needed  -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes  '
SeccompEnabled:   true
AppArmorEnabled:  false

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122579 kubectl auth can-i command behaviour is wrong for impersonated users

- Issue 链接：[#122579](https://github.com/kubernetes/kubernetes/issues/122579)

### Issue 内容

#### What happened?

The kubectl auth can-i command behaves differently when using the --user and --as options.
The --as option only checks the explicit permissions granted to the user via roles and bindings. It does not check the permissions which a user acquires through its associated group.
The --user option works correctly (provided the user is in the kubeconfig file)

#### What did you expect to happen?

The --as option should also check the permissions that a user acquires through its associated group.

#### How can we reproduce it (as minimally and precisely as possible)?

Either a minikube or kubeadm cluster with kubectl installed. openssl is needed to generate CSRs.

1. Create a new `clusterrolebinding` that binds the `cluster-admin` role to a new dummy group called `example-masters`
```
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: example-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: example:masters
EOF
```

2. Create a new user and associate it with the `example:masters` group
```
openssl genrsa -out poweruser.key 2048
openssl req -new -key poweruser.key -out poweruser.csr -subj "/CN=poweruser/O=example:masters"
```

3. Get the base64 encoded output of the csr
cat poweruser.csr | base64 | tr -d "\n"

4. Create a new certificatesigningrequest by passing the base64 output
```
cat <<EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: poweruser
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2N6Q0NBVnNDQVFBd0xqRVNNQkFHQTFVRUF3d0pjRzkzWlhKMWMyVnlNUmd3RmdZRFZRUUtEQTlsZUdGdApjR3hsT20xaGMzUmxjbk13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLQW9JQkFRRDE2dUcxCi9QTWpBMWYyK1ppblZZQWNaZVE0ZFk0ZUhHdGZuOUwwaWpkcG5DN0VnVldXZzVMM2ZkQUFJTVYxTytYams4alQKZlVWRkkrem1GcUJQeWh2eXNBRDhnM051MEMzV1pZZ1RLYTNweHlISVQrQVVjcm5Uek96TFhIY3hZTlFEWmRiagpGeXVRVE5CMS9Hd282TkZHR29XQUhPY3p0SSt4ZVV2TDhSS0JpZ1h2aFQ1Tm9vMUovSklycEhNZldZbGNFSWxICmtyTnhtTmpPelVseVRLanluek1pWC8xK1hWNlR6UEs5aXZ4WStIazA0RVFtUktDMG05ckcvbTVQcGJLSklKVlEKMThLMUxBeElPKzZuYWswQVMwU0dQeTVOWGFrbnFHV0FRRWIwQ3VzU1pHWXlqQjBGSm5DUy9Ga2tkZ1dQWCtpaApOYUxIT3JETnZVTUc1UEYvQWdNQkFBR2dBREFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBSnRoaXdQV2tCaVpPCmtvUisxSXFDTko5V0FYSEh3a05mT3Q4ZjZhbGhHMytqOXpDSERyVk5nVmQvOENtOGJZcy9HWS90b1FUSmJHYTgKWGJsNTljSmtjNk5UMWROT0N6RjlQUHVwNTMzUkZRcC96WjRiM20vVEduQ2daVm8wQWNxT2IvWFNrMWFKS25ZUgpZUmNjelJsN2tWaHEwTGY2N2NVdUZGUW9kNSt0cGRxRG9BNmw4cERhZSsvRzBBSnBFc2pLdkN2akx2WGVra2gzCnFpR082akZNUFExdG82OUtaV3ZGdmZSaWRFVEtOUno5RnpmUzNoQTBZa2dJNm93M0hBYjlvNWJlVkdhZjluN24KT1oxSWwvSXloMThJVEJXQXJ0SWh5TFhjR3ErRERrUVpaR0dTOXphMXdzbUVFMUlKWGR1eTE1RWFXeUgxa0dpOQpaT2xqTlBralFRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
EOF
```

5. Approve the CSR
`kubectl certificate approve poweruser`

6. Extract the certificate in a file called poweruser.crt
`kubectl get csr poweruser -o jsonpath='{.status.certificate}'| base64 -d > poweruser.crt`

7. Add the new credentials to the kubeconfig
`kubectl config set-credentials poweruser --client-key=poweruser.key --client-certificate=poweruser.crt --embed-certs=true`

8. Add a new context
`kubectl config set-context poweruser --cluster=minikube --user=poweruser`

9. Try the kubectl auth can-i command to verify the permissions on the new user by passing the --user option
```
kubectl auth can-i create pods --user=poweruser
yes
```
This is the correct output as the user has cluster-admin permission via the example:masters group.

10. Try the kubectl auth can-i command to verify the permissions on the new user by passing the --as option
```
kubectl auth can-i create pods --as=poweruser
no
```
**_This result is not as per expectation. The logical output should have been ```yes``` as ```poweruser``` has the cluster-admin role via the example:masters group._** 

11. We can prove this by actually performing an action to create a pod by acting as the poweruser
Switch to the context just created
`kubectl config use-context poweruser`

12. Try to create a pod and it is successful.
`kubectl run nginx-poweruser --image=nginx`

```
kubectl get pod nginx-poweruser 
NAME              READY   STATUS    RESTARTS   AGE
nginx-poweruser   1/1     Running   0          26s
```



#### Anything else we need to know?

This is reproducible on a minikube cluster as well as a cluster created through kubeadm.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here

Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>




#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

# paste output here
$ uname -a
# paste output here
Linux kubemaster 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:29:11 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux



</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
sudo crictl version

Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.7.9
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122577 Pause image seems make no sense in kubeadm part

- Issue 链接：[#122577](https://github.com/kubernetes/kubernetes/issues/122577)

### Issue 内容

#### What happened?

I was gonna install k8s cluster through `kubeadm` in an offline environment through pre-prepared images, but pause image is not as my expectations, as a result, the paused image pull failed, and `kubelet` also failed to start.

#### What did you expect to happen?

`kubelet` use the pause image which I override in `kubeadm`, and `kubelet` start success.

#### How can we reproduce it (as minimally and precisely as possible)?

1. At first, I use `kubeadm config images list` to list all k8s images which I need to prepare in advance, and print bellow:
```
I1229 04:29:13.670682   21144 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28
registry.k8s.io/kube-apiserver:v1.28.4
registry.k8s.io/kube-controller-manager:v1.28.4
registry.k8s.io/kube-scheduler:v1.28.4
registry.k8s.io/kube-proxy:v1.28.4
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
```

2. Use the following configuration file to initialize the cluster with kubeadm
```
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
... # other content
```

3. Inspect the pause image actually used.
```
[root@localhost ~]# ps -ef | grep '/usr/local/bin/kubelet'
root     27248     1  4  2023 ?        07:06:00 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
```
```
[root@localhost ~]# docker ps -a | grep pause
a279c1be4c9c        registry.k8s.io/pause:3.6            "/pause"                 6 days ago          Up 6 days                                     k8s_POD_calico-kube-controllers-7fd5bb489f-6pxjx_kube-system_7e9c6ee5-90f7-4484-9de5-9658f5ba430d_1
```

#### Anything else we need to know?

I know `--pod-infra-container-image` flag in `kubelet` was deprecated, and we should override pause image at `container runtime` side, It's ok to me, but what about the config in `kubeadm`, It seems make no sense as soon as `--pod-infra-container-image` removed. Pause image in preflight check make no sense either.

By the way, please think about it, do we need to maintain the default version of pause image in k/k, such as in kubelet and e2e test? Can we obtain the pause image information directly through `CRI`?

#### Kubernetes version

kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.4", GitCommit:"bae2c62678db2b5053817bc97181fcc2e8388103", GitTreeState:"clean", BuildDate:"2023-11-15T16:56:18Z", GoVersion:"go1.20.11", Compiler:"gc", Platform:"linux/amd64"}

#### Cloud provider

none

#### OS version

_No response_

#### Install tools

kubeadm v1.28.4

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及

---

## Issue #122568 Service account token gets invalidated while terminating pod is re-deleted

- Issue 链接：[#122568](https://github.com/kubernetes/kubernetes/issues/122568)

### Issue 内容

#### What happened?

Attempting to kill a pod while it's already terminating causes its bound service account token to be invalidated.

This can happen, for example, when pod termination takes a long time, and an impatient user will re-attempt to delete it, this time with a short grace-period (e.g., via `kubectl delete pod --now`, which sets a grace-period=1s).



#### What did you expect to happen?

I expect the service account token to remain valid throughout the entire lifetime of the pod, and for k8s API calls from within the pod to succeed (well, at least to authenticate successfully).

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a role binding that allows the default service account to read pods:
```sh
kubectl create rolebinding --clusterrole view --serviceaccount default:default test
``` 
2. Create the following test pod.
The PreStop hook of the pod repeatedly runs `kubectl get pods`, and logs any error encountered:
```yaml
kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  terminationGracePeriodSeconds: 100000
  containers:
  - name: test
    image: bitnami/kubectl
    command:
    - "bin/sh"
    - "-c"
    - "sleep 100000000"
    lifecycle:
      preStop:
        exec:
          command:
          - "bin/sh"
          - "-c"
          - |
            echo "PreStop started at $(date +%T)" >> /tmp/prestop.log
            while true; do
              resp=$(kubectl get pods 2>&1)
              if [ $? -ne 0 ]; then
                echo "kubectl failed at $(date +%T)" >> /tmp/prestop.log
                echo $resp >> /tmp/prestop.log
              fi
              sleep 1
            done
EOF
```
3. Wait for the pod to become ready
```sh
kubectl get pod test --watch
```
4. Gracefully delete the pod, and immediately delete it again with gracePeriod=1 (== `--now`):
```sh
kubectl delete pod test --wait=false && kubectl delete pod test --wait=false --grace-period=1
```
5. Tail the logs of the PreStop hook:
```sh
kubectl exec -it test -- tail -f /tmp/prestop.log
```

The observed output is:
```
PreStop started at 07:14:25
kubectl failed at 07:15:30
error: You must be logged in to the server (Unauthorized)
kubectl failed at 07:15:31
error: You must be logged in to the server (Unauthorized)
kubectl failed at 07:15:32
error: You must be logged in to the server (Unauthorized)
kubectl failed at 07:15:33
error: You must be logged in to the server (Unauthorized)
```
6. Finally, cleanup by force deleting the pod.
(note - this only cleanups the pod, but the container remains dangling in the background):
```sh
kubectl delete pod test --force
```

#### Anything else we need to know?

Based on several attempts to reproduce it, it seems that the token gets invalidated after about [60 seconds] + [grace-period seconds] + [a few seconds] - relative to when the pod was initially deleted.

Hence, different grace-period values yielded different token invalidation times:
- grace-period=1 --> invalidated after ~63s from initial pod deletion
- grace-period=30 --> ~95s
- grace-period=120 --> ~187s

On the other hand, if the 2nd delete command is issued at some later time (e.g., delete, sleep, delete --grace-period), then the token seems to be invalidated as soon as the 2nd deleted command is issued (+ [a few seconds]):
- sleep 120 + grace-period=1 --> ~127s
- sleep 120 + grace-period=30 --> ~126s

And yet on another hand (?!?!):
-sleep 120 + grace-period=120 --> ~3m
- sleep 90 + grace-period=300 --> ~6m

So, the invalidation timing seems to depend on multiple factors.
Hopefully this yields some useful clues.

#### Kubernetes version

<details>

Tested on K8s v1.27.3(GKE), but also reproduced on v1.28.X (GKE + vanilla/kOps)
```console
$ kubectl version
Client Version: v1.28.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.3-gke.100
```

</details>


#### Cloud provider

<details>
GKE, but also reproduced on vanilla/kOps.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

不涉及。

---

## Issue #122566 Remove duplicate images used in e2e tests

- Issue 链接：[#122566](https://github.com/kubernetes/kubernetes/issues/122566)

### Issue 内容

We have the following sets of images used in e2e tests, can we try to eliminate duplication?

(context: see https://github.com/kubernetes/kubernetes/pull/122565/files#diff-de4c32013b42db23b321a084234ce02f9671a8cd99875387d7d6a41ca1aad069)

In general, we should find the references of the older image and switch it to the newer version, unless there is a specific reason both versions of images are needed/used in the test harness.

References are here:
https://cs.k8s.io/?q=(gcp-compute-persistent-disk-csi-driver%7Cbusybox%7Ccuda-vector-add%7Chttpd%7Cnginx%7Ccsi-snapshotter%7Chostpathplugin)&i=nope&files=test%2F&excludeFiles=&repos=kubernetes/kubernetes

- [ ] gcp-compute-persistent-disk-csi-driver
  - registry.k8s.io/cloud-provider-gcp/gcp-compute-persistent-disk-csi-driver:v1.2.2
  - registry.k8s.io/cloud-provider-gcp/gcp-compute-persistent-disk-csi-driver:v1.4.0

- [ ] busybox
  - registry.k8s.io/e2e-test-images/busybox:1.29-2
  - registry.k8s.io/e2e-test-images/busybox:1.36.1-1

- [ ] cuda-vector-add
  - registry.k8s.io/e2e-test-images/cuda-vector-add:1.0
  - registry.k8s.io/e2e-test-images/cuda-vector-add:2.3

- [ ] httpd
  - registry.k8s.io/e2e-test-images/httpd:2.4.38-4
  - registry.k8s.io/e2e-test-images/httpd:2.4.39-4

- [ ] nginx
  - registry.k8s.io/e2e-test-images/nginx:1.14-4
  - registry.k8s.io/e2e-test-images/nginx:1.15-4

- [ ] csi-snapshotter
  - registry.k8s.io/sig-storage/csi-snapshotter:v5.0.1
  - registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0

- [ ] hostpathplugin
  - registry.k8s.io/sig-storage/hostpathplugin:v1.11.0
  - registry.k8s.io/sig-storage/hostpathplugin:v1.9.0


### 分析结果

不涉及

---

